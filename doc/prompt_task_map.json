{
    "commonsense_qa": [
        {
            "task": "task073_commonsenseqa_answer_generation",
            "prompt": [
                "most_suitable_answer",
				"question_answering",
				"question_to_answer_index"
            ]
        }
    ],
    "dream": [
        {
            "task": "task247_dream_answer_generation",
            "prompt": [
                "baseline",
				"read_the_following_conversation_and_answer_the_question"
            ]
        }
    ],
    "quail": [
        {
            "task": "task887_quail_answer_generation",
            "prompt": [
                "context_description_question_text",
				"context_question_description_text",
				"description_context_question_text"
            ]
        }
    ],
    "quartz": [
        {
            "task": "task1731_quartz_question_answering",
            "prompt": [
				"answer_question_based_on",
				"answer_question_below",
				"given_the_fact_answer_the_q",
				"having_read_above_passage",
				"paragraph_question_plain_concat",
				"read_passage_below_choose",
				"use_info_from_paragraph_question",
				"use_info_from_question_paragraph"
            ]
        }
    ],
    "social_i_qa": [
        {
            "task": "task384_socialiqa_question_classification",
            "prompt": [
				"Check if a random answer is valid or not"
            ]
        },
        {
            "task": "task580_socialiqa_answer_generation",
            "prompt": [
				"Show choices and generate answer",
				"Show choices and generate index"
            ]
        }
    ],
    "wiqa": [
        {
            "task": "task1727_wiqa_what_is_the_effect",
            "prompt": [
				"effect_with_label_answer",
				"effect_with_string_answer"
            ]
        }
    ], 
    "cosmos_qa": [
        {
            "task": "task024_cosmosqa_answer_generation",
            "prompt": [
				"context_description_question_text",
				"context_question_description_text",
				"description_context_question_text"
            ]
        }
    ],
    "qasc": [
        {
            "task": "task1297_qasc_question_answering",
            "prompt": [
				"qa_with_separated_facts_1",
				"qa_with_separated_facts_2",
				"qa_with_separated_facts_4"
            ]
        }
    ],
    "quarel": [
        {
            "task": "task1378_quarel_correct_answer_generation",
            "prompt": [
				"choose_between",
				"do_not_use",
				"heres_a_story",
				"logic_test",
				"testing_students"
            ]
        }
    ],
    "sciq": [
        {
            "task": "task591_sciq_answer_generation",
            "prompt": [
				"Direct Question (Closed Book)"
            ]
        }
    ],
    "wiki_hop/original": [
		{
            "task": "task1296_wiki_hop_question_answering",
            "prompt": [
				"choose_best_object_affirmative_1",
				"choose_best_object_affirmative_2",
				"choose_best_object_affirmative_3",
				"choose_best_object_interrogative_1",
				"choose_best_object_interrogative_2"
            ]
        }
    ],
    "adversarial_qa/adversarialQA": [
		{
            "task": "task1295_adversarial_qa_question_answering",
            "prompt": [
				"answer_the_following_q",
				"based_on",
				"question_context_answer",
				"tell_what_it_is"
            ]
        }
    ],
    "quoref": [
        {
            "task": "task002_quoref_answer_generation",
            "prompt": [
				"Answer Friend Question",
				"Answer Question Given Context",
				"Answer Test",
				"Context Contains Answer",
				"Find Answer",
				"Found Context Online",
				"Given Context Answer Question",
				"Guess Answer",
				"Read And Extract",
				"What Is The Answer"
            ]
        }
    ],
    "duorc/SelfRC": [
        {
            "task": "task193_duorc_question_generation",
            "prompt": [
				"generate_question"
            ]
        },
        {
            "task": "task194_duorc_answer_generation",
            "prompt": [
				"answer_question",
				"decide_worth_it",
				"question_answering",
				"movie_director",
				"extract_answer"
            ]
        }
    ],
    "duorc/ParaphraseRC": [
        {
            "task": "task193_duorc_question_generation",
            "prompt": [
				"generate_question"
            ]
        },
        {
            "task": "task194_duorc_answer_generation",
            "prompt": [
				"answer_question",
				"decide_worth_it",
				"question_answering",
				"movie_director",
				"extract_answer"
            ]
        }
    ],
    "ropes": [
        {
            "task": "task061_ropes_answer_generation",
            "prompt": [
				"background_new_situation_answer",
				"background_situation_middle",
				"given_background_situation",
				"new_situation_background_answer",
				"plain_background_situation",
				"plain_bottom_hint",
				"prompt_beginning",
				"prompt_bottom_hint_beginning",
				"prompt_mix",
				"read_background_situation"
            ]
        }
    ],
    "kilt_tasks/hotpotqa": [
        {
            "task": "task1293_kilt_tasks_hotpotqa_question_answering",
            "prompt": [
				"straighforward_qa",
				"complex_question",
				"final_exam",
				"formulate",
				"combining_facts"
            ]
        },
	],
    "hotpot_qa/distractor": [
        {
            "task": "task170_hotpotqa_answer_generation",
            "prompt": [
				"generate_answer_affirmative",
				"generate_answer_interrogative"
            ]
        },
        {
            "task": "task191_hotpotqa_question_generation",
            "prompt": [
				"generate_question"
            ]
        },
        {
            "task": "task192_hotpotqa_sentence_generation",
            "prompt": [
				"generate_explanations_affirmative",
				"generate_explanations_interrogative"
            ]
        }
    ],
    "hotpot_qa/fullwiki": [
        {
            "task": "task170_hotpotqa_answer_generation",
            "prompt": [
				"generate_answer_affirmative",
				"generate_answer_interrogative"
            ]
        },
        {
            "task": "task191_hotpotqa_question_generation",
            "prompt": [
				"generate_question"
            ]
        },
        {
            "task": "task192_hotpotqa_sentence_generation",
            "prompt": [
				"generate_explanations_affirmative",
				"generate_explanations_interrogative"
            ]
        }
    ],
    "wiki_qa": [
        {
            "task": "task1294_wiki_qa_answer_verification",
            "prompt": [
				"automatic_system",
				"found_on_google",
				"Is This True?",
				"exercise"
            ]
        }
    ],
    "common_gen": [
        {
            "task": "task102_commongen_sentence_generation",
            "prompt": [
				"Example prompt",
				"Given concepts - type 2",
				"Given concepts type 1",
				"Put together",
				"choice in concept centric sentence generation",
				"random task template prompt"
            ]
        }
    ],
    "wiki_bio": [
    ],
    "amazon_polarity": [
        {
            "task": "task493_review_polarity_classification",
            "prompt": [
				"User_recommend_this_product"
            ]
        }
    ],
    "amazon_reviews_multi/en": [
        {
            "task": "task617_amazonreview_category_text_generation",
            "prompt": [
				"prompt_review_to_category"            ]
        },
        {
            "task": "task618_amazonreview_summary_text_generation",
            "prompt": [
				"generate_title"
            ]
        },
        {
            "task": "task1310_amazonreview_rating_classification",
            "prompt": [
				"prompt_body_title_to_star",
				"prompt_review_to_star"
            ]
        }
    ],
    "amazon_us_reviews/Wireless_v1_00": [
        {
            "task": "task1342_amazon_us_reviews_title",
            "prompt": [
				"Generate review headline based on review body"
            ]
        }
    ],
    "imdb": [
        {
            "task": "task284_imdb_classification",
            "prompt": [
				"Movie Expressed Sentiment",
				"Movie Expressed Sentiment 2",
				"Reviewer Enjoyment",
				"Reviewer Enjoyment Yes No",
				"Reviewer Expressed Sentiment",
				"Reviewer Opinion bad good choices",
				"Reviewer Sentiment Feeling",
				"Sentiment with choices",
				"Text Expressed Sentiment",
				"Writer Expressed Sentiment"
            ]
        }
    ],
    "rotten_tomatoes": [
        {
            "task": "task888_reviews_classification",
            "prompt": [
				"Movie Expressed Sentiment",
				"Movie Expressed Sentiment 2",
				"Reviewer Enjoyment",
				"Reviewer Enjoyment Yes No",
				"Reviewer Expressed Sentiment",
				"Reviewer Opinion bad good choices",
				"Reviewer Sentiment Feeling",
				"Sentiment with choices",
				"Text Expressed Sentiment",
				"Writer Expressed Sentiment"
            ]
        }
    ],
    "yelp_polarity": [
        {
            "task": "task475_yelp_polarity_classification",
            "prompt": [
				"come_again",
				"experience_good_bad",
				"format_come_again",
				"format_good_bad",
				"like_dislike",
				"like_dislike_2",
				"place_good_bad",
				"rating_high_low",
				"regret_yes_or_no"
            ]
        }
    ],
    "yelp_review_full": [
        {
            "task": "task1292_yelp_review_full_text_categorization",
            "prompt": [
				"based_on_that",
				"format_rating",
				"format_score",
				"format_star",
				"on_a_scale",
				"so_i_would",
				"this_place"
            ]
        }
    ],
    "cnn_dailymail/3.0.0": [
        {
            "task": "task1553_cnn_dailymail_summarization",
            "prompt": [
				"2_or_3_sentences",
				"news_card_view",
				"news_stock",
				"news_summary",
				"sum_in_brief",
				"tldr_summary",
				"write_an_outline"
            ]
        }
    ],
    "gigaword": [
        {
            "task": "task288_gigaword_summarization",
            "prompt": [
				"TLDR",
				"first_sentence_title",
				"generate_summary_for_this",
				"in_a_nutshell",
				"make_a_title",
				"write_a_title_for_this_sentence",
				"write_its_sentence"
            ]
        }
    ],
    "multi_news": [
        {
            "task": "task1291_multi_news_summarization",
            "prompt": [
				"distill",
				"summarize",
				"summary scenario",
				"synthesize",
				"what are the key points"
            ]
        }
    ],
    "samsum": [
        {
            "task": "task1572_samsum_summary",
            "prompt": [
				"Generate a summary for this dialogue",
				"Given the above dialogue write a summary",
				"Sum up the following dialogue",
				"Summarize this dialogue:",
				"To sum up this dialog",
				"Summarize:"
            ]
        }
    ],
    "xsum": [
        {
            "task": "task1290_xsum_summarization",
            "prompt": [
				"DOC_boils_down_to_simple_idea_that",
				"DOC_given_above_write_one_sentence",
				"DOC_how_would_you_rephrase_few_words",
				"DOC_tldr",
				"DOC_write_summary_of_above",
				"article_DOC_summary",
				"college_roommate_asked_DOC_so_I_recap",
				"read_below_DOC_write_abstract",
				"summarize_DOC",
				"summarize_this_DOC_summary"
            ]
        }
    ],
    "ag_news": [
        {
            "task": "task1541_agnews_classification",
            "prompt": [
				"classify_with_choices",
				"classify_with_choices_question_first",
				"which_section_choices"
            ]
        }
    ],
    "dbpedia_14": [
        {
            "task": "task629_dbpedia_14_classification",
            "prompt": [
				"given_list_what_category_does_the_paragraph_belong_to",
				"pick_one_category_for_the_following_text"
            ]
        }
    ],
    "trec": [
        {
            "task": "task1289_trec_classification",
            "prompt": [
				"trec1",
				"trec2",
				"what_category_best_describe",
				"which_category_best_describes",
				"pick_the_best_descriptor"
            ]
        }
    ],
    "glue/mrpc": [
        {
            "task": "task1287_glue_qqp_paraphrasing",
            "prompt": [
				"equivalent",
				"paraphrase",
				"replace",
				"same thing",
				"want to know"
            ]
        }
    ],
    "paws/labeled_final": [
        {
            "task": "task400_paws_paraphrase_classification",
            "prompt": [
				"Concatenation",
				"Concatenation-no-label",
				"Meaning",
				"Meaning-no-label",
				"PAWS-ANLI GPT3",
				"PAWS-ANLI GPT3-no-label",
				"Rewrite",
				"Rewrite-no-label",
				"context-question",
				"context-question-no-label",
				"task_description-no-label"
            ]
        }
    ],
    "paws/labeled_swap": [
        {
            "task": "task400_paws_paraphrase_classification",
            "prompt": [
				"Concatenation",
				"Concatenation-no-label",
				"Meaning",
				"Meaning-no-label",
				"PAWS-ANLI GPT3",
				"PAWS-ANLI GPT3-no-label",
				"Rewrite",
				"Rewrite-no-label",
				"context-question",
				"context-question-no-label",
				"task_description-no-label"
            ]
        }
    ],
    "paws/unlabeled_final": [
        {
            "task": "task400_paws_paraphrase_classification",
            "prompt": [
				"Concatenation",
				"Concatenation-no-label",
				"Meaning",
				"Meaning-no-label",
				"PAWS-ANLI GPT3",
				"PAWS-ANLI GPT3-no-label",
				"Rewrite",
				"Rewrite-no-label",
				"context-question",
				"context-question-no-label",
				"task_description-no-label"
            ]
        }
    ],
    "glue/qqp": [
        {
            "task": "task1287_glue_qqp_paraphrasing",
            "prompt": [
				"answer",
				"duplicate",
				"duplicate or not",
				"meaning",
				"quora",
				"same thing"
            ]
        }
    ],
    "ai2_arc/ARC-Challenge": [
        {
            "task": "task229_arc_answer_generation_hard",
            "prompt": [
				"heres_a_problem",
				"i_am_hesitating",
				"multiple_choice",
				"pick_the_most_correct_option",
				"qa_options"
            ]
        }
    ],
    "ai2_arc/ARC-Easy": [
        {
            "task": "task228_arc_answer_generation_easy",
            "prompt": [
				"heres_a_problem",
				"i_am_hesitating",
				"multiple_choice",
				"pick_the_most_correct_option",
				"qa_options"
            ]
        }
    ],
    "openbookqa/additional": [
    ],
    "openbookqa/main": [
        {
            "task": "task1286_openbookqa_question_answering",
            "prompt": [
				"which_correct",
				"which_correct_inverse",
				"pick_using_id",
				"only_options",
				"choose_an_answer_with_options",
				"choices",
				"pick_answer_with_options"
            ]
        }
    ],
    "piqa": [
        {
            "task": "task080_piqa_answer_generation",
            "prompt": [
				"no prompt needed"
            ]
        }
    ],
    "race/all": [
        {
            "task": "task309_race_answer_generation",
            "prompt": [
				"Select the best answer",
				"Select the best answer (generate span)",
				"Select the best answer (no instructions)",
				"Taking a test"
            ]
        }
    ],
    "race/high": [
        {
            "task": "task309_race_answer_generation",
            "prompt": [
				"Select the best answer",
				"Select the best answer (generate span)",
				"Select the best answer (no instructions)",
				"Taking a test"
            ]
        }
    ],
    "race/middle": [
        {
            "task": "task309_race_answer_generation",
            "prompt": [
				"Select the best answer",
				"Select the best answer (generate span)",
				"Select the best answer (no instructions)",
				"Taking a test"
            ]
        }
    ],
    "hellaswag": [
        {
            "task": "task1389_hellaswag_completion",
            "prompt": [
				"Randomized prompts template",
				"complete_first_then",
				"how_ends",
				"if_begins_how_continues"
            ]
        }
    ],
    "squad_v2": [
        {
            "task": "task349_squad2.0_answerable_unanswerable_question_classification",
            "prompt": [
				"Unanwerable question"
            ]
        }
    ],
    "trivia_qa/unfiltered": [
        {
            "task": "task1564_triviaqa_answer_generation",
            "prompt": [
				"first_person_context",
				"formal_description",
				"question_answer",
				"question_with_instruction"
            ]
        }
    ],
    "web_questions": [
        {
            "task": "task1412_web_questions_question_answering",
            "prompt": [
				"get_the_answer",
				"potential-correct-answer",
				"question-answer",
				"short_general_knowledge_q",
				"whats_the_answer"
            ]
        }
    ],
    "super_glue/boolq": [
        {
            "task": "task380_boolq_yes_no_question",
            "prompt": [
				"GPT-3 Style",
				"I wonder…",
				"after_reading",
				"based on the following passage",
				"based on the previous passage",
				"could you tell me…",
				"exam",
				"exercise",
				"valid_binary",
				"yes_no_question"
            ]
        }
    ],
    "super_glue/copa": [
        {
            "task": "task1393_superglue_copa_text_completion",
            "prompt": [
				"C1 or C2? premise, so/because…",
				"best_option",
				"cause_effect",
				"choose",
				"exercise",
				"i_am_hesitating",
				"more likely",
				"plausible_alternatives"
            ]
        }
    ],
    "super_glue/multirc": [
        {
            "task": "task1392_superglue_multirc_answer_verification",
            "prompt": [
				"I was going to say…",
				"Would it be good to answer…",
				"confirm",
				"correct",
				"decide_valid",
				"found_this_answer",
				"grading",
				"is the correct answer…",
				"is… a correct answer?",
				"paragraph… question… is it… ?"
            ]
        }
    ],
    "super_glue/record": [
        {
            "task": "task302_record_classification",
            "prompt": [
				"trying_to_decide",
				"pick_one_option",
				"choose_between",
				"Which one is the placeholder?",
				"What could the placeholder be?"
            ]
        },
        {
            "task": "task339_record_answer_generation",
            "prompt": [
				"the placeholder refers to…",
				"exercise",
				"corrupted",
				"In the question above, the placeholder stands for",
				"Can you figure out…"
            ]
        }
    ],
    "super_glue/wic": [
        {
            "task": "task625_xlwic_true_or_false_answer_generation",
            "prompt": [
				"GPT-3-prompt",
				"GPT-3-prompt-with-label",
				"affirmation_true_or_false",
				"grammar_homework",
				"polysemous",
				"question-context",
				"question-context-meaning",
				"Generalized question-context format with label",
				"same_sense",
				"similar-sense"
            ]
        }
    ],
    "super_glue/wsc.fixed": [
        {
            "task": "task1390_wscfixed_coreference",
            "prompt": [
				"GPT-3 Style",
				"I think they mean",
				"Who or what is/are",
				"by p they mean",
				"does p stand for",
				"does the pronoun refer to",
				"in other words",
				"p is/are r",
				"replaced with",
				"the pronoun refers to"
            ]
        }
    ],
    "anli": [
        {
            "task": "task1385_anli_r1_entailment",
            "prompt": [
				"GPT-3 style",
				"MNLI crowdsource",
				"always/sometimes/never",
				"based on the previous passage",
				"can we infer",
				"claim true/false/inconclusive",
				"consider always/sometimes/never",
				"does it follow that",
				"does this imply",
				"guaranteed true",
				"guaranteed/possible/impossible",
				"justified in saying",
				"must be true",
				"should assume",
				"take the following as truth"
            ]
        },
        {
            "task": "task1386_anli_r2_entailment",
            "prompt": [
				"GPT-3 style",
				"MNLI crowdsource",
				"always/sometimes/never",
				"based on the previous passage",
				"can we infer",
				"claim true/false/inconclusive",
				"consider always/sometimes/never",
				"does it follow that",
				"does this imply",
				"guaranteed true",
				"guaranteed/possible/impossible",
				"justified in saying",
				"must be true",
				"should assume",
				"take the following as truth"
            ]
        },
        {
            "task": "task1387_anli_r3_entailment",
            "prompt": [
				"GPT-3 style",
				"MNLI crowdsource",
				"always/sometimes/never",
				"based on the previous passage",
				"can we infer",
				"claim true/false/inconclusive",
				"consider always/sometimes/never",
				"does it follow that",
				"does this imply",
				"guaranteed true",
				"guaranteed/possible/impossible",
				"justified in saying",
				"must be true",
				"should assume",
				"take the following as truth"
            ]
        }
    ],
    "super_glue/cb": [
        {
            "task": "task1388_cb_entailment",
            "prompt": [
				"GPT-3 style",
				"MNLI crowdsource",
				"always/sometimes/never",
				"based on the previous passage",
				"can we infer",
				"claim true/false/inconclusive",
				"consider always/sometimes/never",
				"does it follow that",
				"does this imply",
				"guaranteed true",
				"guaranteed/possible/impossible",
				"justified in saying",
				"must be true",
				"should assume",
				"take the following as truth"
            ]
        }
    ],
    "glue/rte": [
        {
            "task": "task1344_glue_entailment_classification",
            "prompt": [
				"does the claim… follow the fact…",
				"entailment explained",
				"imply",
				"imply separated",
				"mean"
            ]
        }
    ],
    "winogrande/winogrande_debiased": [
        {
            "task": "task1391_winogrande_easy_answer_generation",
            "prompt": [
				"Replace",
				"does underscore refer to",
				"fill in the blank",
				"stand for",
				"underscore refer to"
            ]
        }
    ],
    "story_cloze/2016": [
        {
            "task": "task296_storycloze_correct_end_classification",
            "prompt": [
				"Answer Given options",
				"Choose Story Ending",
				"Movie What Happens Next",
				"Story Continuation and Options",
				"Generate Ending",
				"Novel Correct Ending"
            ]
        }
    ]
}