{
    "Contributors": [
        "Yeganeh Kordi"
    ],
    "Source": [
        "qasper"
    ],
    "URL": [
        "https://allenai.org/project/qasper/home"
    ],
    "Categories": [
        "Question Answering"
    ],
    "Reasoning": [],
    "Definition": [
        "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context."
    ],
    "Input_language": [
        "English"
    ],
    "Output_language": [
        "English"
    ],
    "Instruction_language": [
        "English"
    ],
    "Domains": [
        "Scientific Research Papers"
    ],
    "Positive Examples": [
        {
            "input": "We evaluate the proposed approach on the Chinese social media text summarization task, based on the sequence-to-sequence model. Large-Scale Chinese Short Text Summarization Dataset (LCSTS) is constructed by BIBREF1 . The dataset consists of more than 2.4 million text-summary pairs in total, constructed from a famous Chinese social media microblogging service Weibo.  \n Question: Are results reported only for English data?",
            "output": "No",
            "explanation": "Based on the context, the dataset is constructed from a famous Chinese social media microblogging service Weibo."
        },
        {
            "input": "Since INLINEFORM0 is constant, we only need to minimize INLINEFORM1 , therefore the loss function becomes: DISPLAYFORM0 \n Question: How is the expectation regularization loss defined?",
            "output": "DISPLAYFORM0",
            "explanation": "Based on the context, the loss function is DISPLAYFORM0."
        },
        {
            "input": "In natural language, subjectivity refers to the aspects of communication used to express opinions, evaluations, and speculationsBIBREF0, often influenced by one's emotional state and viewpoints. In this work, we investigate the application of BERT-based models for the task of subjective language detection. Experiments ::: Dataset and Experimental Settings\nWe perform our experiments on the WNC dataset open-sourced by the authors of BIBREF2. It consists of aligned pre and post neutralized sentences made by Wikipedia editors under the neutral point of view. It contains $180k$ biased sentences, and their neutral counterparts crawled from $423,823$ Wikipedia revisions between 2004 and 2019 \n Question: Which experiments are perfomed?",
            "output": "They used BERT-based models to detect subjective language in the WNC corpus",
            "explanation": "Based on the context, the answer is correct."
        }
    ],
    "Negative Examples": [
        {
            "input": "The minimal change operation substantially changed the meaning of the sentence, and yet the embedding of the transformation lies very closely to the original sentence (average similarity of 0.930). \n Question: Do they do any analysis of of how the modifications changed the starting set of sentences?",
            "output": "No",
            "explanation": "Based on the context, the answer is incorrect."
        },
        {
            "input": "Looking at the current WinoGrande leaderboard, it appears that the previous state of the art is based on RoBERTa BIBREF2, which can be characterized as an encoder-only transformer architecture. \n Question: What is the previous state of the art?",
            "output": "GPT-3",
            "explanation": "Based on the context, the answer should be RoBERTa."
        }
    ],
    "Instances": [
        {
            "id": "task460-9874768ccfbd475282704d4a07b6cbca",
            "input": "Annotator votes for one of the seven emotions, namely Ekman’s six basic emotions BIBREF1, plus the neutral. \n Question: What labels does the dataset have?",
            "output": [
                "Ekman’s six basic emotions  neutral"
            ]
        },
        {
            "id": "task460-a802d275f27d4fb29655a68f691f3bf0",
            "input": "Compared to reading comprehension based QA (RCQA) setup where the answers to a question is usually found in the given small paragraph, in the OpenBookQA setup the open book part is much larger (than a small paragraph) and is not complete as additional common knowledge may be required.  \n Question: How is OpenBookQA different from other natural language QA?",
            "output": [
                "in the OpenBookQA setup the open book part is much larger the open book part is much larger (than a small paragraph) and is not complete as additional common knowledge may be required"
            ]
        },
        {
            "id": "task460-9b1923458fe24d0f9369a7e1178349b5",
            "input": "The average classification accuracy results are summarised in Table TABREF9. \n Question: What evaluation metric is used?",
            "output": [
                "average classification accuracy"
            ]
        },
        {
            "id": "task460-e0ac70fe76a14484b741f4da2eb31dcd",
            "input": "It is true that the above-mentioned associated caption sentences for each concept-set are human-written and do describe scenes that cover all given concepts. However, they are created under specific contexts (i.e. an image or a video) and thus might be less representative for common sense. To better measure the quality and interpretability of generative reasoners, we need to evaluate them with scenes and rationales created by using concept-sets only as the signals for annotators.\n\nWe collect more human-written scenes for each concept-set in dev and test set through crowd-sourcing via the Amazon Mechanical Turk platform. Each input concept-set is annotated by at least three different humans. The annotators are also required to give sentences as the rationales, which further encourage them to use common sense in creating their scenes. \n Question: Are the sentences in the dataset written by humans who were shown the concept-sets?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-918655ebcb8f453486a0f546b9d6236b",
            "input": "We have evaluated our models considering the F1 Score, which is the harmonic mean of precision and recall. We have run ten times the experiment for each model and considered the average F1 Score. The results are mentioned in Table TABREF11. Considering F1 Macro the models that include the multi-attention mechanism outperform the others and particularly the one with the Projected Layer has the highest performance. In three out of four pairs of models, the ones with the Projected Layer achieved better performance, so in most cases the addition of the Projected Layer had a significant enhancement.\n\n \n Question: Which variation provides the best results on this dataset?",
            "output": [
                "the model with multi-attention mechanism and a projected layer"
            ]
        },
        {
            "id": "task460-300b45adaf0b4b28bd3538ffa4d57338",
            "input": "Based on the best results (BLSTM-CNN-CRF), error analysis is performed based on five types of errors (No extraction, No annotation, Wrong range, Wrong tag, Wrong range and tag), in a way similar to BIBREF10, but we analyze on both gold labels and predicted labels (more detail in figure 1 and 2). \n Question: What type of errors were produced by the BLSTM-CNN-CRF system?",
            "output": [
                "No extraction, No annotation, Wrong range, Wrong tag, Wrong range and tag"
            ]
        },
        {
            "id": "task460-d614ddd95fb04440944e92958ca32087",
            "input": "Following the formula, we can calculate the contribution of every input word makes to every output word, forming a contribution matrix of size $M \\times N$, where $N$ is the output sentence length. Given the contribution matrix, we can obtain the word importance of each input word to the entire output sentence. To this end, for each input word, we first aggregate its contribution values to all output words by the sum operation, and then normalize all sums through the Softmax function. \n Question: How do their models decide how much improtance to give to the output words?",
            "output": [
                "Given the contribution matrix, we can obtain the word importance of each input word to the entire output sentence. "
            ]
        },
        {
            "id": "task460-9d38deab5fed47429409d6652bb627e7",
            "input": "We use two different unsupervised approaches for word sense disambiguation.  The second, called `dense model', represents synsets and contexts in a dense, low-dimensional space by averaging word embeddings. In the synset embeddings model approach, we follow SenseGram BIBREF14 and apply it to the synsets induced from a graph of synonyms.  We observe that the SenseGram-based approach for word sense disambiguation yields substantially better results in every case (Table TABREF25 ). The primary reason for that is the implicit handling of similar words due to the averaging of dense word vectors for semantically related words.  \n Question: Do the authors offer any hypothesis about why the dense mode outperformed the sparse one?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-f15647fffb9f416b8a6b6ecf38df1802",
            "input": "The resource is composed of data from two different surveys. In both surveys subjects were asked to draw on a map (displayed under a Mercator projection) a polygon representing a given geographical descriptor, in the context of the geography of Galicia in Northwestern Spain (see Fig. FIGREF1 ). The first survey was run in order to obtain a high number of responses to be used as an evaluation testbed for modeling algorithms. It was answered by 15/16 year old students in a high school in Pontevedra (located in Western Galicia). 99 students provided answers for a list of 7 descriptors (including cardinal points, coast, inland, and a proper name). The second survey was addressed to meteorologists in the Galician Weather Agency BIBREF12 . \n Question: Which two datasets does the resource come from?",
            "output": [
                "two surveys by two groups - school students and meteorologists to draw on a map a polygon representing a given geographical descriptor"
            ]
        },
        {
            "id": "task460-05c1ff0dc0084bb0b7c5c0a533074855",
            "input": "We compare FSDM with four baseline methods and two ablations.\n\nNDM BIBREF7 proposes a modular end-to-end trainable network. It applies de-lexicalization on user utterances and responses.\n\nLIDM BIBREF9 improves over NDM by employing a discrete latent variable to learn underlying dialogue acts. This allows the system to be refined by reinforcement learning.\n\nKVRN BIBREF13 adopts a copy-augmented Seq2Seq model for agent response generation and uses an attention mechanism on the KB. It does not perform belief state tracking.\n\nTSCP/RL BIBREF10 is a two-stage CopyNet which consists of one encoder and two copy-mechanism-augmented decoders for belief state and response generation. \n Question: What baselines have been used in this work?",
            "output": [
                "NDM, LIDM, KVRN, and TSCP/RL"
            ]
        },
        {
            "id": "task460-48a8421bfe2d42208bef02db494f06bc",
            "input": "We evaluate the sequence-to-sequence QRNN architecture described in SECREF5 on a challenging neural machine translation task, IWSLT German–English spoken-domain translation, applying fully character-level segmentation.  \n Question: What languages pairs are used in machine translation?",
            "output": [
                "German–English"
            ]
        },
        {
            "id": "task460-d899498dde324df287d996764618999c",
            "input": "The final embedding dimensionality is equal to the number of unique word labels in the training set, which is 1061. \n Question: Which dimensionality do they use for their embeddings?",
            "output": [
                "1061"
            ]
        },
        {
            "id": "task460-bccdb0ee15704e56aadec9ebdda947eb",
            "input": "Although humor is a universal construct, there is a wide variety between what each individual may find humorous. We attempt to focus on a subset of the population where we can quantitatively measure reactions: the popular Reddit r/Jokes thread. This forum is highly popular - with tens of thousands of jokes being posted monthly and over 16 million members. Although larger joke datasets exist, the r/Jokes thread is unparalleled in the amount of rated jokes it contains. To the best of our knowledge there is no comparable source of rated jokes in any other language. These Reddit posts consist of the body of the joke, the punchline, and the number of reactions or upvotes. Although this type of humor may only be most enjoyable to a subset of the population, it is an effective way to measure responses to jokes in a large group setting. \n Question: What kind of humor they have evaluated?",
            "output": [
                "a subset of the population where we can quantitatively measure reactions: the popular Reddit r/Jokes thread These Reddit posts consist of the body of the joke, the punchline, and the number of reactions or upvotes. "
            ]
        },
        {
            "id": "task460-09d3024fa1b544788becc32c63698ffd",
            "input": "Despite its usefulness, linked entities extracted from ELS's have issues because of low precision rates BIBREF11 and design challenges in training datasets BIBREF12 . First, the extracted entities may be ambiguous. Second, the linked entities may also be too common to be considered an entity.  \n Question: Why are current ELS's not sufficiently effective?",
            "output": [
                "Linked entities may be ambiguous or too common"
            ]
        },
        {
            "id": "task460-ef447620b93e4af5b7e430c26502dd00",
            "input": "Language Model (LM): We train a two layer recurrent neural language model with GRU cells of hidden size 512.\n\n Sequence-to-Sequence Attention Model (S2S): We train a two layer neural sequence to sequence model equipped with bi-linear attention function with GRU cells of hidden size 512.  Linear Dynamical System (LDS): We also train a linear dynamical system as discussed in Section SECREF1 as one of our baselines for fair comparisons. Semi-Supervised SLDS (SLDS-X%): To gauge the usability of semi-supervision, we also train semi-supervised SLDS models with varying amount of labelled sentiment tags unlike the original model which uses 100% tagged data.  \n Question: What baselines are used?",
            "output": [
                "a two layer recurrent neural language model with GRU cells of hidden size 512 a two layer neural sequence to sequence model equipped with bi-linear attention function with GRU cells of hidden size 512 a linear dynamical system semi-supervised SLDS models with varying amount of labelled sentiment tags"
            ]
        },
        {
            "id": "task460-ac2f32758f714f4aacf804a90dfe69da",
            "input": "Irony Classifier: We implement a CNN classifier trained with our irony dataset. Sentiment Classifier for Irony: We first implement a one-layer LSTM network to classify ironic sentences in our dataset into positive and negative ironies. Sentiment Classifier for Non-irony: Similar to the training process of the sentiment classifier for irony, we first implement a one-layer LSTM network trained with the dataset for the sentiment analysis of common twitters to classify the non-ironies into positive and negative non-ironies. In this section, we describe some additional experiments on the transformation from ironic sentences to non-ironic sentences. \n Question: What experiments are conducted?",
            "output": [
                "Irony Classifier Sentiment Classifier for Irony Sentiment Classifier for Non-irony transformation from ironic sentences to non-ironic sentences"
            ]
        },
        {
            "id": "task460-46083b70854c489f946eb6c62362df12",
            "input": "We compare the performance with the Recurrent Entity Networks model (EntNet) BIBREF17 .  The RelNet model achieves a mean error of 0.285% across tasks which is better than the results of the EntNet model BIBREF17 . The RelNet model is able to achieve 0% test error on 11 of the tasks, whereas the EntNet model achieves 0% error on 7 of the tasks. \n Question: What methods is RelNet compared to?",
            "output": [
                "We compare the performance with the Recurrent Entity Networks model (EntNet) BIBREF17"
            ]
        },
        {
            "id": "task460-6d30109682984c85b4f97d798ac95a2f",
            "input": "To assess the consistency of annotations and also eliminate coincidental annotations, we used agreement rates, which is calculated by dividing the number of senses under each category where the annotators annotate consistently by the total number of each kind of sense. And considering the potential impact of unbalanced distribution of senses, we also used the Kappa value. \n Question: Which inter-annotator metric do they use?",
            "output": [
                "agreement rates Kappa value"
            ]
        },
        {
            "id": "task460-8ac00a5b40864a74af6691d10fd9834c",
            "input": "Women represent 33.16% of the speakers, confirming the figures given by the GMMP report BIBREF0. \n Question: How big is imbalance in analyzed corpora?",
            "output": [
                "Women represent 33.16% of the speakers"
            ]
        },
        {
            "id": "task460-77022e5e7e8946599d636af1fada692f",
            "input": "In this paper, we propose to represent each word with an expressive multimodal distribution, for multiple distinct meanings, entailment, heavy tailed uncertainty, and enhanced interpretability. For example, one mode of the word `bank' could overlap with distributions for words such as `finance' and `money', and another mode could overlap with the distributions for `river' and `creek'. It is our contention that such flexibility is critical for both qualitatively learning about the meanings of words, and for optimal performance on many predictive tasks. \n Question: How does this compare to contextual embedding methods?",
            "output": [
                " represent each word with an expressive multimodal distribution, for multiple distinct meanings, entailment, heavy tailed uncertainty, and enhanced interpretability. For example, one mode of the word `bank' could overlap with distributions for words such as `finance' and `money', and another mode could overlap with the distributions for `river' and `creek'."
            ]
        },
        {
            "id": "task460-10b16a19659349b692959fe8b1fe5736",
            "input": "Bag-of-words feature vectors were used to train a multinomial logistic regression model. Let INLINEFORM0 be the true label, where INLINEFORM1 is the total number of labels and INLINEFORM2 is the concatenation of the weight vectors INLINEFORM3 associated with the INLINEFORM4 th party then DISPLAYFORM0 \n Question: What model are the text features used in to provide predictions?",
            "output": [
                " multinomial logistic regression"
            ]
        },
        {
            "id": "task460-8c6fa3a836904462a2bafac7eedcdba5",
            "input": "In this paper we focus on the skipgram approach with random negative examples proposed in BIBREF0 .  \n Question: Do they use skipgram version of word2vec?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-a207cddfb18c4ba4afdef039b70cd734",
            "input": "We perform our experiments using ActivityNet Captions dataset BIBREF2 that is considered as the standard benchmark for dense video captioning task. The dataset contains approximately 20k videos from YouTube and split into 50/25/25 % parts for training, validation, and testing, respectively.  \n Question: What domain does the dataset fall into?",
            "output": [
                "YouTube videos"
            ]
        },
        {
            "id": "task460-805f345efbbe420c953e5c540667c6e7",
            "input": "Probing into the assorted empirical results may help us discover some interesting phenomenons:\n\nThe advantage of pre-training gradually diminishes with the increase of labeled data BIBREF14, BIBREF17, BIBREF18.\n\nFixed representations yield better results than fine-tuning in some cases BIBREF24.\n\nOverall, pre-training the Seq2Seq encoder outperforms pre-training the decoder BIBREF24, BIBREF17, BIBREF15, BIBREF16. \n Question: What experimental phenomena are presented?",
            "output": [
                "The advantage of pre-training gradually diminishes with the increase of labeled data Fixed representations yield better results than fine-tuning in some cases pre-training the Seq2Seq encoder outperforms pre-training the decoder"
            ]
        },
        {
            "id": "task460-c0f4fb1b45bf4ca288adbbbeb9ff1db8",
            "input": "Even though this corpus has incorrect sentences and their emotional labels, they lack their respective corrected sentences, necessary for the training of our model. In order to obtain this missing information, we outsource native English speakers from an unbiased and anonymous platform, called Amazon Mechanical Turk (MTurk) BIBREF19, which is a paid marketplace for Human Intelligence Tasks (HITs). We use this platform to create tasks for native English speakers to format the original incorrect tweets into correct sentences. Some examples are shown in Table TABREF12. The dataset used to evaluate the models' performance is the Chatbot Natural Language Unerstanding (NLU) Evaluation Corpus, introduced by Braun et al. BIBREF20 to test NLU services. It is a publicly available benchmark and is composed of sentences obtained from a German Telegram chatbot used to answer questions about public transport connections. The dataset has two intents, namely Departure Time and Find Connection with 100 train and 106 test samples, shown in Table TABREF18. Even though English is the main language of the benchmark, this dataset contains a few German station and street names. \n Question: Do they report results only on English datasets?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-5e286dd3b4ec4d269738742a75a33469",
            "input": "A maximum of three contributors will listen to any audio clip. If an $<$audio,transcript$>$ pair first receives two up-votes, then the clip is marked as valid. If instead the clip first receives two down-votes, then it is marked as invalid. A contributor may switch between recording and validation as they wish. Only clips marked as valid are included in the official training, development, and testing sets for each language. Clips which did not recieve enough votes to be validated or invalidated by the time of release are released as “other”. The train, test, and development sets are bucketed such that any given speaker may appear in only one. This ensures that contributors seen at train time are not seen at test time, which would skew results. Additionally, repetitions of text sentences are removed from the train, test, and development sets of the corpus. \n Question: How is validation of the data performed?",
            "output": [
                "A maximum of three contributors will listen to any audio clip. If an $<$audio,transcript$>$ pair first receives two up-votes, then the clip is marked as valid. If instead the clip first receives two down-votes, then it is marked as invalid."
            ]
        },
        {
            "id": "task460-80a64569204047869918011f574a1189",
            "input": "his reduces significantly the needed training time: SBERT can be tuned in less than 20 minutes, while yielding better results than comparable sentence embedding methods. \n Question: How much time takes its training?",
            "output": [
                "20 minutes"
            ]
        },
        {
            "id": "task460-3873e26dc4794a26aa4cf48cec3d92dc",
            "input": "As detailed before, we adapted the cluster-linking dataset from rupnik2016news to evaluate our online crosslingual clustering approach. \n Question: What are the sources of the datasets?",
            "output": [
                "rupnik2016news"
            ]
        },
        {
            "id": "task460-938919dcae3f48999ecb52f01496ecd5",
            "input": "The memory mechanism is adopted in order to enable the model to look beyond localized features and have access to the entire sequence. \n Question: Which features do they use?",
            "output": [
                "beyond localized features and have access to the entire sequence"
            ]
        },
        {
            "id": "task460-152a61df8cf445c7b590576aac3a2fcf",
            "input": "We use UltraSuite: a repository of ultrasound and acoustic data gathered from child speech therapy sessions BIBREF15 . We used all three datasets from the repository: UXTD (recorded with typically developing children), and UXSSD and UPX (recorded with children with speech sound disorders). In total, the dataset contains 13,815 spoken utterances from 86 speakers, corresponding to 35.9 hours of recordings. The utterances have been categorised by the type of task the child was given, and are labelled as: Words (A), Non-words (B), Sentence (C), Articulatory (D), Non-speech (E), or Conversations (F).  \n Question: Do they annotate their own dataset or use an existing one?",
            "output": [
                "Use an existing one"
            ]
        },
        {
            "id": "task460-f2bf5313fa504157828c8064ac1d5a14",
            "input": "Adaptation to food domain captioning Consequently, the food dataset has 3,806 images for training and 1,775 for validation. The non-food dataset has 78,976 images for training and 38,749 for validation. \n Question: How many examples are there in the target domain?",
            "output": [
                "the food dataset has 3,806 images for training "
            ]
        },
        {
            "id": "task460-6bdb6f145ae746ba81a5d8e07a2a569b",
            "input": "We obtain word vectors of size 300 from the learned word embeddings. To represent a Twitter profile, we retrieve word vectors for all the words that appear in a particular profile including the words appear in tweets, profile description, words extracted from emoji, cover and profile images converted to textual formats, and words extracted from YouTube video comments and descriptions for all YouTube videos shared in the user's timeline. Those word vectors are combined to compute the final feature vector for the Twitter profile. \n Question: How in YouTube content translated into a vector format?",
            "output": [
                "words extracted from YouTube video comments and descriptions for all YouTube videos shared in the user's timeline"
            ]
        },
        {
            "id": "task460-f278ebdbb63641788f36755be56df7e3",
            "input": "Our goal is to decouple the content selection from the decoder by introducing an extra content selector. The most intuitive way is training the content selector to target some heuristically extracted contents. For example, we can train the selector to select overlapped words between the source and target BIBREF6 , sentences with higher tf-idf scores BIBREF20 or identified image objects that appear in the caption BIBREF21 . INLINEFORM0 as Latent Variable: Another way is to treat INLINEFORM1 as a latent variable and co-train selector and generator by maximizing the marginal data likelihood. By doing so, the selector has the potential to automatically explore optimal selecting strategies best fit for the corresponding generator component. Reinforce-select (RS) BIBREF24 , BIBREF9 utilizes reinforcement learning to approximate the marginal likelihood. We propose Variational Reinforce-Select (VRS) which applies variational inference BIBREF10 for variance reduction. \n Question: How is the model trained to do only content selection?",
            "output": [
                "target some heuristically extracted contents treat INLINEFORM1 as a latent variable and co-train selector and generator by maximizing the marginal data likelihood reinforcement learning to approximate the marginal likelihood  Variational Reinforce-Select (VRS) which applies variational inference BIBREF10 for variance reduction"
            ]
        },
        {
            "id": "task460-4a941f48903a4dfeb8cde054af08c37b",
            "input": " We modeled the relationship between word count and the two metrics of user engagement (overall rating, mean number of turns) in separate linear regressions. \n Question: What are all the metrics to measure user engagement?",
            "output": [
                "overall rating mean number of turns"
            ]
        },
        {
            "id": "task460-7a7d6a6f13474077938256216f143f31",
            "input": "We describe our rules for WikiSQL here. We first detect WHERE values, which exactly match to table cells. After that, if a cell appears at more than one column, we choose the column name with more overlapped words with the question, with a constraint that the number of co-occurred words is larger than 1. By default, a WHERE operator is INLINEFORM0 , except for the case that surrounding words of a value contain keywords for INLINEFORM1 and INLINEFORM2 . Then, we deal with the SELECT column, which has the largest number of co-occurred words and cannot be same with any WHERE column. By default, the SELECT AGG is NONE, except for matching to any keywords in Table TABREF8 . The coverage of our rule on training set is 78.4%, with execution accuracy of 77.9%. Our rule for KBQA is simple without using a curated mapping dictionary. The pipeline of rules in SequentialQA is similar to that of WikiSQL. Compared to the grammar of WikiSQL, the grammar of SequentialQA has additional actions including copying the previous-turn logical form, no greater than, no more than, and negation. \n Question: How many rules had to be defined?",
            "output": [
                "WikiSQL - 2 rules (SELECT, WHERE)\nSimpleQuestions - 1 rule\nSequentialQA - 3 rules (SELECT, WHERE, COPY)"
            ]
        },
        {
            "id": "task460-2f2bcc8c4b324ba3964dc4dbfda57750",
            "input": "First, many news articles begin with reporter names, media agencies, dates or other contents irrelevant to the content, e.g. “New York (CNN) –”, “Jones Smith, May 10th, 2018:”. We therefore apply simple regular expressions to remove these prefixes.\n\nSecond, to ensure that the summary is concise and the article contains enough salient information, we only keep articles with 10-150 words in the top three sentences and 150-1200 words in the rest, and that contain at least 6 sentences in total. In this way, we filter out i) articles with excessively long content to reduce memory consumption; ii) very short leading sentences with little information which are unlikely to be a good summary. To encourage the model to generate abstrative summaries, we also remove articles where any of the top three sentences is exactly repeated in the rest of the article.\n\nThird, we try to remove articles whose top three sentences may not form a relevant summary. \n Question: What does the data cleaning and filtering process consist of?",
            "output": [
                "many news articles begin with reporter names, media agencies, dates or other contents irrelevant to the content to ensure that the summary is concise and the article contains enough salient information, we only keep articles with 10-150 words in the top three sentences and 150-1200 words in the rest, and that contain at least 6 sentences in total we try to remove articles whose top three sentences may not form a relevant summary"
            ]
        },
        {
            "id": "task460-33454f6a962e4828a8ebbd916f28b127",
            "input": "In BIBREF0 , BIBREF1 , BIBREF2 , Pasca et al. firstly extract potential class-attribute pairs using linguistically motivated patterns from unstructured text including query logs and query sessions, and then score the attributes using the Bayes model. In BIBREF3 , Rahul Rai proposed to identify product attributes from customer online reviews using part-of-speech(POS) tagging patterns, and to evaluate their importance with several different frequency metrics. In BIBREF4 , Lee et al. developed a system to extract concept-attribute pairs from multiple data sources, such as Probase, general web documents, query logs and external knowledge base, and aggregate the weights from different sources into one consistent typicality score using a Ranking SVM model. In BIBREF5 , Li et al. introduced the OntoRank algorithm for ranking the importance of semantic web objects at three levels of granularity: document, terms and RDF graphs. The algorithm is based on the rational surfer model, successfully used in the Swoogle semantic web search engine. \n Question: What are the traditional methods to identifying important attributes?",
            "output": [
                "automated attribute-value extraction score the attributes using the Bayes model evaluate their importance with several different frequency metrics aggregate the weights from different sources into one consistent typicality score using a Ranking SVM model OntoRank algorithm"
            ]
        },
        {
            "id": "task460-890b8cb6faee40e6885153e6753a6364",
            "input": "For comparison, we also report the performance of the error detection system by Rei2016, trained using the same FCE dataset. \n Question: What was the baseline used?",
            "output": [
                "error detection system by Rei2016"
            ]
        },
        {
            "id": "task460-419620509c684869a4f71c39e774d412",
            "input": "The question is encoded with a 1024-dimensional LSTM model that takes in a one-hot descriptor of each word in the question. The image is described with the 4096-dimensional output from the last fully connected layer of the Convolutional Neural Network (CNN), VGG16 BIBREF25 . The system performs an element-wise multiplication of the image and question features, after linearly transforming the image descriptor to 1024 dimensions. The final layer of the architecture is a softmax layer. \n Question: What is the model architecture used?",
            "output": [
                "LSTM to encode the question, VGG16 to extract visual features. The outputs of LSTM and VGG16 are multiplied element-wise and sent to a softmax layer."
            ]
        },
        {
            "id": "task460-c21cced9f78344229fdf9248dbdfe172",
            "input": "For the entailment classifier we compare Decomposable Attention BIBREF2 , BIBREF3 as implemented in the official baseline, ESIM BIBREF4 , and a transformer network with pre-trained weights BIBREF5 .  \n Question: Which pre-trained transformer do they use?",
            "output": [
                "BIBREF5"
            ]
        },
        {
            "id": "task460-5b9e4224d15e43b99101dd9d1ae38a7f",
            "input": "To investigate the trade-off between speed and performance, we compare our technique to standard models with and without attention on a Sequence Copy Task of varying length like in BIBREF14 . For this purpose we use 4 large machine translation datasets of WMT'17 on the following language pairs: English-Czech (en-cs, 52M examples), English-German (en-de, 5.9M examples), English-Finish (en-fi, 2.6M examples), and English-Turkish (en-tr, 207,373 examples). \n Question: Which datasets are used in experiments?",
            "output": [
                "Sequence Copy Task and WMT'17"
            ]
        },
        {
            "id": "task460-e94ad134e31c49caab1068dabc87ae7c",
            "input": "We compare our model with two sets of baselines:\n\nMemN2N BIBREF12 is an end-to-end trainable version of memory networks BIBREF9 . Attentive and Impatient Readers BIBREF6 use bidirectional LSTMs to encode question and evidence, and do classification over a large vocabulary based on these two encodings. \n Question: What are the baselines?",
            "output": [
                "MemN2N BIBREF12 Attentive and Impatient Readers BIBREF6"
            ]
        },
        {
            "id": "task460-0afb5f9ae2f241a185434179625a0b03",
            "input": "Hausa language is the second most spoken indigenous language in Africa with over 40 million native speakers BIBREF20, and one of the three major languages in Nigeria, along with Igbo and Yorùbá. The language is native to the Northern part of Nigeria and the southern part of Niger, and it is widely spoken in West and Central Africa as a trade language in eight other countries: Benin, Ghana, Cameroon, Togo, Côte d'Ivoire, Chad, Burkina Faso, and Sudan. Yorùbá language is the third most spoken indigenous language in Africa after Swahilli and Hausa with over 35 million native speakers BIBREF20. The language is native to the South-western part of Nigeria and the Southern part of Benin, and it is also spoken in other countries like Republic of Togo, Ghana, Côte d'Ivoire, Sierra Leone, Cuba and Brazil. \n Question: In which countries are Hausa and Yor\\`ub\\'a spoken?",
            "output": [
                "Nigeria Benin, Ghana, Cameroon, Togo, Côte d'Ivoire, Chad, Burkina Faso, and Sudan Republic of Togo, Ghana, Côte d'Ivoire, Sierra Leone, Cuba and Brazil"
            ]
        },
        {
            "id": "task460-c8c282465234400595dbb687213c7833",
            "input": "In order to derive the discourse features, an entity grid is constructed by feeding the document through an NLP pipeline to identify salient entities. Two flavors of discourse features are created by populating the entity grid with either (i) grammatical relations (GR) or (ii) RST discourse relations (RST).  \n Question: What discourse features are used?",
            "output": [
                "Entity grid with grammatical relations and RST discourse relations."
            ]
        },
        {
            "id": "task460-f0c5efc203444b77a983edaa3f13167f",
            "input": "Our visual model is based on fine-tuning an Inception V3 model BIBREF1 over visual renderings of documents, while our textual model is based on a hierarchical biLSTM. We further combine the two into a joint model.  We applied neural network models to capture visual features given visual renderings of documents. Experimental results show that we achieve a 2.9% higher accuracy than state-of-the-art approaches based on textual features over Wikipedia, and performance competitive with or surpassing state-of-the-art approaches over arXiv. We further proposed a joint model, combining textual and visual representations, to predict the quality of a document.  \n Question: What kind of model do they use?",
            "output": [
                "visual model is based on fine-tuning an Inception V3 model BIBREF1 over visual renderings of documents, while our textual model is based on a hierarchical biLSTM. We further combine the two into a joint model.  neural network models"
            ]
        },
        {
            "id": "task460-7911d3e0af7145e5947b7b06cd6e8bb6",
            "input": "For training data, 3 million webpages from this corpus were processed with a CCG parser to produce logical forms BIBREF10 . We also used the test set created by Krishnamurthy and Mitchell, which contains 220 queries generated in the same fashion as the training data from a separate section of ClueWeb. However, as they did not release a development set with their data, we used this set as a development set. This final test set contains 307 queries. \n Question: How big is their dataset?",
            "output": [
                "3 million webpages processed with a CCG parser for training, 220 queries for development, and 307 queries for testing"
            ]
        },
        {
            "id": "task460-770a64a8648b446d919b5bb46e6085ae",
            "input": "Akkadian is divided into six dialects in the dataset: Old Babylonian, Middle Babylonian peripheral, Standard Babylonian, Neo Babylonian, Late Babylonian, and Neo Assyrian BIBREF14 . \n Question: Which language is divided into six dialects in the task mentioned in the paper?",
            "output": [
                "Akkadian."
            ]
        },
        {
            "id": "task460-3200810321eb4a1085bc1c23d45ca853",
            "input": "In cooperation with the ruling regime, Weibo sets strict control over the content published under its service BIBREF0. According to Zhu et al. zhu-etal:2013, Weibo uses a variety of strategies to target censorable posts, ranging from keyword list filtering to individual user monitoring. Among all posts that are eventually censored, nearly 30% of them are censored within 5–30 minutes, and nearly 90% within 24 hours BIBREF1. We hypothesize that the former are done automatically, while the latter are removed by human censors. \n Question: Is is known whether Sina Weibo posts are censored by humans or some automatic classifier?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-d165653b7aa346c0a9713f29b5e69c8f",
            "input": "The following handcrafted features were used for the model:\n\nBias feature\n\nToken feature\n\nUppercase feature (y/n)\n\nTitlecase feature (y/n)\n\nCharacter trigram feature\n\nQuotation feature (y/n)\n\nWord suffix feature (last three characters)\n\nPOS tag (provided by spaCy utilities)\n\nWord shape (provided by spaCy utilities)\n\nWord embedding (see Table TABREF26) \n Question: What are the handcrafted features used?",
            "output": [
                "Bias feature Token feature Uppercase feature (y/n) Titlecase feature (y/n) Character trigram feature Quotation feature (y/n) Word suffix feature (last three characters) POS tag (provided by spaCy utilities) Word shape (provided by spaCy utilities) Word embedding (see Table TABREF26)"
            ]
        },
        {
            "id": "task460-3a984197ca2049c9bd405fed5aa0a57f",
            "input": " Recurrent Neural Network (RNN) Language Models\nare trained to output the probability distribution of the upcoming word given a context, without explicitly representing the structure of the context BIBREF9, BIBREF10. ActionLSTM\nmodels the linearized bracketed tree structure of a sentence by learning to predict the next action required to construct a phrase-structure parse BIBREF16. Generative Recurrent Neural Network Grammars (RNNG)\njointly model the word sequence as well as the underlying syntactic structure BIBREF18. \n Question: What are the baseline models?",
            "output": [
                "Recurrent Neural Network (RNN) ActionLSTM Generative Recurrent Neural Network Grammars (RNNG)"
            ]
        },
        {
            "id": "task460-38df1f8c8e6b403ba7fc5bf494b15ea1",
            "input": "Experiments on two publicly available datasets (Camrest BIBREF11 and InCar Assistant BIBREF6) confirm the effectiveness of the KB-retriever. \n Question: Which dialog datasets did they experiment with?",
            "output": [
                "Camrest InCar Assistant"
            ]
        },
        {
            "id": "task460-87fe094eda2d48e39ce23eae55acb9ca",
            "input": "Open-domain chatbots are more generic dialogue systems. An example is the Poly-encoder from BIBREF7 humeau2019real. It outperforms the Bi-encoder BIBREF8, BIBREF9 and matches the performance of the Cross-encoder BIBREF10, BIBREF11 while maintaining reasonable computation time. It performs strongly on downstream language understanding tasks involving pairwise comparisons, and demonstrates state-of-the-art results on the ConvAI2 challenge BIBREF12. Feed Yourself BIBREF13 is an open-domain dialogue agent with a self-feeding model. When the conversation goes well, the dialogue becomes part of the training data, and when the conversation does not, the agent asks for feedback. Lastly, Kvmemnn BIBREF14 is a key-value memory network with a knowledge base that uses a key-value retrieval mechanism to train over multiple domains simultaneously. We use all three of these models as baselines for comparison.  We compare against four dialogue system baselines: Kvmemnn, Feed Yourself, Poly-encoder, and a BERT bi-ranker baseline trained on the Persona-Chat dataset using the same training hyperparameters (including learning rate scheduler and length capping settings) described in Section SECREF20. \n Question: What baseline models are used?",
            "output": [
                "the Poly-encoder from BIBREF7 humeau2019real Feed Yourself BIBREF13 is an open-domain dialogue agent with a self-feeding model Kvmemnn BIBREF14 is a key-value memory network with a knowledge base that uses a key-value retrieval mechanism to train over multiple domains simultaneously We compare against four dialogue system baselines: Kvmemnn, Feed Yourself, Poly-encoder, and a BERT bi-ranker baseline trained on the Persona-Chat dataset using the same training hyperparameters (including learning rate scheduler and length capping settings) described in Section SECREF20. a BERT bi-ranker"
            ]
        },
        {
            "id": "task460-a98059116d284874b7fb2576c1640084",
            "input": "We see that A-gen performance improves significantly with the joint model: both F1 and EM increase by about 10 percentage points. \n Question: How much improvement does jointly learning QA and QG give, compared to only training QA?",
            "output": [
                "We see that A-gen performance improves significantly with the joint model: both F1 and EM increase by about 10 percentage points. "
            ]
        },
        {
            "id": "task460-b2cfb3407ecc41dea6b4bfc00587463c",
            "input": "We defined the intents with guidance from queries collected using a scoping crowdsourcing task, which prompted crowd workers to provide questions and commands related to topic domains in the manner they would interact with an artificially intelligent assistant. We manually grouped data generated by scoping tasks into intents.  \n Question: How was the dataset annotated?",
            "output": [
                "intents are annotated manually with guidance from queries collected using a scoping crowdsourcing task"
            ]
        },
        {
            "id": "task460-d49d1d2c20db41baa3e7f3b948c0f7c1",
            "input": "S2-GORC is a large citation graph dataset which includes full texts of 8.1 million scientific documents. We select a subset of 154K computer science articles as our corpus. From these, we extract 622K citing sentences that link back to other documents in our corpus. We hold 2500 examples for each of the validation and test sets. \n Question: What is the size of the corpus?",
            "output": [
                "8.1 million scientific documents 154K computer science articles 622K citing sentences"
            ]
        },
        {
            "id": "task460-2561352b379641dfb872d527f2495784",
            "input": "Observe that in only about 12% (7 of the top 59) of the most cited areas of research, women received higher average citations than men. These include: sentiment analysis, information extraction, document summarization, spoken dialogue, cross lingual (research), dialogue, systems, language generation. (Of course, note that some of the 59 areas, as estimated using title term bigrams, are overlapping. Also, we did not include large scale in the list above because the difference in averages is very small and it is not really an area of research.) Thus, the citation gap is common across a majority of the high-citations areas within NLP. \n Question: Which NLP area have the highest average citation for woman author?",
            "output": [
                "sentiment analysis, information extraction, document summarization, spoken dialogue, cross lingual (research), dialogue, systems, language generation"
            ]
        },
        {
            "id": "task460-c353555a6b4e40778bb3ed5ccde8dece",
            "input": "The input document/summary that may have unordered sentences is processed so that it will have sentences clustered together. To test our approach, we jumble the ordering of sentences in a document, process the unordered document and compare the similarity of the output document with the original document. To structure an unordered document is an essential task in many applications. It is a post-requisite for applications like multiple document extractive text summarization where we have to present a summary of multiple documents. It is a prerequisite for applications like question answering from multiple documents where we have to present an answer by processing multiple documents. \n Question: What is an unordered text document, do these arise in real-world corpora?",
            "output": [
                "A unordered text document is one where sentences in the document are disordered or jumbled. It doesn't appear that unordered text documents appear in corpora, but rather are introduced as part of processing pipeline."
            ]
        },
        {
            "id": "task460-6649727d8d924bbbb3339244c82b1890",
            "input": "We used a set of global network indicators which allow us to encode each network layer by a tuple of features. Then we simply concatenated tuples as to represent each multi-layer network with a single feature vector. We used the following global network properties:\n\nNumber of Strongly Connected Components (SCC): a Strongly Connected Component of a directed graph is a maximal (sub)graph where for each pair of vertices $u,v$ there is a path in each direction ($u\\rightarrow v$, $v\\rightarrow u$).\n\nSize of the Largest Strongly Connected Component (LSCC): the number of nodes in the largest strongly connected component of a given graph.\n\nNumber of Weakly Connected Components (WCC): a Weakly Connected Component of a directed graph is a maximal (sub)graph where for each pair of vertices $(u, v)$ there is a path $u \\leftrightarrow v$ ignoring edge directions.\n\nSize of the Largest Weakly Connected Component (LWCC): the number of nodes in the largest weakly connected component of a given graph.\n\nDiameter of the Largest Weakly Connected Component (DWCC): the largest distance (length of the shortest path) between two nodes in the (undirected version of) largest weakly connected component of a graph.\n\nAverage Clustering Coefficient (CC): the average of the local clustering coefficients of all nodes in a graph; the local clustering coefficient of a node quantifies how close its neighbours are to being a complete graph (or a clique). It is computed according to BIBREF28.\n\nMain K-core Number (KC): a K-core BIBREF13 of a graph is a maximal sub-graph that contains nodes of internal degree $k$ or more; the main K-core number is the highest value of $k$ (in directed graphs the total degree is considered).\n\nDensity (d): the density for directed graphs is $d=\\frac{|E|}{|V||V-1|}$, where $|E|$ is the number of edges and $|N|$ is the number of vertices in the graph; the density equals 0 for a graph without edges and 1 for a complete graph.\n\nStructural virality of the largest weakly connected component (SV): this measure is defined in BIBREF14 as the average distance between all pairs of nodes in a cascade tree or, equivalently, as the average depth of nodes, averaged over all nodes in turn acting as a root; for $|V| > 1$ vertices, $SV=\\frac{1}{|V||V-1|}\\sum _i\\sum _j d_{ij}$ where $d_{ij}$ denotes the length of the shortest path between nodes $i$ and $j$. This is equivalent to compute the Wiener's index BIBREF29 of the graph and multiply it by a factor $\\frac{1}{|V||V-1|}$. In our case we computed it for the undirected equivalent graph of the largest weakly connected component, setting it to 0 whenever $V=1$. \n Question: What are the global network features which quantify different aspects of the sharing process?",
            "output": [
                "Number of Strongly Connected Components (SCC) Size of the Largest Strongly Connected Component (LSCC) Number of Weakly Connected Components (WCC) Size of the Largest Weakly Connected Component (LWCC) Diameter of the Largest Weakly Connected Component (DWCC) Average Clustering Coefficient (CC) Main K-core Number (KC) Density (d)"
            ]
        },
        {
            "id": "task460-78bb409a84f14047b61f8e991d0b09fc",
            "input": "Building on these results, we take a somewhat more systematic approach to looking for interpretable hidden state dimensions, by using decision trees to predict individual hidden state dimensions (Figure 2 ). We visualize the overall dynamics of the hidden states by coloring the training data with the k-means clusters on the state vectors (Figures 3 , 3 ). In Figures 3 and 3 , we color-code the training data with the 10 HMM states. In Figures 3 and 3 , we apply k-means clustering to the LSTM state vectors, and color-code the training data with the clusters. \n Question: Which methods do the authors use to reach the conclusion that LSTMs and HMMs learn complementary information?",
            "output": [
                "decision trees to predict individual hidden state dimensions apply k-means clustering to the LSTM state vectors, and color-code the training data with the clusters"
            ]
        },
        {
            "id": "task460-7950909759854e86a72daad46aab8746",
            "input": "Four datasets are used in our work, including CoNLL 2003 German BIBREF9 , CoNLL 2002 Spanish BIBREF10 , OntoNotes 4 BIBREF11 and Weibo NER BIBREF12 .  Table TABREF22 shows the results on Chinese OntoNotes 4.0.  \n Question: Which languages do they work with?",
            "output": [
                "German Spanish Chinese"
            ]
        },
        {
            "id": "task460-10017b5da7404786aa82917459fc5041",
            "input": "We can see that how 1.10intelligence is used varies by field: in computer science the most similar words include 1.10artificial and 1.10ai; in finance, similar words include 1.10abilities and 1.10consciousness. \n Question: Which words are used differently across ArXiv?",
            "output": [
                "intelligence"
            ]
        },
        {
            "id": "task460-320846ec195f4bcba858c06a20edfe4c",
            "input": "We call the model as a modified Toulmin's model. It contains five argument components, namely claim, premise, backing, rebuttal, and refutation. When annotating a document, any arbitrary token span can be labeled with an argument component; the components do not overlap.  \n Question: What argument components do the ML methods aim to identify?",
            "output": [
                "claim, premise, backing, rebuttal, and refutation"
            ]
        },
        {
            "id": "task460-4cd6b2e6f6304ce196ab6691befc3547",
            "input": "Los resultados de la evaluación se presentan en la Tabla TABREF42, en la forma de promedios normalizados entre [0,1] y de su desviación estándar $\\sigma $. \n Question: What evaluation metrics did they look at?",
            "output": [
                "accuracy with standard deviation"
            ]
        },
        {
            "id": "task460-fb862882b5ac43d09f941806fdbb613d",
            "input": "When evaluating classifiers it is common to use accuracy, precision and recall as well as Hamming loss.  It has been shown that when calculating precision and recall on multi-label classifiers, it can be advantageous to use micro averaged precision and recall BIBREF6 . The formulas for micro averaged precision are expressed as DISPLAYFORM0 DISPLAYFORM1 \n Question: what evaluation metrics are discussed?",
            "output": [
                "precision  recall  Hamming loss micro averaged precision and recall "
            ]
        },
        {
            "id": "task460-8aeb353b3c5c45b5aae00c360bbc25f8",
            "input": "Very recently, the success of deep neural networks in many natural language processing tasks ( BIBREF20 ) has inspired new work in abstractive summarization . BIBREF2 propose a neural attention model with a convolutional encoder to solve this task.  More recently, BIBREF4 extended BIBREF2 's work with an RNN decoder, and BIBREF8 proposed an RNN encoder-decoder architecture for summarization. Both techniques are currently the state-of-the-art on the DUC competition. \n Question: What is the state-of-the art?",
            "output": [
                "neural attention model with a convolutional encoder with an RNN decoder and RNN encoder-decoder"
            ]
        },
        {
            "id": "task460-f3a01ab636ed45898b29e097cf980680",
            "input": "Although we create new models for tasks such as sentiment analysis and gender detection as part of AraNet, our focus is more on putting together the toolkit itself and providing strong baselines that can be compared to. Hence, although we provide some baseline models for some of the tasks, we do not explicitly compare to previous research since most existing works either exploit smaller data (and so it will not be a fair comparison), use methods pre-dating BERT (and so will likely be outperformed by our models) . For many of the tasks we model, there have not been standard benchmarks for comparisons across models. This makes it difficult to measure progress and identify areas worthy of allocating efforts and budgets. \n Question: What models did they compare to?",
            "output": [
                " we do not explicitly compare to previous research since most existing works either exploit smaller data (and so it will not be a fair comparison), use methods pre-dating BERT (and so will likely be outperformed by our models)"
            ]
        },
        {
            "id": "task460-65fbdd9558ac4521b9e1cbf898fe7972",
            "input": "A higher score indicates better step ordering (with a maximum score of 2). tab:coherencemetrics shows that our personalized models achieve average recipe-level coherence scores of 1.78-1.82, surpassing the baseline at 1.77. On average, human evaluators preferred personalized model outputs to baseline 63% of the time, confirming that personalized attention improves the semantic plausibility of generated recipes. \n Question: What were their results on the new dataset?",
            "output": [
                "average recipe-level coherence scores of 1.78-1.82 human evaluators preferred personalized model outputs to baseline 63% of the time"
            ]
        },
        {
            "id": "task460-57f3b7550541459a9c26b812f0de0ba9",
            "input": "We also are especially interested in seeing this model applied to different languages. \n Question: Have the authors tried this approach on other languages?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-f2e4e59649f24819b6f45be6d0894f2c",
            "input": "During each evaluation, target terms are masked, predicted, and then compared to the masked (known) value. \n Question: Are the Transformers masked?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-6f384d8e07c04462b142e0451433a5b1",
            "input": "We use the full-length ROUGE F1 and METEOR as our main evaluation metrics. \n Question: What evaluation metrics do they use?",
            "output": [
                "ROUGE F1 METEOR"
            ]
        },
        {
            "id": "task460-de818dc353e944bd9ecd42b68dfce3fa",
            "input": "Detection: In hypernymy detection, the task is to classify whether pairs of words are in a hypernymy relation. Direction: In direction prediction, the task is to identify which term is broader in a given pair of words. Graded Entailment: In graded entailment, the task is to quantify the degree to which a hypernymy relation holds. \n Question: What hypernymy tasks do they study?",
            "output": [
                "Detection Direction Graded Entailment"
            ]
        },
        {
            "id": "task460-e77df7ce024246eb8507a5bf69dc4731",
            "input": "The current configuration of Aristo comprises of eight solvers, described shortly, each of which attempts to answer a multiple choice question. To study particular phenomena and develop solvers, the project has created larger datasets to amplify and study different problems, resulting in 10 new datasets and 5 large knowledge resources for the community.\n\nThe solvers can be loosely grouped into:\n\nStatistical and information retrieval methods\n\nReasoning methods\n\nLarge-scale language model methods Over the life of the project, the relative importance of the methods has shifted towards large-scale language methods. The field of NLP has advanced substantially with the advent of large-scale language models such as ELMo (BID6), ULMFit (BID37), GPT (BID38), BERT (BID7), and RoBERTa (BID8). These models are trained to perform various language prediction tasks such as predicting a missing word or the next sentence, using large amounts of text (e.g., BERT was trained on Wikipedia + the Google Book Corpus of 10,000 books). They can also be fine-tuned to new language prediction tasks, such as question-answering, and have been remarkably successful in the few months that they have been available. We apply BERT to multiple choice questions by treating the task as classification: Given a question $q$ with answer options $a_{i}$ and optional background knowledge $K_{i}$, we provide it to BERT as:\n\n[CLS] $K_i$ [SEP] $q$ [SEP] $a_{i}$ [SEP] The AristoBERT solver uses three methods to apply BERT more effectively. First, we retrieve and supply background knowledge along with the question when using BERT. This provides the potential for BERT to “read” that background knowledge and apply it to the question, although the exact nature of how it uses background knowledge is more complex and less interpretable. Second, we fine-tune BERT using a curriculum of several datasets, including some that are not science related. Finally, we ensemble different variants of BERT together. \n Question: Is Aristo just some modern NLP model (ex. BERT) finetuned od data specific for this task?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-b2fb73dd2a174108b779ccaa8d7d563d",
            "input": "CoinCollector BIBREF8 is a class of text-based games where the objective is to find and collect a coin from a specific location in a given set of connected rooms . The agent wins the game after it collects the coin, at which point (for the first and only time) a reward of +1 is received by the agent. The environment parses only five admissible commands (go north, go east, go south, go west, and take coin) made by two worlds; CookingWorld BIBREF14 in this challenge, there are 4,440 games with 222 different levels of difficulty, with 20 games per level of difficulty, each with different entities and maps. The goal of each game is to cook and eat food from a given recipe, which includes the task of collecting ingredients (e.g. tomato, potato, etc.), objects (e.g. knife), and processing them according to the recipe (e.g. cook potato, slice tomato, etc.). The parser of each game accepts 18 verbs and 51 entities with a predefined grammar, but the overall size of the vocabulary of the observations is 20,000. In Appendix SECREF36 we provide more details about the levels and the games' grammar. \n Question: On what Text-Based Games are experiments performed?",
            "output": [
                "CoinCollector  CookingWorld "
            ]
        },
        {
            "id": "task460-00071d5e767347f79fc8af787dfa66aa",
            "input": "Sentence classification combines natural language processing (NLP) with machine learning to identify trends in sentence structure, BIBREF14 , BIBREF15 . Each tweet is converted to a numeric word vector in order to identify distinguishing features by training an NLP classifier on a validated set of relevant tweets. The classifier acts as a tool to sift through ads, news, and comments not related to patients. Our scheme combines a logistic regression classifier, BIBREF16 , with a Convolutional Neural Network (CNN), BIBREF17 , BIBREF18 , to identify self-reported diagnostic tweets.\n\nIt is important to be wary of automated accounts (e.g. bots, spam) whose large output of tweets pollute relevant organic content, BIBREF19 , and can distort sentiment analyses, BIBREF20 . Prior to applying sentence classification, we removed tweets containing hyperlinks to remove automated content (some organic content is necessarily lost with this strict constraint). Twitter allows users to spread content from other users via `retweets'. We also removed these posts prior to classification to isolate tweets authored by patients. We also accounted for non-relevant astrological content by removing all tweets containing any of the following horoscope indicators: `astrology',`zodiac',`astronomy',`horoscope',`aquarius',`pisces',`aries',`taurus',`leo',`virgo',`libra', and `scorpio'. We preprocessed tweets by lowercasing and removing punctuation. We also only analyzed tweets for which Twitter had identified `en' for the language English. \n Question: What machine learning and NLP methods were used to sift tweets relevant to breast cancer experiences?",
            "output": [
                "ML  logistic regression classifier combined with a Convolutional Neural Network (CNN) to identify self-reported diagnostic tweets.\nNLP methods:  tweet conversion to numeric word vector,  removing tweets containing hyperlinks, removing \"retweets\", removing all tweets containing horoscope indicators,  lowercasing and  removing punctuation."
            ]
        },
        {
            "id": "task460-4ce35bebe7b842dc967a9719233e7290",
            "input": "The input of our model are the words in the input text $x[1], ... , x[n]$ and query $q[1], ... , q[n]$ . We concatenate pre-trained word embeddings from GloVe BIBREF40 and character embeddings trained by CharCNN BIBREF41 to represent input words. The $2d$ -dimension embedding vectors of input text $x_1, ... , x_n$ and query $q_1, ... , q_n$ are then fed into a Highway Layer BIBREF42 to improve the capability of word embeddings and character embeddings as\n\n$$\\begin{split} g_t &= {\\rm sigmoid}(W_gx_t+b_g) \\\\ s_t &= {\\rm relu } (W_xx_t+b_x) \\\\ u_t &= g_t \\odot s_t + (1 - g_t) \\odot x_t~. \\end{split}$$ (Eq. 18) The same Highway Layer is applied to $q_t$ and produces $v_t$ . Next, $u_t$ and $v_t$ are fed into a Bi-Directional Long Short-Term Memory Network (BiLSTM) BIBREF44 respectively in order to model the temporal interactions between sequence words: Then we feed $\\mathbf {U}$ and $\\mathbf {V}$ into the attention flow layer BIBREF27 to model the interactions between the input text and query. Therefore, we introduce Self-Matching Layer BIBREF29 in our model as\n\n$$\\begin{split} o_t &= {\\rm BiLSTM}(o_{t-1}, [h_t, c_t]) \\\\ s_j^t &= w^T {\\rm tanh}(W_hh_j+\\tilde{W_h}h_t)\\\\ \\alpha _i^t &= {\\rm exp}(s_i^t)/\\Sigma _{j=1}^n{\\rm exp}(s_j^t)\\\\ c_t &= \\Sigma _{i=1}^n\\alpha _i^th_i ~. \\end{split}$$ (Eq. 20) Finally we feed the embeddings $\\mathbf {O} = [o_1, ... , o_n]$ into a Pointer Network BIBREF39 to decode the answer sequence as\n\n$$\\begin{split} p_t &= {\\rm LSTM}(p_{t-1}, c_t) \\\\ s_j^t &= w^T {\\rm tanh}(W_oo_j+W_pp_{t-1})\\\\ \\beta _i^t &= {\\rm exp}(s_i^t)/\\Sigma _{j=1}^n{\\rm exp}(s_j^t)\\\\ c_t &= \\Sigma _{i=1}^n\\beta _i^to_i~. \\end{split}$$ (Eq. 21) Therefore, the probability of generating the answer sequence $\\textbf {a}$ is as follows\n\n$${\\rm P}(\\textbf {a}|\\mathbf {O}) = \\prod _t {\\rm P}(a^t | a^1, ... , a^{t-1}, \\mathbf {O})~.$$ (Eq. 23) \n Question: What QA models were used?",
            "output": [
                "A pointer network decodes the answer from a bidirectional LSTM with attention flow layer and self-matching layer, whose inputs come from word and character embeddings of the query and input text fed through a highway layer."
            ]
        },
        {
            "id": "task460-553ee81e057846f0b5c336639ae2b3b5",
            "input": "In this paper, we introduce automatic `drunk-texting prediction' as a computational task. Given a tweet, the goal is to automatically identify if it was written by a drunk user.  \n Question: Do the authors equate drunk tweeting with drunk texting? ",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-5044ebb7bbdc4ffa99bacb9fef386028",
            "input": "Concretely, we selected gold standards that fit our problem definition and were published in the years 2016 to 2019, have at least $(2019 - publication\\ year) \\times 20$ citations, and bucket them according to the answer selection styles as described in Section SECREF4 \n Question: What modern MRC gold standards are analyzed?",
            "output": [
                "fit our problem definition and were published in the years 2016 to 2019, have at least $(2019 - publication\\ year) \\times 20$ citations"
            ]
        },
        {
            "id": "task460-4f90835737ba459cac2b31cd7be7e83f",
            "input": "We used two parallel corpora: Asian Scientific Paper Excerpt Corpus (ASPEC) BIBREF0 and NTCIR PatentMT Parallel Corpus BIBREF1 . \n Question: What parallel corpus did they use?",
            "output": [
                "Asian Scientific Paper Excerpt Corpus (ASPEC) BIBREF0 NTCIR PatentMT Parallel Corpus BIBREF1"
            ]
        },
        {
            "id": "task460-d72248ab9361496ab80716682644bb63",
            "input": "In order to understand the latent topics of those #MeToo tweets for college followers, we first utilize Latent Dirichlet Allocation (LDA) to label universal topics demonstrated by the users. Since certain words frequently appear in those #MeToo tweets (e.g., sexual harassment, men, women, story, etc.), we transform our corpus using TF-IDF, a term-weighting scheme that discounts the influence of common terms. \n Question: How are the topics embedded in the #MeToo tweets extracted?",
            "output": [
                "Using Latent Dirichlet Allocation on TF-IDF transformed from the corpus"
            ]
        },
        {
            "id": "task460-8005b94027074757a7066e4748f296eb",
            "input": "We evaluate our newly proposed models and related baselines in several low-resource settings across different languages with real, distantly supervised data with non-synthetic noise.  \n Question: How they evaluate their approach?",
            "output": [
                "They  evaluate newly proposed models in several low-resource settings across different languages with real, distantly supervised data with non-synthetic noise"
            ]
        },
        {
            "id": "task460-24648b7408c045a4880249d298a9d4f4",
            "input": "the attention heads in the proposed adaptively sparse Transformer can specialize more and with higher confidence In particular, we analyzed how the attention heads in the proposed adaptively sparse Transformer can specialize more and with higher confidence. \n Question: How does their model improve interpretability compared to softmax transformers?",
            "output": [
                "the attention heads in the proposed adaptively sparse Transformer can specialize more and with higher confidence"
            ]
        },
        {
            "id": "task460-8950fd801b69486599aa7f07b66601fb",
            "input": "For the training data, we use Daily Mail news articles released by BIBREF9 .  \n Question: what dataset was used?",
            "output": [
                "Daily Mail news articles released by BIBREF9 "
            ]
        },
        {
            "id": "task460-7c818dedd9054eaa886b0ac22d795118",
            "input": "GPT-2 achieves the highest score and the $n$-gram the lowest. \n Question: Which of the model yields the best performance?",
            "output": [
                "GPT-2"
            ]
        },
        {
            "id": "task460-ea666a654ce54831a5984069ce9df7a0",
            "input": "The task of humor identification in social media texts is analyzed as a classification problem and several machine learning classification models are used. \n Question: What experiments were carried out on the corpus?",
            "output": [
                "task of humor identification in social media texts is analyzed as a classification problem"
            ]
        },
        {
            "id": "task460-69a585b7aa9546c9b95cbf0963ccdf79",
            "input": "Table TABREF14 show the results obtained for proposed s2sL approach in comparison to that of MLP for the tasks of Speech/Music and Neutral/Sad classification, by considering different proportions of training data. \n Question: Which models/frameworks do they compare to?",
            "output": [
                "MLP"
            ]
        },
        {
            "id": "task460-a1860a51018c49f2b3687e0d204407a7",
            "input": "The involved algorithms are detailed as follows.\n\nCNN-C denotes the CNN with (Chinese) character embedding.\n\nCNN-W denotes the CNN with (Chinese) word embedding.\n\nCNN-Lex-C denotes the algorithm which also integrates polar words in CNN which is proposed by Shin et al. BIBREF24 . The (Chinese) character embedding is used.\n\nCNN-Lex-W denotes the algorithm which also integrates polar words in CNN which is proposed by Shin et al. BIBREF24 . The (Chinese) word embedding is used.\n\nBi-LSTM-C denotes the BI-LSTM with (Chinese) character embedding.\n\nBi-LSTM-W denotes the Bi-LSTM with (Chinese) word embedding.\n\nLex-rule denotes the rule-based approach shows in Fig. 1. This approach is unsupervised.\n\nBOW denotes the conventional algorithm which is based of bag-of-words features. \n Question: What are the other models they compare to?",
            "output": [
                "CNN-C CNN-W CNN-Lex-C CNN-Lex-W Bi-LSTM-C  Bi-LSTM-W Lex-rule BOW"
            ]
        },
        {
            "id": "task460-c000f903161349418414a03f16587bd6",
            "input": "Semantic Retrieval: We treated the neural semantic retrieval at both the paragraph and sentence level as binary classification problems with models' parameters updated by minimizing binary cross entropy loss. To be specific, we fed the query and context into BERT as:\n\nWe applied an affine layer and sigmoid activation on the last layer output of the [$\\mathit {CLS}$] token which is a scalar value. The parameters were updated with the objective function:\n\nwhere $\\hat{p}_i$ is the output of the model, $\\mathbf {T}^{p/s}_{pos}$ is the positive set and $\\mathbf {T}^{p/s}_{neg}$ is the negative set. As shown in Fig. FIGREF2, at sentence level, ground-truth sentences were served as positive examples while other sentences from upstream retrieved set were served as negative examples. Similarly at the paragraph-level, paragraphs having any ground-truth sentence were used as positive examples and other paragraphs from the upstream term-based retrieval processes were used as negative examples. \n Question: How do they train the retrieval modules?",
            "output": [
                "We treated the neural semantic retrieval at both the paragraph and sentence level as binary classification problems with models' parameters updated by minimizing binary cross entropy loss."
            ]
        },
        {
            "id": "task460-5c8fe0fe7a2543b1858c282ea7731a58",
            "input": "This layer consists of two submodules: the Multi-Head Attention (MHA) and the Point-wise Convolution Transformation (PCT). \n Question: Do they use multi-attention heads?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-73de263aa25f48519248dcb7c4388ad3",
            "input": "Word embedding models, and BERT in particular, contain vast amounts of information collected through the course of their training. BERT Base for instance, has 110 Million parameters and was trained on both Wikipedea Corpus and BooksCorpus BIBREF0, a combined collection of over 3 Billion words. \n Question: What is the language model pre-trained on?",
            "output": [
                "Wikipedea Corpus and BooksCorpus"
            ]
        },
        {
            "id": "task460-62d12693ed424527aa6f030a2d7803af",
            "input": "We carried out the experimentation with a range of classifiers of different types: Support Vector Machines (SVM), Gaussian Naive Bayes, Multinomial Naive Bayes, Decision Trees, Random Forests and a Maximum Entropy classifier. \n Question: What model do they train?",
            "output": [
                "Support Vector Machines (SVM), Gaussian Naive Bayes, Multinomial Naive Bayes, Decision Trees, Random Forests and a Maximum Entropy classifier"
            ]
        },
        {
            "id": "task460-0daf998942d644bbb62833e03f0f0f82",
            "input": "As we show below, STransE performs better than the SE and TransE models and other state-of-the-art link prediction models on two standard link prediction datasets WN18 and FB15k, so it can serve as a new baseline for KB completion. \n Question: What datasets are used to evaluate the model?",
            "output": [
                "WN18, FB15k"
            ]
        },
        {
            "id": "task460-abb17750d4b54d0e895412a4d2e5c48d",
            "input": "The experiment dataset comes from Microsoft Research (MSR) . It contains three domains: movie, taxi, and restaurant. The total count of dialogues per domain and train/valid/test split is reported in Table TABREF11. At every turn both user and agent acts are annotated, we use only the agent side as targets in our experiment. The acts are ordered in the dataset (each output sentence aligns with one act). \n Question: What datasets are used for training/testing models? ",
            "output": [
                "Microsoft Research dataset containing movie, taxi and restaurant domains."
            ]
        },
        {
            "id": "task460-2564c22d805a4fc2915ae4783d83a642",
            "input": "We compare our approach with two baseline vision-based methods proposed in BIBREF6 , BIBREF7 , which measure the similarity of two sets of global visual features for bilingual lexicon induction:\n\nCNN-mean: taking the similarity score of the averaged feature of the two image sets.\n\nCNN-avgmax: taking the average of the maximum similarity scores of two image sets. \n Question: Which vision-based approaches does this approach outperform?",
            "output": [
                "CNN-mean CNN-avgmax"
            ]
        },
        {
            "id": "task460-49849a1a2e0b4e678910361379f3742c",
            "input": "Multiple turns: The average number of utterances per dialog is about 23 which ensures context-rich language behaviors. \n Question: What is the average number of turns per dialog?",
            "output": [
                "The average number of utterances per dialog is about 23 "
            ]
        },
        {
            "id": "task460-9758c89e52c44aa4b0673d17bda088ac",
            "input": "For generating a poem from images we use an existing actor-critic architecture BIBREF1. For Shakespearizing modern English texts, we experimented with various types of sequence to sequence models. We use a sequence-to-sequence model which consists of a single layer unidrectional LSTM encoder and a single layer LSTM decoder and pre-trained retrofitted word embeddings shared between source and target sentences. Since a pair of corresponding Shakespeare and modern English sentences have significant vocabulary overlap we extend the sequence-to-sequence model mentioned above using pointer networks BIBREF11 that provide location based attention and have been used to enable copying of tokens directly from the input. \n Question: What models are used for painting embedding and what for language style transfer?",
            "output": [
                "generating a poem from images we use an existing actor-critic architecture various types of sequence to sequence models"
            ]
        },
        {
            "id": "task460-543b146151444d1e987eb4929a56cfa4",
            "input": "Conversation Based Features\nThese features are devoted to exploit the peculiar characteristics of the dataset, which have a tree structure reflecting the conversation thread.\n\nText Similarity to Source Tweet: Jaccard Similarity of each tweet with its source tweet.\n\nText Similarity to Replied Tweet: the degree of similarity between the tweet with the previous tweet in the thread (the tweet is a reply to that tweet).\n\nTweet Depth: the depth value is obtained by counting the node from sources (roots) to each tweet in their hierarchy. \n Question: What conversation-based features are used?",
            "output": [
                "Text Similarity to Source Tweet Text Similarity to Replied Tweet Tweet Depth"
            ]
        },
        {
            "id": "task460-59d141a9fc97410eb832f40e135e5808",
            "input": "Building Extractive CNN/Daily Mail \n Question: What is the problem with existing metrics that they are trying to address?",
            "output": [
                "Answer with content missing: (whole introduction) However, recent\nstudies observe the limits of ROUGE and find in\nsome cases, it fails to reach consensus with human.\njudgment (Paulus et al., 2017; Schluter, 2017)."
            ]
        },
        {
            "id": "task460-5df1301642934680b7c3e2340236e2cd",
            "input": "We use the guidelines of the state-of-the-art approach of BIBREF12 that achieved the highest inflection accuracy in the latest SIGMORPHON 2019 morphological inflection shared task. Our models are implemented in DyNet BIBREF13. We trained an encoder-decoder model that receives the form as character-level input, encodes it with a BiLSTM encoder, and then an attention enhanced decoder BIBREF14 outputs the corresponding sequence of morphological tags, implemented in DyNet. We trained a character level encoder-decoder model, similar to the above-mentioned inflection system, implemented in DyNet. \n Question: What system is used as baseline?",
            "output": [
                "DyNet"
            ]
        },
        {
            "id": "task460-4b0525696491484a8d13a36e3c07d7c3",
            "input": "Clustering was performed separately for each specialty of doctors. We also examined the distribution of doctors' IDs in the obtained clusters. It turned out that some clusters covered almost exactly descriptions written by one doctor. This situation took place in the specialties where clusters are separated with large margins (e.g. psychiatry, pediatrics, cardiology). \n Question: Do they explore similarity of texts across different doctors?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-5c17823d978a494fab74111f6f2d3cba",
            "input": " To be able to employ the trained model, test sets are first translated to English via machine translation and then inference takes place.   For machine translation, Google translation API is used. \n Question: how did the authors translate the reviews to other languages?",
            "output": [
                "Using Google translation API."
            ]
        },
        {
            "id": "task460-b5bbf05055a04cc3aaf7c5ac7b59959d",
            "input": "after a shared CLWE space has been induced, the task is to retrieve target language translations for a test set of source language words retrieve target language translations for a test set of source language words BLI is cast as a ranking task in our BLI setup with only one correct translation for each “query” word, MAP is equal to mean reciprocal rank (MRR) \n Question: How does BLI measure alignment quality?",
            "output": [
                "we use mean average precision (MAP) as the main evaluation metric"
            ]
        },
        {
            "id": "task460-09ec6ffb6bfa4a7aa971c489e1586bc1",
            "input": "We evaluate our approach for six target languages: French (fr), Russian (ru), Arabic (ar), Chinese (zh), Hindi (hi), and Vietnamese (vi).  \n Question: What languages are the model transferred to?",
            "output": [
                "French (fr), Russian (ru), Arabic (ar), Chinese (zh), Hindi (hi), and Vietnamese (vi)"
            ]
        },
        {
            "id": "task460-424bd469ec5d4bb8bc10221f9988e949",
            "input": "We assessed the proposed models on four different NLG domains: finding a restaurant, finding a hotel, buying a laptop, and buying a television. The Restaurant and Hotel were collected in BIBREF4 , while the Laptop and TV datasets have been released by BIBREF22 with a much larger input space but only one training example for each DA so that the system must learn partial realization of concepts and be able to recombine and apply them to unseen DAs.  \n Question: Does the model evaluated on NLG datasets or dialog datasets?",
            "output": [
                "NLG datasets"
            ]
        },
        {
            "id": "task460-cdb7c496a7fa41eaaf71a4c90e356d90",
            "input": "Python package twitterscraper is used to scrap tweets from twitter. 10,478 tweets from the past two years from domains like `sports', `politics', `entertainment' were extracted. \n Question: Where did the texts in the corpus come from?",
            "output": [
                "tweets from the past two years from domains like `sports', `politics', `entertainment'"
            ]
        },
        {
            "id": "task460-909483d48c72419a9d55d1d91c0623fb",
            "input": "Table TABREF31 shows the F-score, the Area Under the ROC Curve (AUC) and the mean accuracy (ACC) of the proposed models when different inputs are available. Fig. FIGREF32 shows the Precision vs Recall plot and the ROC curve (which plots the True Positive Rate vs the False Positive Rate) of the different models. \n Question: What metrics are used to benchmark the results?",
            "output": [
                "F-score Area Under the ROC Curve (AUC) mean accuracy (ACC) Precision vs Recall plot ROC curve (which plots the True Positive Rate vs the False Positive Rate)"
            ]
        },
        {
            "id": "task460-b2eea82be0b84f6f8281795f0e8d2e9d",
            "input": "The D2V model has been rated 80 times as \"bad relevance\" while the pmra returned only 24 times badly relevant documents.  \n Question: How better are results for pmra algorithm  than Doc2Vec in human evaluation? ",
            "output": [
                "The D2V model has been rated 80 times as \"bad relevance\" while the pmra returned only 24 times badly relevant documents."
            ]
        },
        {
            "id": "task460-fa51f1504bec4936afc1d6287f9c96e9",
            "input": "One common point in all the approaches yet has been the use of only textual features available in the dataset. Our model not only incorporates textual features, modeled using BiLSTM and augmented with an attention mechanism, but also considers related images for the task. \n Question: What are the differences with previous applications of neural networks for this task?",
            "output": [
                "This approach considers related images"
            ]
        },
        {
            "id": "task460-02faee05cd324194b02af7bcb987f335",
            "input": "Fenerbahçe We have decided to consider tweets about popular sports clubs as our domain for stance detection.  Hence we have determined our targets as Galatasaray (namely Target-1) and Fenerbahçe (namely, Target-2) which are two of the most popular football clubs in Turkey.  Then, we have annotated the stance information in the tweets for these targets as Favor or Against. For the purposes of the current study, we have not annotated any tweets with the Neither class. \n Question: How were the tweets annotated?",
            "output": [
                "tweets are annotated with only Favor or Against for two targets - Galatasaray and Fenerbahçe"
            ]
        },
        {
            "id": "task460-21487bec2bb04fa98467c4cb286bba3c",
            "input": "The Machine Learning techniques used varied from Maximum Entropy Classifiers (BIBREF4) to Support Vector Machines (BIBREF5,BIBREF6,BIBREF7,BIBREF8), while the deep learning approaches included Recursive Neural Networks (BIBREF9,BIBREF10), Convolutional Neural Networks (BIBREF11) and most recently transfer learning-based architectures like Bidirectional Encoder Representation from Transformers (BERT) (BIBREF12). Figures FIGREF1 and FIGREF1 contain a summary of the papers addressing speculation detection and scope resolution (BIBREF13, BIBREF5, BIBREF9, BIBREF3, BIBREF14, BIBREF15, BIBREF16, BIBREF17, BIBREF6, BIBREF11, BIBREF18, BIBREF10, BIBREF19, BIBREF7, BIBREF4, BIBREF8). \n Question: What were the baselines?",
            "output": [
                "varied from Maximum Entropy Classifiers (BIBREF4) to Support Vector Machines (BIBREF5,BIBREF6,BIBREF7,BIBREF8) Recursive Neural Networks (BIBREF9,BIBREF10), Convolutional Neural Networks (BIBREF11) and most recently transfer learning-based architectures like Bidirectional Encoder Representation from Transformers (BERT) (BIBREF12)"
            ]
        },
        {
            "id": "task460-6cb2b323155848d287e0ba2b7ce94c39",
            "input": "We have used a dataset of 14 TDs to conduct our experiments. \n Question: Which dataset(s) do they use?",
            "output": [
                "14 TDs BIBREF15"
            ]
        },
        {
            "id": "task460-ce30fd4e3c584fddabef8abfbde56d06",
            "input": "After removing 135 questions during the validation, the final dataset comprises 13,939 questions, 3,827 of which require commonsense knowledge (i.e. 27.4%).  \n Question: what is the size of their dataset?",
            "output": [
                "13,939"
            ]
        },
        {
            "id": "task460-4a35f1936b324018a021790484427775",
            "input": "Concerning logical inferences, monotonicity reasoning BIBREF6 , BIBREF7 , which is a type of reasoning based on word replacement, requires the ability to capture the interaction between lexical and syntactic structures. \n Question: What is monotonicity reasoning?",
            "output": [
                "a type of reasoning based on word replacement, requires the ability to capture the interaction between lexical and syntactic structures"
            ]
        },
        {
            "id": "task460-3079848f32ea472ebac26702bbbb1c34",
            "input": " The overall performance of BERT was significantly better than the other models, having the lowest average difference in accuracy of 22.5 points. \n Question: Which model generalized the best?",
            "output": [
                "BERT"
            ]
        },
        {
            "id": "task460-df36bdfbfda34356909e678c3ef12e56",
            "input": "Based on the resulting 1.8 million lists of about 169,000 distinct user ids, we compute a topic model with INLINEFORM0 topics using Latent Dirichlet Allocation BIBREF3 . For each of the user ids, we extract the most probable topic from the inferred user id-topic distribution as cluster id. This results in a thematic cluster id for most of the user ids in our background corpus grouping together accounts such as American or German political actors, musicians, media websites or sports clubs (see Table TABREF17 ).  \n Question: What topic clusters are identified by LDA?",
            "output": [
                "Clusters of Twitter user ids from accounts of American or German political actors, musicians, media websites or sports club"
            ]
        },
        {
            "id": "task460-971405467ead4e3f943e4e36a1fbf40e",
            "input": " BIBREF3 also use bilingual crowd workers, but the studies supporting the use of crowdsourcing for MT evaluation were performed with older MT systems, and their findings may not carry over to the evaluation of contemporary higher-quality neural machine translation (NMT) systems. In addition, the MT developers to which crowd workers were compared are usually not professional translators.  We hypothesise that an evaluation of sentences in isolation, as applied by BIBREF3, precludes raters from detecting translation errors that become apparent only when inter-sentential context is available, and that they will judge MT quality less favourably when evaluating full documents. BIBREF3 used all source texts of the WMT 2017 Chinese–English test set for their experiments, of which only half were originally written in Chinese;  \n Question: What was the weakness in Hassan et al's evaluation design?",
            "output": [
                "MT developers to which crowd workers were compared are usually not professional translators, evaluation of sentences in isolation prevents raters from detecting translation errors, used not originally written Chinese test set\n"
            ]
        },
        {
            "id": "task460-311fe211c6f54058ba5b74f73ff1e4f0",
            "input": "In all our experiments, we used the out-of-the-box BERT models without any task-specific fine-tuning. \n Question: How does their model differ from BERT?",
            "output": [
                "Their model does not differ from BERT."
            ]
        },
        {
            "id": "task460-4b058674845b45c1b82a9e63cda6dbfd",
            "input": "We collected tweets associated to a dozen US mainstream news websites, i.e. most trusted sources described in BIBREF18, with the Streaming API, and we referred to Hoaxy API BIBREF16 for what concerns tweets containing links to 100+ US disinformation outlets. We filtered out articles associated to less than 50 tweets. The resulting dataset contains overall $\\sim $1.7 million tweets for mainstream news, collected in a period of three weeks (February 25th, 2019-March 18th, 2019), which are associated to 6,978 news articles, and $\\sim $1.6 million tweets for disinformation, collected in a period of three months (January 1st, 2019-March 18th, 2019) for sake of balance of the two classes, which hold 5,775 distinct articles. Diffusion censoring effects BIBREF14 were correctly taken into account in both collection procedures. We provide in Figure FIGREF4 the distribution of articles by source and political bias for both news domains. For what concerns the Italian scenario we first collected tweets with the Streaming API in a 3-week period (April 19th, 2019-May 5th, 2019), filtering those containing URLs pointing to Italian official newspapers websites as described in BIBREF22; these correspond to the list provided by the association for the verification of newspaper circulation in Italy (Accertamenti Diffusione Stampa). We instead referred to the dataset provided by BIBREF23 to obtain a set of tweets, collected continuously since January 2019 using the same Twitter endpoint, which contain URLs to 60+ Italian disinformation websites. In order to get balanced classes (April 5th, 2019-May 5th, 2019), we retained data collected in a longer period w.r.t to mainstream news. In both cases we filtered out articles with less than 50 tweets; overall this dataset contains $\\sim $160k mainstream tweets, corresponding to 227 news articles, and $\\sim $100k disinformation tweets, corresponding to 237 news articles. We provide in Figure FIGREF5 the distribution of articles according to distinct sources for both news domains.  \n Question: Which two news domains are country-independent?",
            "output": [
                "mainstream news and disinformation"
            ]
        },
        {
            "id": "task460-09f1500b4763425cbedc07e7d8a8e1e7",
            "input": "For evaluation, the Visual Dialog task employs four metrics. NDCG is the primary metric of the Visual Dialog Challenge which considers multiple similar answers as correct ones. The other three are MRR, recall@k, and mean rank where they only consider the rank of a single answer. \n Question: What metrics are used in challenge?",
            "output": [
                "NDCG MRR recall@k mean rank"
            ]
        },
        {
            "id": "task460-757a686273f24d8b86cc88e6d61d7427",
            "input": "For instance, we observed that winter words were together and far away from summer ones. Week days were grouped as well and far from week-end days. \n Question: What geometric properties do embeddings display?",
            "output": [
                "Winter and summer words formed two separate clusters. Week day and week-end day words also formed separate clusters."
            ]
        },
        {
            "id": "task460-7abb3c9b3f1c4604848dc3b2148c67ed",
            "input": "Three different datasets have been used to train our models: the Toronto book corpus, Wikipedia sentences and tweets.  Our Sent2Vec models also on average outperform or are at par with the C-PHRASE model, despite significantly lagging behind on the STS 2014 WordNet and News subtasks. This observation can be attributed to the fact that a big chunk of the data that the C-PHRASE model is trained on comes from English Wikipedia, helping it to perform well on datasets involving definition and news items.  \n Question: Do they report results only on English data?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-69f4063673fc4cfd83bf55efae788bde",
            "input": "There have been many taxonomies for dialogue acts: speech acts BIBREF14 refer to the utterance, not only to present information but to the action at is performed. Speech acts were later modified into five classes (Assertive, Directive, Commissive, Expressive, Declarative) BIBREF15. There are many such standard taxonomies and schemes to annotate conversational data, and most of them follow the discourse compositionality. These schemes have proven their importance for discourse or conversational analysis BIBREF16. During the increased development of dialogue systems and discourse analysis, the standard taxonomy was introduced in recent decades, called Dialogue Act Markup in Several Layers (DAMSL) tag set. According to DAMSL, each DA has a forward-looking function (such as Statement, Info-request, Thanking) and a backwards-looking function (such as Accept, Reject, Answer) BIBREF17. \n Question: How were dialogue act labels defined?",
            "output": [
                "Dialogue Act Markup in Several Layers (DAMSL) tag set"
            ]
        },
        {
            "id": "task460-1636644a9dac4207ae7821a703010287",
            "input": "For a word belonging to any one of the word-groups representing these concepts, the modified cost term favors an increase for the value of this word's embedding vector dimension corresponding to the concept that the particular word belongs to.  \n Question: Along which dimension do the semantically related words take larger values?",
            "output": [
                "dimension corresponding to the concept that the particular word belongs to"
            ]
        },
        {
            "id": "task460-6fcd4829f3204e23927cf7db77d8b4fb",
            "input": " We then train a paragraph vector model using the Document to Vector (Doc2Vec) framework BIBREF7 on the whole set (13 million) of preprocessed text records, although training on smaller sets (1 million) also produces good results. \n Question: Which text embedding methodologies are used?",
            "output": [
                "Document to Vector (Doc2Vec)"
            ]
        },
        {
            "id": "task460-1833faf6e6f749e88720d7b827901ed5",
            "input": "To develop a generalizable model that avoids overfitting, we perform feature selection using statistical tests and all relevant ensemble learning models. It adds randomness to the data by creating shuffled copies of all features (shadow feature), and then trains Random Forest classifier on the extended data. \n Question: What model is used to achieve 5% improvement on F1 for identifying depressed individuals on Twitter?",
            "output": [
                "Random Forest classifier"
            ]
        },
        {
            "id": "task460-aad53091a27149bb8ce478e19c7074cd",
            "input": "Two-talker overlapped speech is artificially generated by mixing these waveform segments. To maximize the speech overlap, we developed a procedure to mix similarly sized segments at around 0dB. First, we sort the speech segments by length. Then, we take segments in pairs, zero-padding the shorter segment so both have the same length. These pairs are then mixed together to create the overlapped speech data. \n Question: How are the two datasets artificially overlapped?",
            "output": [
                "we sort the speech segments by length we take segments in pairs, zero-padding the shorter segment so both have the same length These pairs are then mixed together"
            ]
        },
        {
            "id": "task460-aac849aab33e4afdaf9b201ccf833900",
            "input": "The BD-4SK-ASR Dataset ::: The Language Model\nWe created the language from the transcriptions. The model was created using CMUSphinx in which (fixed) discount mass is 0.5, and backoffs are computed using the ratio method. The model includes 283 unigrams, 5337 bigrams, and 6935 trigrams. \n Question: What are the results of the experiment?",
            "output": [
                "They were able to create a language model from the dataset, but did not test."
            ]
        },
        {
            "id": "task460-2aa58a33d284434a953a7dc5bd5a2e77",
            "input": "To learn better graph representation of multi-party dialogues, we adopt the dialogues with 8-15 utterances and 3-7 speakers. To simplify the task, we filter the dialogues with long sentences (more than 20 words). Finally, we obtain 52,053 dialogues and 460,358 utterances. \n Question: How large is the proposed dataset?",
            "output": [
                "we obtain 52,053 dialogues and 460,358 utterances"
            ]
        },
        {
            "id": "task460-4119c5b6feb546fe9b10225472c32fe5",
            "input": "The Swissmetro dataset consists of survey data collected on the trains between St. Gallen and Geneva, Switzerland, during March 1998.  \n Question: What datasets are used for evaluation?",
            "output": [
                "Swissmetro dataset"
            ]
        },
        {
            "id": "task460-1ec93c41443848aeba5845af5196b23b",
            "input": ". A highly dynamic community constantly shifts interests from one time window to another, and these temporal variations are reflected in its use of volatile language. Formally, we define the dynamicity of a community INLINEFORM0 as the average volatility of all utterances in INLINEFORM1 .  \n Question: How do the authors measure how temporally dynamic a community is?",
            "output": [
                "the average volatility of all utterances"
            ]
        },
        {
            "id": "task460-ba33c1d512ad457e88ed23d85312bb92",
            "input": "To investigate the extent to which the text obtained from the two platforms of Yahoo! Answers and Twitter reflect the true attributes of neighbourhoods, we first study whether there are significant, strong and meaningful correlations between the terms present in each corpus and the many neighbourhood attributes through the Pearson correlation coefficient $\\rho $ . \n Question: What do the correlation demonstrate? ",
            "output": [
                "the extent to which the text obtained from the two platforms of Yahoo! Answers and Twitter reflect the true attributes of neighbourhoods"
            ]
        },
        {
            "id": "task460-4b620bd2ebc9434f8a7744b1350e6ec5",
            "input": " In this work, we have restricted ourselves to the same datasets as BIBREF7 . These include nine (real-valued) numerical features, which are latitude, longitude, elevation, population, and five climate related features (avg. temperature, avg. precipitation, avg. solar radiation, avg. wind speed, and avg. water vapor pressure). In addition, 180 categorical features were used, which are CORINE land cover classes at level 1 (5 classes), level 2 (15 classes) and level 3 (44 classes) and 116 soil types (SoilGrids). Note that each location should belong to exactly 4 categories: one CORINE class at each of the three levels and a soil type. \n Question: what dataset is used in this paper?",
            "output": [
                " the same datasets as BIBREF7"
            ]
        },
        {
            "id": "task460-791c846071d7407295ee90ca44617313",
            "input": "As a warm-up procedure, the first 100 posts were annotated by two annotators (the author and the supervisor) and the results compared. The remainder of the annotation task was performed by the author, resulting in 3600 annotated samples. \n Question: Who were the annotators?",
            "output": [
                "the author and the supervisor"
            ]
        },
        {
            "id": "task460-a75850e6200b48c8ac9809e3f1c27c5d",
            "input": "For all the classifiers, our feature combination outperforms the baselines (considering only unigram features) as well as BIBREF3 , with the MILR classifier getting an F-score improvement of 3.7% and Kappa difference of 0.08. We also achieve an improvement of 2% over the baseline, using SVM classifier, when we employ our feature set. We also observe that the gaze features alone, also capture the differences between sarcasm and non-sarcasm classes with a high-precision but a low recall. \n Question: What other evaluation metrics are looked at?",
            "output": [
                "F-score Kappa"
            ]
        },
        {
            "id": "task460-80dbcaa114674ecd9e62ee126e8b49a5",
            "input": "CFQ contains 239,357 English question-answer pairs that are answerable using the public Freebase data. \n Question: How big is new question answering dataset?",
            "output": [
                "239,357 English question-answer pairs"
            ]
        },
        {
            "id": "task460-7d34a88df4d441ef8edfa42341f7c6f0",
            "input": "We have collected total 3500 questions from the Internet and other sources such as books of general knowledge questions, history etc. The corpus contains the questions and the classes each question belongs to. \n Question: what datasets did they use?",
            "output": [
                "Dataset of total 3500 questions from the Internet and other sources such as books of general knowledge questions, history, etc."
            ]
        },
        {
            "id": "task460-20de68d0b3714f2caeb405567a9972e9",
            "input": "The clinical notes we used for the experiment are provided by domain experts, consisting of 1,160 physician logs of Medical ICU admission requests at a tertiary care center affiliated to Mount Sanai. The enriched corpus contains 42,506 Wikipedia articles, each of which corresponds to one candidate, 6 research papers and 2 critical care medicine textbooks, besides our raw ICU data. \n Question: Which dataset do they use to build their model?",
            "output": [
                "1,160 physician logs of Medical ICU admission requests 42,506 Wikipedia articles 6 research papers and 2 critical care medicine textbooks"
            ]
        },
        {
            "id": "task460-70e29943604a4b489495113c57d59d72",
            "input": "For English dataset, we crawl 687 cybersecurity articles from a collection of advanced persistent threats (APT) reports which are published from 2008 to 2018.  \n Question: Where are the cybersecurity articles used in the model sourced from?",
            "output": [
                " from a collection of advanced persistent threats (APT) reports which are published from 2008 to 2018"
            ]
        },
        {
            "id": "task460-29d14339d80a40fbb3e39d5b0b4fd958",
            "input": "Table TABREF32 shows the perplexity of the models on each of the test sets. As expected, each model performs better on the test set they have been trained on. When applied to a different test set, both see an increase in perplexity. However, the Leipzig model seems to have more trouble generalizing: its perplexity nearly doubles on the SwissCrawl test set and raises by twenty on the combined test set. \n Question: How is language modelling evaluated?",
            "output": [
                "perplexity of the models"
            ]
        },
        {
            "id": "task460-3161f48fa13b4eed93619eb550a1a775",
            "input": "The results indicate that most performance gains come from words embeddings, style, and morality features. Other features (emotion and sentiment) show lower importance: nevertheless, they still improve the overall system performance (on average 0.35% Macro-F$_1$ improvement) \n Question: Based on this paper, what is the more predictive set of features to detect fake news?",
            "output": [
                "words embeddings, style, and morality features"
            ]
        },
        {
            "id": "task460-77dd413d82f54083a617c1002c14a524",
            "input": "In order to verify the effectiveness of our method (i.e., Multi-linear attention) replacing multi-head attention in Transformer, we carry out two NLP tasks named language modeling (LM) and neural machine translation (NMT). Then, we test different model configurations on the PTB BIBREF25 , WikiText-103 BIBREF26 and One-Billion Word benchmark BIBREF27 datasets and report the results in Table 1 and Table 2 . In this task, we have trained the Transformer model BIBREF2 on WMT 2016 English-German dataset BIBREF36 . \n Question: What datasets or tasks do they conduct experiments on?",
            "output": [
                "Language Modeling (LM) PTB BIBREF25 , WikiText-103 BIBREF26 and One-Billion Word benchmark BIBREF27 datasets neural machine translation (NMT) WMT 2016 English-German dataset"
            ]
        },
        {
            "id": "task460-558d8f86dbe44881a66e8b57d6d41e0a",
            "input": "To understand the reason behind the drop in performance, first we need to understand how BERT processes input text data. BERT uses WordPiece tokenizer to tokenize the text. WordPiece tokenizer utterances based on the longest prefix matching algorithm to generate tokens . The tokens thus obtained are fed as input of the BERT model. When it comes to tokenizing noisy data, we see a very interesting behaviour from WordPiece tokenizer. Owing to the spelling mistakes, these words are not directly found in BERT’s dictionary. Hence WordPiece tokenizer tokenizes noisy words into subwords. However, it ends up breaking them into subwords whose meaning can be very different from the meaning of the original word. Often, this changes the meaning of the sentence completely, therefore leading to substantial dip in the performance. \n Question: What is the reason behind the drop in performance using BERT for some popular task?",
            "output": [
                "Hence WordPiece tokenizer tokenizes noisy words into subwords. However, it ends up breaking them into subwords whose meaning can be very different from the meaning of the original word. Often, this changes the meaning of the sentence completely, therefore leading to substantial dip in the performance."
            ]
        },
        {
            "id": "task460-76f468489ddd45229cc406862072798e",
            "input": "We evaluate our proposed model on the SNAP (Stanford Network Analysis Project) Amazon review datasets BIBREF8, which contain not only reviews and ratings, but also golden summaries. We empirically compare different methods using Amazon SNAP Review Dataset BIBREF20, which is a part of Stanford Network Analysis Project. For fair comparison with previous work, we adopt the same partitions used by previous work BIBREF6, BIBREF7, which is, for each domain, the first 1000 samples are taken as the development set, the following 1000 samples as the test set, and the rest as the training set. \n Question: Which review dataset do they use?",
            "output": [
                "SNAP (Stanford Network Analysis Project)"
            ]
        },
        {
            "id": "task460-85a3927ab574435282e15a287bc810c8",
            "input": "To create minimal pairs exemplifying a wide array of linguistic contrasts, it is necessary to artificially generate all datasets. This ensures both that we have sufficient unacceptable examples, and that the data is fully controlled, allowing for repeated isolation of a single linguistic phenomenon in each paradigm BIBREF30. The data generation scripts use a basic template to create each paradigm, pulling from a vocabulary of over 3000 words annotated for morphological, syntactic, and semantic features needed to create grammatical and semantically felicitous sentences. Examples SECREF6 and SECREF6 show one such template for the `acceptable' and `unacceptable' sentences within a pair: the sole difference between them is the underlined word, which differs only in whether the anaphor agrees in number with its antecedent. Our generation codebase and scripts are freely available. This generation procedure is not without limitations, and despite the very detailed vocabulary we use, implausible sentences are occasionally generated (e.g., `Sam ran around some glaciers'). In these cases, though, both the acceptable and unacceptable sentences will be equally implausible given world knowledge, so any difference in the probability assigned to them is still due to the intended grammatical contrast. \n Question: How is the data automatically generated?",
            "output": [
                " The data generation scripts use a basic template to create each paradigm, pulling from a vocabulary of over 3000 words annotated for morphological, syntactic, and semantic features needed to create grammatical and semantically felicitous sentences."
            ]
        },
        {
            "id": "task460-4af2851e853d460fb34d1f4a2af66018",
            "input": "The term `propaganda' derives from propagare in post-classical Latin, as in “propagation of the faith\" BIBREF1, and thus has from the beginning been associated with an intentional and potentially multicast communication; only later did it become a pejorative term. It was pragmatically defined in the World War II era as “the expression of an opinion or an action by individuals or groups deliberately designed to influence the opinions or the actions of other individuals or groups with reference to predetermined ends\" BIBREF2. \n Question: How is \"propaganda\" defined for the purposes of this study?",
            "output": [
                "an intentional and potentially multicast communication “the expression of an opinion or an action by individuals or groups deliberately designed to influence the opinions or the actions of other individuals or groups with reference to predetermined ends\""
            ]
        },
        {
            "id": "task460-5be54984dd6b42fdbca84dfd4e3fb79a",
            "input": "Also, note that the performance for this task is not expected to achieve a perfect accuracy, as there may be situations where more than one action is reasonable, and also because writers tell a story playing with elements such as surprise or uncertainty. \n Question: Why do they think this task is hard?  What is the baseline performance?",
            "output": [
                "1. there may be situations where more than one action is reasonable, and also because writers tell a story playing with elements such as surprise or uncertainty.\n2. Macro F1 = 14.6 (MLR, length 96 snippet)\nWeighted F1 = 31.1 (LSTM, length 128 snippet)"
            ]
        },
        {
            "id": "task460-4569adbd9a724e4cb64edadb231b9300",
            "input": "In future work we also intend to add these types of studies to the ERP predictions.\n\nDiscussion \n Question: Which two pairs of ERPs from the literature benefit from joint training?",
            "output": [
                "Answer with content missing: (Whole Method and Results sections) Self-paced reading times widely benefit ERP prediction, while eye-tracking data seems to have more limited benefit to just the ELAN, LAN, and PNP ERP components.\nSelect:\n- ELAN, LAN\n- PNP ERP"
            ]
        },
        {
            "id": "task460-a60b641b2f7443739da2c81176cfc927",
            "input": "Nonetheless, the main caveat of this basic pre-training is that the source encoder is trained to be used by an English decoder, while the target decoder is trained to use the outputs of an English encoder — not of a source encoder. \n Question: Is pivot language used in experiments English or some other language?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-86ece9b2a00a4720a419287acd3964fe",
            "input": "We also conduct two human evaluations in order to assess (a) which type of summary participants prefer (we compare extractive and abstractive systems) and (b) how much key information from the document is preserved in the summary (we ask participants to answer questions pertaining to the content in the document by reading system summaries). \n Question: Do they use other evaluation metrics besides ROUGE?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-da54478db9b347328f58cfc0d4928dd3",
            "input": "We used the Twitter definition of hateful conduct in the first survey. This definition was presented at the beginning, and again above every tweet. \n Question: What definition was one of the groups was shown?",
            "output": [
                "Twitter definition of hateful conduct"
            ]
        },
        {
            "id": "task460-07f59a38e1e14d4eb3cc52c3448c3cf3",
            "input": "One example illustrates that the mistakes the model makes can be associated with changes in the party policy. \n Question: Do changes in policies of the political actors account for all of the mistakes the model made?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-9d5a75b4f3d947fdb51869e0924ad668",
            "input": "In our paper, we opt for a state-of-the-art character bigram CNN classifier BIBREF4 , and investigate various ways in which the discourse information can be featurized and integrated into the CNN.  \n Question: What was the previous state-of-the-art?",
            "output": [
                "character bigram CNN classifier"
            ]
        },
        {
            "id": "task460-10a651c9974a4bc2bb5ad01d081cf9d0",
            "input": "This kind of error affects the precision and recall for the “troll” and “engage” classes. A solution to this problem may be the inclusion of longer parts of the conversation.  \n Question: What potential solutions are suggested?",
            "output": [
                " inclusion of longer parts of the conversation"
            ]
        },
        {
            "id": "task460-7fbaadbd247041f0a7b1d1eface6c31f",
            "input": "We view different varieties of Arabic as different domains, and hence introduce a simple, yet effective, `in-domain' training measure where we further pre-train BERT on a dataset closer to task domain (in that it involves dialectal tweet data). \n Question: What in-domain data is used to continue pre-training?",
            "output": [
                "dialectal tweet data"
            ]
        },
        {
            "id": "task460-d5035147a77746de98570173ba8698b2",
            "input": "As far as we know, all existing systems treat this task as a pipeline of two separate subtasks, i.e., event extraction and temporal relation classification, and they also assume that gold events are given when training the relation classifier BIBREF0, BIBREF1, BIBREF2, BIBREF3, BIBREF4, BIBREF5. Specifically, they built end-to-end systems that extract events first and then predict temporal relations between them (Fig. FIGREF1). In these pipeline models, event extraction errors will propagate to the relation classification step and cannot be corrected afterwards. Our first contribution is the proposal of a joint model that extracts both events and temporal relations simultaneously (see Fig. FIGREF1). \n Question: Is this the first paper to propose a joint model for event and temporal relation extraction?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-ba6e97ccbe3b4988a3545ed16ee875c9",
            "input": "Each classifier is implemented with the following specifications:\n\nNaïve Bayes (NB): Multinomial NB with additive smoothing constant 1\n\nLogistic Regression (LR): Linear LR with L2 regularization constant 1 and limited-memory BFGS optimization\n\nSupport Vector Machine (SVM): Linear SVM with L2 regularization constant 1 and logistic loss function\n\nRandom Forests (RF): Averaging probabilistic predictions of 10 randomized decision trees\n\nGradient Boosted Trees (GBT): Tree boosting with learning rate 1 and logistic loss function Along with traditional machine learning approaches, we investigate neural network based models to evaluate their efficacy within a larger dataset. In particular, we explore Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), and their variant models. \n Question: What learning models are used on the dataset?",
            "output": [
                "Naïve Bayes (NB) Logistic Regression (LR) Support Vector Machine (SVM) Random Forests (RF) Gradient Boosted Trees (GBT)  Convolutional Neural Networks (CNN) Recurrent Neural Networks (RNN)"
            ]
        },
        {
            "id": "task460-02242ea892894f2bbff724784402f47a",
            "input": "We use the public financial news dataset released by BIBREF4, which is crawled from Reuters and Bloomberg over the period from October 2006 to November 2013. We conduct our experiments on predicting the Standard & Poor’s 500 stock (S&P 500) index and its selected individual stocks, obtaining indices and prices from Yahoo Finance. Detailed statistics of the training, development and test sets are shown in Table TABREF8. We report the final results on test set after using development set to tune some hyper-parameters. \n Question: What is dataset used for news-driven stock movement prediction?",
            "output": [
                "the public financial news dataset released by BIBREF4"
            ]
        },
        {
            "id": "task460-eefc9418f059484e8131f5a8a00b581e",
            "input": "Abstract meaning representation BIBREF0 , or AMR for short, allows us to do that with the inclusion of most of the shallow-semantic natural language processing (NLP) tasks that are usually addressed separately, such as named entity recognition, semantic role labeling and co-reference resolution. \n Question: Which subtasks do they evaluate on?",
            "output": [
                " entity recognition, semantic role labeling and co-reference resolution"
            ]
        },
        {
            "id": "task460-3e9248bb826949a7a600989c7192692c",
            "input": " Each passage was labelled by 3 unique annotators. \n Question: How many annotators were there?",
            "output": [
                "3 "
            ]
        },
        {
            "id": "task460-ada0a6bc6c834568a132d81c7511d78c",
            "input": "In addition to the majority baseline, we also compare our results with a lexicon-based approach. For experimental results, we report majority baseline for each language where the majority baseline corresponds to a model's accuracy if it always predicts the majority class in the dataset. \n Question: what are the baselines?",
            "output": [
                "majority baseline lexicon-based approach"
            ]
        },
        {
            "id": "task460-ad30568ede6a419782ed683968b11c6a",
            "input": " In contrast to most encoder-decoder architectures, which are purely based on recurrent neural network (RNNs) framework, we construct an encoder with several convolutional layers BIBREF14 followed by NIN layers BIBREF15 as the lower part in the encoder and integrate them with deep bidirectional long short-term memory (Bi-LSTM) BIBREF16 at the higher part. On the decoder side, we use a standard deep unidirectional LSTM with global attention BIBREF13 that is calculated by a multi-layer perceptron (MLP) as described in Eq. EQREF2 . \n Question: Which architecture do they use for the encoder and decoder?",
            "output": [
                "we construct an encoder with several convolutional layers BIBREF14 followed by NIN layers BIBREF15 as the lower part in the encoder and integrate them with deep bidirectional long short-term memory (Bi-LSTM) BIBREF16 at the higher part On the decoder side, we use a standard deep unidirectional LSTM with global attention BIBREF13 that is calculated by a multi-layer perceptron (MLP)"
            ]
        },
        {
            "id": "task460-8b130574803d41d5a3f9ce783f06548f",
            "input": "We follow the same scheme as BIBREF47 to annotate the MT outputs with coreference chains. This scheme allows the annotator to define each markable as a certain mention type (pronoun, NP, VP or clause). The mentions can be defined further in terms of their cohesive function (antecedent, anaphoric, cataphoric, comparative, substitution, ellipsis, apposition). Antecedents can either be marked as simple or split or as entity or event. The annotation scheme also includes pronoun type (personal, possessive, demonstrative, reflexive, relative) and modifier types of NPs (possessive, demonstrative, definite article, or none for proper names), see BIBREF46 for details. The mentions referring to the same discourse item are linked between each other. We use the annotation tool MMAX2 BIBREF48 which was also used for the annotation of ParCorFull. In the next step, chain members are annotated for their correctness. For the incorrect translations of mentions, we include the following error categories: gender, number, case, ambiguous and other. The latter category is open, which means that the annotators can add their own error types during the annotation process. With this, the final typology of errors also considered wrong named entity, wrong word, missing word, wrong syntactic structure, spelling error and addressee reference. \n Question: How are the (possibly incorrect) coreference chains in the MT outputs annotated?",
            "output": [
                "allows the annotator to define each markable as a certain mention type (pronoun, NP, VP or clause) The mentions referring to the same discourse item are linked between each other. chain members are annotated for their correctness"
            ]
        },
        {
            "id": "task460-9c372c277b2c4344acba6273d6f09630",
            "input": "The quality class of a Wikipedia article is assigned by Wikipedia reviewers or any registered user, who can discuss through the article's talk page to reach consensus. The arXiv dataset BIBREF2 consists of three subsets of academic articles under the arXiv repository of Computer Science (cs), from the three subject areas of: Artificial Intelligence (cs.ai), Computation and Language (cs.cl), and Machine Learning (cs.lg). In line with the original dataset formulation BIBREF2 , a paper is considered to have been accepted (i.e. is positively labeled) if it matches a paper in the DBLP database or is otherwise accepted by any of the following conferences: ACL, EMNLP, NAACL, EACL, TACL, NIPS, ICML, ICLR, or AAAI. Failing this, it is considered to be rejected (noting that some of the papers may not have been submitted to one of these conferences). \n Question: Where do they get their ground truth quality judgments?",
            "output": [
                "Wikipedia articles are labelled with one of six quality classes, in descending order of quality: Featured Article (“FA”), Good Article (“GA”), B-class Article (“B”), C-class Article (“C”), Start Article (“Start”), and Stub Article (“Stub”). The quality class of a Wikipedia article is assigned by Wikipedia reviewers or any registered user, who can discuss through the article's talk page to reach consensus. The arXiv dataset BIBREF2 consists of three subsets of academic articles under the arXiv repository of Computer Science (cs), from the three subject areas of: Artificial Intelligence (cs.ai), Computation and Language (cs.cl), and Machine Learning (cs.lg). In line with the original dataset formulation BIBREF2 , a paper is considered to have been accepted (i.e. is positively labeled) if it matches a paper in the DBLP database or is otherwise accepted by any of the following conferences: ACL, EMNLP, NAACL, EACL, TACL, NIPS, ICML, ICLR, or AAAI. Failing this, it is considered to be rejected (noting that some of the papers may not have been submitted to one of these conferences). "
            ]
        },
        {
            "id": "task460-a93b0e044df4466e9d235bcf0baf9686",
            "input": "For both POS tagging and dependency parsing, we run experiments on the Wall Street Journal (WSJ) portion of the Penn Treebank. \n Question: What datasets do they evaluate on?",
            "output": [
                " Wall Street Journal (WSJ) portion of the Penn Treebank"
            ]
        },
        {
            "id": "task460-8bc90dc31307447988b869b7f6be1e12",
            "input": "We make copies of the monolingual model for each language and add additional crosslingual latent variables (CLVs) to couple the monolingual models, capturing crosslingual semantic role patterns. Concretely, when training on parallel sentences, whenever the head words of the arguments are aligned, we add a CLV as a parent of the two corresponding role variables. \n Question: Do they add one latent variable for each language pair in their Bayesian model?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-ea37b346769a478983e6303bbef38b95",
            "input": "One of the major drawbacks of SCRF models is their high computational cost. In our experiments, the CTC model is around 3–4 times faster than the SRNN model that uses the same RNN encoder. To cut down the computational cost, we investigated if CTC can be used to pretrain the RNN encoder to speed up the training of the joint model. Figure 2 shows the convergence curves of the joint model with and without CTC pretraining, and we see pretraining indeed improves the convergence speed of the joint model. \n Question: Can SCRF be used to pretrain the model?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-a3b08937c86c492dadd0b0f89cf4a791",
            "input": "Given a proof state $= (_, _)$ , where $_$ and $_$ denote a substitution set and a proof score, respectively, unification is computed as follows: The resulting proof score of $g$ is given by:\n\n$$ \\begin{aligned} \\max _{f \\in \\mathcal {K}} & \\; {unify}_(g, [f_{p}, f_{s}, f_{o}], (\\emptyset , )) \\\\ & = \\max _{f \\in \\mathcal {K}} \\; \\min \\big \\lbrace , \\operatorname{k}(_{\\scriptsize {grandpaOf}:}, _{f_{p}:}),\\\\ &\\qquad \\qquad \\qquad \\operatorname{k}(_{{abe}:}, _{f_{s}:}), \\operatorname{k}(_{{bart}:}, _{f_{o}:}) \\big \\rbrace , \\end{aligned}$$ (Eq. 3)\n\nwhere $f \\triangleq [f_{p}, f_{s}, f_{o}]$ is a fact in $\\mathcal {K}$ denoting a relationship of type $f_{p}$ between $f_{s}$ and $f_{o}$ , $_{s:}$ is the embedding representation of a symbol $s$ , $$ denotes the initial proof score, and $\\operatorname{k}({}\\cdot {}, {}\\cdot {})$ denotes the RBF kernel. \n Question: How are proof scores calculated?",
            "output": [
                "'= ( , { ll k(h:, g:) if hV, gV\n\n1 otherwise } )\n\nwhere $_{h:}$ and $_{g:}$ denote the embedding representations of $h$ and $g$ , respectively."
            ]
        },
        {
            "id": "task460-2fa15b264b6e4b888ce4399051e4af3b",
            "input": "The main reason is that when the controllability is strong, the change of selection will directly affect the text realization so that a tiny error of content selection might lead to unrealistic text. If the selector is not perfectly trained, the fluency will inevitably be influenced. When the controllability is weaker, like in RS, the fluency is more stable because it will not be affected much by the selection mask. \n Question: Does the performance necessarily drop when more control is desired?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-b4f65a0738b74391a2a9f2c528e8f80d",
            "input": "The dataset consists of a set of polysemous words: 20 nouns, 20 verbs, and 10 adjectives and specifies 20 to 100 contexts per word, with the total of 4,664 contexts, drawn from the Open American National Corpus. Given a set of contexts of a polysemous word, the participants of the competition had to divide them into clusters by sense of the word. The contexts are manually labelled with WordNet senses of the target words, the gold standard clustering is generated from this labelling.\n\nThe task allows two setups: graded WSI where participants can submit multiple senses per word and provide the probability of each sense in a particular context, and non-graded WSI where a model determines a single sense for a word in context. In our experiments we performed non-graded WSI. We considered the most suitable sense as the one with the highest cosine similarity with embeddings of the context, as described in Section SECREF9. \n Question: How are the different senses annotated/labeled? ",
            "output": [
                "The contexts are manually labelled with WordNet senses of the target words"
            ]
        },
        {
            "id": "task460-9981bff80e074a3f8c27ceba51baa1e3",
            "input": "This model uses a particular RNN cell in order to store just relevant information about the given question.  Dynamic Memory stores information of entities present in $T$ . The addition of the $s_t^T q$ term in the gating function is our main contribution.  \n Question: How does the model recognize entities and their relation to answers at inference time when answers are not accessible?",
            "output": [
                "gating function Dynamic Memory"
            ]
        },
        {
            "id": "task460-4db3e71b9d1b429c9ff647bdba47790e",
            "input": "For capturing facial presence, we rely on BIBREF56 's approach that uses multilevel convolutional coarse-to-fine network cascade to tackle facial landmark localization. Facial Expression:\n\nFollowing BIBREF8 's approach, we adopt Ekman's model of six emotions: anger, disgust, fear, joy, sadness and surprise, and use the Face++ API to automatically capture them from the shared images. General Image Features:\n\nThe importance of interpretable computational aesthetic features for studying users' online behavior has been highlighted by several efforts BIBREF55 , BIBREF8 , BIBREF57 .  Qualitative Language Analysis: The recent LIWC version summarizes textual content in terms of language variables such as analytical thinking, clout, authenticity, and emotional tone.  It also measures other linguistic dimensions such as descriptors categories (e.g., percent of target words gleaned by dictionary, or longer than six letters - Sixltr) and informal language markers (e.g., swear words, netspeak), and other linguistic aspects (e.g., 1st person singular pronouns.) \n Question: What types of features are used from each data type?",
            "output": [
                "facial presence Facial Expression General Image Features  textual content analytical thinking clout authenticity emotional tone Sixltr  informal language markers 1st person singular pronouns"
            ]
        },
        {
            "id": "task460-839ab46ba85147b8b440928b79a752bc",
            "input": "We deployed the task on Amazon Mechanical Turk (AMT). To see how reasoning varies across workers, we hire 3 crowdworkers per one instance. We hire reliable crowdworkers with $\\ge 5,000$ HITs experiences and an approval rate of $\\ge $ 99.0%, and pay ¢20 as a reward per instance. \n Question: Did they use any crowdsourcing platform?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-750a832ebc434e29a31b01451c33abad",
            "input": "The same BUTD model that achieves 73.02% on the uncorrected SNLI-VE test set, achieves 73.18% balanced accuracy when tested on the corrected test set from SNLI-VE-2.0. Hence, for this model, we do not notice a significant difference in performance. This could be due to randomness. \n Question: How much is performance difference of existing model between original and corrected corpus?",
            "output": [
                "73.02% on the uncorrected SNLI-VE test set, achieves 73.18% balanced accuracy when tested on the corrected test set"
            ]
        },
        {
            "id": "task460-8281c116e7e745e7b04797844f775545",
            "input": "The Wikipedia dataset consists of articles from English Wikipedia, with quality class labels assigned by the Wikipedia community. \n Question: Which languages do they use?",
            "output": [
                "English"
            ]
        },
        {
            "id": "task460-a7a7e6e96ece4ffa800da90e874632d4",
            "input": "We crowd-sourced the collection of the dataset on Amazon Mechanical Turk (MTurk). \n Question: How was the dataset collected?",
            "output": [
                "crowd-sourced the collection of the dataset on Amazon Mechanical Turk (MTurk)"
            ]
        },
        {
            "id": "task460-d7f7b44ef45e4f089585a6030a13af9e",
            "input": "As shown in Table TABREF48, our two models show competitive results compared to the state-of-the-art on the Visual Dialog challenge 2018 (DL-61 was the winner of the Visual Dialog challenge 2018).  \n Question: What model was winner of the Visual Dialog challenge 2018?",
            "output": [
                "DL-61"
            ]
        },
        {
            "id": "task460-cfae5422681648ec98afab8c810c45c8",
            "input": "The total F-1 score on the OntoNotes dataset is 88%, and the total F-1 cross-validation score on the 112 class Wiki(gold) dataset is 53%.  \n Question: What results do they achieve using their proposed approach?",
            "output": [
                "F-1 score on the OntoNotes is 88%, and it is 53% on Wiki (gold)."
            ]
        },
        {
            "id": "task460-6cead32ff510417ca4c1a873b3f18812",
            "input": "In this section, we empirically compare automatic differentiation (AD, our implementation based on Clad) and numerical differentiation (ND, based on finite difference method) in ROOT. We show that AD can drastically improve accuracy and performance of derivative evaluation, compared to ND. \n Question: How is correctness of automatic derivation proved?",
            "output": [
                "empirically compare automatic differentiation (AD, our implementation based on Clad) and numerical differentiation (ND, based on finite difference method)"
            ]
        },
        {
            "id": "task460-91cc35ccd837444f9d7732d2e1e781e7",
            "input": "EmotionLines BIBREF6 is a dialogue dataset composed of two subsets, Friends and EmotionPush, according to the source of the dialogues. The former comes from the scripts of the Friends TV sitcom. The other is made up of Facebook messenger chats.  \n Question: What are the sources of the datasets?",
            "output": [
                "Friends TV sitcom Facebook messenger chats"
            ]
        },
        {
            "id": "task460-1061fdf1aa4d45019469fa3567539052",
            "input": "TF-IDF: We applied TF-IDF features as inputs to four classifiers: support vector machine classifier (SVM), logistic regression classifier (LR), Naive Bayes classifier (NB) and random forest (RF); CNN: We followed the same architecture as BIBREF9; LSTM: We applied an LSTM model BIBREF20 to classify the sentences with pre-trained word embeddings;LSTM-soft: We then added a soft-attention BIBREF21 layer on top of the LSTM model where we computed soft alignment scores over each of the hidden states; LSTM-self: We applied a self-attention layer BIBREF22 to LSTM model.  \n Question: To what baseline models is proposed model compared?",
            "output": [
                "support vector machine classifier (SVM) logistic regression classifier (LR) Naive Bayes classifier (NB) random forest (RF) CNN LSTM  LSTM-soft LSTM-self"
            ]
        },
        {
            "id": "task460-33b2afbe4b664c85bfafd54c2cfd7b80",
            "input": " In this section, we recommend a set of evaluation design changes that we believe are needed for assessing human–machine parity, and will strengthen the human evaluation of MT in general.\n\nRecommendations ::: (R1) Choose professional translators as raters.\nIn our blind experiment (Section SECREF3), non-experts assess parity between human and machine translation where professional translators do not, indicating that the former neglect more subtle differences between different translation outputs.\n\nRecommendations ::: (R2) Evaluate documents, not sentences.\nWhen evaluating sentences in random order, professional translators judge machine translation more favourably as they cannot identify errors related to textual coherence and cohesion, such as different translations of the same product name. Our experiments show that using whole documents (i. e., full news articles) as unit of evaluation increases the rating gap between human and machine translation (Section SECREF4).\n\nRecommendations ::: (R3) Evaluate fluency in addition to adequacy.\nRaters who judge target language fluency without access to the source texts show a stronger preference for human translation than raters with access to the source texts (Sections SECREF4 and SECREF24). In all of our experiments, raters prefer human translation in terms of fluency while, just as in BIBREF3's BIBREF3 evaluation, they find no significant difference between human and machine translation in sentence-level adequacy (Tables TABREF21 and TABREF30). Our error analysis in Table TABREF34 also indicates that MT still lags behind human translation in fluency, specifically in grammaticality.\n\nRecommendations ::: (R4) Do not heavily edit reference translations for fluency.\nIn professional translation workflows, texts are typically revised with a focus on target language fluency after an initial translation step. As shown in our experiment in Section SECREF24, aggressive revision can make translations more fluent but less accurate, to the degree that they become indistinguishable from MT in terms of accuracy (Table TABREF30).\n\nRecommendations ::: (R5) Use original source texts.\nRaters show a significant preference for human over machine translations of texts that were originally written in the source language, but not for source texts that are translations themselves (Section SECREF35). Our results are further evidence that translated texts tend to be simpler than original texts, and in turn easier to translate with MT. \n Question: What recommendations do they offer?",
            "output": [
                " Choose professional translators as raters  Evaluate documents, not sentences Evaluate fluency in addition to adequacy Do not heavily edit reference translations for fluency Use original source texts"
            ]
        },
        {
            "id": "task460-9c0450de37b54667aa0cfc3d2fc5fc97",
            "input": "I also performed several case studies. I obtained document embeddings, in the same latent space as the topic embeddings, by summing the posterior mean vectors INLINEFORM0 for each token, and visualized them in two dimensions using INLINEFORM1 -SNE BIBREF24 (all vectors were normalized to unit length). The state of the Union addresses (Figure FIGREF27 ) are embedded almost linearly by year, with a major jump around the New Deal (1930s), and are well separated by party at any given time period.  \n Question: What is an example of a computational social science NLP task?",
            "output": [
                "Visualization of State of the union addresses"
            ]
        },
        {
            "id": "task460-6938f9c4b1fe4aeab8376fb326d46f62",
            "input": "Most prior work focuses on a different aspect of offensive language such as abusive language BIBREF0 , BIBREF1 , (cyber-)aggression BIBREF2 , (cyber-)bullying BIBREF3 , BIBREF4 , toxic comments INLINEFORM0 , hate speech BIBREF5 , BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 , and offensive language BIBREF11 . Prior work has focused on these aspects of offensive language in Twitter BIBREF3 , BIBREF7 , BIBREF8 , BIBREF11 , Wikipedia comments, and Facebook posts BIBREF2 . \n Question: What is the definition of offensive language?",
            "output": [
                " Most prior work focuses on a different aspect of offensive language such as abusive language BIBREF0 , BIBREF1 , (cyber-)aggression BIBREF2 , (cyber-)bullying BIBREF3 , BIBREF4 , toxic comments INLINEFORM0 , hate speech BIBREF5 , BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 , and offensive language BIBREF11 . Prior work has focused on these aspects of offensive language in Twitter BIBREF3 , BIBREF7 , BIBREF8 , BIBREF11 , Wikipedia comments, and Facebook posts BIBREF2 ."
            ]
        },
        {
            "id": "task460-997caf76b2b44f49807a3952ad349fde",
            "input": "Manning and Schütze argue that, even though not quite correct, language text can be modeled as stationary, ergodic random processes BIBREF29, an assumption that we follow. Moreover, given the diversity of language production, we assume this stationary ergodic random process with finite alphabet $\\mathcal {A}$ denoted $X = \\lbrace X_i, -\\infty < i < \\infty \\rbrace $ is non-null in the sense that always $P(x_{-m}^{-1}) > 0$ and\n\nThis is sometimes called the smoothing requirement. \n Question: Is the assumption that natural language is stationary and ergodic valid?",
            "output": [
                "It is not completely valid for natural languages because of diversity of language - this is called smoothing requirement."
            ]
        },
        {
            "id": "task460-a0fbf5f5e4ea4a4f8fb3ffb5feb7b670",
            "input": "To fine-tune GPT2 for text generation, it is typical to concatenate the conditioning context $X = x_1 \\ldots x_n$ and citing sentence $Y = y_1 \\ldots y_m$ with a special separator token $\\mho $. The abstract of each document is embedded into a single dense vector by averaging the contextualized embeddings provided by the SciBERT model of BIBREF11 and normalizing. \n Question: Which baselines are explored?",
            "output": [
                "GPT2 SciBERT model of BIBREF11"
            ]
        },
        {
            "id": "task460-ef1addd47c674c659c6d6c45b6080d6c",
            "input": "As shown in Table TABREF46, consensus dropout fusion improves the score of NDCG by around 1.0 from the score of the joint model while still yielding comparable scores for other metrics. As also shown in Table TABREF46, the ensemble model seems to take the best results from each model. \n Question: Which method for integration peforms better ensemble or consensus dropout fusion with shared parameters?",
            "output": [
                "ensemble model"
            ]
        },
        {
            "id": "task460-d90c229e478b4e12b094de7c6572b99b",
            "input": "We evaluated our models on 3 different datasets:\n\nCSAT dataset for CSAT prediction, consisting of spoken transcripts (automatic via ASR).\n\n20 newsgroups for topic identification task, consisting of written text;\n\nFisher Phase 1 corpus for topic identification task, consisting of spoken transcripts (manual); \n Question: What datasets did they use for evaluation?",
            "output": [
                "CSAT dataset 20 newsgroups Fisher Phase 1 corpus"
            ]
        },
        {
            "id": "task460-047c22b090334d32a759e34c3206864b",
            "input": "For the two single sentence tasks—the syntax-oriented CoLA task and the SST sentiment task—we find somewhat deteriorated performance. \n Question: Does the additional training on supervised tasks hurt performance in some tasks?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-1c4e16e86c354183a1c3a25182f418e1",
            "input": "In order to compare with the previous methods, we evaluate our model on AIDA-B and other datasets.  AIDA-CoNLL BIBREF14 is annotated on Reuters news articles. It contains training (AIDA-Train), validation (AIDA-A) and test (AIDA-B) sets.\n\nACE2004 BIBREF15 is a subset of the ACE2004 Coreference documents.\n\nMSNBC BIBREF16 contains top two stories in the ten news categories(Politics, Business, Sports etc.)\n\nAQUAINT BIBREF17 is a news corpus from the Xinhua News Service, the New York Times, and the Associated Press.\n\nWNED-CWEB BIBREF18 is randomly picked from the FACC1 annotated ClueWeb 2012 dataset.\n\nWNED-WIKI BIBREF18 is crawled from Wikipedia pages with its original hyperlink annotation. \n Question: What datasets used for evaluation?",
            "output": [
                "AIDA-B ACE2004 MSNBC AQUAINT WNED-CWEB WNED-WIKI"
            ]
        },
        {
            "id": "task460-9e2aa48d5bfe445087d89559c3065235",
            "input": "Since pre-trained models operate on subword level, we need to estimate subword translation probabilities. For the purpose of evaluating the contextual representations learned by our model, we do not use part-of-speech tags. Contextualized representations are directly fed into Deep-Biaffine layers to predict arc and label scores. Table TABREF34 presents the Labeled Attachment Scores (LAS) for zero-shot dependency parsing. \n Question: What metrics are used for evaluation?",
            "output": [
                "translation probabilities Labeled Attachment Scores (LAS)"
            ]
        },
        {
            "id": "task460-00191064df544b1fbcc5f520fdadd895",
            "input": "In the sentiment analysis experiments, we found that a tweet may contain multiple sentiments. The groundtruth labels contain 210 positive sentiments, 521 neutral sentiments, and 305 negative sentiments to the candidates. \n Question: Is datasets for sentiment analysis balanced?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-60252c9c2f8040e3a5afc83ec52b1e6b",
            "input": "Many practical fake news detection algorithms use a kind of semantic side information, such as whether the generated text is factually correct, in addition to its statistical properties. Although statistical side information would be straightforward to incorporate in the hypothesis testing framework, it remains to understand how to cast such semantic knowledge in a statistical decision theory framework. \n Question: What semantic features help in detecting whether a piece of text is genuine or generated? of ",
            "output": [
                "No feature is given, only discussion that semantic features are use in practice and yet to be discovered how to embed that knowledge into statistical decision theory framework."
            ]
        },
        {
            "id": "task460-ad24faeb45384397b6676ef8dbad3da3",
            "input": "We propose to derive rewards from human-human dialogues by assigning positive values to contextualised responses seen in the data, and negative values to randomly chosen responses due to lacking coherence (also referred to as `non-human-like responses') – see example in Tables TABREF29 and TABREF30 . \n Question: How do they obtain human generated policies?",
            "output": [
                "derive rewards from human-human dialogues by assigning positive values to contextualised responses seen in the data, and negative values to randomly chosen responses due to lacking coherence"
            ]
        },
        {
            "id": "task460-a69399926cc14b71bb808b22e6f87831",
            "input": "The contributions of this work are: \n Question: Do they look for inconsistencies between different UD treebanks?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-385398a6d9c34aa8aa171b9326cb6680",
            "input": "Active learning methods can generally be described into two parts: a learning engine and a selection engine BIBREF28 . The learning engine is essentially a classifier, which is mainly used for training of classification problems. The selection engine is based on the sampling strategy, which chooses samples that need to be relabeled by annotators from unlabeled data. Then, relabeled samples are added to training set for classifier to re-train, thus continuously improving the accuracy of the classifier. In this paper, a CRF-based segmenter and a scoring model are employed as learning engine and selection engine, respectively. \n Question: How does the active learning model work?",
            "output": [
                "Active learning methods has a learning engine (mainly used for training of classification problems) and the selection engine (which chooses samples that need to be relabeled by annotators from unlabeled data). Then, relabeled samples are added to training set for classifier to re-train, thus continuously improving the accuracy of the classifier. In this paper, CRF-based segmenter and a scoring model are employed as learning engine and selection engine, respectively."
            ]
        },
        {
            "id": "task460-f6ae90d0a5f04f9788bf302d76e886eb",
            "input": "Word Lattice\nAs shown in Figure FIGREF4 , a word lattice is a directed graph INLINEFORM0 , where INLINEFORM1 represents a node set and INLINEFORM2 represents a edge set. For a sentence in Chinese, which is a sequence of Chinese characters INLINEFORM3 , all of its possible substrings that can be considered as words are treated as vertexes, i.e. INLINEFORM4 . Then, all neighbor words are connected by directed edges according to their positions in the original sentence, i.e. INLINEFORM5 . \n Question: How do they obtain word lattices from words?",
            "output": [
                "By considering words as vertices and generating directed edges between neighboring words within a sentence"
            ]
        },
        {
            "id": "task460-dbfd9bff19f144aaba02c238745e7030",
            "input": "JESSI is trained using only the datasets given on the shared task, without using any additional external data. \n Question: What datasets were used?",
            "output": [
                "datasets given on the shared task, without using any additional external data"
            ]
        },
        {
            "id": "task460-d826e5169231439788abed7915ac4b50",
            "input": "Based on Euclidean distance between vector representations of visits we applied and compared two clustering algorithms: k-means and hierarchical clustering with Ward's method for merging clusters BIBREF23 . \n Question: Which clustering technique do they use on partients' visits texts?",
            "output": [
                "k-means hierarchical clustering with Ward's method for merging clusters BIBREF23"
            ]
        },
        {
            "id": "task460-b2599a76f60a48edb3ac90ca7ac521ca",
            "input": "We assume that speech transcripts can be extracted from audio signals with high accuracy, given the advancement of ASR technologies BIBREF7 . \n Question: Do they use datasets with transcribed text or do they determine text from the audio?",
            "output": [
                "They use text transcription."
            ]
        },
        {
            "id": "task460-c3c19694ccb9433ea2d20dce1126a2a7",
            "input": "In order to train and evaluate open-domain factoid QA system for real-world questions, we build a new Chinese QA dataset named as WebQA. \n Question: What languages do they experiment with?",
            "output": [
                "Chinese"
            ]
        },
        {
            "id": "task460-8c0a78c524d64088832ac23df739829f",
            "input": "In our experiments we use the same data-sets from other works BIBREF32, BIBREF23, BIBREF33 as well as other datasets that we collected by us using a similar criterion (described in Section SECREF4). We use thirty different discussions that took place between March 2015 and June 2019, half of them with controversy and half without it. We considered discussions in four different languages: English, Portuguese, Spanish and French, occurring in five regions over the world: South and North America, Western Europe, Central and Southern Asia. We also studied these discussions taking first 140 characters and then 280 from each tweet to analyze the difference in performance and computing time wrt the length of the posts. \n Question: What datasets did they use?",
            "output": [
                "BIBREF32, BIBREF23, BIBREF33 discussions in four different languages: English, Portuguese, Spanish and French, occurring in five regions over the world: South and North America, Western Europe, Central and Southern Asia. "
            ]
        },
        {
            "id": "task460-46aa68c75658492a8f4f2213d6fc97ad",
            "input": "We extract unique users and tweets from the combined result set to generate a dataset of about 60,000 unique tweets, pertaining to 51,104 unique users. \n Question: How many tweets are explored in this paper?",
            "output": [
                "60,000 "
            ]
        },
        {
            "id": "task460-669595538a7a46c5a78906d90ee83a20",
            "input": "A recently published dataset called SCAN BIBREF2 (Simplified version of the CommAI Navigation tasks), tests compositional generalization in a sequence-to-sequence (seq2seq) setting by systematically holding out of the training set all inputs containing a basic primitive verb (\"jump\"), and testing on sequences containing that verb. \n Question: How does the SCAN dataset evaluate compositional generalization?",
            "output": [
                "it systematically holds out inputs in the training set containing basic primitive verb, \"jump\", and tests on sequences containing that verb."
            ]
        },
        {
            "id": "task460-87a797dbeec742ad8422a7b6b72bd461",
            "input": "For the future work, we plan to solve the triples with multiple entities as the second entity, which is excluded from problem scope in this paper. The input of QA4IE is a document $D$ with an existing knowledge base $K$ and the output is a set of relation triples $R = \\lbrace e_i, r_{ij}, e_j\\rbrace $ in $D$ where $e_i$ and $e_j$ are two individual entities and $r_{ij}$ is their relation. \n Question: Can this approach model n-ary relations?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-7c6d102fd0e147b8bb77e6a7eef8242a",
            "input": "We embedded COSTRA sentences with LASER BIBREF15, the method that performed very well in revealing linear relations in BaBo2019. \n Question: Are some baseline models trained on this dataset?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-c68496699c75446a86284c6fac6c7152",
            "input": "To demonstrate SPNet's effectiveness, we compare it with two state-of-the-art methods, Pointer-Generator BIBREF5 and Transformer BIBREF6. \n Question: What are previous state-of-the-art document summarization methods used?",
            "output": [
                "Pointer-Generator Transformer"
            ]
        },
        {
            "id": "task460-eba2cd7d7cb8489ebfc4bda4cfff0c68",
            "input": "We compare our sentence classification with the LSTM baseline and evaluate the biggest set of PICO sentence data available at this point BIBREF13. SCIBERT was chosen as an additional baseline model for fine-tuning because it provided the best representation of embedded PICO sentences.  \n Question: What baselines did they consider?",
            "output": [
                "LSTM SCIBERT"
            ]
        },
        {
            "id": "task460-e01dc32addad4b9686e19cd23b290d7c",
            "input": "Word2vec representation is far better, advanced and a recent technique which functions by mapping words to a 300 dimensional vector representations. \n Question: What is the dimension of the embeddings?",
            "output": [
                "300"
            ]
        },
        {
            "id": "task460-2215da8dc21f49c6943c6e17ad9432c1",
            "input": "Our encoder-decoder framework employs separate encoding for different speakers in the dialog. User utterances $x_t^{usr}$ and system utterances $x_t^{sys}$ are fed into a user encoder and a system encoder separately to obtain encoder hidden states $h_{i}^{usr}$ and $h_{i}^{sys}$ . We integrate semantic slot scaffold by performing delexicalization on original dialogs. Delexicalization is a common pre-processing step in dialog modeling. We integrate dialog domain scaffold through a multi-task framework. Dialog domain indicates different conversation task content, for example, booking hotel, restaurant and taxi in MultiWOZ dataset. \n Question: How does SPNet utilize additional speaker role, semantic slot and dialog domain annotations?",
            "output": [
                "Our encoder-decoder framework employs separate encoding for different speakers in the dialog. We integrate semantic slot scaffold by performing delexicalization on original dialogs. We integrate dialog domain scaffold through a multi-task framework."
            ]
        },
        {
            "id": "task460-c4adc5d38e7e458287a4a37f0cb5af6f",
            "input": "A few sample advice rules in English (these are converted to first-order logic format and given as input to our algorithm) are presented in Table TABREF11 .  We modified the work of Odom et al. odomAIME15,odomAAAI15 to learn RDNs in the presence of advice. The key idea is to explicitly represent advice in calculating gradients. \n Question: How do they incorporate human advice?",
            "output": [
                "by converting human advice to first-order logic format and use as an input to calculate gradient"
            ]
        },
        {
            "id": "task460-b920003b202d425e9dce1bb302a66b21",
            "input": "We evaluate our cross-lingual pre-training based transfer approach against several strong baselines on two public datatsets, Europarl BIBREF31 and MultiUN BIBREF32, which contain multi-parallel evaluation data to assess the zero-shot performance. \n Question: which datasets did they experiment with?",
            "output": [
                "Europarl MultiUN"
            ]
        },
        {
            "id": "task460-005bc7f39f2f4550b2df58eafdac174b",
            "input": "For instance, schema 23 from the WSC collection can be translated into both “The girls were bullying the boys so we [punished/rescued] them” and “The boys were bullying the girls, so we [punished/rescued] them,” thus avoiding any presupposition of whether girls are more likely to bully boys or vice versa. \n Question: What data do they look at?",
            "output": [
                "WSC collection"
            ]
        },
        {
            "id": "task460-b6cb31648b0a4e17a308adc41e43af7b",
            "input": "Inspired by the supervised reordering in conventional SMT, in this paper, we propose a Supervised Attention based NMT (SA-NMT) model. Specifically, similar to conventional SMT, we first run off-the-shelf aligners (GIZA++ BIBREF3 or fast_align BIBREF4 etc.) to obtain the alignment of the bilingual training corpus in advance. Then, treating this alignment result as the supervision of attention, we jointly learn attention and translation, both in supervised manners. Since the conventional aligners delivers higher quality alignment, it is expected that the alignment in the supervised attention NMT will be improved leading to better end-to-end translation performance. \n Question: Which conventional alignment models do they use as guidance?",
            "output": [
                "GIZA++ BIBREF3 or fast_align BIBREF4 "
            ]
        },
        {
            "id": "task460-c83b77f207454a3aaca9795b88f33964",
            "input": "We obtained a corpus of approximately 300,000 sentences, from which roughly 1.5 million single-quiz question training examples were derived.  \n Question: What is the size of the dataset?",
            "output": [
                "300,000 sentences with 1.5 million single-quiz questions"
            ]
        },
        {
            "id": "task460-647d19adac044e059147e38f8550bef3",
            "input": "The prediction of outcomes of debates is very interesting in our case. Most of the results seem to match with the views of some experts such as the political pundits of the Washington Post. This implies that certain rules that were used to score the candidates in the debates by said-experts were in fact reflected by reading peoples' sentiments expressed over social media. This opens up a wide variety of learning possibilities from users' sentiments on social media, which is sometimes referred to as the wisdom of crowd. \n Question: Who is the crowd in these experiments?",
            "output": [
                " peoples' sentiments expressed over social media"
            ]
        },
        {
            "id": "task460-349c819833fa42249ecc5ffaeb0a868f",
            "input": "Car-speak is abstract language that pertains to a car's physical attribute(s). In this instance the physical attributes that the term “fast” pertains to could be the horsepower, or it could be the car's form factor (how the car looks). However, we do not know exactly which attributes the term “fast” refers to. \n Question: How does car speak pertains to a car's physical attributes?",
            "output": [
                "we do not know exactly"
            ]
        },
        {
            "id": "task460-20cb085a200a40b88a00d62f31a7c4bd",
            "input": ". This work can be extended to predict future events with one day in advance, where we will use the same method for feature selection in addition to to time series analysis of the historical patterns of the word-pairs. \n Question: Do the authors suggest any future extensions to this work?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-b7f18ad75b464c209d2325817f6e5dd7",
            "input": "We compare our approach with two baseline vision-based methods proposed in BIBREF6 , BIBREF7 , which measure the similarity of two sets of global visual features for bilingual lexicon induction:\n\nCNN-mean: taking the similarity score of the averaged feature of the two image sets.\n\nCNN-avgmax: taking the average of the maximum similarity scores of two image sets. \n Question: What baseline is used for the experimental setup?",
            "output": [
                "CNN-mean CNN-avgmax"
            ]
        },
        {
            "id": "task460-4d79626531be4062b08f3e846320e3fa",
            "input": "The final section of the paper, then, discusses how insights gained from technologically observing opinion dynamics can inform conceptual modelling efforts and approaches to on-line opinion facilitation. As such, the paper brings into view and critically evaluates the fundamental conceptual leap from machine-guided observation to debate facilitation and intervention. \n Question: Does the paper report the results of previous models applied to the same tasks?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-e0ca721dc6164caa9ee5cb384df29153",
            "input": "Our video question answering task is novel and to our knowledge, no model has been designed specifically for this task. As a first step towards solving this problem, we evaluated the performance of state-of-the-art models developed for other QA tasks, including a sentence-level prediction task and two segment retrieval tasks. Baselines ::: Baseline1: Sentence-level prediction\nGiven a transcript (a sequence of sentences) and a question, Baseline1 predicts (starting sentence index, ending sentence index). The model is based on RaSor BIBREF13, which has been developed for the SQuAD QA task BIBREF6. RaSor concatenates the embedding vectors of the starting and the ending words to represent a span. Following this idea, Baseline1 represents a span of sentences by concatenating the vectors of the starting and ending sentences. The left diagram in Fig. FIGREF15 illustrates the Baseline1 model.\n\nModel. The model takes two inputs, a transcript, $\\lbrace s_1, s_2, ... s_n\\rbrace $ where $s_i$ are individual sentences and a question, $q$. The output is the span scores, $y$, the scores over all possible spans. GLoVe BIBREF14 is used for the word representations in the transcript and the questions. We use two bi-LSTMs BIBREF15 to encode the transcript.\n\nwhere n is the number of sentences . The output of Passage-level Encoding, $p$, is a sequence of vector, $p_i$, which represents the latent meaning of each sentence. Then, the model combines each pair of sentence embeddings ($p_i$, $p_j$) to generate a span embedding.\n\nwhere [$\\cdot $,$\\cdot $] indicates the concatenation. Finally, we use a one-layer feed forward network to compute a score between each span and a question.\n\nIn training, we use cross-entropy as an objective function. In testing, the span with the highest score is picked as an answer. Baselines ::: Baseline2: Segment retrieval\nWe also considered a simpler task by casting our problem as a retrieval task. Specifically, in addition to a plain transcript, we also provided the model with the segmentation information which was created during the data collection phrase (See Section. SECREF3). Note that each segments corresponds to a candidate answer. Then, the task is to pick the best segment for given a query. This task is easier than Baseline1's task in that the segmentation information is provided to the model. Unlike Baseline1, however, it is unable to return an answer span at various granularities. Baseline2 is based on the attentive LSTM BIBREF17, which has been developed for the InsuranceQA task. The right diagram in Fig. FIGREF15 illustrates the Baseline2 model.\n\nModel. The two inputs, $s$ and $q$ represent the segment text and a question. The model first encodes the two inputs.\n\n$h^s$ is then re-weighted using attention weights.\n\nwhere $\\odot $ denotes the element-wise multiplication operation. The final score is computed using a one-layer feed-forward network.\n\nDuring training, the model requires negative samples. For each positive example, (question, ground-truth segment), all the other segments in the same transcript are used as negative samples. Cross entropy is used as an objective function.\n\n Baselines ::: Baseline3: Pipeline Segment retrieval\nWe construct a pipelined approach through another segment retrieval task, calculating the cosine similarities between the segment and question embeddings. In this task however, we want to test the accuracy of retrieving the segments given that we first retrieve the correct video from our 76 videos. First, we generate the TF-IDF embeddings for the whole video transcripts and questions. The next step involves retrieving the videos which have the lowest cosine distance between the video transcripts and question. We then filter and store the top ten videos, reducing the number of computations required in the next step. Finally, we calculate the cosine distances between the question and the segments which belong to the filtered top 10 videos, marking it as correct if found in these videos. While the task is less computationally expensive than the previous baseline, we do not learn the segment representations, as this task is a simple retrieval task based on TF-IDF embeddings.\n\nModel. The first two inputs are are the question, q, and video transcript, v, encoded by their TF-IDF vectors: BIBREF18:\n\nWe then filter the top 10 video transcripts(out of 76) with the minimum cosine distance, and further compute the TF-IDF vectors for their segments, Stop10n, where n = 10. We repeat the process for the corresponding segments:\n\nselecting the segment with the minimal cosine distance distance to the query. \n Question: What baseline algorithms were presented?",
            "output": [
                "a sentence-level prediction algorithm, a segment retrieval algorithm and a pipeline segment retrieval algorithm"
            ]
        },
        {
            "id": "task460-66e22b2bbdbb4d00bd9dd823ff1177cb",
            "input": "The core idea of dual learning is to leverage the duality between the primal task (mapping from domain $\\mathcal {X}$ to domain $\\mathcal {Y}$) and dual task (mapping from domain $\\mathcal {Y}$ to $\\mathcal {X}$ ) to boost the performances of both tasks. MADL BIBREF0 extends the dual learning BIBREF1, BIBREF2 framework by introducing multiple primal and dual models. \n Question: How does muli-agent dual learning work?",
            "output": [
                "MADL BIBREF0 extends the dual learning BIBREF1, BIBREF2 framework by introducing multiple primal and dual models."
            ]
        },
        {
            "id": "task460-34698919d67048719aa79aff18abf6e7",
            "input": "Firstly, LastStateRNN is the classic RNN model, where the last state passes through an MLP and then the LR Layer estimates the corresponding probability. In contrast, in the AvgRNN model we consider the average vector of all states that come out of the cells. The AttentionRNN model is the one that it has been presented in BIBREF9. Moreover, we introduce the MultiAttentionRNN model for the harassment language detection, which instead of one attention, it includes four attentions, one for each category. \n Question: What baseline model is used?",
            "output": [
                " LastStateRNN AvgRNN AttentionRNN"
            ]
        },
        {
            "id": "task460-f73a6b53ca174ff6ae3b2c5ad515a9fb",
            "input": "MATT-BiE-LSTM and WATT-BiE-LSTM obtain similar performances when tested on both Vader and human annotated samples, though their ways of computing the attention (weights and vectors) are different that WATT computes attention weights and the senti-emoji embeddings guided by each word, and MATT obtains the senti-emoji embedding based on the LSTM encoder on the whole contexts and computes the attention weights of the senti-emoji embedding across all words. Both models outperforms the state-of-the-art baseline models including ATT-E-LSTM Attention-based LSTM with emojis:We also use the word-emoji embedding to calculate the emoji-word attention following Equation EQREF20 and EQREF21 , and the only difference is that we replace the attention-derived senti-emoji embedding with the pre-trained word-emoji embedding by fasttext, denoted as ATT-E-LSTM. LSTM with bi-sense emoji embedding (proposed): As we have introduced in Section SECREF13 , we propose two attention-based LSTM networks based on bi-sense emoji embedding, denoted as MATT-BiE-LSTM and WATT-BiE-LSTM. \n Question: Which SOTA models are outperformed?",
            "output": [
                "Attention-based LSTM with emojis"
            ]
        },
        {
            "id": "task460-a21fba379a6d4f8db756c43c09fde2c4",
            "input": "In our experiments, we use as input the 2210 tokenized sentences of the Stanford Sentiment Treebank test set BIBREF2 , preprocessing them by lowercasing as was done in BIBREF8 . \n Question: Which datasets are used for evaluation?",
            "output": [
                "Stanford Sentiment Treebank"
            ]
        },
        {
            "id": "task460-ce638df62f63435f912750fec9b2d61e",
            "input": "The resulting set of 46 documents makes up our base corpus. Note that these documents vary greatly in size, which means the resulting corpus is varied, but not balanced in terms of size (Table TABREF43). \n Question: How big PIE datasets are obtained from dictionaries?",
            "output": [
                "46 documents makes up our base corpus"
            ]
        },
        {
            "id": "task460-e8bc852aec514e408561e0dfaf73dc7c",
            "input": "We compared our models with the following state-of-the-art baselines:\n\nSequence to Sequence (Seq2Seq): A simple encoder-decoder model which concatenates four sentences to a long sentence with an attention mechanism BIBREF31 .\n\nHierarchical LSTM (HLSTM): The story context is represented by a hierarchical LSTM: a word-level LSTM for each sentence and a sentence-level LSTM connecting the four sentences BIBREF29 . A hierarchical attention mechanism is applied, which attends to the states of the two LSTMs respectively.\n\nHLSTM+Copy: The copy mechanism BIBREF32 is applied to hierarchical states to copy the words in the story context for generation.\n\nHLSTM+Graph Attention(GA): We applied multi-source attention HLSTM where commonsense knowledge is encoded by graph attention.\n\nHLSTM+Contextual Attention(CA): Contextual attention is applied to represent commonsense knowledge. \n Question: Which baselines are they using?",
            "output": [
                "Seq2Seq HLSTM HLSTM+Copy HLSTM+Graph Attention HLSTM+Contextual Attention"
            ]
        },
        {
            "id": "task460-4aa87cb96c404d839fcf4d2def102f29",
            "input": "GP-GNNs first build a fully-connected graph $\\mathcal {G} = (\\mathcal {V}, \\mathcal {E})$ , where $\\mathcal {V}$ is the set of entities, and each edge $(v_i, v_j) \\in \\mathcal {E}, v_i, v_j \\in \\mathcal {V}$ corresponds to a sequence $s = x_0^{i,j}, x_1^{i,j}, \\dots , x_{l-1}^{i,j}$ extracted from the text.  \n Question: So this paper turns unstructured text inputs to parameters that GNNs can read?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-cd3e724dae414b4a976f55fefc516a77",
            "input": "The first challenge is to acquire very large Vietnamese corpus and to use them in building a classifier, which could further improve accuracy.  The second challenge is design and development of big data warehouse and analytic framework for Vietnamese documents, which corresponds to the rapid and continuous growth of gigantic volume of articles and/or documents from Web 2.0 applications, such as, Facebook, Twitter, and so on. The final challenge relates to building a system, which is able to incrementally learn new corpora and interactively process feedback. \n Question: Why challenges does word segmentation in Vietnamese pose?",
            "output": [
                "Acquire very large Vietnamese corpus and build a classifier with it, design a develop a big data warehouse and analytic framework, build a system to incrementally learn new corpora and interactively process feedback."
            ]
        },
        {
            "id": "task460-fa60ee3fc1344da4871451ab47b8ddc5",
            "input": "For multi-modal sentiment analysis, we can simply split the image into two parts. One for the text input, and the other for the tabular data. \n Question: How is Super Character method modified to handle tabular data also?",
            "output": [
                "simply split the image into two parts. One for the text input, and the other for the tabular data"
            ]
        },
        {
            "id": "task460-124e447aa3e6461682b99a88143e0a53",
            "input": "We observe $R^2$ correlation coefficients between 0.11 and 0.22 for the input and output length ratios and between 0.81 and 0.88 for the compound divergence.  \n Question: How strong is negative correlation between compound divergence and accuracy in performed experiment?",
            "output": [
                " between 0.81 and 0.88"
            ]
        },
        {
            "id": "task460-d38377382f234cc4a620762109c6dd68",
            "input": "Figure FIGREF23 shows the proposed ontology, which, in our evaluation procedure, was populated with 3121 events entries from 51 documents. \n Question: How is the effectiveness of this pipeline approach evaluated?",
            "output": [
                "proposed ontology, which, in our evaluation procedure, was populated with 3121 events entries from 51 documents."
            ]
        },
        {
            "id": "task460-c960c7d6d3a640a48468325ab38e6d86",
            "input": "Here we consider five baselines to compare with GraLap: (i) Uniform: assign 3 to all the references assuming equal intensity, (ii) SVR+W: recently proposed Support Vector Regression (SVR) with the feature set mentioned in BIBREF4 , (iii) SVR+O: SVR model with our feature set, (iv) C4.5SSL: C4.5 semi-supervised algorithm with our feature set BIBREF23 , and (v) GLM: the traditional graph-based LP model with our feature set BIBREF9 . \n Question: What are the baselines model?",
            "output": [
                "(i) Uniform (ii) SVR+W (iii) SVR+O (iv) C4.5SSL (v) GLM"
            ]
        },
        {
            "id": "task460-5afbafa1ca944902954e9276c77252bd",
            "input": "We evaluate our approaches on two public Chinese span-extraction machine reading comprehension datasets: CMRC 2018 (simplified Chinese) BIBREF8 and DRCD (traditional Chinese) BIBREF9. The statistics of the two datasets are listed in Table TABREF29. \n Question: Is this a span-based (extractive) QA task?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-9dd1cc93b78e48df86b9b1407460612f",
            "input": "We explored different normalization techniques. FBanks with cepstral mean normalization (CMN) perform better than raw FBanks. We found using variance with mean normalization (CMVN) unnecessary for the task. Using deltas and delta-deltas improves model, so we used them in other experiments. Models trained with spectrogram features converge slower and to worse minimum, but the difference when using CMN is not very big compared to FBanks. \n Question: What normalization techniques are mentioned?",
            "output": [
                "FBanks with cepstral mean normalization (CMN) variance with mean normalization (CMVN)"
            ]
        },
        {
            "id": "task460-6a070edc590b4651a2df48e1f14eb086",
            "input": "We first pre-train our generator on the Gutenberg dataset BIBREF24 for 20 epochs and then fine-tune BIBREF19 them to our target datasets with a language modeling objective. \n Question: What objective function is used in the GAN?",
            "output": [
                "language modeling objective"
            ]
        },
        {
            "id": "task460-56d8fb25f01642a38bc4f37ac546b624",
            "input": "A meta-research study found only 1 in 9 qualitative papers in Human-Computer Interaction reported inter-rater reliability metrics BIBREF31. Another related area are meta-research and methods papers focused on identifying or preventing low-effort responses from crowdworkers — sometimes called “spam” or “random” responses, or alternatively ”fraudsters” or ”cheaters.” Rates of “self-agreement” are often used, determining if the same person labels the same item differently at a later stage. \n Question: What are the key issues around whether the gold standard data produced in such an annotation is reliable? ",
            "output": [
                " only 1 in 9 qualitative papers in Human-Computer Interaction reported inter-rater reliability metrics low-effort responses from crowdworkers"
            ]
        },
        {
            "id": "task460-02f7095656ee4b1cafeeda0d9633b1ff",
            "input": "As it is reported that conservatives and liberals exhibit different behaviors on online social platforms BIBREF19BIBREF20BIBREF21, we further assigned a political bias label to different US outlets (and therefore news articles) following the procedure described in BIBREF2. In order to assess the robustness of our method, we performed classification experiments by training only on left-biased (or right-biased) outlets of both disinformation and mainstream domains and testing on the entire set of sources, as well as excluding particular sources that outweigh the others in terms of samples to avoid over-fitting. \n Question: How is the political bias of different sources included in the model?",
            "output": [
                "By assigning a political bias label to each news article and training only on left-biased or right-biased outlets of both disinformation and mainstream domains"
            ]
        },
        {
            "id": "task460-1d44f32c67f24227a15b9d503a1f70be",
            "input": "We use two datasets for experiments (Table TABREF29 ): (a) STAN INLINEFORM0 , created by BIBREF10 BansalBV15, which consists of 1,108 unique English hashtags from 1,268 randomly selected tweets in the Stanford Sentiment Analysis Dataset BIBREF36 along with their crowdsourced segmentations and our additional corrections; and (b) STAN INLINEFORM1 , our new expert curated dataset, which includes all 12,594 unique English hashtags and their associated tweets from the same Stanford dataset. \n Question: How is the dataset of hashtags sourced?",
            "output": [
                "1,268 randomly selected tweets in the Stanford Sentiment Analysis Dataset BIBREF36 all 12,594 unique English hashtags and their associated tweets from the same Stanford dataset"
            ]
        },
        {
            "id": "task460-9dec0b5706454d088fb9960e29056ec8",
            "input": "One of the solutions that has been proposed for mitigating gender bias on the word embedding level is Counterfactual Data Augmentation (CDA) BIBREF25. We apply this method by augmenting our dataset with a copy of every dialogue with gendered words swapped using the gendered word pair list provided by BIBREF21. \n Question: How does counterfactual data augmentation aim to tackle bias?",
            "output": [
                "The training dataset is augmented by swapping all gendered words by their other gender counterparts"
            ]
        },
        {
            "id": "task460-b73a5b05a44e43c0a6356dd8ef3c1110",
            "input": "When generating the word $y_t$ at time step $t$, the context $\\mathbf {X}$ is encoded into a fixed-sized dialog context vector $\\mathbf {c}_t$ by following the hierarchical attention structure in HRAN BIBREF13. Additionally, we extract the emotion information from the utterances in $\\mathbf {X}$ by leveraging an external text analysis program, and use an RNN to encode it into an emotion context vector $\\mathbf {e}$, which is combined with $\\mathbf {c}_t$ to produce the distribution. \n Question: How does the multi-turn dialog system learns?",
            "output": [
                "we extract the emotion information from the utterances in $\\mathbf {X}$ by leveraging an external text analysis program, and use an RNN to encode it into an emotion context vector $\\mathbf {e}$, which is combined with $\\mathbf {c}_t$ to produce the distribution"
            ]
        },
        {
            "id": "task460-905457472478450e8fe8f634d19d4afb",
            "input": "To evaluate the usefulness of our corpus for SMT purposes, we used it to train an automatic translator with Moses BIBREF8 . \n Question: What SMT models did they look at?",
            "output": [
                "automatic translator with Moses"
            ]
        },
        {
            "id": "task460-5149160dcbd6445ba4a757a50f785e8c",
            "input": "Following prior work BIBREF56 , BIBREF57 , BIBREF17 , we employ the MNCut spectral clustering algorithm BIBREF58 , which has wide applicability in similar NLP tasks which involve high-dimensional feature spaces BIBREF59 , BIBREF60 , BIBREF18 . \n Question: What clustering algorithm is used on top of the VerbNet-specialized representations?",
            "output": [
                "MNCut spectral clustering algorithm BIBREF58"
            ]
        },
        {
            "id": "task460-3d45c8738300459199dd1ce12c4adcdc",
            "input": " We show that using external knowledge outside the tweet text (from landing pages of URLs) and user features can significantly improve performance.  \n Question: What external sources of information are used?",
            "output": [
                "landing pages of URLs"
            ]
        },
        {
            "id": "task460-3d90aa722ab74403aaa645ba97f70700",
            "input": "Data collected by BIBREF3 , which we term the Sexist/Racist (SR) data set, was collected using an initial Twitter search followed by analysis and filtering by the authors and their team who identified 17 common phrases, hashtags, and users that were indicative of abusive speech. BIBREF4 collected the HATE dataset by searching for tweets using a lexicon provided by Hatebase.org. The final data set we used, which we call HAR, was collected by BIBREF9 ; we removed all retweets reducing the dataset to 20,000 tweets. \n Question: which datasets were used?",
            "output": [
                "Sexist/Racist (SR) data set HATE dataset HAR"
            ]
        },
        {
            "id": "task460-89c8e687f465402c91394b85b0561674",
            "input": " The Base is trained only on clean data while Base+Noise is trained on both the clean and the noisy data without noise handling. Global-CM uses a global confusion matrix for all noisy instances to model the noise as proposed by BIBREF3 and presented in Section SECREF3. The same architecture is used for Global-ID-CM, but the confusion matrix is initialized with the identity matrix (instead of Formula DISPLAY_FORM5) and only adapted during training.\n\nThe cluster-based models we propose in Section SECREF4 are Brown-CM and K-Means-CM. We experimented with numbers of clusters of 5, 10, 25 and 50. The models that select only the largest groups $G$ are marked as *-Freq and select either 30% or 50% of the clusters. The interpolation models have the postfix *-IP with $\\lambda \\in \\lbrace 0.3, 0.5, 0.7\\rbrace $ . The combination of both is named *-Freq-IP. As for all other hyperparameters, the choice was taken on the development set.\n\nWe implemented the Cleaning BIBREF15 and Dynamic-CM BIBREF14 models. Both were not developed for sequence labeling tasks and therefore needed to be adapted. For the Cleaning model, we followed the instructions by BIBREF3. The embedding and prediction components of the Dynamic-CM model were replaced according to our base model. The output of the dense layer was used as input to the dynamic matrix generation. We experimented with and without their proposed trace loss. \n Question: What is baseline used?",
            "output": [
                "Base  Base+Noise Cleaning  Dynamic-CM   Global-CM  Global-ID-CM Brown-CM   K-Means-CM"
            ]
        },
        {
            "id": "task460-347a442afd5a4ffcbd7f133ff9bb9b78",
            "input": "Self-Attention on Question. As the question has integrated previous utterances, the model needs to directly relate previously mentioned concept with the current question. This is helpful for concept carry-over and coreference resolution. We thus employ self-attention on question. \n Question: Does the model incorporate coreference and entailment?",
            "output": [
                "As the question has integrated previous utterances, the model needs to directly relate previously mentioned concept with the current question. This is helpful for concept carry-over and coreference resolution."
            ]
        },
        {
            "id": "task460-458457ce320f42d0a78510b352bc4935",
            "input": "In order to decode spatial connections between the electrodes from the channel covariance matrix, we use a CNN BIBREF19 , in particular a four-layered 2D CNN stacking two convolutional and two fully connected hidden layers. The INLINEFORM0 feature map at a given CNN layer with input INLINEFORM1 , weight matrix INLINEFORM2 and bias INLINEFORM3 is obtained as: INLINEFORM4 . At this first level of hierarchy, the network is trained with the corresponding labels as target outputs, optimizing a cross-entropy cost function. In parallel, we apply a four-layered recurrent neural network on the channel covariance matrices to explore the hidden temporal features of the electrodes. Namely, we exploit an LSTM BIBREF20 consisting of two fully connected hidden layers, stacked with two LSTM layers and trained in a similar manner as CNN. \n Question: How was the spatial aspect of the EEG signal computed?",
            "output": [
                "we use a CNN BIBREF19 , in particular a four-layered 2D CNN stacking two convolutional and two fully connected hidden layers."
            ]
        },
        {
            "id": "task460-e5335d77b8d4462db7cb11e7aa3a305c",
            "input": "For the NMT experiment, we used the Torch implementation to train a 2-layer LSTM model with 500 hidden units in both encoder and decoder, with 12 epochs. \n Question: Which NMT models did they experiment with?",
            "output": [
                "2-layer LSTM model with 500 hidden units in both encoder and decoder"
            ]
        },
        {
            "id": "task460-ef71e19fa4fd47f38cca09c7d0be153b",
            "input": "In later phase, phrase-based word embedding can be incorporated for improved vocabulary mapping. To get more accurate target code for each line, Abstract Syntax Tree(AST) can be beneficial. \n Question: What additional techniques could be incorporated to further improve accuracy?",
            "output": [
                "phrase-based word embedding Abstract Syntax Tree(AST)"
            ]
        },
        {
            "id": "task460-779c82b055204bcbb6864ef76a2fde35",
            "input": "It also needs to be noted that due to the issues in actual practice, the objective of this work is to generate candidate prescriptions to facilitate the prescribing procedure instead of substituting the human practitioners completely. \n Question: Why did they think this was a good idea?",
            "output": [
                "They think it will help human TCM practitioners make prescriptions."
            ]
        },
        {
            "id": "task460-ce36075ae34d4a14a4f78e58fd5dfb0b",
            "input": "The first phase is to extract a knowledge graph from the story that depicts locations, characters, objects, and the relations between these entities. We present two techniques. The first uses neural question-answering technique to extract relations from a story text. The second, provided as a baseline, uses OpenIE5, a commonly used rule-based information extraction technique. For the sake of simplicity, we considered primarily the location-location and location-character/object relations, represented by the “next to” and “has” edges respectively in Figure FIGREF4. We instead propose a new method that leverages models trained for context-grounded question-answering tasks to do entity extraction with no task dependent data or fine-tuning necessary. Our method, dubbed AskBERT, leverages the Question-Answering (QA) model ALBERT BIBREF15. AskBERT consists of two main steps as shown in Figure FIGREF7: vertex extraction and graph construction. The first step is to extract the set of entities—graph vertices—from the story. We are looking to extract information specifically regarding characters, locations, and objects. This is done by using asking the QA model questions such as “Who is a character in the story?”. BIBREF16 have shown that the phrasing of questions given to a QA model is important and this forms the basis of how we formulate our questions—questions are asked so that they are more likely to return a single answer, e.g. asking “Where is a location in the story?” as opposed to “Where are the locations in the story?”. In particular, we notice that pronoun choice can be crucial; “Where is a location in the story?” yielded more consistent extraction than “What is a location in the story?”. ALBERT QA is trained to also output a special <$no$-$answer$> token when it cannot find an answer to the question within the story. Our method makes use of this by iteratively asking QA model a question and masking out the most likely answer outputted on the previous step. This process continues until the <$no$-$answer$> token becomes the most likely answer. The first step is to extract the set of entities—graph vertices—from the story. We are looking to extract information specifically regarding characters, locations, and objects. This is done by using asking the QA model questions such as “Who is a character in the story?”. BIBREF16 have shown that the phrasing of questions given to a QA model is important and this forms the basis of how we formulate our questions—questions are asked so that they are more likely to return a single answer, e.g. asking “Where is a location in the story?” as opposed to “Where are the locations in the story?”. In particular, we notice that pronoun choice can be crucial; “Where is a location in the story?” yielded more consistent extraction than “What is a location in the story?”. ALBERT QA is trained to also output a special <$no$-$answer$> token when it cannot find an answer to the question within the story. Our method makes use of this by iteratively asking QA model a question and masking out the most likely answer outputted on the previous step. This process continues until the <$no$-$answer$> token becomes the most likely answer.\n\nThe next step is graph construction. Typical interactive fiction worlds are usually structured as trees, i.e. no cycles except between locations. Using this fact, we use an approach that builds a graph from the vertex set by one relation—or edge—at a time. Once again using the entire story plot as context, we query the ALBERT-QA model picking a random starting location $x$ from the set of vertices previously extracted.and asking the questions “What location can I visit from $x$?” and “Who/What is in $x$?”. The methodology for phrasing these questions follows that described for the vertex extraction. The answer given by the QA model is matched to the vertex set by picking the vertex $u$ that contains the best word-token overlap with the answer. Relations between vertices are added by computing a relation probability on the basis of the output probabilities of the answer given by the QA model. We compared our proposed AskBERT method with a non-neural, rule-based approach. This approach is based on the information extracted by OpenIE5, followed by some post-processing such as named-entity recognition and part-of-speech tagging. OpenIE5 combines several cutting-edge ideas from several existing papers BIBREF17, BIBREF18, BIBREF19 to create a powerful information extraction tools. For a given sentence, OpenIE5 generates multiple triples in the format of $\\langle entity, relation, entity\\rangle $ as concise representations of the sentence, each with a confidence score. These triples are also occasionally annotated with location information indicating that a triple happened in a location.\n\nAs in the neural AskBERT model, we attempt to extract information regarding locations, characters, and objects. The entire story plot is passed into the OpenIE5 and we receive a set of triples. The location annotations on the triples are used to create a set of locations. We mark which sentences in the story contain these locations. POS tagging based on marking noun-phrases is then used in conjunction with NER to further filter the set of triples—identifying the set of characters and objects in the story.\n\nThe graph is constructed by linking the set of triples on the basis of the location they belong to. While some sentences contain very explicit location information for OpenIE5 to mark it out in the triples, most of them do not. We therefore make the assumption that the location remains the same for all triples extracted in between sentences where locations are explicitly mentioned. For example, if there exists $location A$ in the 1st sentence and $location B$ in the 5th sentence of the story, all the events described in sentences 1-4 are considered to take place in $location A$. The entities mentioned in these events are connected to $location A$ in the graph. \n Question: How is the information extracted?",
            "output": [
                "neural question-answering technique to extract relations from a story text OpenIE5, a commonly used rule-based information extraction technique"
            ]
        },
        {
            "id": "task460-c6f43f12538644d28e7945d60967a9a0",
            "input": "BioASQ organizers provide the training and testing data. The training data consists of questions, gold standard documents, snippets, concepts, and ideal answers (which we did not use in this paper, but we used last year BIBREF2). The test data is split between phases A and B. The Phase A dataset consists of the questions, unique ids, question types. The Phase B dataset consists of the questions, golden standard documents, snippets, unique ids and question types. \n Question: What dataset did they use?",
            "output": [
                "BioASQ  dataset"
            ]
        },
        {
            "id": "task460-1b15736448534733b3aca8a71b442eaf",
            "input": "Traditional text-to-speech (TTS) systems are composed of complex pipelines BIBREF0 , these often include accoustic frontends, duration model, acoustic prediction model and vocoder models The open source LJSpeech Dataset was used to train our TTS model. The architecture of our model utilizes RNN-based Seq2Seq model for generating mel spectrogram from text \n Question: Which dataset(s) do they evaluate on?",
            "output": [
                "LJSpeech"
            ]
        },
        {
            "id": "task460-6fdc8125fdaa4583b4e092fafe2c4f24",
            "input": "For all experiments, the dimensions of word embeddings and recurrent hidden states are both set to 512. \n Question: Do they use the same architecture as LSTM-s and GRUs with just replacing with the LAU unit?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-feb1deba32fe4d88a46f145799eac5c9",
            "input": "A single speaker narrated the 2000 sentences, which took several days.  \n Question: How many annotators participated?",
            "output": [
                "1"
            ]
        },
        {
            "id": "task460-730ee74e14064243839ec9ddf767927e",
            "input": "We further use as features the embeddings of the claim, of the best-scoring snippet, and of the best-scoring sentence triplet from a Web page. We calculate these embeddings (i) as the average of the embeddings of the words in the text, and also (ii) using LSTM encodings, which we train for the task as part of a deep neural network (NN). We also use a task-specific embedding of the claim together with all the above evidence about it, which comes from the last hidden layer of the NN. \n Question: What algorithm and embedding dimensions are used to build the task-specific embeddings?",
            "output": [
                " task-specific embedding of the claim together with all the above evidence about it, which comes from the last hidden layer of the NN"
            ]
        },
        {
            "id": "task460-a781239fc9214bd08d36e2823154edd1",
            "input": "We present a comparison of the proposed models to existing word embeddings approaches. These are: the Bernoulli embeddings (b-emb) BIBREF1 , continuous bag-of-words (CBOW) BIBREF5 , Distributed Memory version of Paragraph Vector (PV-DM) BIBREF11 and the Global Vectors (GloVe) BIBREF6 model. \n Question: What word embeddings do they test?",
            "output": [
                "Bernoulli embeddings (b-emb) BIBREF1 , continuous bag-of-words (CBOW) BIBREF5 , Distributed Memory version of Paragraph Vector (PV-DM) BIBREF11 and the Global Vectors (GloVe) BIBREF6 model"
            ]
        },
        {
            "id": "task460-e32aec18764744e69c7f26522f263ca9",
            "input": "The summaries generated by our ILP-based approach were compared with the corresponding manual summaries using the ROUGE BIBREF22 unigram score. \n Question: What evaluation metrics were used for the summarization task?",
            "output": [
                "ROUGE BIBREF22 unigram score"
            ]
        },
        {
            "id": "task460-5f23e66365604153afbe08a39537f4ab",
            "input": "Our first task is the recently-introduced Visual Question Answering challenge (VQA) BIBREF22 . The next set of experiments we consider focuses on GeoQA, a geographical question-answering task first introduced by Krish2013Grounded. \n Question: What benchmark datasets they use?",
            "output": [
                "VQA and GeoQA"
            ]
        },
        {
            "id": "task460-fc8b89022ac64fb18e56d90d087b89e2",
            "input": "Stanford Sentiment Treebank Stanford Sentiment Treebank (SST) BIBREF14 . This concerns predicting movie review sentiment. Two datasets are derived from this corpus: (1) SST-1, containing five classes: very negative, negative, neutral, positive, and very positive. (2) SST-2, which has only two classes: negative and positive. For both, we remove phrases of length less than 4 from the training set. Subj BIBREF15 . The aim here is to classify sentences as either subjective or objective. This comprises 5000 instances of each. TREC BIBREF16 . A question classification dataset containing six classes: abbreviation, entity, description, human, location and numeric. There are 5500 training and 500 test instances. Irony BIBREF17 . This dataset contains 16,006 sentences from reddit labeled as ironic (or not). The dataset is imbalanced (relatively few sentences are ironic). Thus before training, we under-sampled negative instances to make classes sizes equal. Note that for this dataset we report the Area Under Curve (AUC), rather than accuracy, because it is imbalanced. \n Question: What dataset/corpus is this evaluated over?",
            "output": [
                " SST-1 SST-2 Subj  TREC  Irony "
            ]
        },
        {
            "id": "task460-e53a7c901c1e4c00b1c66de6e3821fa3",
            "input": "Different from previous work, we make our model conceptually simple and modular, so that the most important sub module, namely a five-character window context, can be pretrained using external data.  \n Question: What submodules does the model consist of?",
            "output": [
                "five-character window context"
            ]
        },
        {
            "id": "task460-7ed89ef7e186471e90787ba88812b6ea",
            "input": "To simulate the scenario, we split the Microsoft COCO dataset into food and non-food domain datasets. the food dataset has 3,806 images for training and 1,775 for validation. The non-food dataset has 78,976 images for training and 38,749 for validation. Adaptation to food domain captioning \n Question: How many examples are there in the source domain?",
            "output": [
                "78,976"
            ]
        },
        {
            "id": "task460-0d1df8b288eb48fd8ed5a68ce1a023a9",
            "input": "The focus of this publication is on representing information that is valuable for these tasks but that hitherto has largely been ignored in machine learning approaches centering around simplified language, specifically, text structure (e.g., paragraphs, lines), typography (e.g., font type, font style), and image (content, position, and dimensions) information. Information on the font type and font style (e.g., italics, bold print) of a token and its position on the physical page (for PDFs only) was specified as attributes to the token elements of the tokens layer (cf. Figure FIGREF34 for an example) \n Question: Which information about typography is included in the corpus?",
            "output": [
                "font type font style Information on the font type and font style (e.g., italics, bold print) of a token and its position on the physical page"
            ]
        },
        {
            "id": "task460-634df917c96645b2bef967f76762e010",
            "input": "The Style baseline achieves an Accuracy of 83%, LDA features 89% and a combination of the two gets 90%. However, training on full poems reduces this to 42—52%. \n Question: Is the outcome of the LDA analysis evaluated in any way?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-b8832904197f49ca92594d3e569f2dec",
            "input": "Cuneiform is an ancient writing system invented by the Sumerians for more than three millennia. \n Question: What is one of the first writing systems in the world?",
            "output": [
                "Cuneiform"
            ]
        },
        {
            "id": "task460-f2e79cb148b74769a64bcd31f4f90fb9",
            "input": "We collect a novel dataset of 230K+ recipe texts and 1M+ user interactions (reviews) over 18 years (2000-2018) from Food.com. \n Question: How did they obtain the interactions?",
            "output": [
                "from Food.com"
            ]
        },
        {
            "id": "task460-3bf298ac3efc43e884d0f5e9ac3f1517",
            "input": "Despite this, we found that the training of our embeddings was not considerably slower than the training of order-2 equivalents such as SGNS. Explicitly, our GPU trained CBOW vectors (using the experimental settings found below) in 3568 seconds, whereas training CP-S and JCP-S took 6786 and 8686 seconds respectively. \n Question: Do they measure computation time of their factorizations compared to other word embeddings?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-bdeae457bc26444db62689537326d5a0",
            "input": "The comparative evaluator is trained with maximum likelihood estimation (MLE) objective, as described in eq DISPLAY_FORM6\n\nwhere $\\mathcal {X}$ is the set of pairwise training examples contructed as described above, $Q(x_1, x_2) \\in \\lbrace >,<,\\approx \\rbrace $ is the true label for the pair ($x_1$, $x_2$), $D_\\phi ^q(x_1, x_2)$ is the probability of the comparative discriminator's prediction being $q$ ($q \\in \\lbrace >,<,\\approx \\rbrace $) for the pair ($x_1$, $x_2$). \n Question: How they add human prefference annotation to fine-tuning process?",
            "output": [
                "human preference annotation is available $Q(x_1, x_2) \\in \\lbrace >,<,\\approx \\rbrace $ is the true label for the pair"
            ]
        },
        {
            "id": "task460-b73991a56c59443fa3c393cff9846ebb",
            "input": "We create two models both of which constitutes of three main parts: encoder, interaction and classifier and take two sequences as input. The encoder is shared among the sequences simply uses two stacked GRU layers. The interaction part consists of only attention for one model while for the another one it consists of attention and conflict combined as shown in (eqn.11) . The classifier part is simply stacked fully-connected layers.  \n Question: Which neural architecture do they use as a base for their attention conflict mechanisms?",
            "output": [
                "GRU-based encoder, interaction block, and classifier consisting of stacked fully-connected layers."
            ]
        },
        {
            "id": "task460-b7ebaeffc6bf4556a26222f4a2b5301d",
            "input": "We use the Universal Dependencies' Hindi-English codemixed data set BIBREF9 to test the model's ability to label code-mixed data. This dataset is based on code-switching tweets of Hindi and English multilingual speakers. We use the Devanagari script provided by the data set as input tokens. \n Question: What codemixed language pairs are evaluated?",
            "output": [
                "Hindi-English"
            ]
        },
        {
            "id": "task460-fba77f9e6a0341dab689bd51ada1557f",
            "input": "We compare the performance of our Chinese word embedding vectors in the task of synonym discovery against another set of embedding vectors that was constructed with a co-occurrence model BIBREF1 . Our embeddings also proved to perform better than our benchmark dataset. \n Question: Does this approach perform better than context-based word embeddings?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-d16a631f66fe4bde9c3ff2248bc52f72",
            "input": "We evaluate our model on two benchmark datasets BIBREF9 . The homographic dataset contains 2,250 contexts, 1,607 of which contain a pun. The heterographic dataset consists of 1,780 contexts with 1,271 containing a pun. We notice there is no standard splitting information provided for both datasets. Thus we apply 10-fold cross validation. To make direct comparisons with prior studies, following BIBREF4 , we accumulated the predictions for all ten folds and calculate the scores in the end. \n Question: What datasets are used in evaluation?",
            "output": [
                "The homographic dataset contains 2,250 contexts, 1,607 of which contain a pun. The heterographic dataset consists of 1,780 contexts with 1,271 containing a pun."
            ]
        },
        {
            "id": "task460-ea23d7319ac14b8192bfac77c5eac312",
            "input": "We experiment with small feed-forward networks for four diverse NLP tasks: language identification, part-of-speech tagging, word segmentation, and preordering for statistical machine translation. \n Question: What NLP tasks do the authors evaluate feed-forward networks on?",
            "output": [
                "language identification, part-of-speech tagging, word segmentation, and preordering for statistical machine translation"
            ]
        },
        {
            "id": "task460-a12b501c7f3b49b79d667b64939ec796",
            "input": "We evaluate our models on two typical tasks: text classification and text semantic matching. \n Question: What tasks do they experiment with?",
            "output": [
                "text classification and text semantic matching"
            ]
        },
        {
            "id": "task460-2c58bbad2c6f4447bab75b673b5fa1c4",
            "input": "We model the ASP placement task as a successor of the AEP task. For all the `relevant' news entity pairs, the task is to determine the correct entity section. Each section in a Wikipedia entity page represents a different topic. For example, Barack Obama has the sections `Early Life', `Presidency', `Family and Personal Life' etc. Article-Section Ground-truth. The dataset consists of the triple INLINEFORM0 , where INLINEFORM1 , where we assume that INLINEFORM2 has already been determined as relevant. We therefore have a multi-class classification problem where we need to determine the section of INLINEFORM3 where INLINEFORM4 is cited.  \n Question: How do they determine the exact section to use the input article?",
            "output": [
                "They use a multi-class classifier to determine the section it should be cited"
            ]
        },
        {
            "id": "task460-1f191582515a4f73866c67d58e355b4e",
            "input": "We conduct all our experiments on 635hrs of audio data for 7 Indian languages collected from $\\textbf {All India Radio}$ news channel.  We collected and curated around 635Hrs of audio data for 7 Indian languages, namely Kannada, Hindi, Telugu, Malayalam, Bengali, and English. We collected the data from the All India Radio news channel where an actor will be reading news for about 5-10 mins. To cover many speakers for the dataset, we crawled data from 2010 to 2019. Since the audio is very long to train any deep neural network directly, we segment the audio clips into smaller chunks using Voice activity detector. Since the audio clips will have music embedded during the news, we use Inhouse music detection model to remove the music segments from the dataset to make the dataset clean and our dataset contains 635Hrs of clean audio which is divided into 520Hrs of training data containing 165K utterances and 115Hrs of testing data containing 35K utterances.  \n Question: How was the audio data gathered?",
            "output": [
                "Through the All India Radio new channel where actors read news."
            ]
        },
        {
            "id": "task460-fec35feb7cc24f648177c9a954939e2a",
            "input": "We use the BLEU BIBREF30 metric on the validation set for the VQG model training. BLEU is a measure of similitude between generated and target sequences of words, widely used in natural language processing. It assumes that valid generated responses have significant word overlap with the ground truth responses. We use it because in this case we have five different references for each of the generated questions. We obtain a BLEU score of 2.07.\n\nOur chatbot model instead, only have one reference ground truth in training when generating a sequence of words. We considered that it was not a good metric to apply as in some occasions responses have the same meaning, but do not share any words in common. Thus, we save several models with different hyperparameters and at different number of training iterations and compare them using human evaluation, to chose the model that performs better in a conversation. \n Question: How is performance of this system measured?",
            "output": [
                "using the BLEU score as a quantitative metric and human evaluation for quality"
            ]
        },
        {
            "id": "task460-c3e481012ade432dbb7240697a6c5af7",
            "input": "(proposed) Bi-LSTM/CRF + Bi-CharLSTM with modality attention (W+C): uses the modality attention to merge word and character embeddings.\n\n(proposed) Bi-LSTM/CRF + Bi-CharLSTM + Inception (W+C+V): takes as input visual contexts extracted from InceptionNet as well, concatenated with word and char vectors.\n\n(proposed) Bi-LSTM/CRF + Bi-CharLSTM + Inception with modality attention (W+C+V): uses the modality attention to merge word, character, and visual embeddings as input to entity LSTM. \n Question: Does their NER model learn NER from both text and images?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-326d4556029b4c068b940bfc4de9b82d",
            "input": "FarsNet [20] [21] is the first WordNet for Persian, developed by the NLP Lab at Shahid Beheshti University and it follows the same structure as the original WordNet. \n Question: What is the WordNet counterpart for Persian?",
            "output": [
                "FarsNet"
            ]
        },
        {
            "id": "task460-bd201ae29f3c4123aca957a0fa85612a",
            "input": "Here I would use Informap algorithm BIBREF12. To make a comparison to this method, I am using CCM and SCA for distance measurement in this experiment, too. UPGMA algorithm would be used accordingly in these two cases. \n Question: Is the proposed method compared to previous methods?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-c4de8b6ae96b479a82e5f614f534cf8e",
            "input": "Our Transformer Transducer model architecture has 18 audio and 2 label encoder layers. Every layer is identical for both audio and label encoders. The details of computations in a layer are shown in Figure FIGREF10 and Table TABREF11. All the models for experiments presented in this paper are trained on 8x8 TPU with a per-core batch size of 16 (effective batch size of 2048). \n Question: Does model uses pretrained Transformer encoders?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-fc9a3a0e8ae2428eb23659fae933b350",
            "input": "Unsupervised Evaluation\nThe unsupervised tasks include five tasks from SemEval Semantic Textual Similarity (STS) in 2012-2016 BIBREF30 , BIBREF31 , BIBREF32 , BIBREF33 , BIBREF34 and the SemEval2014 Semantic Relatedness task (SICK-R) BIBREF35 .\n\nThe cosine similarity between vector representations of two sentences determines the textual similarity of two sentences, and the performance is reported in Pearson's correlation score between human-annotated labels and the model predictions on each dataset.\n\nSupervised Evaluation\nIt includes Semantic relatedness (SICK) BIBREF35 , SemEval (STS-B) BIBREF36 , paraphrase detection (MRPC) BIBREF37 , question-type classification (TREC) BIBREF38 , movie review sentiment (MR) BIBREF39 , Stanford Sentiment Treebank (SST) BIBREF40 , customer product reviews (CR) BIBREF41 , subjectivity/objectivity classification (SUBJ) BIBREF42 , opinion polarity (MPQA) BIBREF43 . \n Question: How do they evaluate the sentence representations?",
            "output": [
                "The unsupervised tasks include five tasks from SemEval Semantic Textual Similarity (STS) in 2012-2016 BIBREF30 , BIBREF31 , BIBREF32 , BIBREF33 , BIBREF34 and the SemEval2014 Semantic Relatedness task (SICK-R) BIBREF35 .\n\nThe cosine similarity between vector representations of two sentences determines the textual similarity of two sentences, and the performance is reported in Pearson's correlation score between human-annotated labels and the model predictions on each dataset. Supervised Evaluation\nIt includes Semantic relatedness (SICK) BIBREF35 , SemEval (STS-B) BIBREF36 , paraphrase detection (MRPC) BIBREF37 , question-type classification (TREC) BIBREF38 , movie review sentiment (MR) BIBREF39 , Stanford Sentiment Treebank (SST) BIBREF40 , customer product reviews (CR) BIBREF41 , subjectivity/objectivity classification (SUBJ) BIBREF42 , opinion polarity (MPQA) BIBREF43 ."
            ]
        },
        {
            "id": "task460-888b1b40cb5d46bc83a0e8d734e0c51d",
            "input": "As we mention in the Section 2, CAEVO is the current state-of-the-art system for feature-based temporal event relation extraction BIBREF10 . It's widely used as the baseline for evaluating TB-Dense data. We adopt it as our baseline for evaluating CaTeRS and RED datasets. \n Question: What type of baseline are established for the two datasets?",
            "output": [
                "CAEVO"
            ]
        },
        {
            "id": "task460-e5797d71a295421c871ffa105349b72e",
            "input": "Results on LangID and NoLangID are compared to the system presented by deri2016grapheme, which is identified in our results as wFST. \n Question: what was the baseline?",
            "output": [
                "system presented by deri2016grapheme"
            ]
        },
        {
            "id": "task460-6eb1565d7b0d4a3594c0e9b9f86c22bd",
            "input": "To evaluate the responses generated by all compared methods, we compute the following automatic metrics on our test set:\n\nBLEU: BLEU-n measures the average n-gram precision on a set of reference responses. We report BLEU-n with n=1,2,3,4.\n\nDistinct-1 & distinct-2 BIBREF5: We count the numbers of distinct uni-grams and bi-grams in the generated responses and divide the numbers by the total number of generated uni-grams and bi-grams in the test set. These metrics can be regarded as an automatic metric to evaluate the diversity of the responses. \n Question: What automatic metrics are used?",
            "output": [
                "BLEU Distinct-1 & distinct-2"
            ]
        },
        {
            "id": "task460-f9d0a937832f4848b3f4f721e88eb84a",
            "input": "We use 10-fold cross-validation, and only two types of features: n-grams and Word2Vec word embeddings.  The n-gram features include unigrams, bigrams, and trigrams, including sequences of punctuation (for example, ellipses or \"!!!\"), and emoticons. We use GoogleNews Word2Vec features BIBREF28 . \n Question: What simple features are used?",
            "output": [
                "unigrams, bigrams, and trigrams, including sequences of punctuation Word2Vec word embeddings"
            ]
        },
        {
            "id": "task460-4b1f98fcfdce4b04955816ddb25e07da",
            "input": "The Flickr30K dataset BIBREF0 is a collection of over 30,000 images with 5 crowdsourced descriptions each. It is commonly used to train and evaluate neural network models that generate image descriptions (e.g. BIBREF2 ). This paper aims to give an overview of linguistic bias and unwarranted inferences resulting from stereotypes and prejudices. I will build on earlier work on linguistic bias in general BIBREF3 , providing examples from the Flickr30K data, and present a taxonomy of unwarranted inferences. Finally, I will discuss several methods to analyze the data in order to detect biases. \n Question: What is the size of the dataset?",
            "output": [
                "30,000"
            ]
        },
        {
            "id": "task460-d3bddfc04d764ef797ede65803c56808",
            "input": "For example, the original sentence `We went shop on Saturday' and the corrected version `We went shopping on Saturday' would produce the following pattern:\n\n(VVD shop_VV0 II, VVD shopping_VVG II)\n\nAfter collecting statistics from the background corpus, errors can be inserted into error-free text. The learned patterns are now reversed, looking for the correct side of the tuple in the input sentence. We only use patterns with frequency INLINEFORM0 , which yields a total of 35,625 patterns from our training data.  \n Question: What textual patterns are extracted?",
            "output": [
                "(VVD shop_VV0 II, VVD shopping_VVG II)"
            ]
        },
        {
            "id": "task460-e4c58237d58b4f18a0ecb6ec74d8bcc9",
            "input": "Hence, the contributions of this work are two-fold: (1) we develop a linguistically-infused neural network model to classify reactions in social media posts, and (2) we apply our model to label 10.8M Twitter posts and 6.2M Reddit comments in order to evaluate the speed and type of user reactions to various news sources. We develop a neural network architecture that relies on content and other linguistic signals extracted from reactions and parent posts, and takes advantage of a “late fusion” approach previously used effectively in vision tasks BIBREF13 , BIBREF14 . More specifically, we combine a text sequence sub-network with a vector representation sub-network as shown in Figure FIGREF5 . The text sequence sub-network consists of an embedding layer initialized with 200-dimensional GloVe embeddings BIBREF15 followed by two 1-dimensional convolution layers, then a max-pooling layer followed by a dense layer. The vector representation sub-network consists of two dense layers. We incorporate information from both sub-networks through concatenated padded text sequences and vector representations of normalized Linguistic Inquiry and Word Count (LIWC) features BIBREF16 for the text of each post and its parent. \n Question: What is the architecture of their model?",
            "output": [
                "we combine a text sequence sub-network with a vector representation sub-network as shown in Figure FIGREF5 . The text sequence sub-network consists of an embedding layer initialized with 200-dimensional GloVe embeddings BIBREF15 followed by two 1-dimensional convolution layers, then a max-pooling layer followed by a dense layer. The vector representation sub-network consists of two dense layers. We incorporate information from both sub-networks through concatenated padded text sequences and vector representations of normalized Linguistic Inquiry and Word Count (LIWC) features BIBREF16 for the text of each post and its parent."
            ]
        },
        {
            "id": "task460-8ed423e6656646deb7552abe2a893720",
            "input": "We use the Dialogue State Tracking Competition 2 (DSTC2) dataset BIBREF27 which is the most widely used dataset for research on task-oriented chatbots.  We also used two other datasets recently open-sourced by Google Research BIBREF28 which are M2M-sim-M (dataset in movie domain) and M2M-sim-R (dataset in restaurant domain).  \n Question: What are the three datasets used?",
            "output": [
                "DSTC2 M2M-sim-M M2M-sim-R"
            ]
        },
        {
            "id": "task460-5f692ce087a742a5b29f3b2d44a5c318",
            "input": "In general, the model transforms the segments into object-pairs by the TC-CNN and passes sentence through bi-GRU to obtain the global representation. Then we integrate object-pairs with global representation and make a pair-wise inference to detect the relationship among the segments. Ablation studies show that the proposed SCRN at the segment level has the capacity for relational reasoning and promotes the result significantly. \n Question: How is Relation network used to infer causality at segment level?",
            "output": [
                "we integrate object-pairs with global representation and make a pair-wise inference to detect the relationship among the segments"
            ]
        },
        {
            "id": "task460-c8df5461e638464d86ebff34fcf72f6e",
            "input": "We collected all available comments in the stories from Reddit from August 2015.  \n Question: what is the source of the new dataset?",
            "output": [
                "Reddit"
            ]
        },
        {
            "id": "task460-b2b83fb707c6446b93e8c2faaceffc74",
            "input": "To exemplify the INLINEFORM0 score we evaluated and compared the performance of two different SBD systems over a set of YouTube videos in a multi-reference enviroment. The first system (S1) employs a Convolutional Neural Network to determine if the middle word of a sliding window corresponds to a SU boundary or not BIBREF30 . The second approach (S2) by contrast, introduces a bidirectional Recurrent Neural Network model with attention mechanism for boundary detection BIBREF31 . \n Question: Which SBD systems did they compare?",
            "output": [
                "Convolutional Neural Network  bidirectional Recurrent Neural Network model with attention mechanism"
            ]
        },
        {
            "id": "task460-daeb98dd0db84df08bea7a16585a0cf7",
            "input": "Paraphrase Identification (PI) is the task of determining whether two sentences are paraphrase or not. We have implemented the cross-lingual variant of kernel functions for PI and RE tasks as described in section SECREF3 and measured the accuracy of models by testing them on the parallel data set. \n Question: What classification task was used to evaluate the cross-lingual adaptation method described in this work?",
            "output": [
                "Paraphrase Identification"
            ]
        },
        {
            "id": "task460-daeabded9bc745708fd6ff21519e738d",
            "input": "The system consists of: (i) Local, shallow features based mostly on orthographic, word shape and n-gram features plus their context; and (ii) three types of simple clustering features, based on unigram matching: (i) Brown BIBREF32 clusters, taking the 4th, 8th, 12th and 20th node in the path; (ii) Clark BIBREF33 clusters and, (iii) Word2vec BIBREF34 clusters, based on K-means applied over the extracted word vectors using the skip-gram algorithm. \n Question: What shallow local features are extracted?",
            "output": [
                " Local, shallow features based mostly on orthographic, word shape and n-gram features plus their context"
            ]
        },
        {
            "id": "task460-2a0ed3a7c7c24ca9abedc715c77abd9a",
            "input": "We report segmentation performance using precision, recall, and F-measure on boundaries (BP, BR, BF), and tokens (WP, WR, WF). We also report the exact-match (X) metric which computes the proportion of correctly segmented utterances. \n Question: How is the word segmentation task evaluated?",
            "output": [
                "precision, recall, and F-measure on boundaries (BP, BR, BF), and tokens (WP, WR, WF)  exact-match (X) metric"
            ]
        },
        {
            "id": "task460-413b59e9536148beb3d0cb880e156a68",
            "input": "For DQA four participants answered each question, therefore we took the average INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 values over the four evaluators as the result per question. \n Question: How do they measure performance?",
            "output": [
                "average INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 values"
            ]
        },
        {
            "id": "task460-10ba4f68736f40e59ad7d0236b528d79",
            "input": "Slot filling models are a useful method for simple natural language understanding tasks, where information can be extracted from a sentence and used to perform some structured action As candidate tasks, we consider the actions that a user might perform via apps on their phone. Crowd-sourced data was collected simulating common use cases for four different apps: United Airlines, Airbnb, Greyhound bus service and OpenTable. The corresponding actions are booking a flight, renting a home, buying bus tickets, and making a reservation at a restaurant. \n Question: What tasks are they experimenting with in this paper?",
            "output": [
                "Slot filling we consider the actions that a user might perform via apps on their phone The corresponding actions are booking a flight, renting a home, buying bus tickets, and making a reservation at a restaurant"
            ]
        },
        {
            "id": "task460-769a6c9e4fc84abe8bd96665cf2d4683",
            "input": "Results form Table TABREF31 may give an idea that INLINEFORM0 is just an scaled INLINEFORM1 . While it is true that they show a linear correlation, INLINEFORM2 may produce a different system ranking than INLINEFORM3 given the integral multi-reference principle it follows. However, what we consider the most profitable about INLINEFORM4 is the twofold inclusion of all available references it performs. First, the construction of INLINEFORM5 to provide a more inclusive reference against to whom be evaluated and then, the computation of INLINEFORM6 , which scales the result depending of the agreement between references. We showed how INLINEFORM0 is an inclusive metric which not only evaluates the performance of a system against all references, but also takes into account the agreement between them.  \n Question: What makes it a more reliable metric?",
            "output": [
                "It takes into account the agreement between different systems"
            ]
        },
        {
            "id": "task460-84370a1c68f246229386ae2024e19c74",
            "input": "Given that there is no public dataset available with financial intents in Portuguese, we have employed the incremental approach to create our own training set for the Intent Classifier. We have created domain-specific word vectors by considering a set 246,945 documents, corresponding to of 184,001 Twitter posts and and 62,949 news articles, all related to finance . \n Question: What datasets are used?",
            "output": [
                "Custom dataset with user questions; set of documents, twitter posts and news articles, all related to finance."
            ]
        },
        {
            "id": "task460-d34598f43fad49b986710ac7d9b31c0c",
            "input": "Results on all sets show an important difference between precision and recall, precision being significantly higher than recall. There is also a significant difference between the results obtained on development and test set (F1 = 89.60, F1 = 87.82) and the results on the supplemental test set (F1 = 71.49). The time difference between the supplemental test set and the development and test set (the headlines from the the supplemental test set being from a different time period to the training set) can probably explain these differences. \n Question: What is the performance of the CRF model on the task described?",
            "output": [
                "the results obtained on development and test set (F1 = 89.60, F1 = 87.82) and the results on the supplemental test set (F1 = 71.49)"
            ]
        },
        {
            "id": "task460-60c7bfe377fa4b4e91ee757f08074610",
            "input": "Although our approach strongly outperforms random baselines, the relatively low F1 scores indicate that predicting which word is echoed in explanations is a very challenging task. It follows that we are only able to derive a limited understanding of how people choose to echo words in explanations. The extent to which explanation construction is fundamentally random BIBREF47, or whether there exist other unidentified patterns, is of course an open question. We hope that our study and the resources that we release encourage further work in understanding the pragmatics of explanations. \n Question: Do authors provide any explanation for intriguing patterns of word being echoed?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-cd3e4937dafc4fe68bcad3b0955ef8a6",
            "input": "Multi-task Pairwise Neural Ranking\nWe propose a multi-task pairwise neural ranking approach to better incorporate and distinguish the relative order between the candidate segmentations of a given hashtag. Our model adapts to address single- and multi-token hashtags differently via a multi-task learning strategy without requiring additional annotations. In this section, we describe the task setup and three variants of pairwise neural ranking models (Figure FIGREF11 ). Pairwise Neural Ranking Model Margin Ranking (MR) Loss Adaptive Multi-task Learning \n Question: What set of approaches to hashtag segmentation are proposed?",
            "output": [
                "Adaptive Multi-task Learning\n Margin Ranking (MR) Loss\n Pairwise Neural Ranking Model\n"
            ]
        },
        {
            "id": "task460-d387e1058612422f9932b12323500cc9",
            "input": "In ConvE, only $v_h$ and $v_r$ are reshaped and then concatenated into an input matrix which is fed to the convolution layer \n Question: Did the authors try stacking multiple convolutional layers?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-99b9db333db849938e380990b40eddff",
            "input": "To identify the most suitable classifier for classifying the scalars associated with each text, we perform evaluations using the stochastic gradient descent, naive bayes, decision tree, and random forest classifiers. \n Question: What was the baseline?",
            "output": [
                "stochastic gradient descent, naive bayes, decision tree"
            ]
        },
        {
            "id": "task460-5c7bb419888649dc948e1bd6846d08b7",
            "input": "In order to obtain a large number of Brazilian music lyrics, we created a crawler to navigate into the Vagalume website, extracting, for each musical genre, all the songs by all the listed authors. \n Question: what is the source of the song lyrics?",
            "output": [
                "Vagalume website"
            ]
        },
        {
            "id": "task460-99f8e156d0844cfe8927991d9a3860f1",
            "input": "Our data are collected from the yelp academic dataset, provided by Yelp.com, a popular restaurant review website. The data set contains three types of objects: business, user, and review, where business objects contain basic information about local businesses (i.e. restaurants), review objects contain review texts and star rating, and user objects contain aggregate information about a single user across all of Yelp. Table TABREF31 illustrates the general statistics of the dataset. \n Question: Does they focus on any specific product/service domain?",
            "output": [
                "local businesses (i.e. restaurants)"
            ]
        },
        {
            "id": "task460-117f0322f5994eaebdbd9e4598851f22",
            "input": "Following titovcrosslingual, we run our experiments on the English (EN) and German (DE) sections of the CoNLL 2009 corpus BIBREF13 , and EN-DE section of the Europarl corpus BIBREF14 . \n Question: Which parallel corpora are used?",
            "output": [
                "English (EN) and German (DE) sections of the CoNLL 2009 corpus BIBREF13 EN-DE section of the Europarl corpus BIBREF14"
            ]
        },
        {
            "id": "task460-b6335da0f90f4381acac0069a3ef2485",
            "input": "KGR10, also known as plWordNet Corpus 10.0 (PLWNC 10.0), is the result of the work on the toolchain to automatic acquisition and extraction of the website content, called CorpoGrabber BIBREF19 . It is a pipeline of tools to get the most relevant content of the website, including all subsites (up to the user-defined depth). The proposed toolchain can be used to build a big Web corpus of text documents. It requires the list of the root websites as the input. Tools composing CorpoGrabber are adapted to Polish, but most subtasks are language independent. The whole process can be run in parallel on a single machine and includes the following tasks: download of the HTML subpages of each input page URL with HTTrack, extraction of plain text from each subpage by removing boilerplate content (such as navigation links, headers, footers, advertisements from HTML pages) BIBREF20 , deduplication of plain text BIBREF20 , bad quality documents removal utilising Morphological Analysis Converter and Aggregator (MACA) BIBREF21 , documents tagging using Wrocław CRF Tagger (WCRFT) BIBREF22 . Last two steps are available only for Polish. \n Question: How was the KGR10 corpus created?",
            "output": [
                "most relevant content of the website, including all subsites"
            ]
        },
        {
            "id": "task460-1c4396f5da434da7bf62551111159957",
            "input": "The Y-axis in the figure is the success rate of the agent (measured in terms of number of dialogs that resulted in launching a skill divided by total number of dialogs), and the X-axis is the number of learning steps.  \n Question: How did they measure effectiveness?",
            "output": [
                "number of dialogs that resulted in launching a skill divided by total number of dialogs"
            ]
        },
        {
            "id": "task460-7619b0068eed43e59f6a119a6bc0a1aa",
            "input": "We used dataset, HEOT obtained from one of the past studies done by Mathur et al. where they annotated a set of cleaned tweets obtained from twitter for the conversations happening in Indian subcontinent. A labelled dataset for a corresponding english tweets were also obtained from a study conducted by Davidson et al. \n Question: What dataset is used?",
            "output": [
                "HEOT  A labelled dataset for a corresponding english tweets ",
                "HEOT  A labelled dataset for a corresponding english tweets"
            ]
        },
        {
            "id": "task460-1b600e1e66dd4211a684858179db1c8d",
            "input": "We extracted 200 sentence pairs from BIBREF3 's dataset and provided each pair with a document context consisting of a preceding and a following sentence, as in the following example. \n Question: What document context was added?",
            "output": [
                "Preceding and following sentence of each metaphor and paraphrase are added as document context"
            ]
        },
        {
            "id": "task460-5552a1674272430fa49edcda50f31739",
            "input": "In this section, we analyze the Hamming distance between the projections of the sentences from the enwik9 dataset and the corresponding projections of the same sentences after applying character level perturbations. We experiment with three types of character level perturbation BIBREF11 and two types of word level perturbation operations.\n\nPerturbation Study ::: Character Level Perturbation Operations\ninsert(word, n) : We randomly choose n characters from the character vocabulary and insert them at random locations into the input word. We however retain the first and last characters of the word as is. Ex. transformation: $sample \\rightarrow samnple$.\n\nswap(word, n): We randomly swap the location of two characters in the word n times. As with the insert operation, we retain the first and last characters of the word as is and only apply the swap operation to the remaining characters. Ex. transformation: $sample \\rightarrow sapmle$.\n\nduplicate(word, n): We randomly duplicate a character in the word by n times. Ex. transformation: $sample \\rightarrow saample$.\n\nPerturbation Study ::: Character Level Perturbation Operations ::: Word Level Perturbation Operations\ndrop(sentence, n): We randomly drop n words from the sentence. Ex. transformation: This is a big cat. $\\rightarrow $ This is a cat.\n\nduplicate(sentence, n): Similar to duplicate(word, n) above, we randomly duplicate a word in the sentence n times. Ex. transformation: This is a big cat. $\\rightarrow $ This is a big big cat.\n\nswap(sentence, n): Similar to swap(word, n), we randomly swap the location of two words in the sentence n times. Ex. transformation: This is a big cat. $\\rightarrow $ This cat is big. \n Question: How does their perturbation algorihm work?",
            "output": [
                "same sentences after applying character level perturbations"
            ]
        },
        {
            "id": "task460-536f784f8bae47bea450f55693a8eb77",
            "input": "Methods ::: Length Encoding Method\nInspired by BIBREF11, we use length encoding to provide the network with information about the remaining sentence length during decoding. We propose two types of length encoding: absolute and relative. Let pos and len be, respectively, a token position and the end of the sequence, both expressed in terms of number characters. Then, the absolute approach encodes the remaining length:\n\nwhere $i=1,\\ldots ,d/2$.\n\nSimilarly, the relative difference encodes the relative position to the end. This representation is made consistent with the absolute encoding by quantizing the space of the relative positions into a finite set of $N$ integers:\n\nwhere $q_N: [0, 1] \\rightarrow \\lbrace 0, 1, .., N\\rbrace $ is simply defined as $q_N(x) = \\lfloor {x \\times N}\\rfloor $. As we are interested in the character length of the target sequence, len and pos are given in terms of characters, but we represent the sequence as a sequence of BPE-segmented subwords BIBREF17. To solve the ambiguity, len is the character length of the sequence, while pos is the character count of all the preceding tokens. \n Question: How do they enrich the positional embedding with length information",
            "output": [
                "They introduce new trigonometric encoding which besides information about position uses additional length information (abs or relative)."
            ]
        },
        {
            "id": "task460-9b391b2d61924d0ca95d55b87a003dec",
            "input": "Advanced neural architectures based on character input (CNNs, BPE, etc) are supposed to be able to learn how to handle spelling and morphology variations themselves, even for languages with rich morphology: `just add more layers!'. Contextualised embedding models follow this tradition: as a rule, they are trained on raw text collections, with minimal linguistic pre-processing. Below, we show that this is not entirely true. \n Question: Why is lemmatization not necessary in English?",
            "output": [
                "Advanced neural architectures and contextualized embedding models learn how to handle spelling and morphology variations."
            ]
        },
        {
            "id": "task460-2ddd590d02de4238b3fc35008e540e57",
            "input": " We opted for an automatic approach instead, that can be scaled arbitrarily and at little cost: we generate synthetic sentence pairs $(, \\tilde{})$ by randomly perturbing 1.8 million segments $$ from Wikipedia. We use three techniques: mask-filling with BERT, backtranslation, and randomly dropping out words.  \n Question: How are the synthetic examples generated?",
            "output": [
                "Random perturbation of Wikipedia sentences using mask-filling with BERT, backtranslation and randomly drop out"
            ]
        },
        {
            "id": "task460-898fbcad6406484f9e83e4bd76caab1d",
            "input": "For both datasets, we follow the evaluation metrics used in the original evaluation tasks BIBREF13 . For DBQA, P@1 (Precision@1), MAP (Mean Average Precision) and MRR (Mean Reciprocal Rank) are adopted. For KBRE, since only one golden candidate is labeled for each question, only P@1 and MRR are used. \n Question: Which metrics do they use to evaluate matching?",
            "output": [
                "Precision@1 Mean Average Precision Mean Reciprocal Rank"
            ]
        },
        {
            "id": "task460-fc318081d9a640358e8ad7bf89a92327",
            "input": "Note that GANE-AP delivers better results compared with GANE-OT, suggesting the attention parsing mechanism can further improve the low-level mutual attention matrix. \n Question: Which of their proposed attention methods works better overall?",
            "output": [
                "attention parsing"
            ]
        },
        {
            "id": "task460-005020b3c66a4e87a221b7a5e523d0c7",
            "input": "We use TB-Dense and MATRES in our experiments and briefly summarize the data statistics in Table TABREF33. \n Question: What datasets were used for this work?",
            "output": [
                "TB-Dense  MATRES"
            ]
        },
        {
            "id": "task460-4016f5f7b5cf4bdfb8866b9f7116a5f7",
            "input": "WikiSmall consists of ordinary and simplified sentences from the ordinary and simple English Wikipedias, which has been used as benchmark for evaluating text simplification BIBREF17 , BIBREF18 , BIBREF8 . The training set has 89,042 sentence pairs, and the test set has 100 pairs. WikiLarge is also from Wikipedia corpus whose training set contains 296,402 sentence pairs BIBREF19 , BIBREF20 . WikiLarge includes 8 (reference) simplifications for 2,359 sentences split into 2,000 for development and 359 for testing. \n Question: what are the sizes of both datasets?",
            "output": [
                "training set has 89,042 sentence pairs, and the test set has 100 pairs training set contains 296,402 2,000 for development and 359 for testing"
            ]
        },
        {
            "id": "task460-f4067d9c7b4d4fb0bfd74a8e344a866f",
            "input": "Another problem we face is that the BERT model released by Google is trained only on Arabic Wikipedia, which is almost exclusively Modern Standard Arabic (MSA). The shared task dataset contains 5,030 tweets related to different political issues and events in the Middle East taking place between 2011 and 2018. Tweets involve both MSA as well as dialects at various degrees of granularity such as Egyptian, Gulf, and Levantine. \n Question: What dialect is used in the Google BERT model and what is used in the task data?",
            "output": [
                "Modern Standard Arabic (MSA) MSA as well as dialects at various degrees of granularity such as Egyptian, Gulf, and Levantine"
            ]
        },
        {
            "id": "task460-c1278469e7374b6da085a6f75f5aa3a3",
            "input": "Using a set of semantically oriented tasks that require explicit semantic cross-lingual representations, we showed that mBERT contextual embeddings do not represent similar semantic phenomena similarly and therefore they are not directly usable for zero-shot cross-lingual tasks. \n Question: What challenges this work presents that must be solved to build better language-neutral representations?",
            "output": [
                "contextual embeddings do not represent similar semantic phenomena similarly and therefore they are not directly usable for zero-shot cross-lingual tasks"
            ]
        },
        {
            "id": "task460-1b716e77308244e39cd312f282e87680",
            "input": "The final annotation categories for the dataset are: Joy, Sadness, Anger, Fear, Anticipation, Surprise, Love, Disgust, Neutral \n Question: How many emotions do they look at?",
            "output": [
                "9"
            ]
        },
        {
            "id": "task460-1c6aea98aa27428a8514c24dfc457622",
            "input": "As a first baseline we use Bag-of-Words, a well-known and robust text representations used in various domains BIBREF21 , combined with a standard shallow classifier, namely, a Support Vector Machine with linear kernel. We used LIBSVM implementation of SVM. \n Question: Which shallow approaches did they experiment with?",
            "output": [
                "SVM"
            ]
        },
        {
            "id": "task460-5a41cdbf9b19436ea8ec170d46beb54b",
            "input": "Among all single models, LFT performs the best, followed by MinAvgOut. RL is also comparable with previous state-of-the-art models VHRED (attn) and Reranking-RL. We think that this is because LFT exerts no force in pulling the model predictions away from the ground-truth tokens, but rather just makes itself aware of how dull each response is. Consequently, its responses appear more relevant than the other two approaches. Moreover, the hybrid model (last row) outperforms all other models by a large margin.  \n Question: Which one of the four proposed models performed best?",
            "output": [
                "the hybrid model MinAvgOut + RL"
            ]
        },
        {
            "id": "task460-559557b478a84cc68ce47e234f4dc01f",
            "input": "The task is set up to mimic (albeit, in an oversimplified manner) the input-output symbol alignments and local syntactic properties that models must learn in many natural language tasks, such as translation, tagging and summarization. \n Question: Why does the proposed task a good proxy for the general-purpose sequence to sequence tasks?",
            "output": [
                "The task is set up to mimic (albeit, in an oversimplified manner) the input-output symbol alignments and local syntactic properties that models must learn in many natural language tasks, such as translation, tagging and summarization."
            ]
        },
        {
            "id": "task460-90a8bcb786dc482cacdc058318e404b3",
            "input": "We first investigated overall detection of any NMT-Fake reviews (1,006 fake reviews and 994 real reviews). \n Question: How many reviews in total (both generated and true) do they evaluate on Amazon Mechanical Turk?",
            "output": [
                "1,006 fake reviews and 994 real reviews"
            ]
        },
        {
            "id": "task460-1357be86b53d4c739ed42fb4802adf51",
            "input": "We evaluate our model on two benchmark treebanks, English Penn Treebank (PTB) and Chinese Penn Treebank (CTB5.1) following standard data splitting BIBREF30, BIBREF31. POS tags are predicted by the Stanford Tagger BIBREF32. For constituent parsing, we use the standard evalb tool to evaluate the F1 score. For dependency parsing, we apply Stanford basic dependencies (SD) representation BIBREF4 converted by the Stanford parser. Following previous work BIBREF27, BIBREF33, we report the results without punctuations for both treebanks. Tables TABREF17, TABREF18 and TABREF19 compare our model to existing state-of-the-art, in which indicator Separate with our model shows the results of our model learning constituent or dependency parsing separately, (Sum) and (Concat) respectively represent the results with the indicated input token representation setting. On PTB, our model achieves 93.90 F1 score of constituent parsing and 95.91 UAS and 93.86 LAS of dependency parsing. On CTB, our model achieves a new state-of-the-art result on both constituent and dependency parsing. At last, our model is evaluated on two benchmark treebanks for both constituent and dependency parsing. The empirical results show that our parser reaches new state-of-the-art for all parsing tasks. \n Question: How is dependency parsing empirically verified?",
            "output": [
                " At last, our model is evaluated on two benchmark treebanks for both constituent and dependency parsing. The empirical results show that our parser reaches new state-of-the-art for all parsing tasks."
            ]
        },
        {
            "id": "task460-da14853b93354b32b0d4fd2f91a05036",
            "input": "Knowledge Base Question Answering (KBQA) systems answer questions by obtaining information from KB tuples BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 \n Question: What does KBQA abbreviate for",
            "output": [
                "Knowledge Base Question Answering"
            ]
        },
        {
            "id": "task460-15e50ac042c7402eb70cba7235c64681",
            "input": "After using Kytea, Japanese texts are applied LSW algorithm to replace OOV words by their synonyms.  \n Question: Are synonymous relation taken into account in the Japanese-Vietnamese task?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-ba6482a812964dd5a4b242d504774bcb",
            "input": "Detection: In hypernymy detection, the task is to classify whether pairs of words are in a hypernymy relation. For this task, we evaluate all models on five benchmark datasets: First, we employ the noun-noun subset of bless, which contains hypernymy annotations for 200 concrete, mostly unambiguous nouns. Second, we evaluate on leds BIBREF13 , which consists of 2,770 noun pairs balanced between positive hypernymy examples, and randomly shuffled negative pairs. Direction: In direction prediction, the task is to identify which term is broader in a given pair of words. For this task, we evaluate all models on three datasets described by BIBREF16 : On bless, the task is to predict the direction for all 1337 positive pairs in the dataset. Pairs are only counted correct if the hypernymy direction scores higher than the reverse direction, i.e. INLINEFORM0 . We reserve 10% of the data for validation, and test on the remaining 90%. On wbless, we follow prior work BIBREF17 , BIBREF18 and perform 1000 random iterations in which 2% of the data is used as a validation set to learn a classification threshold, and test on the remainder of the data. We report average accuracy across all iterations. Finally, we evaluate on bibless BIBREF16 , a variant of wbless with hypernymy and hyponymy pairs explicitly annotated for their direction. Graded Entailment: In graded entailment, the task is to quantify the degree to which a hypernymy relation holds. For this task, we follow prior work BIBREF19 , BIBREF18 and use the noun part of hyperlex BIBREF20 , consisting of 2,163 noun pairs which are annotated to what degree INLINEFORM0 is-a INLINEFORM1 holds on a scale of INLINEFORM2 . \n Question: Which benchmark datasets are used?",
            "output": [
                "noun-noun subset of bless leds BIBREF13 bless wbless bibless hyperlex BIBREF20"
            ]
        },
        {
            "id": "task460-d6138ca58f6c454393d2ad8817910bf1",
            "input": "We evaluate our model on a publicly available dataset, KARA ONE BIBREF17 , composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw). The dataset consists of 14 participants, with each prompt presented 11 times to each individual.  \n Question: How many subjects does the EEG data come from?",
            "output": [
                "14"
            ]
        },
        {
            "id": "task460-f30add8f8f5741cea75fcc52c2e59803",
            "input": "A community with a very distinctive identity will tend to have distinctive interests, expressed through specialized language. Formally, we define the distinctiveness of a community INLINEFORM0 as the average specificity of all utterances in INLINEFORM1  \n Question: How do the authors measure how distinctive a community is?",
            "output": [
                " the average specificity of all utterances"
            ]
        },
        {
            "id": "task460-d3db449f8e204d92b2dcf8de067df332",
            "input": "Because our conversion rules are interpretable, we identify shortcomings in both resources, using each as validation for the other. We were able to find specific instances of incorrectly applied UniMorph annotation, as well as specific instances of cross-lingual inconsistency in both resources. \n Question: Do they look for inconsistencies between different languages' annotations in UniMorph?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-e675531f55e342229eaac4c8db0741f6",
            "input": "We evaluate a character-level variant of our proposed language model over a preprocessed version of the Penn Treebank (PTB) and Text8 datasets. The unsupervised constituency parsing task compares hte tree structure inferred by the model with those annotated by human experts. The experiment is performed on WSJ10 dataset. \n Question: Which dataset do they experiment with?",
            "output": [
                "Penn Treebank Text8 WSJ10"
            ]
        },
        {
            "id": "task460-529b5a6a556b47278b42e1674eabf396",
            "input": "In this section, we present the details of the analysis performed on the data obtained pertaining to Twitter messages from January 2020 upto now, that is the time since the news of the Coronavirus outbreak in China was spread across nations. Also, the data prominently captures the tweets in English, Spanish, and French languages. \n Question: Do they collect only English data?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-e2567a17b2a3451db684f8eec44c38ec",
            "input": "Our findings paves the way for research in several new directions. While we probed the effectiveness of virtual edges in a specific text classification task, we could extend this approach for general classification tasks. A systematic comparison of embeddings techniques could also be performed to include other recent techniques BIBREF54, BIBREF55. We could also identify other relevant techniques to create virtual edges, allowing thus the use of the methodology in other networked systems other than texts. For example, a network could be enriched with embeddings obtained from graph embeddings techniques. A simpler approach could also consider link prediction BIBREF56 to create virtual edges. Finally, other interesting family of studies concerns the discrimination between co-occurrence and virtual edges, possibly by creating novel network measurements considering heterogeneous links. \n Question: What other natural processing tasks authors think could be studied by using word embeddings?",
            "output": [
                "general classification tasks use of the methodology in other networked systems a network could be enriched with embeddings obtained from graph embeddings techniques"
            ]
        },
        {
            "id": "task460-9b976c2453b244508ef66e146fefb91f",
            "input": " In this work, we develop a technique to rapidly transfer an existing pre-trained model from English to other languages in an energy efficient way BIBREF8. As the first step, we focus on building a bilingual language model (LM) of English and a target language. Starting from a pre-trained English LM, we learn the target language specific parameters (i.e., word embeddings), while keeping the encoder layers of the pre-trained English LM fixed. We then fine-tune both English and target model to obtain the bilingual LM.  \n Question: How is the model transferred to other languages?",
            "output": [
                "Build a bilingual language model,   learn the target language specific parameters starting from a pretrained English LM , fine-tune both English and target model to obtain the bilingual LM."
            ]
        },
        {
            "id": "task460-f9b8b4976bf643959aece8792053d51c",
            "input": "We define hateful speech to be the language which explicitly or implicitly threatens or demeans a person or a group based upon a facet of their identity such as gender, ethnicity, or sexual orientation. \n Question: What is their definition of hate speech?",
            "output": [
                "language which explicitly or implicitly threatens or demeans a person or a group based upon a facet of their identity such as gender, ethnicity, or sexual orientation"
            ]
        },
        {
            "id": "task460-2c99a9bedf3f443d819cc473b64aecd2",
            "input": "According the distribution of the sentiment score, the sentiment on Tesla is slightly skewed towards positive during the testing period. \n Question: Do the authors give any examples of major events which draw the public's attention and the impact they have on stock price?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-847e7b91f27a4509a4f51e442650f319",
            "input": "Considering the improvements over the majority baseline achieved by the RNN model for both non-English (on the average 22.76% relative improvement; 15.82% relative improvement on Spanish, 72.71% vs. 84.21%, 30.53% relative improvement on Turkish, 56.97% vs. 74.36%, 37.13% relative improvement on Dutch, 59.63% vs. 81.77%, and 7.55% relative improvement on Russian, 79.60% vs. 85.62%) and English test sets (27.34% relative improvement), we can draw the conclusion that our model is robust to handle multiple languages. \n Question: which non-english language was the had the worst results?",
            "output": [
                "Turkish"
            ]
        },
        {
            "id": "task460-7d8c977f0d5b43999f6ec88822d3a15f",
            "input": "Tweets IDs 6, 8, and 10 are some samples containing offensive words and slurs which arenot hate or offensive in all cases and writers of them used this type of language in their daily communications. Given these pieces of evidence, by considering the content of tweets, we can see in tweets IDs 3, 4, and 9 that our BERT-based classifier can discriminate tweets in which neither and implicit hatred content exist. \n Question: What evidence do the authors present that the model can capture some biases in data annotation and collection?",
            "output": [
                "The authors showed few tweets where neither and implicit hatred content exist but the model was able to discriminate"
            ]
        },
        {
            "id": "task460-63d9983c7ffe463e8af35a703be9ba77",
            "input": "WAS: The architecture used in BIBREF3 without the audio input. The decoder output Chinese character at each timestep. Others keep unchanged to the original implementation. LipCH-Net-seq: For a fair comparison, we use sequence-to-sequence with attention framework to replace the Connectionist temporal classification (CTC) loss BIBREF14 used in LipCH-Net BIBREF5 when converting picture to pinyin. CSSMCM-w/o video: To evaluate the necessity of video information when predicting tone, the video stream is removed when predicting tone and Chinese characters. In other word, video is only used when predicting the pinyin sequence. The tone is predicted from the pinyin sequence. Tone information and pinyin information work together to predict Chinese character. \n Question: What was the previous state of the art model for this task?",
            "output": [
                "WAS LipCH-Net-seq CSSMCM-w/o video"
            ]
        },
        {
            "id": "task460-7f7d4fa87310426b90422ef3dc2cf284",
            "input": "We introduce a new model for difficulty prediction combining learned representations induced via a pre-trained `universal' sentence encoder BIBREF6 , and a sentence encoder learned from scratch for this task. In total, this resulted in 57,505 and 2,428 sentences in the train and test set abstracts, respectively. \n Question: How much data is needed to train the task-specific encoder?",
            "output": [
                "57,505 sentences"
            ]
        },
        {
            "id": "task460-9de6fe99c46a45a5976c98cf6272e3c6",
            "input": "This metric is used to evaluate the accuracy of dialogue belief tracker BIBREF1.\n\nAPRA: Action Per-Response Accuracy (APRA) evaluates the per-turn accuracy of the dialogue actions generated by dialogue policy maker. For baselines, APRA evaluates the classification accuracy of the dialogue policy maker. But our model actually generates each individual token of actions, and we consider a prediction to be correct only if every token of the model output matches the corresponding token in the ground truth.\n\nBLEU BIBREF19: The metric evaluates the quality of the final response generated by natural language generator. The metric is usually used to measure the performance of the task-oriented dialogue system. \n Question: What metrics are used to measure performance of models?",
            "output": [
                "BPRA APRA BLEU"
            ]
        },
        {
            "id": "task460-450c06ca958441118b2f7ed36804f3df",
            "input": "We are in line with recent work BIBREF16 , proposing to shift evaluation from absolute values to more exploratory evaluations focusing on weaknesses and strengths of the embeddings and not so much in generic scores. For example, one metric could consist in checking whether for any given word, all words that are known to belong to the same class are closer than any words belonging to different classes, independently of the actual cosine. \n Question: What new metrics are suggested to track progress?",
            "output": [
                " For example, one metric could consist in checking whether for any given word, all words that are known to belong to the same class are closer than any words belonging to different classes, independently of the actual cosine"
            ]
        },
        {
            "id": "task460-2d401a9ef5d84ff9b6306fdb81a64941",
            "input": "However, we also showed difference in produced errors for each method and different impact at word-level depending of the approach or units. Thus, future work will focus on analysing the orthographic output of these systems in two ways: 1) investigate errors produced by the end-to-end methods and explore several approaches to correct common errors done in French and 2) compare the end-to-end methods in a SLU context and evaluate the semantic value of the partially correct produced words. \n Question: What will be in focus for future work?",
            "output": [
                "1) investigate errors produced by the end-to-end methods and explore several approaches to correct common errors done in French 2) compare the end-to-end methods in a SLU context and evaluate the semantic value of the partially correct produced words"
            ]
        },
        {
            "id": "task460-4b7efc33d73c4fb4b79d016a226df507",
            "input": "Adhering to the experimental settings of BIBREF1 , we concatenate sentence representations produced from our model with those obtained from the state-of-the-art unsupervised learning model (Layer Normalized Skip-Thoughts, ST-LN) BIBREF31 . Following the experimental design of BIBREF1 , we conduct experiments on three different learning objectives: Cap2All, Cap2Cap, Cap2Img. \n Question: What baselines are the proposed method compared against?",
            "output": [
                "(Layer Normalized Skip-Thoughts, ST-LN) BIBREF31 Cap2All, Cap2Cap, Cap2Img"
            ]
        },
        {
            "id": "task460-7455189984eb4f61826d5ef9cfa2b8c6",
            "input": "In the first scenario we equip the decoder with an additional morphology table including target-side affixes. We place an attention module on top of the table which is controlled by the decoder. At each step, as the decoder samples a character, it searches the table to find the most relevant information which can enrich its state. Signals sent from the table can be interpreted as additional constraints. In the second scenario we share the decoder between two output channels. The first one samples the target character and the other one predicts the morphological annotation of the character. This multi-tasking approach forces the decoder to send morphology-aware information to the final layer which results in better predictions. In the third scenario we combine these two models.  \n Question: How are the auxiliary signals from the morphology table incorporated in the decoder?",
            "output": [
                "an additional morphology table including target-side affixes. We inject the decoder with morphological properties of the target language."
            ]
        },
        {
            "id": "task460-f15f9ce8183c4d1f8c577fd45fb1caed",
            "input": "WikiTableQuestions contains 22,033 questions and is an order of magnitude larger than previous state-of-the-art datasets. Its questions were not designed by predefined templates but were hand crafted by users, demonstrating high linguistic variance. \n Question: How do they gather data for the query explanation problem?",
            "output": [
                "hand crafted by users"
            ]
        },
        {
            "id": "task460-97414c87a70b4e36a28e904a3c26f97e",
            "input": "While the neural syntax-agnostic system obtains superior performance on the L1 data, the two syntax-based systems both produce better analyses on the L2 data. Furthermore, as illustrated in the comparison between different parsers, the better the parsing results we get, the better the performance on L2 we achieve. This shows that syntactic parsing is important in semantic construction for learner Chinese. The main reason, according to our analysis, is that the syntax-based system may generate correct syntactic analyses for partial grammatical fragments in L2 texts, which provides crucial information for SRL. \n Question: Do the authors suggest why syntactic parsing is so important for semantic role labelling for interlanguages?",
            "output": [
                "syntax-based system may generate correct syntactic analyses for partial grammatical fragments"
            ]
        },
        {
            "id": "task460-cf5e5eb2adf04d688344e51103a22722",
            "input": "The seed lexicon consists of positive and negative predicates. If the predicate of an extracted event is in the seed lexicon and does not involve complex phenomena like negation, we assign the corresponding polarity score ($+1$ for positive events and $-1$ for negative events) to the event. It is a  \n Question: What is the seed lexicon?",
            "output": [
                "a vocabulary of positive and negative predicates that helps determine the polarity score of an event"
            ]
        },
        {
            "id": "task460-94363fb77e02458ba6e8d1276215549f",
            "input": "Additionally, we also test their performance only on the items from the Zurich and Visp dialect, because most of the samples are from this two dialects. In Table TABREF15 we show the result of the comparison of the two models. \n Question: Is the model evaluated on the graphemes-to-phonemes task?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-e52d145252494a5f9421d562bd2300e6",
            "input": "To test the effectiveness of this new architecture in forecasting derailment of online conversations, we develop and distribute two new datasets. The first triples in size the highly curated `Conversations Gone Awry' dataset BIBREF9, where civil-starting Wikipedia Talk Page conversations are crowd-labeled according to whether they eventually lead to personal attacks; the second relies on in-the-wild moderation of the popular subreddit ChangeMyView, where the aim is to forecast whether a discussion will later be subject to moderator action due to “rude or hostile” behavior.  \n Question: What are two datasets model is applied to?",
            "output": [
                " `Conversations Gone Awry' dataset subreddit ChangeMyView"
            ]
        },
        {
            "id": "task460-4e319c7ce61c40538f5c395900f0af61",
            "input": "In this work, we use the datasets released by BIBREF1 and HEOT dataset provided by BIBREF0 .  Both the HEOT and BIBREF1 datasets contain tweets which are annotated in three categories: offensive, abusive and none (or benign) \n Question: Do they perform some annotation?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-e5854dc03e0d491eaf773f1656dbfa84",
            "input": "In this section, we propose new simple disentanglement models that perform better than prior methods, and re-examine prior work. \n Question: Did they experiment with the corpus?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-3dc316c542b14c34b1447e494444f679",
            "input": "This effect of context on human ratings is very similar to the one reported in BIBREF5 . They find that sentences rated as ill formed out of context are improved when they are presented in their document contexts. BIBREF5 suggest that adding context causes speakers to focus on broader semantic and pragmatic issues of discourse coherence, rather than simply judging syntactic well formedness (measured as naturalness) when a sentence is considered in isolation. \n Question: What provisional explanation do the authors give for the impact of document context?",
            "output": [
                "adding context causes speakers to focus on broader semantic and pragmatic issues of discourse coherence"
            ]
        },
        {
            "id": "task460-e0eadf1abce943ba815e658ea51cd663",
            "input": "Furthermore, the gains in WER over the baseline are significantly larger for the Density Ratio method than for Shallow Fusion, with up to 28% relative reduction in WER (17.5% $\\rightarrow $ 12.5%) compared to up to 17% relative reduction (17.5% $\\rightarrow $ 14.5%) for Shallow Fusion, in the no fine-tuning scenario. \n Question: What metrics are used for evaluation?",
            "output": [
                "word error rate"
            ]
        },
        {
            "id": "task460-6246ae4712af4cd6adb27a341cd27fb0",
            "input": "The resulting dataset is nearly balanced, with 52.3% of the data (1,857 instances) labeled stressful. \n Question: Is the dataset balanced across categories?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-da2033a5f69f435b9842c411c7e23dc0",
            "input": "This framework has potential applications when comparing different gold standards, considering the design choices for a new gold standard and performing qualitative error analyses for a proposed approach. \n Question: Have they made any attempt to correct MRC gold standards according to their findings? ",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-b9cc2892fa7d4bc196560cc923912c0e",
            "input": "We create a new manually annotated multimodal hate speech dataset formed by $150,000$ tweets, each one of them containing text and an image. We call the dataset MMHS150K, and made it available online . In this section, we explain the dataset creation steps. \n Question: How many tweats does MMHS150k contains, 150000?",
            "output": [
                "$150,000$ tweets"
            ]
        },
        {
            "id": "task460-dc02bf70855d481c8775bb30b9787ab5",
            "input": "Instead of providing an analytic approximation, AEM uses a discriminator network to discriminate between the reconstructed documents from latent events and the original input documents. This essentially helps the generator to construct a more realistic document from a random noise drawn from a Dirichlet distribution. Due to the flexibility of neural networks, the generator is capable of learning complicated nonlinear distributions. And the supervision signal provided by the discriminator will help generator to capture the event-related patterns. \n Question: How does this model overcome the assumption that all words in a document are generated from a single event?",
            "output": [
                "flexibility of neural networks, the generator is capable of learning complicated nonlinear distributions supervision signal provided by the discriminator will help generator to capture the event-related patterns"
            ]
        },
        {
            "id": "task460-9f66e63ea0094995a622348801d403fc",
            "input": "Baseline Experiments\nIn this section, we briefly describe a baseline and evaluation scripts that we release, with a detailed documentation, along with the corpus. \n Question: What type of evaluation is proposed for this task?",
            "output": [
                "Answer with content missing: (Evaluation Metrics section) Precision, Recall, F1-scores, Strict match, METEOR, ROUGE-2"
            ]
        },
        {
            "id": "task460-452ef530b7d8474981d333726e188d3b",
            "input": "We end up eliminating 5.86% of total characters, and end up with 45,821 characters and 12,815 unique HLA, resulting in 945,519 total character-HLA pairs.  We end up eliminating 5.86% of total characters, and end up with 45,821 characters and 12,815 unique HLA, resulting in 945,519 total character-HLA pairs. \n Question: How many different characters were in dataset?",
            "output": [
                "45,821 characters"
            ]
        },
        {
            "id": "task460-a9b25af55e314769876b1c779bee5ac0",
            "input": "Our baseline decoder is a standard beam search decoder BIBREF5 with several straightforward performance optimizations \n Question: What baseline decoder do they use?",
            "output": [
                "a standard beam search decoder BIBREF5 with several straightforward performance optimizations"
            ]
        },
        {
            "id": "task460-1dba55d5b3c24d99b2508db0823c872c",
            "input": "We crowdsourced multiple-choice questions in two parts, encouraging workers to be imaginative and varied in their use of language. First, workers were given a seed qualitative relation q+/-( INLINEFORM0 ) in the domain, expressed in English (e.g., “If a surface has more friction, then an object will travel slower”), and asked to enter two objects, people, or situations to compare. \n Question: Do all questions in the dataset allow the answers to pick from 2 options?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-35e90d85736a4318b956e3160db15274",
            "input": " Impact votes are provided by the users of the platform to evaluate how impactful a particular claim is. Users can pick one of 5 possible impact labels for a particular claim: no impact, low impact, medium impact, high impact and very high impact. While evaluating the impact of a claim, users have access to the full argument context and therefore, they can assess how impactful a claim is in the given context of an argument. \n Question: What annotations are available in the dataset?",
            "output": [
                "5 possible impact labels for a particular claim: no impact, low impact, medium impact, high impact and very high impact"
            ]
        },
        {
            "id": "task460-be3b9c83756e4c8ea63fb68800c98bcb",
            "input": "We create three additional training datasets by adding sentences involving object RCs to the original Wikipedia corpus (Section lm). To this end, we randomly pick up 30 million sentences from Wikipedia (not overlapped to any sentences in the original corpus), parse by the same parser, and filter sentences containing an object RC, amounting to 680,000 sentences.  \n Question: How do they perform data augmentation?",
            "output": [
                "They randomly sample sentences from Wikipedia that contains an object RC and add them to training data"
            ]
        },
        {
            "id": "task460-253651739f3f42b1952a01cd2a32e796",
            "input": "We consider two types of architectures: a bi-directional language model to augment the sequence to sequence encoder and a uni-directional model to augment the decoder. Both use self-attention BIBREF16 and the uni-directional model contains INLINEFORM0 transformer blocks, followed by a word classifier to predict the next word on the right. bi-directional language model to augment the sequence to sequence encoder \n Question: What language model architectures are used?",
            "output": [
                "uni-directional model to augment the decoder"
            ]
        },
        {
            "id": "task460-9fc18678cfe2416585801f70c3e3abd4",
            "input": "Results reveal that, contrary to the common belief, correlations between Rouge and the Pyramid scores are weak, which challenges its effectiveness for scientific summarization. Furthermore, we show a large variance of correlations between different Rouge variants and the manual evaluations which further makes the reliability of Rouge for evaluating scientific summaries less clear. \n Question: What is the common belief that this paper refutes? (c.f. 'contrary to the common belief, ROUGE is not much [sic] reliable'",
            "output": [
                "correlations between Rouge and the Pyramid scores are weak, which challenges its effectiveness for scientific summarization"
            ]
        },
        {
            "id": "task460-9d914b3eb4d14ab2b45eb0f0ed6d1658",
            "input": "We use the Fisher Spanish speech corpus BIBREF11, which consists of 819 phone calls, with an average duration of 12 minutes, amounting to a total of 160 hours of data. \n Question: What language do they look at?",
            "output": [
                "Spanish"
            ]
        },
        {
            "id": "task460-777fd15dd77143c381c756b9e8d75f1c",
            "input": "Impact votes are provided by the users of the platform to evaluate how impactful a particular claim is. Users can pick one of 5 possible impact labels for a particular claim: no impact, low impact, medium impact, high impact and very high impact. While evaluating the impact of a claim, users have access to the full argument context and therefore, they can assess how impactful a claim is in the given context of an argument. An interesting observation is that, in this dataset, the same claim can have different impact labels depending on the context in which it is presented. \n Question: How is pargmative and discourse context added to the dataset?",
            "output": [
                "While evaluating the impact of a claim, users have access to the full argument context and therefore, they can assess how impactful a claim is in the given context of an argument."
            ]
        },
        {
            "id": "task460-1596e9dfb6724da2992f2419adef84b9",
            "input": "For our experiment we decided to use the Europarl dataset, using the data from the WMT11 . \n Question: Are any experiments performed to try this approach to word embeddings?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-30ac7465d12e43fdb05d6053004c1092",
            "input": "In our experiments, the Memory Neural Networks (MemNNs) proposed in babidataset serve as the baselines. \n Question: What word level and character level model baselines are used?",
            "output": [
                "None"
            ]
        },
        {
            "id": "task460-5d14014b34a14047951ee0ac55761f0e",
            "input": "We evaluate our newly proposed models and related baselines in several low-resource settings across different languages with real, distantly supervised data with non-synthetic noise. The advanced modeling of the noisy labels substantially improves the performance up to 36% over methods without noise-handling and up to 9% over all other noise-handling baselines. \n Question: Did they evaluate against baseline?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-0d2af0d613514eddaac2e0b862bb06f3",
            "input": "We evaluate our model on both English and Chinese segmentation. For both languages we used standard datasets for word segmentation and language modeling. English\nThe Brent corpus is a standard corpus used in statistical modeling of child language acquisition BIBREF15 , BIBREF16  We use the commonly used version of the PTB prepared by BIBREF17 Since Chinese orthography does not mark spaces between words, there have been a number of efforts to annotate word boundaries. We evaluate against two corpora that have been manually segmented according different segmentation standards. The Beijing University Corpus was one of the corpora used for the International Chinese Word Segmentation Bakeoff BIBREF18 . We use the Penn Chinese Treebank Version 5.1 BIBREF19 . \n Question: What dataset is used?",
            "output": [
                "Brent corpus PTB  Beijing University Corpus Penn Chinese Treebank"
            ]
        },
        {
            "id": "task460-dd6ed674326a4ed1b1930abb2904e19d",
            "input": "However, all previous IE benchmarks BIBREF18 are too small to train neural network models typically used in QA, and thus we need to build a large benchmark. Therefore, we build a large scale benchmark named QA4IE benchmark which consists of 293K Wikipedia articles and 2M golden relation triples with 636 different relation types. We manually find 148 relations which can be projected to a WikiData relation out of 2064 DBpedia relations. \n Question: Was this benchmark automatically created from an existing dataset?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-c9dafba67e614048be0d5594c1675729",
            "input": "We conducted our experiments on the CMU ARCTIC database BIBREF33, which contains parallel recordings of professional US English speakers sampled at 16 kHz. One female (slt) was chosen as the target speaker and one male (bdl) and one female (clb) were chosen as sources. We selected 100 utterances each for validation and evaluation, and the other 932 utterances were used as training data. For the TTS corpus, we chose a US female English speaker (judy bieber) from the M-AILABS speech dataset BIBREF34 to train a single-speaker Transformer-TTS model. With the sampling rate also at 16 kHz, the training set contained 15,200 utterances, which were roughly 32 hours long. \n Question: What datasets are experimented with?",
            "output": [
                "the CMU ARCTIC database BIBREF33  the M-AILABS speech dataset BIBREF34 "
            ]
        },
        {
            "id": "task460-a2c32928b17142d2959e210ad715c88e",
            "input": "Follow the prior works BIBREF6, BIBREF7, BIBREF9, we adopt the BLEU and the Micro Entity F1 to evaluate our model performance.  We provide human evaluation on our framework and the compared models.  We hire several human experts and ask them to judge the quality of the responses according to correctness, fluency, and humanlikeness on a scale from 1 to 5. \n Question: What were the evaluation metrics?",
            "output": [
                "BLEU Micro Entity F1 quality of the responses according to correctness, fluency, and humanlikeness on a scale from 1 to 5"
            ]
        },
        {
            "id": "task460-45dac2d6fcf1465ab047e15a707c040c",
            "input": "We compare classification and regression approaches and show that classification produces better results than regression but the quality of the results depends on the approach followed to annotate the data labels. \n Question: Did classification models perform better than previous regression one?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-351d3f6507ba4000ba8b6297be8956af",
            "input": "The speech content was obtained from the closed captions (CC) provided by the YouTube ASR system which can be though as subtitles. \n Question: What ASR system do they use?",
            "output": [
                "YouTube ASR system "
            ]
        },
        {
            "id": "task460-db939941fad44ad0af889caea6db49e8",
            "input": "Fig. FIGREF4 shows the distributions of seven question types grouped deterministically from the lexicons. \n Question: How many question types do they find in the datasets analyzed?",
            "output": [
                "seven "
            ]
        },
        {
            "id": "task460-12c6f7e10b6748d49409b10ff9bc6b5d",
            "input": "Recently, MADAMIRA BIBREF6 system has been evaluated using a blind testset (25K words for Modern Standard Arabic (MSA) selected from Penn Arabic Tree bank (PATB)), and the reported accuracy was 96.2% as the percentage of words where the chosen analysis (provided by SAMA morphological analyzer BIBREF7 ) has the correct lemma. As MSA is usually written without diacritics and IR systems normally remove all diacritics from search queries and indexed data as a basic preprocessing step, so another column for undiacritized lemma is added and it's used for evaluating our lemmatizer and comparing with state-of-the-art system for lemmatization; MADAMIRA. \n Question: What is the state of the art?",
            "output": [
                " MADAMIRA BIBREF6 system"
            ]
        },
        {
            "id": "task460-b70a08759f8248379dba5295158b03ec",
            "input": "Twitter data: We used the Twitter API to scrap tweets with hashtags. All non-English tweets were filtered out by the API. \n Question: Do they evaluate only on English datasets?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-30959de5ba634f319178d10ba3e7a00c",
            "input": "For every input data point and possible target class, LRP delivers one scalar relevance value per input variable, hereby indicating whether the corresponding part of the input is contributing for or against a specific classifier decision, or if this input variable is rather uninvolved and irrelevant to the classification task at all. \n Question: Does the LRP method work in settings that contextualize the words with respect to one another?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-344629eb537b4b6d86d101ca673b415a",
            "input": "Amazon Reviews Dataset BIBREF24 is a large dataset with millions of reviews from different product categories. For our experiments, we consider a subset of 20000 reviews from the domains Cell Phones and Accessories(C), Clothing and Shoes(S), Home and Kitchen(H) and Tools and Home Improvement(T). Out of 20000 reviews, 10000 are positive and 10000 are negative. We use 12800 reviews for training, 3200 reviews for validation and 4000 reviews for testing from each domain. \n Question: For the purposes of this paper, how is something determined to be domain specific knowledge?",
            "output": [
                "reviews under distinct product categories are considered specific domain knowledge"
            ]
        },
        {
            "id": "task460-47bd91e06f9040b69e4eac41c63d02cf",
            "input": "Chowdhury BIBREF14 and Thomas et al. BIBREF11 proposed methods that use linguistic phenomenons and two-stage SVM to classify DDIs. FBK-irst BIBREF10 is a follow-on work which applies kernel method to the existing model and outperforms it. Neural network based approaches have been proposed by several works. Liu et al. BIBREF9 employ CNN for DDI extraction for the first time which outperforms the traditional machine learning based methods.  Sahu et al. BIBREF12 proposed LSTM based DDI extraction approach and outperforms CNN based approach, since LSTM handles sentence as a sequence instead of slide windows. \n Question: What are the existing methods mentioned in the paper?",
            "output": [
                "Chowdhury BIBREF14 and Thomas et al. BIBREF11 FBK-irst BIBREF10 Liu et al. BIBREF9 Sahu et al. BIBREF12"
            ]
        },
        {
            "id": "task460-3c60a6c9451546fd9cb50aea63eae44d",
            "input": "Our data collection setup uses a dialogue simulator to generate dialogue outlines first and then paraphrase them to obtain natural utterances. \n Question: Where is the dataset from?",
            "output": [
                "dialogue simulator"
            ]
        },
        {
            "id": "task460-25b1dfc78a964130a986f7e688b2ea08",
            "input": "We conduct our annotation study on Amazon Mechanical Turk, presenting Turkers with Human Intelligence Tasks (henceforth, HITs) consisting of a single conversation between a customer and an agent. In each HIT, we present Turkers with a definition of each dialogue act, as well as a sample annotated dialogue for reference. For each turn in the conversation, we allow Turkers to select as many labels from our taxonomy as required to fully characterize the intent of the turn. Additionally, annotators are asked three questions at the end of each conversation HIT, to which they could respond that they agreed, disagreed, or could not tell: \n Question: How are customer satisfaction, customer frustration and overall problem resolution data collected?",
            "output": [
                "By annotators on Amazon Mechanical Turk."
            ]
        },
        {
            "id": "task460-1bf1e824e1cc4a2c9f93a236364c90d1",
            "input": "We started by answering always YES (in batch 2 and 3) to get the baseline performance. For batch 4 we used entailment. \n Question: What was the baseline model?",
            "output": [
                "by answering always YES (in batch 2 and 3) "
            ]
        },
        {
            "id": "task460-b4b2905a7bd7456a9c9e40e4af9a4df4",
            "input": "terms extracted from Yahoo! Answers tend to be more related, in terms of the number of correlated terms, to attributes related to religion or ethnicity compared to terms from Twitter. However, for two particular attributes (i.e., Price and Buddhist), the number of correlated terms from Twitter is higher than the ones from Yahoo! Answers .  \n Question: On Twitter, do the demographic attributes and answers show more correlations than on Yahoo! Answers?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-4a6a6e3746294212bed92394a711cc11",
            "input": "Results in Table TABREF64 show the correctness rates of these scenarios. User correctness score is superior to that of the baseline parser by 7.5% (from 37.1% to 44.6%), while the hybrid approach outscores both with a correctness of 48.7% improving the baseline by 11.6%. \n Question: Which query explanation method was preffered by the users in terms of correctness?",
            "output": [
                "hybrid approach"
            ]
        },
        {
            "id": "task460-b32bd746249d48e598daf0b86c8b148c",
            "input": "When testing across languages, we report accuracy for two setups: average accuracy over each single-language model (Avg), and accuracy obtained when training on the concatenation of all languages but the target one (All). The latter setting is also used for the embeddings model. We report accuracy for all experiments. \n Question: What are the evaluation metrics used?",
            "output": [
                "average accuracy over each single-language model (Avg), and accuracy obtained when training on the concatenation of all languages but the target one (All)"
            ]
        },
        {
            "id": "task460-fb81666f66154faa8b64cd07882e4f55",
            "input": "The two main data sets that we will use to evaluate our architecture are the 2014 i2b2 de-identification challenge data set BIBREF2 and the nursing notes corpus BIBREF3 . \n Question: Which two datasets is the system tested on?",
            "output": [
                "2014 i2b2 de-identification challenge data set BIBREF2 nursing notes corpus BIBREF3"
            ]
        },
        {
            "id": "task460-4576e8c51d1e496eb846f46f4170012e",
            "input": "First preference is given to the labels that are perfectly matching in all the neural annotators. In Table TABREF11, we can see that both datasets have about 40% of exactly matching labels over all models (AM). Then priority is given to the context-based models to check if the label in all context models is matching perfectly. In case two out of three context models are correct, then it is being checked if that label is also produced by at least one of the non-context models. Then, we allow labels to rely on these at least two context models. As a result, about 47% of the labels are taken based on the context models (CM). When we see that none of the context models is producing the same results, then we rank the labels with their respective confidence values produced as a probability distribution using the $softmax$ function. The labels are sorted in descending order according to confidence values. Then we check if the first three (case when one context model and both non-context models produce the same label) or at least two labels are matching, then we allow to pick that one. There are about 3% in IEMOCAP and 5% in MELD (BM).\n\nFinally, when none the above conditions are fulfilled, we leave out the label with an unknown category. This unknown category of the dialogue act is labeled with `xx' in the final annotations, and they are about 7% in IEMOCAP and 11% in MELD (NM). \n Question: How does the ensemble annotator extract the final label?",
            "output": [
                "First preference is given to the labels that are perfectly matching in all the neural annotators. In case two out of three context models are correct, then it is being checked if that label is also produced by at least one of the non-context models. When we see that none of the context models is producing the same results, then we rank the labels with their respective confidence values produced as a probability distribution using the $softmax$ function. The labels are sorted in descending order according to confidence values. Then we check if the first three (case when one context model and both non-context models produce the same label) or at least two labels are matching, then we allow to pick that one.  Finally, when none the above conditions are fulfilled, we leave out the label with an unknown category."
            ]
        },
        {
            "id": "task460-033b8670ae954044a96a63f5be97456b",
            "input": "For our classification experiments, we use the MH17 Twitter dataset introduced by BIBREF4, a dataset collected in order to study the flow of (dis)information about the MH17 plane crash on Twitter. \n Question: What dataset is used for this study?",
            "output": [
                "MH17 Twitter dataset"
            ]
        },
        {
            "id": "task460-5efc8a4d9f5b43ed8b8f8ba85178b29f",
            "input": " In this task English tweets from conversation threads, each associated to a newsworthy event and the rumours around it, are provided as data. The goal is to determine whether a tweet in the thread is supporting, denying, querying, or commenting the original rumour which started the conversation.  \n Question: Is this an English-language dataset?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-d911c2e28ce34f17a45d5dcc1e51b3a3",
            "input": "In the sixth international workshop on Vietnamese Language and Speech Processing (VLSP 2019), the Hate Speech Detection (HSD) task is proposed as one of the shared-tasks to handle the problem related to controlling content in SNSs. The fundamental idea of this system is how to make a system that has the diversity of viewing an input. That because of the variety of the meaning in Vietnamese language especially with the acronym, teen code type. \n Question: Is the data all in Vietnamese?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-f3ccb61c28f74adda3873a23f7309ebc",
            "input": "We compare our model with the following state-of-the-art joint entity and relation extraction models:\n\n(1) SPTree BIBREF4: This is an end-to-end neural entity and relation extraction model using sequence LSTM and Tree LSTM. (2) Tagging BIBREF5: This is a neural sequence tagging model which jointly extracts the entities and relations using an LSTM encoder and an LSTM decoder. (3) CopyR BIBREF6: This model uses an encoder-decoder approach for joint extraction of entities and relations. (4) HRL BIBREF11: This model uses a reinforcement learning (RL) algorithm with two levels of hierarchy for tuple extraction. (5) GraphR BIBREF14: This model considers each token in a sentence as a node in a graph, and edges connecting the nodes as relations between them. (6) N-gram Attention BIBREF9: This model uses an encoder-decoder approach with N-gram attention mechanism for knowledge-base completion using distantly supervised data. \n Question: What is previous work authors reffer to?",
            "output": [
                "SPTree Tagging CopyR HRL GraphR N-gram Attention"
            ]
        },
        {
            "id": "task460-0fba2cae0ae749339fb03edd1963789e",
            "input": "GPT-2 achieves the highest score and the $n$-gram the lowest. Transformer-XL and the LSTM LM perform in the middle, and at roughly the same level as each other. We report the 12-category accuracy results for all models and human evaluation in Table TABREF14. \n Question: What is the performance of the models on the tasks?",
            "output": [
                "Overall accuracy per model is: 5-gram (60.5), LSTM (68.9), TXL (68.7), GPT-2 (80.1)"
            ]
        },
        {
            "id": "task460-c97481c8abb34a0f80598ceb0081d29b",
            "input": "We follow a similar evaluation protocol to those presented in BIBREF6 , BIBREF8 , BIBREF9 which is to use our learned representations as features for a low complexity classifier (typically linear) on a novel supervised task/domain unseen during training without updating the parameters of our sentence representation model. We also consider such a transfer learning evaluation in an artificially constructed low-resource setting. In addition, we also evaluate the quality of our learned individual word representations using standard benchmarks BIBREF36 , BIBREF37 . \n Question: How do they evaluate their sentence representations?",
            "output": [
                "standard benchmarks BIBREF36 , BIBREF37 to use our learned representations as features for a low complexity classifier (typically linear) on a novel supervised task/domain unseen during training without updating the parameters transfer learning evaluation in an artificially constructed low-resource setting"
            ]
        },
        {
            "id": "task460-c36ccaddcaa34daba49b96aafacfc54e",
            "input": "Our desiderata for the task collection were: sufficient diversity, existence of fairly large datasets for training, and success as standalone training objectives for sentence representations.\n\nMulti-task training setup \n Question: Which model architecture do they for sentence encoding?",
            "output": [
                "Answer with content missing: (Skip-thought vectors-Natural Language Inference paragraphs) The encoder for the current sentence and the decoders for the previous (STP) and next sentence (STN) are typically parameterized as separate RNNs\n- RNN"
            ]
        },
        {
            "id": "task460-996b2eab3b6d43faadd20ef9498e59cd",
            "input": "The resulting sequence of vectors is encoded using an LSTM encoder.  \n Question: What architecture does the encoder have?",
            "output": [
                "LSTM"
            ]
        },
        {
            "id": "task460-d6700d690ff3486e88d54f7714e21b2b",
            "input": "We compared our model with MLE, RL and GAN baselines. Since COCO and EMNLP2017 WMT don't have input while WeiboDial regards posts as input, we chose the following baselines respectively:\n\nMLE: a RNN model trained with MLE objective BIBREF4 . Its extension, Seq2Seq, can work on the dialogue dataset BIBREF2 .\n\nSeqGAN: The first text GAN model that updates the generator with policy gradient based on the rewards from the discriminator BIBREF7 .\n\nLeakGAN: A variant of SeqGAN that provides rewards based on the leaked information of the discriminator for the generator BIBREF11 .\n\nMaliGAN: A variant of SeqGAN that optimizes the generator with a normalized maximum likelihood objective BIBREF8 .\n\nIRL: This inverse reinforcement learning method replaces the discriminator with a reward approximator to provide dense rewards BIBREF12 .\n\nRAML: A RL approach to incorporate MLE objective into RL training framework, which regards BLEU as rewards BIBREF17 .\n\nDialogGAN: An extension of SeqGAN tuned to dialogue generation task with MLE objective added to the adversarial objective BIBREF16 .\n\nDPGAN: A variant of DialogGAN which uses a language model based discriminator and regards cross-entropy as rewards BIBREF13 . \n Question: What GAN models were used as baselines to compare against?",
            "output": [
                "MLE SeqGAN LeakGAN MaliGAN IRL RAML DialogGAN DPGAN"
            ]
        },
        {
            "id": "task460-9aba7c1a7b9d4ec984d206e5881441bf",
            "input": "We recruited 102 pairs of participants from Amazon Mechanical Turk and randomly assigned speaker and listener roles \n Question: Was this experiment done in a lab?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-28a9d501e21a4aecbcb372b7232fe820",
            "input": "Following recent works, we compare on two widely used datasets, the Penn Treebank (PTB) BIBREF28 as prepared by BIBREF29 and WikiText2 (WT-2) BIBREF20 . \n Question: what data did they use?",
            "output": [
                " Penn Treebank WikiText2"
            ]
        },
        {
            "id": "task460-d211d511a13948c9ae9f2aad76efa8f4",
            "input": "To analyse the results we chose to use the test provided by BIBREF10, which consists of $19\\,791$ analogies divided into 19 different categories: 6 related to the “semantic\" macro-area (8915 analogies) and 13 to the “syntactic\" one (10876 analogies). All the analogies are composed by two pairs of words that share a relation, schematized with the equation: $a:a^{*}=b:b^{*}$ (e.g. “man : woman = king : queen\"); where $b^{*}$ is the word to be guessed (“queen\"), $b$ is the word coupled to it (“king\"), $a$ is the word for the components to be eliminated (“man\"), and $a^{*}$ is the word for the components to be added (“woman\"). \n Question: Are the word embeddings tested on a NLP task?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-962ff8cf364e40aba76406cede3d41ed",
            "input": "Topology only embeddings: MMB BIBREF45 , DeepWalk BIBREF1 , LINE BIBREF33 , Node2vec BIBREF46 . ( INLINEFORM1 ) Joint embedding of topology & text: Naive combination, TADW BIBREF5 , CENE BIBREF6 , CANE BIBREF9 , WANE BIBREF10 , DMTE BIBREF34 .  \n Question: Which other embeddings do they compare against?",
            "output": [
                "MMB DeepWalk LINE  Node2vec TADW CENE CANE WANE DMTE"
            ]
        },
        {
            "id": "task460-0ad51eabc5c849c2805ab50d05686476",
            "input": " Moreover, due to their noisy nature, they are also processed using some Twitter-specific techniques such as substitution/removal of URLs, of user mentions, of hashtags, and of emoticons, spelling correction, elongation normalization, abbreviation lookup, punctuation removal, detection of amplifiers and diminishers, negation scope detection, etc. That language proved to be quite challenging with its use of creative spelling and punctuation, misspellings, slang, new words, URLs, and genre-specific terminology and abbreviations, e.g., RT for re-tweet and #hashtags. In addition to the genre difference, there is also a difference in length: social media messages are generally short, often length-limited by design as in Twitter, i.e., a sentence or a headline rather than a full document \n Question: What difficulties does sentiment analysis on Twitter have, compared to sentiment analysis in other domains?",
            "output": [
                "Tweets noisy nature, use of creative spelling and punctuation, misspellings, slang, new words, URLs, and genre-specific terminology and abbreviations, short (length limited) text"
            ]
        },
        {
            "id": "task460-1efbc25302404c0fb5d6837352a6c8f1",
            "input": "The second series of tests consisted of using all the documents of the subcorpus “specialist” $E$, because the documents of the subcorpus of Annodis are not identical.  As far as system evaluations are concerned, we use the 78 $E$ documents as reference.  We calculate the precision $P$, the recall $R$ and the $F$-score on the text corpus used in our tests, as follow: \n Question: How is segmentation quality evaluated?",
            "output": [
                "Segmentation quality is evaluated by calculating the precision, recall, and F-score of the automatic segmentations in comparison to the segmentations made by expert annotators from the ANNODIS subcorpus."
            ]
        },
        {
            "id": "task460-33ecb67b58904e1bbfbe76e93d8b90f4",
            "input": "We draw from research in social psychology to inform our methodology, most prominently Moral Foundations Theory BIBREF26. MFT seeks to explain the structure and variation of human morality across cultures, and proposes five moral foundations: Care / Harm, Fairness / Cheating, Loyalty / Betrayal, Authority / Subversion, and Sanctity / Degradation. Each foundation is summarized by a positive and a negative pole, resulting in ten fine-grained moral categories. \n Question: Which fine-grained moral dimension examples do they showcase?",
            "output": [
                "Care / Harm, Fairness / Cheating, Loyalty / Betrayal, Authority / Subversion, and Sanctity / Degradation"
            ]
        },
        {
            "id": "task460-f6cbeedd43df481fbacd525eda614912",
            "input": "We started from an initial corpus that did not have revisions, and kept adding new documents and revising existing documents. The revision operations we used were deletion, addition and replacement of words, sentences, paragraphs, section names and document titles. The addition of words, ..., section names, and new documents were pulled from the Wikipedia abstracts. We generated six data sets using different random seeds, and each data set contained six corpora (Corpus 0 - 5). \n Question: What are simulated datasets collected?",
            "output": [
                "There are 6 simulated datasets collected which is initialised with a corpus of size 550 and simulated by generating new documents from Wikipedia extracts and replacing existing documents"
            ]
        },
        {
            "id": "task460-e2e81d35579042d0aeb25ab7927a12b5",
            "input": "For the language modeling evaluation, we also evaluate a baseline without knowledge distillation (termed NoKD), with a model parameterized identically to the distilled student models but trained directly on the teacher model objective from scratch. For downstream tasks, we compare with NoKD as well as Patient Knowledge Distillation (PKD) from BIBREF34, who distill the 12-layer BERTBASE model into 3 and 6-layer BERT models by using the teacher model's hidden states. \n Question: What state-of-the-art compression techniques were used in the comparison?",
            "output": [
                "baseline without knowledge distillation (termed NoKD) Patient Knowledge Distillation (PKD)"
            ]
        },
        {
            "id": "task460-0b2a9d255a5a4fdfbc8e8438ec005d92",
            "input": "Once the S-V-O is generated, Text2Visual provides users with visual components that convey the S-V-O text meanings. \n Question: Does their solution involve connecting images and text?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-653be0d8a9864bdfbb4c6b3669a9f318",
            "input": "Table TABREF21 shows results for Korean INLINEFORM0 English, using the same configurations (1, 2 and 8) as for German–English. Our results confirm that the techniques we apply are successful across datasets, and result in stronger systems than previously reported on this dataset, achieving 10.37 BLEU as compared to 5.97 BLEU reported by gu-EtAl:2018:EMNLP1. \n Question: what were their experimental results in the low-resource dataset?",
            "output": [
                "10.37 BLEU"
            ]
        },
        {
            "id": "task460-4ee051cac8bc448a8a910bfc1db320c1",
            "input": "Various multilingual extensions of NMT have already been proposed in the literature. The authors of BIBREF18 , BIBREF19 apply multitask learning to train models for multiple languages. Zoph and Knight BIBREF20 propose a multi-source model and BIBREF21 introduces a character-level encoder that is shared across several source languages. In our setup, we will follow the main idea proposed by Johnson et al. BIBREF22 . The authors of that paper suggest a simple addition by means of a language flag on the source language side (see Figure 2 ) to indicate the target language that needs to be produced by the decoder. This flag will be mapped on a dense vector representation and can be used to trigger the generation of the selected language. The authors of the paper argue that the model enables transfer learning and supports the translation between languages that are not explicitly available in training. \n Question: What neural machine translation models can learn in terms of transfer learning?",
            "output": [
                "Multilingual Neural Machine Translation Models"
            ]
        },
        {
            "id": "task460-379e2671df5e4c03a8a45f7efc15ce18",
            "input": "We collect utterances from the $\\mathbf {C}$hinese $\\mathbf {A}$rtificial $\\mathbf {I}$ntelligence $\\mathbf {S}$peakers (CAIS), and annotate them with slot tags and intent labels. The training, validation and test sets are split by the distribution of intents, where detailed statistics are provided in the supplementary material. Since the utterances are collected from speaker systems in the real world, intent labels are partial to the PlayMusic option. We adopt the BIOES tagging scheme for slots instead of the BIO2 used in the ATIS, since previous studies have highlighted meaningful improvements with this scheme BIBREF30 in the sequence labeling field. \n Question: What is the source of the CAIS dataset?",
            "output": [
                "the $\\mathbf {C}$hinese $\\mathbf {A}$rtificial $\\mathbf {I}$ntelligence $\\mathbf {S}$peakers (CAIS)"
            ]
        },
        {
            "id": "task460-96f2c1569ed64a52bd3aec4a3db9b5b7",
            "input": "Three datasets annotated with emotions are commonly used for the development and evaluation of emotion detection systems, namely the Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset. A summary is provided in Table TABREF8 , which also shows, in the bottom row, what role each dataset has in our experiments: apart from the development portion of the Affective Text, which we used to develop our models (Section SECREF4 ), all three have been used as benchmarks for our evaluation. \n Question: Which existing benchmarks did they compare to?",
            "output": [
                "Affective Text Fairy Tales ISEAR"
            ]
        },
        {
            "id": "task460-b77d3e8fd6b740b78a63e6090706c9f0",
            "input": "For the proposed model, we denote INLINEFORM0 parameterized by INLINEFORM1 as a neural-based feature encoder that maps documents from both domains to a shared feature space, and INLINEFORM2 parameterized by INLINEFORM3 as a fully connected layer with softmax activation serving as the sentiment classifier. We have left the feature encoder INLINEFORM0 unspecified, for which, a few options can be considered. In our implementation, we adopt a one-layer CNN structure from previous works BIBREF22 , BIBREF4 , as it has been demonstrated to work well for sentiment classification tasks. \n Question: What is the architecture of the model?",
            "output": [
                "one-layer CNN structure from previous works BIBREF22 , BIBREF4"
            ]
        },
        {
            "id": "task460-7c6eb5070ca64ac8b3a37bd3dbf0d9ff",
            "input": "We use the unshuffled version of the French OSCAR corpus, which amounts to 138GB of uncompressed text and 32.7B SentencePiece tokens. \n Question: What is CamemBERT trained on?",
            "output": [
                "unshuffled version of the French OSCAR corpus"
            ]
        },
        {
            "id": "task460-6123cf29bd324b9a98ace18de8d535a6",
            "input": "According to the categories mentioned in BIBREF5 , irony can be classified into 3 classes: verbal irony by means of a polarity contrast, the sentences containing expression whose polarity is inverted between the intended and the literal evaluation; other types of verbal irony, the sentences that show no polarity contrast between the literal and intended meaning but are still ironic; and situational irony, the sentences that describe situations that fail to meet some expectations. As ironies in the latter two categories are obscure and hard to understand, we decide to only focus on ironies in the first category in this work.   Because of the lack of previous work and baselines on irony generation, we implement our model based on style transfer.  \n Question: What are the difficulties in modelling the ironic pattern?",
            "output": [
                "obscure and hard to understand  lack of previous work and baselines on irony generation"
            ]
        },
        {
            "id": "task460-3415a571fa074cfeb25eccf316759648",
            "input": "Experiments ::: Baselines\nTo comprehensively evaluate our AGDT, we compare the AGDT with several competitive models. \n Question: Is the model evaluated against other Aspect-Based models?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-112e509334a94ee49395c6a7f7cc2d26",
            "input": "Internally, the ASR system maintains a rich hypothesis space in the form of speech lattices or confusion networks (cnets). \n Question: What is a word confusion network?",
            "output": [
                "It is a network used to encode speech lattices to maintain a rich hypothesis space."
            ]
        },
        {
            "id": "task460-5c7d2062dcac49388deb78543ab271a8",
            "input": "To control the quality, we ensured that a single annotator annotates maximum 120 headlines (this protects the annotators from reading too many news headlines and from dominating the annotations). Secondly, we let only annotators who geographically reside in the U.S. contribute to the task.\n\nWe test the annotators on a set of $1,100$ test questions for the first phase (about 10% of the data) and 500 for the second phase. Annotators were required to pass 95%. \n Question: How is quality of annotation measured?",
            "output": [
                "Annotators went through various phases to make sure their annotations did not deviate from the mean."
            ]
        },
        {
            "id": "task460-c763de3926c24d808190bfbea238fc4c",
            "input": "Later, BIBREF8 introduced an RNN with an external stack memory to learn simple context-free languages, such as $a^n b^m$ , $a^nb^ncb^ma^m$ , and $a^{n+m} b^n c^m$ . Similar studies BIBREF15 , BIBREF16 , BIBREF17 , BIBREF10 , BIBREF11 have explored the existence of stable counting mechanisms in simple RNNs, which would enable them to learn various context-free and context-sensitive languages BIBREF9 , on the other hand, proposed a variant of Long Short-Term Memory (LSTM) networks to learn two context-free languages, $a^n b^n$ , $a^n b^m B^m A^n$ , and one strictly context-sensitive language, $a^n b^n c^n$ . \n Question: How do they get the formal languages?",
            "output": [
                "These are well-known formal languages some of which was used in the literature to evaluate the learning capabilities of RNNs."
            ]
        },
        {
            "id": "task460-f111763ef0974336b74299a7a3588bf6",
            "input": "Compared with 2015, our QA-SRL gold yielded 1593 annotations, with 989 core and 604 adjuncts, while theirs yielded 1315 annotations, 979 core and 336 adjuncts.  \n Question: How much more coverage is in the new dataset?",
            "output": [
                "278 more annotations"
            ]
        },
        {
            "id": "task460-42da268044e54530beb339fbb438cd07",
            "input": "Both the distributional hypothesis itself and Tugendhat's interpretation of Frege's work are examples of holistic approaches to meaning, where the meaning of the whole determines the meaning of parts. \n Question: How does Frege's holistic and functional approach to meaning relates to general distributional hypothesis?",
            "output": [
                "interpretation of Frege's work are examples of holistic approaches to meaning"
            ]
        },
        {
            "id": "task460-40c35fd60c5242baa0571461e959eab1",
            "input": "We use the Bayesian model of garg2012unsupervised as our base monolingual model. The semantic roles are predicate-specific. \n Question: What does an individual model consist of?",
            "output": [
                "Bayesian model of garg2012unsupervised as our base monolingual model"
            ]
        },
        {
            "id": "task460-33a2d27eddbd4c3e9ade6f600e662986",
            "input": "In this section we present two datasets used in our experiments: The NowThisNews dataset, collected for the purpose of this paper, and The BreakingNews dataset BIBREF4 , publicly available dataset of news articles.\n\ncontains 4090 posts with associated videos from NowThisNews Facebook page collected between 07/2015 and 07/2016. For each post we collected its title and the number of views of the corresponding video, which we consider our popularity metric. Due to a fairly lengthy data collection process, we decided to normalize our data by first grouping posts according to their publication month and then labeling the posts for which the popularity metric exceeds the median monthly value as popular, the remaining part as unpopular. \n Question: Where do they obtain the news videos from?",
            "output": [
                "NowThisNews Facebook page"
            ]
        },
        {
            "id": "task460-92781116cef24548a582fd2ee060fddc",
            "input": "However, recent advances in Deep Learning techniques have shown that the NER task can benefit from the use of neural architectures, such as biLSTM-networks BIBREF3 , BIBREF4 . We use the implementation proposed in BIBREF24 for conducting three different experiments. \n Question: Which machine learning algorithms did the explore?",
            "output": [
                "biLSTM-networks"
            ]
        },
        {
            "id": "task460-b55ad695eaa84d7fa99ef1e86572332c",
            "input": "A total of 2,50,000 tweets over a period of August 31st, 2015 to August 25th,2016 on Microsoft are extracted from twitter API BIBREF15 . The news on twitter about Microsoft and tweets regarding the product releases were also included. Stock opening and closing prices of Microsoft from August 31st, 2015 to August 25th, 2016 are obtained from Yahoo! Finance BIBREF16 . \n Question: What dataset is used to train the model?",
            "output": [
                "2,50,000 tweets Stock opening and closing prices of Microsoft from August 31st, 2015 to August 25th, 2016"
            ]
        },
        {
            "id": "task460-b8d1157399694a7e83f1256b98f5de14",
            "input": "This section reports the results of the experiments conducted on two data sets for evaluating the performances of wDTW and wTED against other baseline methods. We denote the following distance/similarity measures. WMD: The Word Mover's Distance introduced in Section SECREF1 . VSM: The similarity measure introduced in Section UID12 . PV-DTW: PV-DTW is the same as Algorithm SECREF21 except that the distance between two paragraphs is not based on Algorithm SECREF20 but rather computed as INLINEFORM0 where INLINEFORM1 is the PV embedding of paragraph INLINEFORM2 . PV-TED: PV-TED is the same as Algorithm SECREF23 except that the distance between two paragraphs is not based on Algorithm SECREF20 but rather computed as INLINEFORM0 . \n Question: Which are the state-of-the-art models?",
            "output": [
                "WMD VSM PV-DTW PV-TED"
            ]
        },
        {
            "id": "task460-a0d59f9fbdbf4440bae3e55fa9466493",
            "input": "For the evaluation of summaries we use the standard ROGUE metric. For comparison with previous AMR based summarization methods, we report the Recall, Precision and INLINEFORM0 scores for ROGUE-1. \n Question: Which evaluation methods are used?",
            "output": [
                "Quantitative evaluation methods using ROUGE, Recall, Precision and F1."
            ]
        },
        {
            "id": "task460-426cb29477794ce5b3f9782f92f3a512",
            "input": "The semi-parametric models were trained on 32 GPUs with each replica split over 2 GPUs, one to train the translation model and the other for computing the CSTM. \n Question: Does their combination of a non-parametric retrieval and neural network get trained end-to-end?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-aea696bf7f9d44688890170c5a9ca39a",
            "input": "The previously mentioned datasets are all in English \n Question: For which languages most of the existing MRC datasets are created?",
            "output": [
                "English"
            ]
        },
        {
            "id": "task460-25d9b181c8494c9ebdefda2a509e3d7f",
            "input": "LUWAK is implemented in pure JavaScript code, and it uses the LocalStorage of a web browser. \n Question: What programming language is the tool written in?",
            "output": [
                "JavaScript"
            ]
        },
        {
            "id": "task460-651b25c4e006472da6117a20daae03ac",
            "input": "We characterize the evaluation of Emotionally-Aware Chatbot into two different parts, qualitative and quantitative assessment. Qualitative assessment will focus on assessing the functionality of the software, while quantitative more focus on measure the chatbots' performance with a number. Based on our investigation of several previous studies, we found that most of the works utilized ISO 9241 to assess chatbots' quality by focusing on the usability aspect. This aspect can be grouped into three focuses, including efficiency, effectiveness, and satisfaction, concerning systems' performance to achieve the specified goals. In automatic evaluation, some studies focus on evaluating the system at emotion level BIBREF15 , BIBREF28 . Therefore, some common metrics such as precision, recall, and accuracy are used to measure system performance, compared to the gold label. This evaluation is similar to emotion classification tasks such as previous SemEval 2018 BIBREF32 and SemEval 2019 . Other studies also proposed to use perplexity to evaluate the model at the content level (to determine whether the content is relevant and grammatical) BIBREF14 , BIBREF39 , BIBREF28 . This evaluation metric is widely used to evaluate dialogue-based systems which rely on probabilistic approach BIBREF61 . Another work by BIBREF14 used BLEU to evaluate the machine response and compare against the gold response (the actual response), although using BLEU to measure conversation generation task is not recommended by BIBREF62 due to its low correlation with human judgment. This evaluation involves human judgement to measure the chatbots' performance, based on several criteria. BIBREF15 used three annotators to rate chatbots' response in two criteria, content (scale 0,1,2) and emotion (scale 0,1). Content is focused on measuring whether the response is natural acceptable and could plausible produced by a human. This metric measurement is already adopted and recommended by researchers and conversation challenging tasks, as proposed in BIBREF38 .  \n Question: How are EAC evaluated?",
            "output": [
                "Qualitatively through efficiency, effectiveness and satisfaction aspects and quantitatively through metrics such as precision, recall, accuracy, BLEU score and even human judgement."
            ]
        },
        {
            "id": "task460-66865157a6e14341bbec3d8f7746946e",
            "input": "Our model got the first position in the German sub-task with a macro F1 score of 0.62. \n Question: What is the performance of the model for the German sub-task A?",
            "output": [
                "macro F1 score of 0.62"
            ]
        },
        {
            "id": "task460-8030a8174f964780849ce3b3e4853f66",
            "input": "We consider three tasks representing a broad selection of natural language understanding scenarios: paper acceptance prediction based on the PeerRead data set BIBREF2, Named Entity Recognition (NER) based on the Broad Twitter Corpus BIBREF3, and author stance prediction based on the RumEval-19 data set BIBREF6.  \n Question: What are three challenging tasks authors evaluated their sequentially aligned representations?",
            "output": [
                "paper acceptance prediction Named Entity Recognition (NER) author stance prediction"
            ]
        },
        {
            "id": "task460-169cfcd164514751869cc7615ca2ec4f",
            "input": "Our model yields the best results on both UAS and LAS metrics of all languages except the Japanese. As for Japanese, our model gives unsatisfactory results because the original treebank was written in Roman phonetic characters instead of hiragana, which is used by both common Japanese writing and our pre-trained embeddings. Despite this, our model overall still gives 1.0% higher average UAS and LAS than the previous best parser, BIAF. Although both BIAF and STACKPTR parsers have achieved relatively high parsing accuracies on the 12 languages and have all UAS higher than 90%, our model achieves state-of-the-art results in all languages for both UAS and LAS. Overall, our model reports more than 1.0% higher average UAS than STACKPTR and 0.3% higher than BIAF. \n Question: What are performance compared to former models?",
            "output": [
                "model overall still gives 1.0% higher average UAS and LAS than the previous best parser, BIAF our model reports more than 1.0% higher average UAS than STACKPTR and 0.3% higher than BIAF"
            ]
        },
        {
            "id": "task460-85c2bdb8396a4167a198b684eb734865",
            "input": "if there is an error in the graph, the triple is likely to be inconsistent with its neighborhood, and thus the model should put least trust on this triple. In other words, the error triple should have the least influence on the model's prediction of the training data. Here, we demonstrate another potential use of adversarial modifications: finding erroneous triples in the knowledge graph. Intuitively, if there is an error in the graph, the triple is likely to be inconsistent with its neighborhood, and thus the model should put least trust on this triple. In other words, the error triple should have the least influence on the model's prediction of the training data. Formally, to find the incorrect triple $\\langle s^{\\prime }, r^{\\prime }, o\\rangle $ in the neighborhood of the train triple $\\langle s, r, o\\rangle $ , we need to find the triple $\\langle s^{\\prime },r^{\\prime },o\\rangle $ that results in the least change $\\Delta _{(s^{\\prime },r^{\\prime })}(s,r,o)$ when removed from the graph. \n Question: How is this approach used to detect incorrect facts?",
            "output": [
                "if there is an error in the graph, the triple is likely to be inconsistent with its neighborhood, and thus the model should put least trust on this triple. In other words, the error triple should have the least influence on the model's prediction of the training data. "
            ]
        },
        {
            "id": "task460-882fefe57e224657ba5b09820bcc5b2e",
            "input": "The Sentence corpus (S) consists of domain-targeted $~$ 80K sentences and 280 GB of plain text extracted from web pages used by BIBREF6 aristo2016:combining. We use the text corpora (S) from BIBREF6 aristo2016:combining to build our tuple KB.  We take the top 200 hits, run Open IE v4, and aggregate the resulting tuples over all $a \\in A$ and over all questions in $Q_\\mathit {tr}$ to create the tuple KB (T). \n Question: What was the textual source to which OpenIE was applied?",
            "output": [
                "domain-targeted $~$ 80K sentences and 280 GB of plain text extracted from web pages used by BIBREF6 aristo2016:combining"
            ]
        },
        {
            "id": "task460-00a28000219f4021b2dcc04266aa0e42",
            "input": "Our Recurrent +ELMo model uses the language model from BIBREF9 to provide contextualized embeddings to the baseline model outlined above, as recommended by the authors.\n\nOur OpenAI GPT model fine-tunes the 12 layer 768 dimensional uni-directional transformer from BIBREF27 , which has been pre-trained as a language model on the Books corpus BIBREF36 . \n Question: did they use other pretrained language models besides bert?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-59a0288777e045daa96ea238c5c21fdb",
            "input": "We use a benchmark dataset created by ceccarelli2013learning from the CoNLL 2003 data.  \n Question: What is the benchmark dataset?",
            "output": [
                "a benchmark dataset created by ceccarelli2013learning from the CoNLL 2003 data"
            ]
        },
        {
            "id": "task460-9962f3e6b0fb42479b4fc9f9a0deffea",
            "input": "To develop the dataset, we extracted 200 sentences from Sorani Kurdish books of grades one to three of the primary school in the Kurdistan Region of Iraq. We randomly created 2000 sentences from the extracted sentences. \n Question: What is the size of the dataset?",
            "output": [
                "2000 sentences"
            ]
        },
        {
            "id": "task460-0641afa4c5b34b05893d04bb49382dd3",
            "input": "BERT-ADA BIBREF33 is a domain-adapted BERT-based model proposed for the APC task, which fine-tuned the BERT-BASE model on task-related corpus. This model obtained state-of-the-art accuracy on the Laptops dataset. We build a joint model for the multi-task of ATE and APC based on the BERT-BASE model. After optimizing the model parameters according to the empirical result, the joint model based on BERT-BASE achieved hopeful performance on all three datasets and even surpassed other proposed BERT based improved models on some datasets, such as BERT-PT, AEN-BERT, SDGCN-BERT, and so on. \n Question: What was state of the art on SemEval-2014 task4 Restaurant and Laptop dataset?",
            "output": [
                "BERT-ADA BERT-PT, AEN-BERT, SDGCN-BERT"
            ]
        },
        {
            "id": "task460-a941afb500c542e8936de933fe9ea24e",
            "input": "Figure FIGREF5 shows that E-BERT performs comparable to BERT and ERNIE on unfiltered LAMA. \n Question: Which of the two ensembles yields the best performance?",
            "output": [
                "Answer with content missing: (Table 2) CONCAT ensemble"
            ]
        },
        {
            "id": "task460-a1027f9f6c494dda959157dab0404013",
            "input": "The goal of an language model is to assign meaningful probabilities to a sequence of words. Given a set of tokens $\\mathbf {X}=(x_1,....,x_T)$, where $T$ is the length of a sequence, our task is to estimate the joint conditional probability $P(\\mathbf {X})$ which is\n\nwere $(x_{1}, \\ldots , x_{i-1})$ is the context. An Intrinsic evaluation of the performance of Language Models is perplexity (PPL) which is defined as the inverse probability of the set of the tokens and taking the $T^{th}$ root were $T$ is the number of tokens We propose an approximation of the joint probability as,\n\nThis type of approximations has been previously explored with Bi-directional RNN LM's BIBREF9 but not for deep transformer models. We therefore, define a pseudo-perplexity score from the above approximated joint probability. \n Question: How is pseudo-perplexity defined?",
            "output": [
                "Answer with content missing: (formulas in selection): Pseudo-perplexity is perplexity where conditional joint probability is approximated."
            ]
        },
        {
            "id": "task460-963634353361455692d9d7f984631552",
            "input": "Compared with the BERT-BASE model, BERT-SPC significantly improves the accuracy and F1 score of aspect polarity classification. \n Question: How much better is performance of the proposed model compared to the state of the art in these various experiments?",
            "output": [
                "significantly improves the accuracy and F1 score of aspect polarity classification"
            ]
        },
        {
            "id": "task460-39b97929ba754ba7bd3a09a3a619d6cb",
            "input": "Through experiments on two large complex cross-domain datasets, SParC BIBREF2 and CoSQL BIBREF6, we carefully compare and analyze the performance of different context modeling methods.  \n Question: What two large datasets are used for evaluation?",
            "output": [
                "SParC BIBREF2 and CoSQL BIBREF6"
            ]
        },
        {
            "id": "task460-ea69894f39aa476b8dc3431e2fa38bb2",
            "input": "First, workers were given a seed qualitative relation q+/-( INLINEFORM0 ) in the domain, expressed in English (e.g., “If a surface has more friction, then an object will travel slower”), and asked to enter two objects, people, or situations to compare. They then created a question, guided by a large number of examples, and were encouraged to be imaginative and use their own words. Second, the LFs were elicited using a novel technique of reverse-engineering them from a set of follow-up questions, without exposing workers to the underlying formalism. This is possible because of the constrained space of LFs. Referring to LF templates (1) and (2) earlier (Section SECREF13 ), these questions are as follows:\n\nFrom this information, we can deduce the target LF ( INLINEFORM0 is the complement of INLINEFORM1 , INLINEFORM2 , we arbitrarily set INLINEFORM3 =world1, hence all other variables can be inferred). \n Question: How do they obtain the logical forms of their questions in their dataset?",
            "output": [
                " workers were given a seed qualitative relation asked to enter two objects, people, or situations to compare created a question, guided by a large number of examples LFs were elicited using a novel technique of reverse-engineering them from a set of follow-up questions"
            ]
        },
        {
            "id": "task460-ed939b37e6f74255a0b8a221395ae320",
            "input": "Table TABREF16 shows the confusion matrices for facial and audio emotion recognition on our complete AMMER data set and Table TABREF17 shows the results per class for each method, including facial and audio data and micro and macro averages. The classification from facial expressions yields a macro-averaged $\\text{F}_1$ score of 33 % across the three emotions joy, insecurity, and annoyance (P=0.31, R=0.35). \n Question: How is face and audio data analysis evaluated?",
            "output": [
                "confusion matrices $\\text{F}_1$ score"
            ]
        },
        {
            "id": "task460-c81bb01adadc43ae9d9fadb3df25adac",
            "input": "In order to obtain insights about why the more fine-grained bi-sense emoji embedding helps in understanding the complexed sentiments behind tweets, we visualize the attention weights for ATT-E-LSTM and MATT-BiE-LSTM for comparison. Therefore, we construct the new input INLINEFORM0 to each LSTM unit by concatenating the original word embedding and the attention vector in Equation EQREF21 to distribute the senti-emoji information to each step. This model is called Multi-level Attention-based LSTM with Bi-sense Emoji Embedding (MATT-BiE-LSTM) Attention-based LSTM with emojis:We also use the word-emoji embedding to calculate the emoji-word attention following Equation EQREF20 and EQREF21 , and the only difference is that we replace the attention-derived senti-emoji embedding with the pre-trained word-emoji embedding by fasttext, denoted as ATT-E-LSTM. In Figure FIGREF27 (a), the ATT-E-LSTM model (baseline) assigns relatively more weights on the word “no” and “pressure”, while MATT-BiE-LSTM attends mostly on the word “happy” and “lovely”. The different attention distributions suggest that the proposed senti-emoji embedding is capable of recognizing words with strong sentiments that are closely related to the true sentiment even with the presence of words with conflicting sentiments, such as “pressure” and “happy”. while ATT-E-LSTM tends to pick up all sentimental words which could raise confusions. The senti-emoji embedding is capable of extracting representations of complexed semantics and sentiments which help guide the attentions even in cases when the word sentiment and emoji sentiment are somewhat contradictory to each other. From Figure FIGREF27 (b) and (c) we can observe that the ATT-E-LSTM assigns more weights on the sentiment-irrelevant words than the MATT-BiE-LSTM such as “hoodies”, “wait” and “after”, indicating that the proposed model is more robust to irrelevant words and concentrates better on important words. Because of the senti-emoji embedding obtained through bi-sense emoji embedding and the sentence-level LSTM encoding on the text input (described in Section SECREF13 ), we are able to construct a more robust embedding based on the semantic and sentiment information from the whole context compared to the word-emoji embedding used in ATT-E-LSTM which takes only word-level information into account. \n Question: What evidence does visualizing the attention give to show that it helps to obtain a more robust understanding of semantics and sentiments?",
            "output": [
                "The different attention distributions suggest that the proposed senti-emoji embedding is capable of recognizing words with strong sentiments that are closely related to the true sentiment even with the presence of words with conflicting sentiments"
            ]
        },
        {
            "id": "task460-48baedba40a64091880ea0bd960dabc7",
            "input": "In the proposed model, in addition to the co-occurrence edges, we link two nodes (words) if the corresponding word embedding representation is similar. \n Question: Do the use word embeddings alone or they replace some previous features of the model with word embeddings?",
            "output": [
                "They use it as addition to previous model - they add new edge between words if word embeddings are similar."
            ]
        },
        {
            "id": "task460-d3e3f6cbb6d44cee9d0c6b6a64d329db",
            "input": "While manually analyzing the raw dataset, we noticed that looking at the tweet one has replied to or has quoted, provides significant contextual information. We call these, “context tweets\". As humans can better understand a tweet with the reference of its context, our assumption is that computers also benefit from taking context tweets into account in detecting abusive language.\n\nAs shown in the examples below, (2) is labeled abusive due to the use of vulgar language. However, the intention of the user can be better understood with its context tweet (1).\n\n(1) I hate when I'm sitting in front of the bus and somebody with a wheelchair get on.\n\nINLINEFORM0 (2) I hate it when I'm trying to board a bus and there's already an as**ole on it.\n\nSimilarly, context tweet (3) is important in understanding the abusive tweet (4), especially in identifying the target of the malice.\n\n(3) Survivors of #Syria Gas Attack Recount `a Cruel Scene'.\n\nINLINEFORM0 (4) Who the HELL is “LIKE\" ING this post? Sick people....\n\nHuang et al. huang2016modeling used several attributes of context tweets for sentiment analysis in order to improve the baseline LSTM model. However, their approach was limited because the meta-information they focused on—author information, conversation type, use of the same hashtags or emojis—are all highly dependent on data.\n\nIn order to avoid data dependency, text sequences of context tweets are directly used as an additional feature of neural network models. We use the same baseline model to convert context tweets to vectors, then concatenate these vectors with outputs of their corresponding labeled tweets. More specifically, we concatenate max-pooled layers of context and labeled tweets for the CNN baseline model. As for RNN, the last hidden states of context and labeled tweets are concatenated. \n Question: What additional features and context are proposed?",
            "output": [
                "using tweets that one has replied or quoted to as contextual information"
            ]
        },
        {
            "id": "task460-e08aee30e8644a8897b88145fdce3bf4",
            "input": "These actions consist of the following components:\n\n[leftmargin=*]\n\nCo-Reference Resolution: To support multi-turn interactions, it is sometimes necessary to use co-reference resolution techniques for effective retrieval. Query Generation: This component generates a query based on the past user-system interactions. Retrieval Model: This is the core ranking component that retrieves documents or passages from a large collection. Result Generation: The retrieved documents can be too long to be presented using some interfaces. \n Question: What are the different modules in Macaw?",
            "output": [
                "Co-Reference Resolution Query Generation Retrieval Model Result Generation"
            ]
        },
        {
            "id": "task460-7c7e81ecc628404d8d304a20939db7ad",
            "input": "Our AV in-cabin data-set includes 30 hours of multimodal data collected from 30 passengers (15 female, 15 male) in 20 rides/sessions. 10 types of passenger intents are identified and annotated as: Set/Change Destination, Set/Change Route (including turn-by-turn instructions), Go Faster, Go Slower, Stop, Park, Pull Over, Drop Off, Open Door, and Other (turn music/radio on/off, open/close window/trunk, change AC/temp, show map, etc.). Relevant slots are identified and annotated as: Location, Position/Direction, Object, Time-Guidance, Person, Gesture/Gaze (this, that, over there, etc.), and None. In addition to utterance-level intent types and their slots, word-level intent keywords are annotated as Intent as well. We obtained 1260 unique utterances having commands to AMIE from our in-cabin data-set. We expanded this data-set via Amazon Mechanical Turk and ended up with 3347 utterances having intents. The annotations for intents and slots are obtained on the transcribed utterances by majority voting of 3 annotators. \n Question: What are the supported natural commands?",
            "output": [
                "Set/Change Destination Set/Change Route Go Faster Go Slower Stop Park Pull Over Drop Off Open Door Other "
            ]
        },
        {
            "id": "task460-bcd449ce38304940b5c57961aaff53c3",
            "input": "We developed an AdaBoost-based classifier to detect our new fake reviews, consisting of 200 shallow decision trees (depth 2). \n Question: What kind of model do they use for detection?",
            "output": [
                "AdaBoost-based classifier"
            ]
        },
        {
            "id": "task460-6495e703cfd34b1ca24e314df030988d",
            "input": "We collected the dataset with 1900 dialogs and 8533 turns.  \n Question: What is the average length of dialog?",
            "output": [
                "4.49 turns"
            ]
        },
        {
            "id": "task460-ffeb12285cfd422884658eb622ca6850",
            "input": "Both the audio and the transcript are de-identified (by removing the identifying information) with digital zeros and [de-identified] tags, respectively. \n Question: Is the data de-identified?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-3297d2cf51a7462c91df7e57b3e131ba",
            "input": "Our transference model extends the original transformer model to multi-encoder based transformer architecture. The transformer architecture BIBREF12 is built solely upon such attention mechanisms completely replacing recurrence and convolutions. The transformer uses positional encoding to encode the input and output sequences, and computes both self- and cross-attention through so-called multi-head attentions, which are facilitated by parallelization. We use multi-head attention to jointly attend to information at different positions from different representation subspaces. \n Question: Which algorithm is used in the UDS-DFKI system?",
            "output": [
                "Our transference model extends the original transformer model to multi-encoder based transformer architecture. The transformer architecture BIBREF12 is built solely upon such attention mechanisms completely replacing recurrence and convolutions. "
            ]
        },
        {
            "id": "task460-f8ffe690aafd43228e7a69e0f6754eb9",
            "input": "The dataset (composed of 2.6 GB of raw text) includes $421\\,829\\,960$ words divided into $17\\,305\\,401$ sentences. \n Question: How big is dataset used to train Word2Vec for the Italian Language?",
            "output": [
                "$421\\,829\\,960$ words divided into $17\\,305\\,401$ sentences"
            ]
        },
        {
            "id": "task460-566af393d6d348a8965ec3b8f13a09c9",
            "input": "As in the work of E. Tong et al. ( BIBREF9 ), we pre-train word embeddings using a skip-gram model BIBREF4 applied to unlabeled data from escort ads, however, we go further by analyzing the emojis' embeddings and thereby expand the trafficking lexicon. \n Question: Do they use pretrained word embeddings?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-cbc801afc9d0475a9441e00c85736c8c",
            "input": "We evaluate the proposed models on three conversationet dataset such as MojiTalk BIBREF16, PersonaChat BIBREF11, Empathetic-Dialogues BIBREF26. \n Question: What three conversational datasets are used for evaluation?",
            "output": [
                "MojiTalk  PersonaChat  Empathetic-Dialogues"
            ]
        },
        {
            "id": "task460-721955c8e03a41ad87bdae9696083c1b",
            "input": "SVM: We define 3 sets of features to characterize each question. The first is a simple bag-of-words set of features over the question (SVM-BOW), the second is bag-of-words features of the question as well as length of the question in words (SVM-BOW + LEN), and lastly we extract bag-of-words features, length of the question in words as well as part-of-speech tags for the question (SVM-BOW + LEN + POS). This results in vectors of 200, 201 and 228 dimensions respectively, which are provided to an SVM with a linear kernel. No-Answer Baseline (NA) : Most of the questions we receive are difficult to answer in a legally-sound way on the basis of information present in the privacy policy. We establish a simple baseline to quantify the effect of identifying every question as unanswerable. Word Count Baseline : To quantify the effect of using simple lexical matching to answer the questions, we retrieve the top candidate policy sentences for each question using a word count baseline BIBREF53, which counts the number of question words that also appear in a sentence. We include the top 2, 3 and 5 candidates as baselines. Human Performance: We pick each reference answer provided by an annotator, and compute the F1 with respect to the remaining references, as described in section 4.2.1. Each reference answer is treated as the prediction, and the remaining n-1 answers are treated as the gold reference. The average of the maximum F1 across all reference answers is computed as the human baseline. \n Question: Were other baselines tested to compare with the neural baseline?",
            "output": [
                "SVM No-Answer Baseline (NA)  Word Count Baseline Human Performance"
            ]
        },
        {
            "id": "task460-916744a389564179abc9a1498d4008f3",
            "input": "The corpus is freely available at the following link \n Question: Is this dataset publicly available?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-55e9d0fb758c4b24bcf11561bf4dda25",
            "input": " We draw our data from news publications, wine reviews, and Reddit, which in addition to large volume, also let us characterize binomials in new ways, and analyze differences in binomial orderings across communities and over time.  We develop new metrics for the agreement of binomial orderings across communities and the movement of binomial orderings over time. Using subreddits as communities, these metrics reveal variations in orderings, some of which suggest cultural change influencing language. We develop a null model to determine how much variation in binomial orderings we might expect across communities and across time, if binomial orderings were randomly ordered according to global asymmetry values.  \n Question: How is order of binomials tracked across time?",
            "output": [
                "draw our data from news publications, wine reviews, and Reddit develop new metrics for the agreement of binomial orderings across communities and the movement of binomial orderings over time  develop a null model to determine how much variation in binomial orderings we might expect across communities and across time"
            ]
        },
        {
            "id": "task460-c7f61cb532614926827f05c110779449",
            "input": "In terms of the future development of this technology, we emphasize the importance of answering four questions: 1) How to introduce unsupervised pre-training into NLG tasks with cross-modal context? 2) How to design a generic pre-training algorithm to fit a wide range of NLG tasks? 3) How to reduce the computing resources required for large-scale pre-training? 4) What aspect of knowledge do the pre-trained models provide for better language generation? \n Question: Which future direction in NLG are discussed?",
            "output": [
                "1) How to introduce unsupervised pre-training into NLG tasks with cross-modal context? 2) How to design a generic pre-training algorithm to fit a wide range of NLG tasks? 3) How to reduce the computing resources required for large-scale pre-training? 4) What aspect of knowledge do the pre-trained models provide for better language generation?"
            ]
        },
        {
            "id": "task460-224a1f412190497dac2cf1bea2632bab",
            "input": "We evaluate an RNN model which uses bidirectionally summed GRU memory cells BIBREF18 and uses the final states as embeddings; a CNN model which uses sentence-max-pooled convolutional filters as embeddings BIBREF19 ; an RNN-CNN model which puts the CNN on top of per-token GRU outputs rather than the word embeddings BIBREF20 ; and an attn1511 model inspired by BIBREF20 that integrates the RNN-CNN model with per-word attention to build hypothesis-specific evidence embeddings. We also report the baseline results of avg mean of word embeddings in the sentence with projection matrix and DAN Deep Averaging Network model that employs word-level dropout and adds multiple nonlinear transformations on top of the averaged embeddings BIBREF21 . \n Question: what were the baselines?",
            "output": [
                "RNN model CNN model  RNN-CNN model attn1511 model Deep Averaging Network model avg mean of word embeddings in the sentence with projection matrix"
            ]
        },
        {
            "id": "task460-43b0a107bf4740d8806b7ec8458dc3c7",
            "input": "As shown in Table TABREF3 , we collect Amazon review keywords for 2,896 e-books (publishers: Kiwi, Rowohlt, Fischer, and Droemer), which leads to 33,663 distinct review keywords and on average 30 keyword assignments per e-book. \n Question: how large is the vocabulary?",
            "output": [
                "33,663"
            ]
        },
        {
            "id": "task460-c83262f1484942bdbb9ed5a7a399a45b",
            "input": "We evaluate the proposed methods in 8 languages, showing a significant ability to learn from partial data. We additionally experiment with initializing CBL with domain-specific instance-weighting schemes, showing mixed results. In the process, we use weighted variants of popular NER models, showing strong performance in both non-neural and neural settings. Finally, we show experiments in a real-world setting, by employing non-speakers to manually annotate romanized Bengali text. We experiment on 8 languages. Four languages – English, German, Spanish, Dutch – come from the CoNLL 2002/2003 shared tasks BIBREF21, BIBREF22. These are taken from newswire text, and have labelset of Person, Organization, Location, Miscellaneous.\n\nThe remaining four languages come from the LORELEI project BIBREF23. These languages are: Amharic (amh: LDC2016E87), Arabic (ara: LDC2016E89), Hindi (hin: LDC2017E62), and Somali (som: LDC2016E91). These come from a variety of sources including discussion forums, newswire, and social media.  \n Question: Which languages are evaluated?",
            "output": [
                "Bengali English, German, Spanish, Dutch Amharic Arabic Hindi Somali "
            ]
        },
        {
            "id": "task460-71643bdac3eb4f23ab0d801090079a57",
            "input": "Traditional text-to-speech (TTS) systems are composed of complex pipelines BIBREF0 , these often include accoustic frontends, duration model, acoustic prediction model and vocoder models. Neural text-to-speech systems have garnered large research interest in the past 2 years. The first to fully explore this avenue of research was Google's tacotron BIBREF1 system. The architecture of our model utilizes RNN-based Seq2Seq model for generating mel spectrogram from text. The architecture is similar to that of Tacotron 2 BIBREF4 In the original Tacotron 2, the attention mechanism used was location sensitive attention BIBREF12 combined the original additive Seq2Seq BIBREF7 Bahdanau attention.\n\nWe propose to replace this attention with the simpler query-key attention from transformer model Following the logic above, we utilize a similar method from BIBREF6 that adds an additional guided attention loss to the overall loss objective, which acts to help the attention mechanism become monotoic as early as possible. As seen from FIGREF24 , an attention loss mask, INLINEFORM0 , is created applies a loss to force the attention alignment, INLINEFORM1 , to be nearly diagonal. \n Question: Which modifications do they make to well-established Seq2seq architectures?",
            "output": [
                "Replacing attention mechanism to query-key attention, and adding a loss to make the attention mask as diagonal as possible"
            ]
        },
        {
            "id": "task460-04275e5e03af4f8abac0cf4ab8043fd1",
            "input": "We work with a dataset composed of 3,206 news articles, each labeled real or fake, with a perfect 50/50 split between 1,603 real and fake articles, respectively. \n Question: What is the size of the dataset?",
            "output": [
                "3,206"
            ]
        },
        {
            "id": "task460-68bf2a66b7994172be7d9f6af0d46b25",
            "input": "Multimodal representation.\nWe combined textual and image representations in two simple ways. The first method is concatenation of the text and image representation (concat). Before concatenation we applied the L2 normalization to each of the modalities. The second method it to learn a common space for the two modalities before concatenation (project).\n\nThe projection of each modality learns a space of $d$-dimensions, so that $h_{1}, h_{2} \\in \\mathbb {R}^{d}$. Once the multimodal representation is produced ($h_{m}$) for the left and right pairs, vectors are directly plugged into the regression layers. Projections are learned end-to-end with the regression layers and the MSE as loss function. \n Question: What multimodal representations are used in the experiments?",
            "output": [
                "The second method it to learn a common space for the two modalities before concatenation (project) The first method is concatenation of the text and image representation (concat)"
            ]
        },
        {
            "id": "task460-140990e119044479b9bf711b4d6aeeda",
            "input": "To show the effectiveness of our approach, we show results on the SICK dataset BIBREF1, a common benchmark for logic-based NLI, and find MonaLog to be competitive with more complicated logic-based approaches (many of which require full semantic parsing and more complex logical machinery). \n Question: Do they beat current state-of-the-art on SICK?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-788129889c1e4425978c0bd0c64a0f53",
            "input": "We measure how contextual a word representation is using three different metrics: self-similarity, intra-sentence similarity, and maximum explainable variance. \n Question: What experiments are proposed to test that upper layers produce context-specific embeddings?",
            "output": [
                "They measure self-similarity, intra-sentence similarity and maximum explainable variance of the embeddings in the upper layers."
            ]
        },
        {
            "id": "task460-129f6b61468f412b9bac4ab60afc7788",
            "input": "In this paper, we study verifiable robustness, i.e., providing a certificate that for a given network and test input, no attack or perturbation under the specification can change predictions, using the example of text classification tasks, Stanford Sentiment Treebank (SST) BIBREF15 and AG News BIBREF16.  \n Question: Which dataset do they use?",
            "output": [
                "Stanford Sentiment Treebank (SST) BIBREF15 and AG News BIBREF16"
            ]
        },
        {
            "id": "task460-e28df9267769448d960cf2905fd6cd90",
            "input": " A dialog turn from one speaker may not only be a direct response to the other speaker's query, but also likely to be a continuation of his own previous statement. Thus, when modeling turn $k$ in a dialog, we propose to connect the last RNN state of turn $k-2$ directly to the starting RNN state of turn $k$ , instead of letting it to propagate through the RNN for turn $k-1$ . \n Question: How long of dialog history is captured?",
            "output": [
                "two previous turns"
            ]
        },
        {
            "id": "task460-2acec34256814460a54873ec350674ff",
            "input": "In this work, we look at a wide range of attributes and report prediction results on 62 demographic attributes. \n Question: How many demographic attributes they try to predict?",
            "output": [
                "62"
            ]
        },
        {
            "id": "task460-5fae311087cb4cf7965aaef2883536d3",
            "input": " Conditional Random Fields\nConditional Random Fields (CRF) BIBREF10 are a standard approach when dealing with sequential data in the context of sequence labeling. BiLSTM-CRF\nPrior to the emergence of deep neural language models, BiLSTM-CRF models BIBREF11 had achieved state-of-the-art results for the task of sequence labeling. Multi-Task Learning\nMulti-Task Learning (MTL) BIBREF15 has become popular with the progress in deep learning. BioBERT\nDeep neural language models have recently evolved to a successful method for representing text. In particular, Bidirectional Encoder Representations from Transformers (BERT) outperformed previous state-of-the-art methods by a large margin on various NLP tasks BIBREF17. \n Question: What baselines do they introduce?",
            "output": [
                "Conditional Random Fields BiLSTM-CRF Multi-Task Learning BioBERT\n"
            ]
        },
        {
            "id": "task460-703f2682b8e0475893c18fbaecbf5598",
            "input": "We evaluate the proposed transfer learning techniques in two non-English language pairs of WMT 2019 news translation tasks: French$\\rightarrow $German and German$\\rightarrow $Czech. \n Question: Are experiments performed with any other pair of languages, how did proposed method perform compared to other models?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-d0442264f7f0434081a8099855c642c0",
            "input": "Following previous work BIBREF13, BIBREF12, BIBREF10, BIBREF17, BIBREF9, BIBREF7, we choose SemCor3.0 as training corpus, which is the largest corpus manually annotated with WordNet sense for WSD. \n Question: Is SemCor3.0 reflective of English language data in general?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-01d3ceb6ebe64ab3ba056a40486739ac",
            "input": "We provide three pairs of short/long datasets from different domains (movies and restaurants) and from different languages (English and Korean) suitable for the task: Mov_en, Res_en, and Mov_ko. Most of the datasets are from previous literature and are gathered differently The Mov_en datasets are gathered from different websites; the short dataset consists of hand-picked sentences by BIBREF19 from document-level reviews from the Rotten Tomatoes website, while the long dataset consists of reviews from the IMDB website obtained by BIBREF20. \n Question: What dierse domains and languages are present in new datasets?",
            "output": [
                "movies  restaurants English  Korean"
            ]
        },
        {
            "id": "task460-07c6ca24d88c4810b433cb71c23b0d5f",
            "input": "According to our observation, if a vernacular paragraph contains more poetic images used in classical literature, its generated poem usually achieves higher score. We also observed that poems generated from descriptive paragraphs achieve higher scores than from logical or philosophical paragraphs. \n Question: What are some guidelines in writing input vernacular so model can generate ",
            "output": [
                " if a vernacular paragraph contains more poetic images used in classical literature, its generated poem usually achieves higher score poems generated from descriptive paragraphs achieve higher scores than from logical or philosophical paragraphs"
            ]
        },
        {
            "id": "task460-5db7f884cad94e98b5b2caf7c4f82227",
            "input": "The data presented in this paper was collected and validated via Mozilla's Common Voice initiative. Using either the Common Voice website or iPhone app, contributors record their voice by reading sentences displayed on the screen (see Figure (FIGREF5)). The recordings are later verified by other contributors using a simple voting system. Shown in Figure (FIGREF6), this validation interface has contributors mark $<$audio,transcript$>$ pairs as being either correct (up-vote) or incorrect (down-vote). \n Question: What crowdsourcing platform is used for data collection and data validation?",
            "output": [
                "the Common Voice website  iPhone app"
            ]
        },
        {
            "id": "task460-c49765d9dfb843daaa043a3da6aefe39",
            "input": "Our multi-task BERT models involve six different Arabic classification tasks. Author profiling and deception detection in Arabic (APDA). LAMA+DINA Emotion detection. Sentiment analysis in Arabic tweets. \n Question: What are the tasks used in the mulit-task learning setup?",
            "output": [
                "Author profiling and deception detection in Arabic LAMA+DINA Emotion detection Sentiment analysis in Arabic tweets"
            ]
        },
        {
            "id": "task460-204b46ab75bd4333a2e721233f73fcf8",
            "input": "We use a siamese neural network, shown to perform state-of-the-art few-shot learning BIBREF11, as our baseline model. We modify the original to account for sequential data, with each twin composed of an embedding layer, a Long-Short Term Memory (LSTM) BIBREF12 layer, and a feed-forward layer with Rectified Linear Unit (ReLU) activations. \n Question: What were the baselines?",
            "output": [
                "Siamese neural network consisting of an embedding layer, a LSTM layer and a feed-forward layer with ReLU activations"
            ]
        },
        {
            "id": "task460-b39492cf20a4472daf0b9c1a29034a0f",
            "input": "Moreover, QA and QG have probabilistic correlation as both tasks relate to the joint probability between $q$ and $a$ . Given a question-answer pair $\\langle q, a \\rangle $ , the joint probability $P(q, a)$ can be computed in two equivalent ways.\r\n\r\n$$P(q, a) = P(a) P(q|a) = P(q)P(a|q)$$ (Eq. 1)\r\n\r\nThe conditional distribution $P(q|a)$ is exactly the QG model, and the conditional distribution $P(a|q)$ is closely related to the QA model. Existing studies typically learn the QA model and the QG model separately by minimizing their own loss functions, while ignoring the probabilistic correlation between them.\r\n\r\nBased on these considerations, we introduce a training framework that exploits the duality of QA and QG to improve both tasks. There might be different ways of exploiting the duality of QA and QG. In this work, we leverage the probabilistic correlation between QA and QG as the regularization term to influence the training process of both tasks. Specifically, the training objective of our framework is to jointly learn the QA model parameterized by $\\theta _{qa}$ and the QG model parameterized by $\\theta _{qg}$ by minimizing their loss functions subject to the following constraint.\r\n\r\n$$P_a(a) P(q|a;\\theta _{qg}) = P_q(q)P(a|q;\\theta _{qa})$$ (Eq. 3)\r\n\r\n$P_a(a)$ and $P_q(q)$ are the language models for answer sentences and question sentences, respectively. Overall, the framework includes three components, namely a QA model, a QG model and a regularization term that reflects the duality of QA and QG. The QA specific objective aims to minimize the loss function $l_{qa}(f_{qa}(a,q;\\theta _{qa}), label)$ , where $label$ is 0 or 1 that indicates whether $a$ is the correct answer of $q$ or not. For each correct question-answer pair, the QG specific objective is to minimize the following loss function,\r\n\r\n$$l_{qg}(q, a) = -log P_{qg}(q|a;\\theta _{qg})$$ (Eq. 6)\r\n\r\nwhere $a$ is the correct answer of $q$ . The third objective is the regularization term which satisfies the probabilistic duality constrains as given in Equation 3 . Specifically, given a correct $\\langle q, a \\rangle $ pair, we would like to minimize the following loss function,\r\n\r\n$$ \\nonumber l_{dual}(a,q;\\theta _{qa}, \\theta _{qg}) &= [logP_a(a) + log P(q|a;\\theta _{qg}) \\\\ & - logP_q(q) - logP(a|q;\\theta _{qa})]^2$$ (Eq. 9)\r\n\r\nwhere $P_a(a)$ and $P_q(q)$ are marginal distributions, which could be easily obtained through language model. \n Question: What does \"explicitly leverages their probabilistic correlation to guide the training process of both models\" mean?",
            "output": [
                "The framework jointly learns parametrized QA and QG models subject to the constraint in equation 2. In more detail, they minimize QA and QG loss functions, with a third dual loss for regularization."
            ]
        },
        {
            "id": "task460-499fb42148da47e8ada26318b527bceb",
            "input": "It should also be noted that signals can be recognized even when the model misclassifies relations, since ${\\Delta }_s$ does not rely on correct classification: it merely quantifies the contribution of a word in context toward the correct label's score. If we examine the influence of each word on the score of the correct relation, that impact should and does still correlate with human judgments based on what the system may tag as the second or third best class to choose. \n Question: Where does proposed metric overlap with juman judgement?",
            "output": [
                "influence of each word on the score of the correct relation, that impact should and does still correlate with human judgments"
            ]
        },
        {
            "id": "task460-1039018c700349dc8b13452d81713e90",
            "input": "We choose Zulu (ZUL), a Bantoid language. In contrast to the first two, it is strongly prefixing. \n Question: What is an example of a prefixing language?",
            "output": [
                "Zulu"
            ]
        },
        {
            "id": "task460-56f55eb3dfba40c39ab67958b3b56ba1",
            "input": "Although the competition proposes two different scenarios, in fact, both are guided by the snomed ct ontology —for subtask 1, entities must be identified with offsets and mapped to a predefined set of four classes (PROTEINAS, NORMALIZABLES, NO_NORMALIZABLES and UNCLEAR); for subtask 2, a list of all snomed ct ids (sctid) for entities occurring in the text must be given, which has been called concept indexing by the shared task organizers. \n Question: What are the two PharmaCoNER subtasks?",
            "output": [
                "Entity identification with offset mapping and concept indexing"
            ]
        },
        {
            "id": "task460-533eaf9938014089ac8e52b24f5ca518",
            "input": "The Siamese neural network has two parallel convolutional networks, INLINEFORM0 , that share the same set of weights, INLINEFORM1 , as shown in Figure FIGREF5 (a). \n Question: What is the architecture of the siamese neural network?",
            "output": [
                "two parallel convolutional networks, INLINEFORM0 , that share the same set of weights"
            ]
        },
        {
            "id": "task460-13979a2697c64b8c92109669a60ffcb0",
            "input": " We also use the final hidden layer of the neural network as a task-specific embedding of the claim, together with the Web evidence.  \n Question: What data is used to build the task-specific embeddings?",
            "output": [
                "embedding of the claim Web evidence"
            ]
        },
        {
            "id": "task460-e70566fe09a54a36953f0b7af56c00aa",
            "input": "This way, we can label 77 new pro-Russian edges by looking at 415 tweets, which means that 19% of the candidates are hits. For the pro-Ukrainian class, we can label 110 new edges by looking at 611 tweets (18% hits). Hence even though the quality of the classifier predictions is too low to be integrated into the network analysis right away, the classifier drastically facilitates the annotation process for human annotators compared to annotating unfiltered tweets (from the original labels we infer that for unfiltered tweets, only 6% are hits for the pro-Russian class, and 11% for the pro-Ukrainian class). \n Question: How can the classifier facilitate the annotation task for human annotators?",
            "output": [
                "quality of the classifier predictions is too low to be integrated into the network analysis right away, the classifier drastically facilitates the annotation process for human annotators compared to annotating unfiltered tweets"
            ]
        },
        {
            "id": "task460-5026bac0b9824a7eb75f9cffdb8df3a4",
            "input": "For each model, we examined word-level perplexity, R@3 in next-word prediction, latency (ms/q), and energy usage (mJ/q). To explore the perplexity–recall relationship, we collected individual perplexity and recall statistics for each sentence in the test set. \n Question: What aspects have been compared between various language models?",
            "output": [
                "Quality measures using perplexity and recall, and performance measured using latency and energy usage. "
            ]
        },
        {
            "id": "task460-02ddcc889f0d42018f30202cd73665c7",
            "input": "Figure FIGREF10 illustrates the UTCNN model. As more than one user may interact with a given post, we first add a maximum pooling layer after the user matrix embedding layer and user vector embedding layer to form a moderator matrix embedding INLINEFORM0 and a moderator vector embedding INLINEFORM1 for moderator INLINEFORM2 respectively, where INLINEFORM3 is used for the semantic transformation in the document composition process, as mentioned in the previous section. The term moderator here is to denote the pseudo user who provides the overall semantic/sentiment of all the engaged users for one document. The embedding INLINEFORM4 models the moderator stance preference, that is, the pattern of the revealed user stance: whether a user is willing to show his preference, whether a user likes to show impartiality with neutral statements and reasonable arguments, or just wants to show strong support for one stance. Ideally, the latent user stance is modeled by INLINEFORM5 for each user. Likewise, for topic information, a maximum pooling layer is added after the topic matrix embedding layer and topic vector embedding layer to form a joint topic matrix embedding INLINEFORM6 and a joint topic vector embedding INLINEFORM7 for topic INLINEFORM8 respectively, where INLINEFORM9 models the semantic transformation of topic INLINEFORM10 as in users and INLINEFORM11 models the topic stance tendency. The latent topic stance is also modeled by INLINEFORM12 for each topic.\n\nAs for comments, we view them as short documents with authors only but without likers nor their own comments. Therefore we apply document composition on comments although here users are commenters (users who comment). It is noticed that the word embeddings INLINEFORM0 for the same word in the posts and comments are the same, but after being transformed to INLINEFORM1 in the document composition process shown in Figure FIGREF4 , they might become different because of their different engaged users. The output comment representation together with the commenter vector embedding INLINEFORM2 and topic vector embedding INLINEFORM3 are concatenated and a maximum pooling layer is added to select the most important feature for comments. Instead of requiring that the comment stance agree with the post, UTCNN simply extracts the most important features of the comment contents; they could be helpful, whether they show obvious agreement or disagreement. Therefore when combining comment information here, the maximum pooling layer is more appropriate than other pooling or merging layers. Indeed, we believe this is one reason for UTCNN's performance gains.\n\nFinally, the pooled comment representation, together with user vector embedding INLINEFORM0 , topic vector embedding INLINEFORM1 , and document representation are fed to a fully connected network, and softmax is applied to yield the final stance label prediction for the post. \n Question: How many layers does the UTCNN model have?",
            "output": [
                "eight layers"
            ]
        },
        {
            "id": "task460-ddffd35dcece4c57b697305192f4a438",
            "input": "We first run our experiment on BiLSTM, BiLSTM-CNN, BiLSTM-CRF BiLSTM-CNN-CRF using the hyper-parameters mentioned in Table TABREF30.  \n Question: Which machine learning models do they explore?",
            "output": [
                "BiLSTM BiLSTM-CNN BiLSTM-CRF BiLSTM-CNN-CRF"
            ]
        },
        {
            "id": "task460-193a880c27e5400ba02b9634de5f9ae3",
            "input": "The FBFans dataset contains data from anti-nuclear-power Chinese Facebook fan groups from September 2013 to August 2014, including posts and their author and liker IDs. \n Question: What topic is covered in the Chinese Facebook data? ",
            "output": [
                "anti-nuclear-power"
            ]
        },
        {
            "id": "task460-3b2d7b6527844b1ab91f3d959e4df49b",
            "input": "Active learning sharply increases the performance of iteratively trained machine learning models by selectively determining which unlabeled samples should be annotated.  \n Question: What is active learning?",
            "output": [
                "A process of training a model when selected unlabeled samples are annotated on each iteration."
            ]
        },
        {
            "id": "task460-6456291330034958b0e47fe5b2510b4e",
            "input": "In the first one, shown in Figure FIGREF12, a special dedicated discriminator is added to the model to control that the latent representation does not contain stylistic information. The second extension of the baseline architecture does not use an adversarial component $D_z$ that is trying to eradicate information on $c$ from component $z$. Instead, the system, shown in Figure FIGREF16 feeds the \"soft\" generated sentence $\\tilde{G}$ into encoder $E$ and checks how close is the representation $E(\\tilde{G} )$ to the original representation $z = E(x)$ in terms of the cosine distance. We further refer to it as shifted autoencoder or SAE. We also study a combination of both approaches described above, shown on Figure FIGREF17. \n Question: What are three new proposed architectures?",
            "output": [
                "special dedicated discriminator is added to the model to control that the latent representation does not contain stylistic information shifted autoencoder or SAE combination of both approaches"
            ]
        },
        {
            "id": "task460-84ed8aa477074239b6993b1625fdb226",
            "input": "Each tweet was tokenized using NLTK TweetTokenizer and classified as one of 10 potential accounts from which it may have originated. The accounts were chosen based on the distinct topics each is known to typically tweet about. \n Question: What text classification task is considered?",
            "output": [
                "To classify a text as belonging to one of the ten possible classes."
            ]
        },
        {
            "id": "task460-c1a5dd598dfa44968c99c44259fe0a39",
            "input": "We thus try to remove the language-specific information from the representations by centering the representations of sentences in each language so that their average lies at the origin of the vector space. \n Question: Are language-specific and language-neutral components disjunctive?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-6c765c7d478e459181ff6d57cc025d6d",
            "input": "The data for this project are two parts, the first part is the historical S&P 500 component stocks, which are downloaded from the Yahoo Finance. We use the data over the period of from 12/07/2017 to 06/01/2018. The second part is the news article from financial domain are collected with the same time period as stock data. Hence, only news article from financial domain are collected. The data is mainly taken from Webhose archived data, which consists of 306242 news articles present in JSON format, dating from December 2017 up to end of June 2018. \n Question: What is the dataset used in the paper?",
            "output": [
                "historical S&P 500 component stocks\n 306242 news articles"
            ]
        },
        {
            "id": "task460-96997d5127c345ebb960942ec0cdbf09",
            "input": "Evaluating FastText, GloVe and word2vec, we show that compared to other word representation learning algorithms, the FastText performs best. \n Question: What do you use to calculate word/sub-word embeddings",
            "output": [
                "FastText"
            ]
        },
        {
            "id": "task460-2efd3c9235874ed78672e4bf09baf82c",
            "input": "One of the most important operations on a word is to obtain the set of words whose meaning is similar to the word, or whose usage in text is similar to the word. We call this set the neighbor of the word. We have observed that the neighbor of a polysemic word consists of words that resemble the primary sense of the polysemic word.  Even though a word may be a polysemic, it usually corresponds to a single vector in distributed representation. This vector is primarily determined by the major sense, which is most frequently used. The information about a word's minor sense is subtle, and the effect of a minor sense is difficult to distinguish from statistical fluctuation. To measure the effect of a minor sense, this paper proposes to use the concept of surrounding uniformity. The surrounding uniformity roughly corresponds to statistical fluctuation in the vectors that correspond to the words in the neighbor Surrounding Uniformity (SU) can be expressed as follows: $SU(\\vec{w}) = \\frac{|\\vec{s}(\\vec{w})|}{|\\vec{w}| + \\sum _{i}^{N}|\\vec{a_i}(\\vec{w})|}$\n\nwhere $\\vec{s}(\\vec{w}) = \\vec{w} + \\sum _{i}^{N} \\vec{a_i}(\\vec{w}).$ \n Question: How is the fluctuation in the sense of the word and its neighbors measured?",
            "output": [
                "Our method performs a statistical test to determine whether a given word is used polysemously in the text, according to the following steps:\n1) Setting N, the size of the neighbor.\n2) Choosing N neighboring words ai in the order whose angle with the vector of the given word w is the smallest.\n3) Computing the surrounding uniformity for ai(0 < i ≤ N) and w.\n4) Computing the mean m and the sample variance σ for the uniformities of ai .\n5) Checking whether the uniformity of w is less than m − 3σ. If the value is less than m − 3σ, we may regard w as a polysemic word."
            ]
        },
        {
            "id": "task460-ee108ef053de4d5eb63990fbef0455e0",
            "input": "After averaging over all concepts, we lose information on the lexical variation that each concept presents but on the other hand one can now investigate which regions show similar geolectal variation, yielding well defined linguistic varieties. Those cells that have similar colors in either figure FIGREF16 or figure FIGREF17 are expected to be ascribed to the same dialect zone. Thus, we can distinguish two main regions or clusters in the maps. The purple background covers most of the map and represents rural regions with small, scattered population. Our analysis shows that this group of cells possesses more specific words in their lexicon. In contrast, the green and yellow cells form a second cluster that is largely concentrated on the center and along the coastline, which correspond to big cities and industrialized areas. In these cells, the use of standard Spanish language is widespread due probably to school education, media, travelers, etc. The character of its vocabulary is more uniform as compared with the purple group. While the purple cluster prefer particular utterances, the lexicon of the urban group includes most of the keywords. \n Question: What are the characteristics of the city dialect?",
            "output": [
                "Lexicon of the cities tend to use most forms of a particular concept"
            ]
        },
        {
            "id": "task460-fa487eb712e24fd4be65b1e55d86c5c7",
            "input": "Following previous studies BIBREF1, we collect event-related microposts from Twitter using 11 and 8 seed events (see Section SECREF2) for CyberAttack and PoliticianDeath, respectively. \n Question: Which real-world datasets are used?",
            "output": [
                "Tweets related to CyberAttack and tweets related to PoliticianDeath"
            ]
        },
        {
            "id": "task460-c09fd88911f14f2bae48d9ea589b5f4c",
            "input": "We first evaluate our model on MultiWOZ 2.0 dataset as shown in Table TABREF16. We compare with five published baselines. TRADE BIBREF3 is the current published state-of-the-art model. Table TABREF24 shows the results on WOZ $2.0$ dataset. We compare with four published baselines. SUMBT BIBREF17 is the current state-of-the-art model on WOZ 2.0 dataset. \n Question: What is current state-of-the-art model?",
            "output": [
                "SUMBT BIBREF17 is the current state-of-the-art model on WOZ 2.0 TRADE BIBREF3 is the current published state-of-the-art model"
            ]
        },
        {
            "id": "task460-8e22a51bc94048e38ba99c83078e4b9a",
            "input": "The basic idea of the visualization, drawing on Isaac Newton’s visualization of the color spectrum BIBREF8 , is to express a mixture in terms of its constituents as represented in barycentric coordinates. This visualization allows an intuitive interpretation of which country a recipe belongs to. If the probability of Japanese is high, the recipe is mapped near the Japanese. The countries on the Newton diagram are placed by spectral graph drawing BIBREF9 , so that similar countries are placed nearby on the circle.  \n Question: What is barycentric Newton diagram?",
            "output": [
                " The basic idea of the visualization, drawing on Isaac Newton’s visualization of the color spectrum BIBREF8 , is to express a mixture in terms of its constituents as represented in barycentric coordinates."
            ]
        },
        {
            "id": "task460-37de6dc188694514b14bfe3ba582752e",
            "input": "We model interactions between observations from the environment, goal, and document using layers. We first encode text inputs using bidirectional LSTMs, then compute summaries using self-attention and conditional summaries using attention. We concatenate text summaries into text features, which, along with visual features, are processed through consecutive layers. In this case of a textual environment, we consider the grid of word embeddings as the visual features for . The final output is further processed by MLPs to compute a policy distribution over actions and a baseline for advantage estimation.  \n Question: How does propose model model that capture three-way interactions?",
            "output": [
                " We first encode text inputs using bidirectional LSTMs, then compute summaries using self-attention and conditional summaries using attention. We concatenate text summaries into text features, which, along with visual features, are processed through consecutive layers. In this case of a textual environment, we consider the grid of word embeddings as the visual features for . The final output is further processed by MLPs to compute a policy distribution over actions and a baseline for advantage estimation."
            ]
        },
        {
            "id": "task460-39d947da456340a4a74b30bb555a3d9a",
            "input": "(2) Different from LEM and DPEMM, AEM uses a generator network to capture the event-related patterns and is able to mine events from different text sources (short and long). Moreover, unlike traditional inference procedure, such as Gibbs sampling used in LEM and DPEMM, AEM could extract the events more efficiently due to the CUDA acceleration; \n Question: What alternative to Gibbs sampling is used?",
            "output": [
                "generator network to capture the event-related patterns"
            ]
        },
        {
            "id": "task460-1487b4579ab045dfae794b2a8627c053",
            "input": "Numerically, Our model improves the DAR accuracy over Bi-LSTM-CRF by 2.1% and 0.8% on SwDA and MRDA respectively. \n Question: By how much do they outperform state-of-the-art solutions on SWDA and MRDA?",
            "output": [
                "improves the DAR accuracy over Bi-LSTM-CRF by 2.1% and 0.8% on SwDA and MRDA respectively"
            ]
        },
        {
            "id": "task460-aa3f0a8a12f84442bd12a84e6bc05050",
            "input": "The adaptive multi-curricula learning framework is established upon the reinforcement learning (RL) paradigm. Figure FIGREF18 illustrates the overall learning process. The multi-curricula learning scheme is scheduled according to the model's performance on the validation set, where the scheduling mechanism acts as the policy $\\pi $ interacting with the dialogue model to acquire the learning status $s$. The reward of the multi-curricula learning mechanism $m_t$ indicates how well the current dialogue model performs. A positive reward is expected if a multi-curricula scheduling action $a_t$ brings improvements on the model's performance, and the current mini-batch of training samples is drawn consulting with the scheduling action $a_t$. \n Question: How does framework automatically chooses different curricula at the evolving learning process according to the learning status of the neural dialogue generation model?",
            "output": [
                "The multi-curricula learning scheme is scheduled according to the model's performance on the validation set, where the scheduling mechanism acts as the policy $\\pi $ interacting with the dialogue model to acquire the learning status $s$. The reward of the multi-curricula learning mechanism $m_t$ indicates how well the current dialogue model performs."
            ]
        },
        {
            "id": "task460-51831509c1a343299d0ad9e310f5d9de",
            "input": "The graphs in figure 1 show the distances between duplicate and non-duplicate questions using different embedding systems. Large achieved a F1 score of 0.71 for this task without any specific training, simply by choosing a threshold below which all sentence pairs are considered duplicates. In order to test whether these results generalised to our domain, we devised a test that would make use of what little data we had to evaluate. \n Question: How is the accuracy of the system measured?",
            "output": [
                "F1 score of 0.71 for this task without any specific training, simply by choosing a threshold below which all sentence pairs are considered duplicates distances between duplicate and non-duplicate questions using different embedding systems"
            ]
        },
        {
            "id": "task460-f4fdcc3a0350412085535e427bad722e",
            "input": "Second, we provide an extensive analysis of the state-of-the-art model for the NLI task and show that our methods reveal interesting insights not available from traditional methods of inspecting attention and word saliency. \n Question: Did they use the state-of-the-art model to analyze the attention?",
            "output": [
                "we provide an extensive analysis of the state-of-the-art model"
            ]
        },
        {
            "id": "task460-a232881c2d784bc2857729f50c040042",
            "input": "the new DMN+ model does not require that supporting facts In addition, we introduce a new input module to represent images. \n Question: What improvements they did for DMN?",
            "output": [
                "the new DMN+ model does not require that supporting facts (i.e. the facts that are relevant for answering a particular question) are labeled during training. In addition, we introduce a new input module to represent images."
            ]
        },
        {
            "id": "task460-7d99d25e85fa4069b55c178db2db3e6a",
            "input": "We show that by determining and integrating heterogeneous set of features from different modalities – aesthetic features from posted images (colorfulness, hue variance, sharpness, brightness, blurriness, naturalness), choice of profile picture (for gender, age, and facial expression), the screen name, the language features from both textual content and profile's description (n-gram, emotion, sentiment), and finally sociability from ego-network, and user engagement – we can reliably detect likely depressed individuals in a data set of 8,770 human-annotated Twitter users. \n Question: What is the source of the user interaction data? ",
            "output": [
                "Sociability from ego-network on Twitter"
            ]
        },
        {
            "id": "task460-2145decfdeed441b88d2ff5bd45d983e",
            "input": "To study how the multimodal context can boost the performance compared to an unimodal context we evaluate different models: a Feature Concatenation Model (FCM), a Spatial Concatenation Model (SCM) and a Textual Kernels Model (TKM) \n Question: What models do they propose?",
            "output": [
                "Feature Concatenation Model (FCM) Spatial Concatenation Model (SCM) Textual Kernels Model (TKM)"
            ]
        },
        {
            "id": "task460-c3e0605f8d844f11bf415f197630d3b9",
            "input": "We plan to apply semantic slot scaffold to news summarization. Specifically, we can annotate the critical entities such as person names or location names to ensure that they are captured correctly in the generated summary. We also plan to collect a human-human dialog dataset with more diverse human-written summaries. \n Question: Is it expected to have speaker role, semantic slot and dialog domain annotations in real world datasets?",
            "output": [
                "Not at the moment, but summaries can be additionaly extended with this annotations."
            ]
        },
        {
            "id": "task460-e686c68fba934138b580b7cd97289826",
            "input": "We used recordings of nurse-initiated telephone conversations for congestive heart failure patients undergoing telemonitoring, post-discharge from the hospital. To analyze the linguistic structure of the inquiry-response pairs in the entire 41-hour dataset, we randomly sampled a seed dataset consisting of 1,200 turns and manually categorized them to different types, which are summarized in Table TABREF14 along with the corresponding occurrence frequency statistics. \n Question: Which data do they use as a starting point for the dialogue dataset?",
            "output": [
                "A sample from nurse-initiated telephone conversations for congestive heart failure patients undergoing telepmonitoring, post-discharge from the Health Management Unit at Changi General Hospital"
            ]
        },
        {
            "id": "task460-2e00405d9f154cba9be395f829715ae5",
            "input": "Our baselines, Honk( UID9 ), DeepSpeech-finetune( UID10 ), had comparatively both lower recall and precision. \n Question: What are the baselines?",
            "output": [
                "Honk DeepSpeech-finetune"
            ]
        },
        {
            "id": "task460-58ff38fba89347f4aff36919d60fe680",
            "input": "These datasets are part of SemEval-2016 Challenge Task 5 BIBREF27 , BIBREF28 . Table TABREF7 shows the number of observations in each test corpus. \n Question: what datasets were used in evaluation?",
            "output": [
                "SemEval-2016 Challenge Task 5 BIBREF27 , BIBREF28"
            ]
        },
        {
            "id": "task460-df33b032957a470fbed84a9210a544ff",
            "input": "SVMRank is a modification to SVM that assigns scores to each data point and allows the results to be ranked ( BIBREF26 ). We use SVMRank in the experiments below.  \n Question: what is the supervised model they developed?",
            "output": [
                "SVMRank"
            ]
        },
        {
            "id": "task460-ad2fe44cb68f4397b9b1b996caede0e9",
            "input": "For training, three types of Recurrent Neural Network (RNN) layers are used – an encoder layer, a decoder layer and an output layer. These layers together form a LSTM model. LSTM is typically used in seq2seq translation. \n Question: What is the architecture of the system?",
            "output": [
                "seq2seq translation"
            ]
        },
        {
            "id": "task460-d3b45ef57c4b4c5aad165652a2708505",
            "input": "We evaluate our proposed models on three commonly used knowledge graph datasets—WN18RR BIBREF26, FB15k-237 BIBREF18, and YAGO3-10 BIBREF27. \n Question: What benchmark datasets are used for the link prediction task?",
            "output": [
                "WN18RR FB15k-237 YAGO3-10"
            ]
        },
        {
            "id": "task460-437142d9541149e5af8b68e9ccc01c33",
            "input": "Our corpus comprises a total of 6,127 scientific entities, including 2,112 Process, 258 Method, 2,099 Material, and 1,658 Data entities.  \n Question: How large is the dataset?",
            "output": [
                "6,127 scientific entities, including 2,112 Process, 258 Method, 2,099 Material, and 1,658 Data entities"
            ]
        },
        {
            "id": "task460-0d6bd59826fe42e792e5f24053b714b5",
            "input": "Here we introduce the evaluation setup and analyze the results for the article–entity (AEP) placement task. We only report the evaluation metrics for the `relevant' news-entity pairs.  Baselines. We consider the following baselines for this task.\n\nB1. The first baseline uses only the salience-based features by Dunietz and Gillick BIBREF11 .\n\nB2. The second baseline assigns the value relevant to a pair INLINEFORM0 , if and only if INLINEFORM1 appears in the title of INLINEFORM2 . Here we show the evaluation setup for ASP task and discuss the results with a focus on three main aspects, (i) the overall performance across the years, (ii) the entity class specific performance, and (iii) the impact on entity profile expansion by suggesting missing sections to entities based on the pre-computed templates. Baselines. To the best of our knowledge, we are not aware of any comparable approach for this task. Therefore, the baselines we consider are the following:\n\nS1: Pick the section from template INLINEFORM0 with the highest lexical similarity to INLINEFORM1 : S1 INLINEFORM2\n\nS2: Place the news into the most frequent section in INLINEFORM0 \n Question: What baseline model is used?",
            "output": [
                "For Article-Entity placement, they consider two baselines: the first one using only salience-based features, and the second baseline checks if the entity appears in the title of the article. \n\nFor Article-Section Placement, they consider two baselines: the first picks the section with the highest lexical similarity to the article, and the second one picks the most frequent section."
            ]
        },
        {
            "id": "task460-a1e7a119de9345f781b07be53a942fb6",
            "input": "Our ASR and ST models follow the architecture in berard2018end, but have 3 decoder layers like that in pino2019harnessing. \n Question: What is the architecture of their model?",
            "output": [
                "follow the architecture in berard2018end, but have 3 decoder layers like that in pino2019harnessing"
            ]
        },
        {
            "id": "task460-06fd033d8f2b4a14a972f495492c8773",
            "input": "Automation of cQA forums can be divided into three tasks: question-comment relevance (Task A), question-question relevance (Task B), and question-external comment relevance (Task C). In our cQA tasks, the pair of objects are (question, question) or (question, comment), and the relationship is relevant/irrelevant. For task C, in addition to an original question (oriQ) and an external comment (relC), the question which relC commented on is also given (relQ). To incorporate this extra information, we consider a multitask learning framework which jointly learns to predict the relationships of the three pairs (oriQ/relQ, oriQ/relC, relQ/relC). \n Question: What supplemental tasks are used for multitask learning?",
            "output": [
                "Multitask learning is used for the task of predicting relevance of a comment on a different question to a given question, where the supplemental tasks are predicting relevance between the questions, and between the comment and the corresponding question"
            ]
        },
        {
            "id": "task460-d1bf22c5be6a476899bc833d47846d8d",
            "input": "The following data sources were used to train the RNN-T and associated RNN-LMs in this study.\n\nSource-domain baseline RNN-T: approximately 120M segmented utterances (190,000 hours of audio) from YouTube videos, with associated transcripts obtained from semi-supervised caption filtering BIBREF28.\n\nSource-domain normalizing RNN-LM: transcripts from the same 120M utterance YouTube training set. This corresponds to about 3B tokens of the sub-word units used (see below, Section SECREF30).\n\nTarget-domain RNN-LM: 21M text-only utterance-level transcripts from anonymized, manually transcribed audio data, representative of data from a Voice Search service. This corresponds to about 275M sub-word tokens.\n\nTarget-domain RNN-T fine-tuning data: 10K, 100K, 1M and 21M utterance-level {audio, transcript} pairs taken from anonymized, transcribed Voice Search data. These fine-tuning sets roughly correspond to 10 hours, 100 hours, 1000 hours and 21,000 hours of audio, respectively. \n Question: How much training data is used?",
            "output": [
                "163,110,000 utterances"
            ]
        },
        {
            "id": "task460-7398ff8ae2364888bc1cb5a699e11c24",
            "input": "The library design of Torch-Struct follows the distributions API used by both TensorFlow and PyTorch BIBREF29. \n Question: Does API provide ability to connect to models written in some other deep learning framework?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-64bde79ba75540b688efbb7dcf6d2ec6",
            "input": "Multiple papers have shown that 'shallow' naive Bayes classifiers BIBREF12, BIBREF8, BIBREF13, BIBREF14, SVMs BIBREF15 and similar models work very well for doing LID. The DSL 2017 paper BIBREF1, for example, gives an overview of the solutions of all of the teams that competed on the shared task and the winning approach BIBREF16 used an SVM with character n-gram, parts of speech tag features and some other engineered features. The winning approach for DSL 2015 used an ensemble naive Bayes classifier. The fasttext classifier BIBREF17 is perhaps one of the best known efficient 'shallow' text classifiers that have been used for LID . Multiple papers have proposed hierarchical stacked classifiers (including lexicons) that would for example first classify a piece of text by language group and then by exact language BIBREF18, BIBREF19, BIBREF8, BIBREF0. Researchers have investigated deeper LID models li \n Question: What is the approach of previous work?",
            "output": [
                "'shallow' naive Bayes SVM hierarchical stacked classifiers bidirectional recurrent neural networks"
            ]
        },
        {
            "id": "task460-c0a26eee0e854645807f18ef51fa2277",
            "input": "First we are comparing BiLSTMBIBREF14, BiLSTM+CNNBIBREF20, BiLSTM+CRFBIBREF1, BiLSTM+CNN+CRFBIBREF2 models with CNN modelBIBREF0 and Stanford CRF modelBIBREF21. \n Question: Which models are used to solve NER for Nepali?",
            "output": [
                "BiLSTMBIBREF14, BiLSTM+CNNBIBREF20, BiLSTM+CRFBIBREF1, BiLSTM+CNN+CRFBIBREF2 CNN modelBIBREF0 and Stanford CRF modelBIBREF21"
            ]
        },
        {
            "id": "task460-8273acb782ab4548be386db84bd591c3",
            "input": "Our system significantly out-performed the baseline on the joint task, as the baseline scored significantly lower for the gender task than for the variety task. \n Question: On which task does do model do best?",
            "output": [
                "Variety prediction task"
            ]
        },
        {
            "id": "task460-f930a824ba4e4188a43b967737c1be46",
            "input": "Secondly, texts go through a cascade of annotation tools, enriching it with the following information:\n\nMorphosyntactic interpretations (sets of tags), using Morfeusz 0.82 BIBREF25 ,\n\nTagging (selection of the most probable interpretation), using a transformation-based learning tagger, PANTERA 0.9.1 BIBREF26 ,\n\nSyntactic groups (possibly nested) with syntactic and semantic heads, using a rule-based shallow parser Spejd 1.3.7 BIBREF27 with a Polish grammar, including improved version of modifications by BIBREF28 , enabling lemmatisation of nominal syntactic groups,\n\nNamed entities, using two available tools: NERF 0.1 BIBREF29 and Liner2 2.3 BIBREF30 .\n\n \n Question: How is the data in RAFAEL labelled?",
            "output": [
                "Using a set of annotation tools such as Morfeusz, PANTERA, Spejd, NERF and Liner"
            ]
        },
        {
            "id": "task460-5a57e7c09782466b915f51bd093dc2ab",
            "input": "We compare the SBERT sentence embeddings to other sentence embeddings methods on the following seven SentEval transfer tasks:\n\nMR: Sentiment prediction for movie reviews snippets on a five start scale BIBREF25.\n\nCR: Sentiment prediction of customer product reviews BIBREF26.\n\nSUBJ: Subjectivity prediction of sentences from movie reviews and plot summaries BIBREF27.\n\nMPQA: Phrase level opinion polarity classification from newswire BIBREF28.\n\nSST: Stanford Sentiment Treebank with binary labels BIBREF29.\n\nTREC: Fine grained question-type classification from TREC BIBREF30.\n\nMRPC: Microsoft Research Paraphrase Corpus from parallel news sources BIBREF31. \n Question: What transfer learning tasks are evaluated?",
            "output": [
                "MR CR SUBJ MPQA SST TREC MRPC"
            ]
        },
        {
            "id": "task460-66959e883ef24e91a6157c6bbe4e8ce9",
            "input": "We use Gaussian Processes as this probabilistic kernelised framework avoids the need for expensive cross-validation for hyperparameter selection \n Question: Why is a Gaussian process an especially appropriate method for this classification problem?",
            "output": [
                "avoids the need for expensive cross-validation for hyperparameter selection"
            ]
        },
        {
            "id": "task460-4957da3bcf9b40688e17ef3abaa8122b",
            "input": "We used the Visual storytelling (VIST) dataset comprising of image sequences obtained from Flickr albums and respective annotated descriptions collected through Amazon Mechanical Turk BIBREF1. Each sequence has 5 images with corresponding descriptions that together make up for a story. Furthermore, for each Flickr album there are 5 permutations of a selected set of its images. In the overall available data there are 40,071 training, 4,988 validation, and 5,050 usable testing stories. \n Question: What statistics on the VIST dataset are reported?",
            "output": [
                "In the overall available data there are 40,071 training, 4,988 validation, and 5,050 usable testing stories."
            ]
        },
        {
            "id": "task460-86dcf07828364812b1eaf07a561051ee",
            "input": "As a further application of our work, we have carried out a supervised classification task aimed at predicting the degree of harm of an incident directly from the text and the hand-coded features (e.g., external category, medical specialty, location).  We also checked if using our unsupervised content-driven cluster labels as additional features can improve the performance of the supervised classification. \n Question: How are content clusters used to improve the prediction of incident severity?",
            "output": [
                "they are used as additional features in a supervised classification task"
            ]
        },
        {
            "id": "task460-5b5f1b9f6bb94128b57f028f57ea7b31",
            "input": "We have also included the unlabelled set of 2472 telephone calls from both minor (Cebuano and Mandarin) and major (Tagalog and Cantonese) languages provided by NIST in the system training. \n Question: Which are the novel languages on which SRE placed emphasis on?",
            "output": [
                "Cebuano and Mandarin Tagalog and Cantonese"
            ]
        },
        {
            "id": "task460-95c97a65b6124114b23ac6b74fd0d0ff",
            "input": "In order to achieve these configuration the TIMIT data was split. Fig. FIGREF12 illustrates the split of the data into 8 subsets (A–H). The TIMIT dataset contains speech from 462 speakers in training and 168 speakers in the test set, with 8 utterances for each speaker. The TIMIT training and test set are split into 8 blocks, where each block contains 2 utterances per speaker, randomly chosen. Thus each block A,B,C,D contains data from 462 speakers with 924 utterances taken from the training sets, and each block E,F,G,H contains speech from 168 test set speakers with 336 utterances.\n\nFor Task a training of embeddings and the classifier is identical, namely consisting of data from blocks (A+B+C+E+F+G). The test data is the remainder, namely blocks (D+H). For Task b the training of embeddings and classifiers uses (A+B+E+F) and (C+G) respectively, while again using (D+H) for test. Task c keeps both separate: embeddings are trained on (A+B+C+D), classifiers on (E+G) and tests are conducted on (F+H). Note that H is part of all tasks, and that Task c is considerably easier as the number of speakers to separate is only 168, although training conditions are more difficult. \n Question: What TIMIT datasets are used for testing?",
            "output": [
                "Once split into 8 subsets (A-H), the test set used are blocks D+H and blocks F+H"
            ]
        },
        {
            "id": "task460-fa76cbfd514f4d799322d67c95a93cca",
            "input": "A context is upward entailing (shown by [... $\\leavevmode {\\color {red!80!black}\\uparrow }$ ]) that allows an inference from ( \"Introduction\" ) to ( \"Introduction\" ), where French dinner is replaced by a more general concept dinner.  On the other hand, a downward entailing context (shown by [... $\\leavevmode {\\color {blue!80!black}\\downarrow }$ ]) allows an inference from ( \"Introduction\" ) to ( \"Introduction\" ), where workers is replaced by a more specific concept new workers. All [ workers $\\leavevmode {\\color {blue!80!black}\\downarrow }$ ] [joined for a French dinner $\\leavevmode {\\color {red!80!black}\\uparrow }$ ] All workers joined for a dinner All new workers joined for a French dinner Not all [new workers $\\leavevmode {\\color {red!80!black}\\uparrow }$ ] joined for a dinner Not all workers joined for a dinner \n Question: How do they define upward and downward reasoning?",
            "output": [
                "Upward reasoning is defined as going from one specific concept to a more general one. Downward reasoning is defined as the opposite, going from a general concept to one that is more specific."
            ]
        },
        {
            "id": "task460-c343875aeebd4ab082c28b64a498f76c",
            "input": "Since our methodology does not use a seed list of offensive words, it is not biased by topic, target, or dialect. Using our methodology, we tagged 10,000 Arabic tweet dataset for offensiveness, where offensive tweets account for roughly 19% of the tweets. Further, we labeled tweets as vulgar or hate speech. To date, this is the largest available dataset, which we plan to make publicly available along with annotation guidelines. We use this dataset to characterize Arabic offensive language to ascertain the topics, dialects, and users' gender that are most associated with the use of offensive language.  \n Question: In what way is the offensive dataset not biased by topic, dialect or target?",
            "output": [
                "It does not use a seed list to gather tweets so the dataset does not skew to specific topics, dialect, targets."
            ]
        },
        {
            "id": "task460-756ac4301a2f4ea9befb5b507bdf4155",
            "input": "As neural classification model, we use a convolutional neural network (CNN) BIBREF29, which has previously shown good results for tweet classification BIBREF30, BIBREF27. \n Question: What neural classifiers are used?",
            "output": [
                " convolutional neural network (CNN) BIBREF29"
            ]
        },
        {
            "id": "task460-59d259c5c42f43b99b6d3c7b010780d2",
            "input": "This paper explores the potential of predicting a user's industry –the aggregate of enterprises in a particular field– by identifying industry indicative text in social media.  \n Question: What do they mean by a person's industry?",
            "output": [
                "the aggregate of enterprises in a particular field"
            ]
        },
        {
            "id": "task460-38fd4a38506b4fbba6b8f022685b66d5",
            "input": "CodeInternational: A tool which can translate code between human languages, powered by Google Translate.\n\n \n Question: Is this auto translation tool based on neural networks?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-cd44ecadad914c36b51e411dee74f760",
            "input": "For each variable in encoding set, learn the new embeddings using the embeddings train set . Since there is stochasticity in the embeddings training model, we will repeat the above multiple times, for the different experiments in the paper (and report the respective mean and standard deviation statistics). \n Question: How do their train their embeddings?",
            "output": [
                "The embeddings are learned several times using the training set, then the average is taken."
            ]
        },
        {
            "id": "task460-1758c8021c204bd9855e64d2881b8bfe",
            "input": "DeepMine is publicly available for everybody with a variety of licenses for different users. It was collected using crowdsourcing BIBREF4. The data collection was done using an Android application. Each respondent installed the application on his/her personal device and recorded several phrases in different sessions.  \n Question: how was the speech collected?",
            "output": [
                "The speech was collected from respondents using an android application."
            ]
        },
        {
            "id": "task460-ed8b77a778974842a39480d37486a4ae",
            "input": "The theorem is proven constructively by selecting the parameters of the multi-head self-attention layer so that the latter acts like a convolutional layer. \n Question: How they prove that multi-head self-attention is at least as powerful as convolution layer? ",
            "output": [
                "constructively by selecting the parameters of the multi-head self-attention layer so that the latter acts like a convolutional layer"
            ]
        },
        {
            "id": "task460-32cd50bf07414714bbcb8f15ccc8c54d",
            "input": "We use the guidelines of the state-of-the-art approach of BIBREF12 that achieved the highest inflection accuracy in the latest SIGMORPHON 2019 morphological inflection shared task. Our models are implemented in DyNet BIBREF13. We trained an encoder-decoder model that receives the form as character-level input, encodes it with a BiLSTM encoder, and then an attention enhanced decoder BIBREF14 outputs the corresponding sequence of morphological tags, implemented in DyNet. We trained a character level encoder-decoder model, similar to the above-mentioned inflection system, implemented in DyNet. \n Question: What are the architectures used for the three tasks?",
            "output": [
                "DyNet"
            ]
        },
        {
            "id": "task460-6b3f9ae5108948b4a0a7f1166dc605e3",
            "input": "Since our objective is to compare character-based and word-based approaches, we have also implemented a simple word-level encoder for tweets. The input tweet is first split into tokens along white-spaces. A more sophisticated tokenizer may be used, but for a fair comparison we wanted to keep language specific preprocessing to a minimum. The encoder is essentially the same as tweet2vec, with the input as words instead of characters. A lookup table stores word vectors for the $V$ (20K here) most common words, and the rest are grouped together under the `UNK' token. \n Question: What is the word-level baseline?",
            "output": [
                "a simple word-level encoder The encoder is essentially the same as tweet2vec, with the input as words instead of characters."
            ]
        },
        {
            "id": "task460-14579c1c9a88466e942aea3d580ca5ca",
            "input": "to answer how offensive they thought the tweet was on a 6-point Likert scale from 1 (Not offensive at all) to 6 (Very offensive). If they answered 4 or higher, the participants had the option to state which particular words they found offensive. \n Question: Was the degree of offensiveness taken as how generally offensive the text was, or how personally offensive it was to the annotator?",
            "output": [
                "Personal thought of the annotator."
            ]
        },
        {
            "id": "task460-bb9a5a64017b4157a0d5fb06ef636fa5",
            "input": "We have shown that neural network-based models can outperform feature-based models with wide margins, and we conducted an ablation study to show that contextualized representation learning can boost performance of NN models. \n Question: What conclusions do the authors draw from their detailed analyses?",
            "output": [
                "neural network-based models can outperform feature-based models with wide margins contextualized representation learning can boost performance of NN models"
            ]
        },
        {
            "id": "task460-27fb9def65e74e05a01f864a27c150c2",
            "input": "Ablation Study\nTo further investigate the efficacy of the key components in our framework, namely, THA and STN, we perform ablation study as shown in the second block of Table TABREF39 . The results show that each of THA and STN is helpful for improving the performance, and the contribution of STN is slightly larger than THA. “OURS w/o THA & STN” only keeps the basic bi-linear attention. Although it performs not bad, it is still less competitive compared with the strongest baseline (i.e., CMLA), suggesting that only using attention mechanism to distill opinion summary is not enough. After inserting the STN component before the bi-linear attention, i.e. “OURS w/o THA”, we get about 1% absolute gains on each dataset, and then the performance is comparable to CMLA. By adding THA, i.e. “OURS”, the performance is further improved, and all state-of-the-art methods are surpassed. \n Question: Do they explore how useful is the detection history and opinion summary?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-bbc19373d9f641abbe7f64557f5db23d",
            "input": "Three annotators, who have no knowledge about which system the response is from, are then required to evaluate among win (response$_1$ is better), loss (response$_2$ is better) and tie (they are equally good or bad) independently, considering four aspects: coherence, logical consistency, fluency and diversity. \n Question: What human judgement metrics are used?",
            "output": [
                "coherence, logical consistency, fluency and diversity"
            ]
        },
        {
            "id": "task460-d6502dc56e0e47ebb316577c76e4d00e",
            "input": "The data stems from a joint ADAPT-Microsoft project.  \n Question: what dataset was used?",
            "output": [
                "The dataset from a joint ADAPT-Microsoft project"
            ]
        },
        {
            "id": "task460-b632171055fd4e088ffbd8fcfc813f58",
            "input": "Our in-house dataset includes manually annotated RE data for 6 languages: English, German, Spanish, Italian, Japanese and Portuguese.  The ACE05 dataset includes manually annotated RE data for 3 languages: English, Arabic and Chinese. It defines 7 entity types (Person, Organization, Geo-Political Entity, Location, Facility, Weapon, Vehicle) and 6 relation types between the entities (Agent-Artifact, General-Affiliation, ORG-Affiliation, Part-Whole, Personal-Social, Physical). \n Question: What languages do they experiment on?",
            "output": [
                "English, German, Spanish, Italian, Japanese and Portuguese  English, Arabic and Chinese"
            ]
        },
        {
            "id": "task460-4a87ed3dde8340108e158aac2c69cf1f",
            "input": "The final results for our F1-score from this classification technique are 0.52 and 0.31 for the single and multi-turn data respectively.\n\nWe compare our results for both the single-turn and multi-turn experiments to the accuracies on the test data based off the BLEU score. \n Question: which existing metrics do they compare with?",
            "output": [
                "F1-score BLEU score"
            ]
        },
        {
            "id": "task460-857a843ce8044fb5b7b85764d5ae2487",
            "input": "For annotation purposes, we created snippets of conversations as the ones shown in Example 1 and Example 2 consisting of the parent of the suspected trolling event, the suspected trolling event comment, and all of the direct responses to the suspected trolling event. However, the trade off is that snippets like this allow us to make use of Amazon Mechanical Turk (AMT) to have the dataset annotated, because it is not a big burden for a “turker” to work on an individual snippet in exchange for a small pay, and expedites the annotation process by distributing it over dozens of people. Specifically, for each snippet, we requested three annotators to label the four aspects previously described. \n Question: how was annotation done?",
            "output": [
                "Annotation was done with the help of annotators from Amazon Mechanical Turk on snippets of conversations"
            ]
        },
        {
            "id": "task460-35de5cde75284833bc802df569417672",
            "input": "Data was collected from a 10% uniform sample of Twitter posts made during 2013, specifically the Gardenhose API. Control documents were also selected. These documents did not contain any of `caused', `causing', or `causes', nor any bidirectional words, and are further matched temporally to obtain the same number of control documents as causal documents in each fifteen-minute period during 2013. Control documents were otherwise selected randomly; causal synonyms may be present.  \n Question: How do they collect the control corpus?",
            "output": [
                "Randomly from Twitter"
            ]
        },
        {
            "id": "task460-ada3b46c38ce47bc9d7ac1566d303a65",
            "input": "machine comprehension  Nelufar  \n Question: What MC abbreviate for?",
            "output": [
                "machine comprehension"
            ]
        },
        {
            "id": "task460-edce46dc72a14cc684acbc0f31a68e91",
            "input": "We optimized our single-task baseline to get a strong baseline in order to exclude better results in multi-task learning in comparison to single-task learning only because of these two following points: network parameters suit the multi-task learning approach better and a better randomness while training in the multi-task learning. To exclude the first point, we tested different hyperparameters for the single-task baseline. We tested all the combinations of the following hyperparameter values: 256, 512, or 1024 as the sizes for the hidden states of the LSTMs, 256, 512, or 1024 as word embedding sizes, and a dropout of 30 %, 40 %, or 50 %. We used subword units generated by byte-pair encoding (BPE) BIBREF16 as inputs for our model. To avoid bad subword generation for the synthetic datasets, in addition to the training dataset, we considered the validation and test dataset for the generating of the BPE merge operations list. We trained the configurations for 14 epochs and trained every configuration three times. We chose the training with the best quality with regard to the validation F1-score to exclude disadvantages of a bad randomness. We got the best quality with regard to the F1-score with 256 as the size of the hidden states of the LSTMs, 1024 as word embedding size, and a dropout of 30 %. For the batch size, we used 64. \n Question: What are the strong baselines you have?",
            "output": [
                "optimize single task with no synthetic data"
            ]
        },
        {
            "id": "task460-535782699c8e4153abf90c25c00b8e88",
            "input": "TABREF5 displays the output behavior of models that we damaged to resemble the damage that causes aphasia. Because the output languages in all of our domains use tokens to represent meanings in many cases, it is expected that the analog to Wernicke's area is responsible for maintaining a high precision. \n Question: Do they perform a quantitative analysis of their model displaying knowledge distortions?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-4dca0b96a7994f299f024037e99e916d",
            "input": "For the image-aided model (W+C+V; upper row in Figure FIGREF19 ), we confirm that the modality attention successfully attenuates irrelevant signals (selfies, etc.) and amplifies relevant modality-based contexts in prediction of a given token. \n Question: Do they inspect their model to see if their model learned to associate image parts with words related to entities?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-c30e940137f2408e8647c0fbb9a9c855",
            "input": "Here, we outline the required steps for developing a knowledge graph of interlinked events. Figure FIGREF2 illustrates the high-level overview of the full pipeline. This pipeline contains the following main steps, to be discussed in detail later. (1) Collecting tweets from the stream of several news channels such as BBC and CNN on Twitter. (2) Agreeing upon background data model. (3) Event annotation potentially contains two subtasks (i) event recognition and (ii) event classification. (4) Entity/relation annotation possibly comprises a series of tasks as (i) entity recognition, (ii) entity linking, (iii) entity disambiguation, (iv) semantic role labeling of entities and (v) inferring implicit entities. (5) Interlinking events across time and media. (6) Publishing event knowledge graph based on the best practices of Linked Open Data. \n Question: Which news organisations are the headlines sourced from?",
            "output": [
                "BBC and CNN "
            ]
        },
        {
            "id": "task460-68b8e5c489394e29a7abadae6633ec28",
            "input": "We presented a new approach for converting between masculine-inflected and feminine-inflected noun phrases in morphologically rich languages. To do this, we introduced a Markov random field with an optional neural parameterization that infers the manner in which a sentence must change to preserve morpho-syntactic agreement when altering the grammatical gender of particular nouns. \n Question: Which model do they use to convert between masculine-inflected and feminine-inflected sentences?",
            "output": [
                "Markov random field with an optional neural parameterization"
            ]
        },
        {
            "id": "task460-60bfd1be210e463b89c6e895fa52951d",
            "input": "The performances of the NER experiments are reported separately for three different parts of the system proposed. Results shown that, in the first case, in the training set the F1 score is always greater than 97%, with a maximum of 99.65%. With both test sets performances decrease, varying between 94-97%. In the case of UGC, comparing the F1 score we can observe how performances significantly decrease. \n Question: What are their results on the entity recognition task?",
            "output": [
                "With both test sets performances decrease, varying between 94-97%"
            ]
        },
        {
            "id": "task460-5bce4bf477d54814aeb5835bd1b1ef24",
            "input": "VQA research began in earnest in late 2014 when the DAQUAR dataset was released BIBREF0 \n Question: From when are many VQA datasets collected?",
            "output": [
                "late 2014"
            ]
        },
        {
            "id": "task460-9d39b3ca94d746378887b6a28740909b",
            "input": "The first type of classifier we evaluate in this work are feedforward neural networks (DNNs) consisting of 3 hidden layers, each with 512 rectified linear units (ReLUs) with a softmax activation function.  As a second classifier to evaluate, we use convolutional neural networks (CNNs) with 2 convolutional and max pooling layers, followed by 2 fully-connected ReLU layers with 512 nodes.  \n Question: What model do they use to classify phonetic segments? ",
            "output": [
                "feedforward neural networks convolutional neural networks"
            ]
        },
        {
            "id": "task460-8d0324e17e9d44d6b8734bd3531f3508",
            "input": "Their results improved the baseline provided by BIBREF8 . Another influential work was BIBREF12 , an unsupervised algorithm called Double Propagation which roughly consists of incrementally augmenting a set of seeds via dependency parsing. n spite of this, Table TABREF20 shows that our system outperforms the best previous approaches across the five languages. In some cases, such as Turkish and Russian, the best previous scores were the baselines provided by the ABSA organizers, but for Dutch, French and Spanish our system is significantly better than current state-of-the-art. In particular, and despite using the same system for every language, we improve over GTI's submission, which implemented a CRF system with linguistic features specific to Spanish BIBREF28 . \n Question: What was the baseline?",
            "output": [
                "the baseline provided by BIBREF8 the baselines provided by the ABSA organizers"
            ]
        },
        {
            "id": "task460-95672f1f35f64d428549493215606606",
            "input": "Subsequently, an LSTM decoder generates the characters in the output word form using encoder states and an attention mechanism. \n Question: What architecture does the decoder have?",
            "output": [
                "LSTM"
            ]
        },
        {
            "id": "task460-ad97552a708748d1adf6207a3828467f",
            "input": "The datasets used for training, validation and testing contain sentences extracted from the Europarl corpus BIBREF1 and SoNaR corpus BIBREF2. The Europarl corpus is an open-source parallel corpus containing proceedings of the European Parliament. The Dutch section consists of 2,333,816 sentences and 53,487,257 words. The SoNaR corpus comprises two corpora: SONAR500 and SONAR1. The SONAR500 corpus consists of more than 500 million words obtained from different domains. \n Question: What are the sizes of both datasets?",
            "output": [
                "The Dutch section consists of 2,333,816 sentences and 53,487,257 words. The SONAR500 corpus consists of more than 500 million words obtained from different domains."
            ]
        },
        {
            "id": "task460-af2f8a8778a242ae903bc098c9c0aae7",
            "input": "This decoding approach closely follows Algorithm SECREF7 , but along with soft back pointers, we also compute hard back pointers at each time step. After computing all the relevant quantities like model score, loss etc., we follow the hard backpointers to obtain the best sequence INLINEFORM0 . \n Question: Do they compare partially complete sequences (created during steps of beam search) to gold/target sequences?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-4a7cf4560f464b37aaaec1ca14bf9539",
            "input": "We used fastText and SVM BIBREF16 for preliminary experiments.  \n Question: What classification models were used?",
            "output": [
                "fastText and SVM BIBREF16"
            ]
        },
        {
            "id": "task460-8d87e9d977ff4e2ab7cde1311b42a28c",
            "input": "For English all-words WSD, we train our WSD model on SemCor BIBREF24, and test it on Senseval-2 (SE2), Senseval-3 (SE3), SemEval 2013 task 12 (SE13), and SemEval 2015 task 13 (SE15). We use OntoNotes Release 5.0, which contains a number of annotations including word senses for Chinese. \n Question: What datasets are used for testing?",
            "output": [
                "Senseval-2 (SE2), Senseval-3 (SE3), SemEval 2013 task 12 (SE13), and SemEval 2015 task 13 (SE15) OntoNotes Release 5.0"
            ]
        },
        {
            "id": "task460-150b2619aad64b33bcf0e43eb586ced7",
            "input": "We used Amazon Mechanical Turk (AMT) for our annotations, restricting the task to workers in five English-speaking countries (USA, UK, Canada, New Zealand, and Australia), more than 1000 finished HITs and at least a 95% acceptance rate. \n Question: What crowdsourcing platform did they use?",
            "output": [
                "Amazon Mechanical Turk (AMT)"
            ]
        },
        {
            "id": "task460-caef761b2f00404199c00e1ae6d9a9d5",
            "input": "This work analyzes the understanding of pretrained language models of factual and commonsense knowledge stored in negated statements. To this end, we introduce the negated LAMA dataset. We construct it by simply inserting negation elements (e.g., “not”) in LAMA cloze statement (e.g., “The theory of relativity was not developed by [MASK].”). \n Question: How did they extend LAMA evaluation framework to focus on negation?",
            "output": [
                "To this end, we introduce the negated LAMA dataset. We construct it by simply inserting negation elements (e.g., “not”) in LAMA cloze statement"
            ]
        },
        {
            "id": "task460-0d2903be3ab540a7a43272b3abb73f61",
            "input": "The following data sources were used to train the RNN-T and associated RNN-LMs in this study.\n\nSource-domain baseline RNN-T: approximately 120M segmented utterances (190,000 hours of audio) from YouTube videos, with associated transcripts obtained from semi-supervised caption filtering BIBREF28.\n\nSource-domain normalizing RNN-LM: transcripts from the same 120M utterance YouTube training set. This corresponds to about 3B tokens of the sub-word units used (see below, Section SECREF30).\n\nTarget-domain RNN-LM: 21M text-only utterance-level transcripts from anonymized, manually transcribed audio data, representative of data from a Voice Search service. This corresponds to about 275M sub-word tokens.\n\nTarget-domain RNN-T fine-tuning data: 10K, 100K, 1M and 21M utterance-level {audio, transcript} pairs taken from anonymized, transcribed Voice Search data. These fine-tuning sets roughly correspond to 10 hours, 100 hours, 1000 hours and 21,000 hours of audio, respectively. \n Question: How is the training data collected?",
            "output": [
                "from YouTube videos, with associated transcripts obtained from semi-supervised caption filtering from a Voice Search service"
            ]
        },
        {
            "id": "task460-50029070e9414f1598720a987f1cf538",
            "input": "The problem with morpheme segmentation is that the vocabulary of stem units is still very large, which leads to many rare and unknown words at the training time.  Therefore, we learn a BPE model on the stem units in the training corpus rather than the words, and then apply it on the stem unit of each word after morpheme segmentation. \n Question: How is morphology knowledge implemented in the method?",
            "output": [
                "A BPE model is applied to the stem after morpheme segmentation."
            ]
        },
        {
            "id": "task460-544ee97932ae48d0a9da90d1502fdad9",
            "input": "As shown in Figure FIGREF7 , KAR is an end-to-end MRC model consisting of five layers:\n\nLexicon Embedding Layer. This layer maps the words to the lexicon embeddings. Context Embedding Layer. This layer maps the lexicon embeddings to the context embeddings. Coarse Memory Layer. This layer maps the context embeddings to the coarse memories. Refined Memory Layer. This layer maps the coarse memories to the refined memories. Answer Span Prediction Layer. This layer predicts the answer start position and the answer end position based on the above layers. \n Question: What type of model is KAR?",
            "output": [
                "Lexicon Embedding Layer Context Embedding Layer Coarse Memory Layer Refined Memory Layer Answer Span Prediction Layer"
            ]
        },
        {
            "id": "task460-5ba07c0462bc42ed8fa9561c5dfc80bd",
            "input": "We employ the standard ROUGE-1, ROUGE-2 and ROUGE-L metrics BIBREF29 to evaluate all summarization models. Following BIBREF22, BIBREF23, we use F-measure ROUGE on XSUM and CNN/DailyMail, and use limited-length recall-measure ROUGE on NYT and DUC. \n Question: What metric was used in the evaluation step?",
            "output": [
                "ROUGE-1, ROUGE-2 and ROUGE-L F-measure ROUGE on XSUM and CNN/DailyMail, and use limited-length recall-measure ROUGE on NYT and DUC"
            ]
        },
        {
            "id": "task460-7b88b3a7fe024656a236c90a456b09b6",
            "input": " Our baseline is a three-layer LSTM-LM with 1,150 hidden units at internal layers trained with the standard cross-entropy loss.  \n Question: What neural language models are explored?",
            "output": [
                "LSTM-LM "
            ]
        },
        {
            "id": "task460-3ef7007dee204d4a9761b68808c5e08e",
            "input": "$\\mathit {\\texttt {+}PPMI}$ only falters on the RW and analogy tasks, and we hypothesize this is where $\\mathit {\\texttt {-}PMI}$ is useful: in the absence of positive information, negative information can be used to improve rare word representations and word analogies. \n Question: What are the disadvantages to clipping negative PMI?",
            "output": [
                "It may lead to poor rare word representations and word analogies."
            ]
        },
        {
            "id": "task460-a2bce257649647e18da0b3202bc4ea08",
            "input": "We work with four individual datasets. The datasets contain addition, subtraction, multiplication, and division word problems.\n\nAI2 BIBREF2. AI2 is a collection of 395 addition and subtraction problems, containing numeric values, where some may not be relevant to the question.\n\nCC BIBREF19. The Common Core dataset contains 600 2-step questions. The Cognitive Computation Group at the University of Pennsylvania gathered these questions.\n\nIL BIBREF4. The Illinois dataset contains 562 1-step algebra word questions. The Cognitive Computation Group compiled these questions also.\n\nMAWPS BIBREF20. MAWPS is a relatively large collection, primarily from other MWP datasets. We use 2,373 of 3,915 MWPs from this set. \n Question: What datasets do they use?",
            "output": [
                "AI2 BIBREF2 CC BIBREF19 IL BIBREF4 MAWPS BIBREF20"
            ]
        },
        {
            "id": "task460-6cda9338a13c48b5a83687a2e4bb7bb4",
            "input": "The only remaining question is what makes two environments similar enough to infer the existence of a common category. There is, again, a large literature on this question (including the aforementioned language modeling, unsupervised parsing, and alignment work), but in the current work we will make use of a very simple criterion: fragments are interchangeable if they occur in at least one lexical environment that is exactly the same. \n Question: How do they determine similar environments for fragments in their data augmentation scheme?",
            "output": [
                "fragments are interchangeable if they occur in at least one lexical environment that is exactly the same"
            ]
        },
        {
            "id": "task460-b89007938ae34b4eb54014bf16602b25",
            "input": "The third person plural pronoun `they' has no gender in English and most other languages (unlike the third person singular pronouns `he', `she', and `it').  If these sentences are translated into French, then `they' in the first sentence should be translated `elles', as referring to Jane and Susan, and `they' in the second sentence should be translated `ils', as referring to Fred and George. A couple of examples: The word `sie' in German serves as both the formal second person prounoun (always capitalized), the third person feminine singular, and the third person plural.  \n Question: What language do they explore?",
            "output": [
                "English French German "
            ]
        },
        {
            "id": "task460-518bc53f535c4523ab32b91d83b4b915",
            "input": "We use two popular knowledge bases: WordNet BIBREF0 and Freebase BIBREF1 in this paper. Specifically, we use WN18 (a subset of WordNet) BIBREF24 and FB15K (a subset of Freebase) BIBREF2 since their text descriptions are easily publicly available. Table 1 lists statistics of the two datasets. \n Question: What datasets are used to evaluate this paper?",
            "output": [
                "WordNet BIBREF0 Freebase BIBREF1 WN18 (a subset of WordNet) BIBREF24  FB15K (a subset of Freebase) BIBREF2"
            ]
        },
        {
            "id": "task460-f16cc4429f1046feb1fb7487032c9d40",
            "input": "We found that properties of the emerging lexicon related to the combinatoriality, namely the words length distribution, the frequency of use of the different forms and a measure for the combinatoriality itself, reflect both qualitatively and quantitatively the corresponding properties as measured in human languages, provided that the memory parameter $\\tau $ is sufficiently high, that is that a sufficiently high effort is required in order to understand and learn brand new forms. \n Question: What empirical data are the Blending Game predictions compared to?",
            "output": [
                "words length distribution, the frequency of use of the different forms and a measure for the combinatoriality"
            ]
        },
        {
            "id": "task460-56d38e99201a48bc9d81c9b515cdc4fd",
            "input": "A baseline model for automatic extraction of anglicisms was created using the annotated corpus we just presented as training material. As mentioned in Section 3, the task of detecting anglicisms can be approached as a sequence labeling problem where only certain spans of texts will be labeled as anglicism (in a similar way to an NER task). The chosen model was conditional random field model (CRF), which was also the most popular model in both Shared Tasks on Language Identification for Code-Switched Data BIBREF23, BIBREF24. \n Question: Does the paper motivate the use of CRF as the baseline model?",
            "output": [
                "the task of detecting anglicisms can be approached as a sequence labeling problem where only certain spans of texts will be labeled as anglicism (in a similar way to an NER task). The chosen model was conditional random field model (CRF), which was also the most popular model in both Shared Tasks on Language Identification for Code-Switched Data"
            ]
        },
        {
            "id": "task460-812830ed2e2c43baa392534c1d5274de",
            "input": "Corpus Embedding\nSuppose that INLINEFORM0 is the target low-resource corpus, we are interested in optimizing the acoustic model with a much larger training corpora set INLINEFORM1 where INLINEFORM2 is the number of corpora and INLINEFORM3 . Each corpus INLINEFORM4 is a collection of INLINEFORM5 pairs where INLINEFORM6 is the input features and INLINEFORM7 is its target.\n\nOur purpose here is to compute the embedding INLINEFORM0 for each corpus INLINEFORM1 where INLINEFORM2 is expected to encode information about its corpus INLINEFORM3 . Those embeddings can be jointly trained with the standard multilingual model BIBREF4 . First, the embedding matrix INLINEFORM4 for all corpora is initialized, the INLINEFORM5 -th row of INLINEFORM6 is corresponding to the embedding INLINEFORM7 of the corpus INLINEFORM8 . Next, during the training phase, INLINEFORM9 can be used to bias the input feature INLINEFORM10 as follows. DISPLAYFORM0\n\nwhere INLINEFORM0 is an utterance sampled randomly from INLINEFORM1 , INLINEFORM2 is its hidden features, INLINEFORM3 is the parameter of the acoustic model and Encoder is the stacked bidirectional LSTM as shown in Figure. FIGREF5 . Next, we apply the language specific softmax to compute logits INLINEFORM4 and optimize them with the CTC objective BIBREF29 . The embedding matrix INLINEFORM5 can be optimized together with the model during the training process. \n Question: How do they compute corpus-level embeddings?",
            "output": [
                "First, the embedding matrix INLINEFORM4 for all corpora is initialized during the training phase, INLINEFORM9 can be used to bias the input feature Next, we apply the language specific softmax to compute logits INLINEFORM4 and optimize them with the CTC objective"
            ]
        },
        {
            "id": "task460-3ddbd299c68740d2b36be52b4ff4c67f",
            "input": "As a first corpus, we compiled INLINEFORM0 that represents a collection of 80 excerpts from scientific works including papers, dissertations, book chapters and technical reports, which we have chosen from the well-known Digital Bibliography & Library Project (DBLP) platform. As a second corpus, we compiled INLINEFORM0 , which represents a collection of 1,645 chat conversations of 550 sex offenders crawled from the Perverted-Justice portal. As a third corpus, we compiled INLINEFORM0 , which is a collection of 200 aggregated postings crawled from the Reddit platform. \n Question: What size are the corpora?",
            "output": [
                "80 excerpts from scientific works collection of 1,645 chat conversations collection of 200 aggregated postings"
            ]
        },
        {
            "id": "task460-3418d919f23e4162a7191b2c78374cf0",
            "input": "1. BERT based fine-tuning: In the first approach, which is shown in Figure FIGREF8, very few changes are applied to the BERTbase. In this architecture, only the [CLS] token output provided by BERT is used. The [CLS] output, which is equivalent to the [CLS] token output of the 12th transformer encoder, a vector of size 768, is given as input to a fully connected network without hidden layer. The softmax activation function is applied to the hidden layer to classify.\n\n2. Insert nonlinear layers: Here, the first architecture is upgraded and an architecture with a more robust classifier is provided in which instead of using a fully connected network without hidden layer, a fully connected network with two hidden layers in size 768 is used. The first two layers use the Leaky Relu activation function with negative slope = 0.01, but the final layer, as the first architecture, uses softmax activation function as shown in Figure FIGREF8.\n\n3. Insert Bi-LSTM layer: Unlike previous architectures that only use [CLS] as the input for the classifier, in this architecture all outputs of the latest transformer encoder are used in such a way that they are given as inputs to a bidirectional recurrent neural network (Bi-LSTM) as shown in Figure FIGREF8. After processing the input, the network sends the final hidden state to a fully connected network that performs classification using the softmax activation function.\n\n4. Insert CNN layer: In this architecture shown in Figure FIGREF8, the outputs of all transformer encoders are used instead of using the output of the latest transformer encoder. So that the output vectors of each transformer encoder are concatenated, and a matrix is produced. The convolutional operation is performed with a window of size (3, hidden size of BERT which is 768 in BERTbase model) and the maximum value is generated for each transformer encoder by applying max pooling on the convolution output. By concatenating these values, a vector is generated which is given as input to a fully connected network. By applying softmax on the input, the classification operation is performed. \n Question: What new fine-tuning methods are presented?",
            "output": [
                "BERT based fine-tuning Insert nonlinear layers Insert Bi-LSTM layer Insert CNN layer"
            ]
        },
        {
            "id": "task460-f29be625e1ec4abb85ee1b015cb35f6f",
            "input": "1) Typical Methods. Three typical knowledge graph embedding methods includes TransE, TransR and TransH are selected as baselines. 2) Path-based Methods. We compare our method with two typical path-based model include PTransE, and ALL-PATHS BIBREF18. 3) Attribute-incorporated Methods. Several state-of-art attribute-incorporated methods including R-GCN BIBREF24 and KR-EAR BIBREF26 are used to compare with our methods on three real datasets. \n Question: What seven state-of-the-art methods are used for comparison?",
            "output": [
                "TransE, TransR and TransH PTransE, and ALL-PATHS R-GCN BIBREF24 and KR-EAR BIBREF26"
            ]
        },
        {
            "id": "task460-756864195e8246dd8f27fd7a28b6bb46",
            "input": "Currently, the following WSD models induced from a text corpus are available: Word senses based on cluster word features. This model uses the cluster words from the induced word sense inventory as sparse features that represent the sense.\n\nWord senses based on context word features. This representation is based on a sum of word vectors of all cluster words in the induced sense inventory weighted by distributional similarity scores.\n\nSuper senses based on cluster word features. To build this model, induced word senses are first globally clustered using the Chinese Whispers graph clustering algorithm BIBREF9 . The edges in this sense graph are established by disambiguation of the related words BIBREF11 , BIBREF12 . The resulting clusters represent semantic classes grouping words sharing a common hypernym, e.g. “animal”. This set of semantic classes is used as an automatically learned inventory of super senses: There is only one global sense inventory shared among all words in contrast to the two previous traditional “per word” models. Each semantic class is labeled with hypernyms. This model uses words belonging to the semantic class as features.\n\nSuper senses based on context word features. This model relies on the same semantic classes as the previous one but, instead, sense representations are obtained by averaging vectors of words sharing the same class. \n Question: Do they use a neural model for their task?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-5a6d4448c6c94425992b456aa295c6b2",
            "input": "CoVoST is currently built on that release and covers the following 11 languages: French, German, Dutch, Russian, Spanish, Italian, Turkish, Persian, Swedish, Mongolian and Chinese. \n Question: Is Arabic one of the 11 languages in CoVost?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-27d354d894194bb5a8ccf1c98a9e15d4",
            "input": "We also measure the usage of words related to people's core values as reported by Boyd et al. boyd2015. The sets of words, or themes, were excavated using the Meaning Extraction Method (MEM) BIBREF10 . \n Question: How do they obtain psychological dimensions of people?",
            "output": [
                "using the Meaning Extraction Method"
            ]
        },
        {
            "id": "task460-3d8b8dcad80e4e0a9a94b3138e2e0410",
            "input": "Table TABREF21 shows results for Korean INLINEFORM0 English, using the same configurations (1, 2 and 8) as for German–English. Our results confirm that the techniques we apply are successful across datasets, and result in stronger systems than previously reported on this dataset, achieving 10.37 BLEU as compared to 5.97 BLEU reported by gu-EtAl:2018:EMNLP1. \n Question: what are the methods they compare with in the korean-english dataset?",
            "output": [
                "gu-EtAl:2018:EMNLP1"
            ]
        },
        {
            "id": "task460-2a07f37bfb4348ed86b4212f167eb26a",
            "input": "Slot Filling is an information extraction task which has become popular in the last years BIBREF3 . The task aims at extracting information about persons, organizations or geo-political entities from a large collection of news, web and discussion forum documents. \n Question: What is the task of slot filling?",
            "output": [
                "The task aims at extracting information about persons, organizations or geo-political entities from a large collection of news, web and discussion forum documents."
            ]
        },
        {
            "id": "task460-f16fb2725b5b40eaa4365f88cbd823a5",
            "input": "The release of the FEVER fact extraction and verification dataset BIBREF0 provides a large-scale challenge that tests a combination of retrieval and textual entailment capabilities. To verify a claim in the dataset as supported, refuted, or undecided, a system must retrieve relevant articles and sentences from Wikipedia. Then it must decide whether each of those sentences, or some combination of them, entails or refutes the claim, which is an entailment problem. Systems are evaluated on the accuracy of the claim predictions, with credit only given when correct evidence is submitted. As entailment data, premises in FEVER data differ substantially from those in the image caption data used as the basis for the Stanford Natural Language Inference (SNLI) BIBREF1 dataset. Sentences are longer (31 compared to 14 words on average), vocabulary is more abstract, and the prevalence of named entities and out-of-vocabulary terms is higher. The retrieval aspect of FEVER is not straightforward either. A claim may have small word overlap with the relevant evidence, especially if the claim is refuted by the evidence. \n Question: What is the FEVER task?",
            "output": [
                "tests a combination of retrieval and textual entailment capabilities. To verify a claim in the dataset as supported, refuted, or undecided, a system must retrieve relevant articles and sentences from Wikipedia. Then it must decide whether each of those sentences, or some combination of them, entails or refutes the claim, which is an entailment problem"
            ]
        },
        {
            "id": "task460-2761d9d451f4474ca6e4728647aba859",
            "input": "Manual inspection of others examples also supports our claim. \n Question: Do they perform manual evaluation?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-43c1834ebc7d40ed969794c68bae0f6c",
            "input": "The first metric we report is the reaction type. The second metric we report is reaction speed. A study by Jin et al. jin2013epidemiological found that trusted news stories spread faster than misinformation or rumor; Zeng et al. zeng2016rumors found that tweets which deny rumors had shorter delays than tweets of support. Our second goal is to determine if these trends are maintained for various types of news sources on Twitter and Reddit. To examine whether users react to content from trusted sources differently than from deceptive sources, we measure the reaction delay, which we define as the time elapsed between the moment the link or content was posted/tweeted and the moment that the reaction comment or tweet occurred. We report the cumulative distribution functions (CDFs) for each source type and use Mann Whitney U (MWU) tests to compare whether users respond with a given reaction type with significantly different delays to news sources of different levels of credibility. \n Question: How is speed measured?",
            "output": [
                "time elapsed between the moment the link or content was posted/tweeted and the moment that the reaction comment or tweet occurred"
            ]
        },
        {
            "id": "task460-e2ce9b5becb049f783a947e3b303e7e5",
            "input": "We also perform a fine-grained syntactic analysis that shows how our method enables to control the morphological realization of first and second-person pronouns, together with verbs and adjectives related to them. \n Question: What conclusions are drawn from the syntactic analysis?",
            "output": [
                " our method enables to control the morphological realization of first and second-person pronouns, together with verbs and adjectives related to them"
            ]
        },
        {
            "id": "task460-9aaaab40989949bba0557e4f9ce92d7e",
            "input": "We use 16 different datasets from several popular review corpora used in BIBREF20 . These datasets consist of 14 product review datasets and two movie review datasets. We use CoNLL 2000 BIBREF22 sequence labeling dataset for both POS Tagging and Chunking tasks. \n Question: What dataset did they use?",
            "output": [
                "16 different datasets from several popular review corpora used in BIBREF20 CoNLL 2000 BIBREF22"
            ]
        },
        {
            "id": "task460-e800a17258be46f18d1827ef3780c693",
            "input": "We use a default train-validation-test split of 70-15-15 for each dataset, and use all 4 datasets (BF, BA, SFU and Sherlock). \n Question: Which multiple datasets did they train on during joint training?",
            "output": [
                "BF, BA, SFU and Sherlock"
            ]
        },
        {
            "id": "task460-d0140f7ce95640d8b0de188e31b30994",
            "input": "Methodology ::: Corpus-based Approach\nContextual information is informative in the sense that, in general, similar words tend to appear in the same contexts.  In the corpus-based approach, we capture both of these characteristics and generate word embeddings specific to a domain. Methodology ::: Dictionary-based Approach\nIn Turkish, there do not exist well-established sentiment lexicons as in English. In this approach, we made use of the TDK (Türk Dil Kurumu - “Turkish Language Institution”) dictionary to obtain word polarities. \n Question: What word-based and dictionary-based feature are used?",
            "output": [
                "generate word embeddings specific to a domain TDK (Türk Dil Kurumu - “Turkish Language Institution”) dictionary to obtain word polarities"
            ]
        },
        {
            "id": "task460-f8436a6654f74e2482e5345f31d84698",
            "input": "We investigate the following six idiom resources:\n\nWiktionary;\n\nthe Oxford Dictionary of English Idioms (ODEI, BIBREF31);\n\nUsingEnglish.com (UE);\n\nthe Sporleder corpus BIBREF10;\n\nthe VNC dataset BIBREF9;\n\nand the SemEval-2013 Task 5 dataset BIBREF15. \n Question: What dictionaries are used for automatic extraction of PIEs?",
            "output": [
                "Wiktionary Oxford Dictionary of English Idioms UsingEnglish.com (UE) Sporleder corpus VNC dataset SemEval-2013 Task 5 dataset"
            ]
        },
        {
            "id": "task460-198f4950b3e243f088a8439846d026d4",
            "input": "The improved performance of our attention models that actively select their optimal context, over a model with the complete thread as context, hLSTM, shows that the context inference improves intervention prediction over using the default full context. \n Question: What aspects of discussion are relevant to instructor intervention, according to the attention mechanism?",
            "output": [
                "context inference"
            ]
        },
        {
            "id": "task460-25499dbdfec442758a6fd01ad4a4cbde",
            "input": "In this paper, we work on recognizing the polarity of an affective event that is represented by a score ranging from $-1$ (negative) to 1 (positive). \n Question: What are labels available in dataset for supervision?",
            "output": [
                "negative positive"
            ]
        },
        {
            "id": "task460-1e02b6c38be34bed992ec8580b13de1b",
            "input": "Dataset: Based on some earlier work, only available labelled dataset had 3189 rows of text messages of average length of 116 words and with a range of 1, 1295. \n Question: How big is the dataset?",
            "output": [
                "3189 rows of text messages"
            ]
        },
        {
            "id": "task460-b7d0d60fe6c14e81943019dd7d8d6ab1",
            "input": "The multi-task model outperforms the single-task model at all data sizes, and the relative performance increases as the size of the training data decreases. When only 200 sentences of training data are used, the performance of the multi-task model is about 60% better than the single-task model for both the Airbnb and Greyhound apps. The relative gain for the OpenTable app is 26%. The results differ between the tasks, but none have an overall benefit from the open vocabulary system. \n Question: Does the performance increase using their method?",
            "output": [
                "The multi-task model outperforms the single-task model at all data sizes but none have an overall benefit from the open vocabulary system"
            ]
        },
        {
            "id": "task460-51fd7f60c8f94c429bbbd65525f18b27",
            "input": "Conclusion \n Question: What is triangulation?",
            "output": [
                "Answer with content missing: (Chapter 3) The concept can be easily explained with an example, visualized in Figure 1. Consider the Portuguese (Pt) word trabalho which, according to the MUSE Pt–En dictionary, has the words job and work as possible En translations. In turn, these two En words can be translated to 4 and 5 Czech (Cs) words respectively. By utilizing the transitive property (which translation should exhibit) we can identify the set of 7 possible Cs translations for the Pt word trabalho."
            ]
        },
        {
            "id": "task460-f515f1d6e37c42139870e8c75c1a8982",
            "input": "All baselines are Transformer models in their base configuration BIBREF11, using 6 encoder and decoder layers, with model and hidden dimensions of 512 and 2048 respectively, and 8 heads for all attention layers. For initial multi-task experiments, all model parameters were shared BIBREF12, but performance was down by multiple BLEU points compared to the baselines. \n Question: What baselines non-adaptive baselines are used?",
            "output": [
                "Transformer models in their base configuration BIBREF11, using 6 encoder and decoder layers, with model and hidden dimensions of 512 and 2048 respectively, and 8 heads for all attention layers"
            ]
        },
        {
            "id": "task460-d488b857c2154d24b0457d61cc003c3e",
            "input": "replacing this single GRU with two different components first component is a sentence reader second component is the input fusion layer \n Question: What changes they did on input module?",
            "output": [
                "For the DMN+, we propose replacing this single GRU with two different components. The first component is a sentence reader The second component is the input fusion layer"
            ]
        },
        {
            "id": "task460-51fe9bb735f24f4a9d1faea8459935a9",
            "input": "In this section we discuss the state of the art on conversational systems in three perspectives: types of interactions, types of architecture, and types of context reasoning. ELIZA BIBREF11 was one of the first softwares created to understand natural language processing.  Right after ELIZA came PARRY, developed by Kenneth Colby, who is psychiatrist at Stanford University in the early 1970s. A.L.I.C.E. (Artificial Linguistic Internet Computer Entity) BIBREF12 appeared in 1995 but current version utilizes AIML, an XML language designed for creating stimulus-response chat robots BIBREF13 . Cleverbot (1997-2014) is a chatbot developed by the British AI scientist Rollo Carpenter.  \n Question: What is the state of the art described in the paper?",
            "output": [
                "ELIZA  PARRY A.L.I.C.E. Cleverbot"
            ]
        },
        {
            "id": "task460-3ae3a95379c24708aa2437146220e550",
            "input": "The results in Table TABREF38 confirm the results of BIBREF13 and suggest that we successfully replicated a large proportion of their features. The results for all three prediction settings (one outgoing edge: INLINEFORM0 , support/attack: INLINEFORM1 and support/attack/neither: INLINEFORM2 ) across all type variables ( INLINEFORM3 , INLINEFORM4 and INLINEFORM5 ) are displayed in Table TABREF39 . All models significantly outperform the majority baseline with respect to macro F1. \n Question: What baseline and classification systems are used in experiments?",
            "output": [
                "BIBREF13 majority baseline"
            ]
        },
        {
            "id": "task460-bcab915f261b4efeaf7961b4d2193ce2",
            "input": "Following the previous works, we perform 10-fold cross validation and report the average results.  \n Question: How is the robustness of the model evaluated?",
            "output": [
                "10-fold cross validation"
            ]
        },
        {
            "id": "task460-7c3eebcb25514f0185e7f638512ff8f9",
            "input": "All documents are segmented into paragraphs and processed at the paragraph level (both training and inference); this is acceptable because we observe that most paragraphs are less than 200 characters. The input sequences are segmented by the BERT tokenizer, with the special [CLS] token inserted at the beginning and the special [SEP] token added at the end. \n Question: At what text unit/level were documents processed?",
            "output": [
                "documents are segmented into paragraphs and processed at the paragraph level"
            ]
        },
        {
            "id": "task460-b1a52e4f6ff4495d81fa1cf2e5b04f35",
            "input": " On high level, we indirectly evaluate the content relevance between the candidate summary and the human summary using information retrieval. To accomplish this, we use the summaries as search queries and compare the overlaps of the retrieved results.  \n Question: In the proposed metric, how is content relevance measured?",
            "output": [
                "The content relevance between the candidate summary and the human summary is evaluated using information retrieval - using the summaries as search queries and compare the overlaps of the retrieved results. "
            ]
        },
        {
            "id": "task460-20627d778f9b4201be9acaef7658480e",
            "input": "We investigate both methods, either in isolation or combined, on two translation directions (En-It and En-De) for which the length of the target is on average longer than the length of the source. En-It, En-De in both directions \n Question: Which languages do they focus on?",
            "output": [
                "two translation directions (En-It and En-De)"
            ]
        },
        {
            "id": "task460-d2f5ef97fd434d10b94729d3e2765aef",
            "input": "We further combine the supervised IR with our unsupervised model, which is trained with full unpaired data (4.8M) and different number of paired data (from 50K to 4.8M). \n Question: How many comments were used?",
            "output": [
                "from 50K to 4.8M"
            ]
        },
        {
            "id": "task460-35b3dc4219294a66b7ecaff6b092e685",
            "input": "Various automated evaluation approaches are proposed to facilitate the development and evaluation of NLG models. We summarize these evaluation approaches below. Text Overlap Metrics, including BLEU BIBREF5, METEOR BIBREF6 and ROUGE BIBREF7, are the most popular metrics employed in the evaluation of NLG models. Perplexity is commonly used to evaluate the quality of a language model. Parameterized Metrics learn a parameterized model to evaluate generated text. \n Question: What previous automated evalution approaches authors mention?",
            "output": [
                "Text Overlap Metrics, including BLEU Perplexity Parameterized Metrics"
            ]
        },
        {
            "id": "task460-a4969e233ff44e28b8db5928f21b6bd8",
            "input": "In Table TABREF15 we measure BLEU BIBREF19, NIST BIBREF20, METEOR BIBREF21, ROUGE-L BIBREF22 and CIDEr BIBREF23 metrics on the 2018 E2E NLG Challenge test data using the evaluation script provided by the organizers.  We use the gold standard selection during training and validation of the text generation model, as well as the automatic evaluation. As we deploy our text generation model for manual evaluation, we use a Conditional Random Field (CRF) model to predict which events to mention. The second human evaluation aimed at judging the acceptability of the output for production use in a news agency.  In the minimum edit evaluation, carried out by the annotator who created the news corpus, only factual mistakes and grammatical errors are corrected, resulting in text which may remain awkward or unfluent. The word error rate (WER) of the generated text compared to its corrected variant as a reference is 5.6% (6.2% disregarding punctuation).  The factual errors and their types are summarized in Table TABREF23. From the total of 510 game events generated by the system, 78 of these contained a factual error, i.e. 84.7% were generated without factual errors. Most fluency issues relate to the overall flow and structure of the report. Addressing these issues would require the model to take into account multiple events in a game, and combine the information more flexibly to avoid repetition.  The second human evaluation aimed at judging the acceptability of the output for production use in a news agency. The output is evaluated in terms of its usability for a news channel labelled as being machine-generated, i.e. not aiming at the level of a human journalist equipped with substantial background information. The evaluation was carried out by two journalists from the STT agency, who split the 59 games among themselves approximately evenly. \n Question: What evaluation criteria and metrics were used to evaluate the generated text?",
            "output": [
                "BLEU  NIST  METEOR  ROUGE-L CIDEr  evaluation script automatic evaluation human evaluation minimum edit evaluation word error rate (WER) factual errors and their types fluency issues acceptability of the output for production use in a news agency"
            ]
        },
        {
            "id": "task460-8150a828fade48708542ce352240236b",
            "input": "We use MS COCO, Bing and Flickr datasets from BIBREF26 to train the model that generates questions. These datasets contain natural questions about images with the purpose of knowing more about the picture. As can be seen in the Figure FIGREF8, questions cannot be answered by only looking at the image. Each source contains 5,000 images with 5 questions per image, adding a total of 15,000 images with 75,000 questions. We use two datasets to train our chatbot model. The first one is the Persona-chat BIBREF15 which contains dialogues between two people with different profiles that are trying to know each other. It is complemented by the Cornell-movie dialogues dataset BIBREF27, which contains a collection of fictional conversations extracted from raw movie scripts. Persona-chat's sentences have a maximum of 15 words, making it easier to learn for machines and a total of 162,064 utterances over 10,907 dialogues. While Cornell-movie dataset contains 304,713 utterances over 220,579 conversational exchanges between 10,292 pairs of movie characters. \n Question: How big dataset is used for training this system?",
            "output": [
                "For the question generation model 15,000 images with 75,000 questions. For the chatbot model, around 460k utterances over 230k dialogues."
            ]
        },
        {
            "id": "task460-59b699da476945cf9b1ee088abf91c95",
            "input": "Our experiments showed that using the objects color or shape to uniquely identify an object allows the robot successfully complete the binning task in 97.6% and 96.0% of the cases. However, using the shape alone as a unique identifier, the task could only be completed in 79.0% of the cases. \n Question: What is task success rate achieved? ",
            "output": [
                "96-97.6% using the objects color or shape and 79% using shape alone"
            ]
        },
        {
            "id": "task460-3db0c7e9698749a8a5ec86ef9c7dd3a6",
            "input": " As well as refining the annotation guidelines, the development process trained annotators who were not security experts. The development and test sets for Darkode and Hack Forums were annotated by additional team members (five for Darkode, one for Hack Forums), and then every disagreement was discussed and resolved to produce a final annotation. The authors, who are researchers in either NLP or computer security, did all of the annotation. \n Question: Who annotated the data?",
            "output": [
                "annotators who were not security experts researchers in either NLP or computer security"
            ]
        },
        {
            "id": "task460-6799e6854e2f4d588240637e4376ffcc",
            "input": "In HotpotQA, on average we can find 6 candidate chains (2-hop) in a instance, and the human labeled true reasoning chain is unique. A predicted chain is correct if the chain only contains all supporting passages (exact match of passages).\n\nIn MedHop, on average we can find 30 candidate chains (3-hop). For each candidate chain our human annotators labeled whether it is correct or not, and the correct reasoning chain is not unique. A predicted chain is correct if it is one of the chains that human labeled as correct.\n\nThe accuracy is defined as the ratio: The accuracy is defined as the ratio: \n Question: What benchmarks are created?",
            "output": [
                "Answer with content missing: (formula) The accuracy is defined as the ratio # of correct chains predicted to # of evaluation samples"
            ]
        },
        {
            "id": "task460-9558a4ce581b4b1488e4bf5502faecea",
            "input": "In target data, the code is written in Python programming language. \n Question: What programming language is target language?",
            "output": [
                "Python"
            ]
        },
        {
            "id": "task460-196331127afa4f5198ceda47f7e916f7",
            "input": "Before designing this NLP toolkit, we conducted a survey among engineers and identified a spectrum of three typical personas. \n Question: How do the authors evidence the claim that many engineers find it a big overhead to choose from multiple frameworks, models and optimization techniques?",
            "output": [
                "By conducting a survey among engineers"
            ]
        },
        {
            "id": "task460-ab622a4264f342b184a97dede78a67b6",
            "input": "As discussed in Section SECREF2 , a variety of features that measure salience of an entity in text are available from the NLP community. We reimplemented the ones in Dunietz and Gillick BIBREF11 . This includes a variety of features, e.g. positional features, occurrence frequency and the internal POS structure of the entity and the sentence it occurs in.  \n Question: What features are used to represent the salience and relative authority of entities?",
            "output": [
                "Salience features positional features, occurrence frequency and the internal POS structure of the entity and the sentence it occurs in.\nThe relative authority of entity features:   comparative relevance of the news article to the different entities occurring in it."
            ]
        },
        {
            "id": "task460-1c3f9e604c294c8dadd52569aeed09de",
            "input": "Last but not least, ethics and fairness are important considerations, that deserve to be studied. In that sense, detection of individual and global bias should be prioritized in order to give useful feedbacks to practitioners. Furthermore we are considering using adversarial learning as in BIBREF33 in order to ensure fairness during the training process. \n Question: Is there any ethical consideration in the research?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-07c8b885e1e943039f30cf43facf0ba1",
            "input": "For reliable multi-hop reasoning using OpenIE tuples, we can add inter-tuple connections to the support graph search, controlled by a small number of rules over the OpenIE predicates. Learning such rules for the Science domain is an open problem and potential avenue of future work. \n Question: Can the method answer multi-hop questions?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-fca7626484d742b59959e4cf8a542f6c",
            "input": "We hypothesized that question answering can benefit from synergistic interaction between the two tasks through parameter sharing and joint training under this multitask setting. \n Question: Which components of QA and QG models are shared during training?",
            "output": [
                "parameter sharing"
            ]
        },
        {
            "id": "task460-5dc191419b7c4a2b9c8dae25bb230a3d",
            "input": "Querying posts on Twitter with extracted lexicons led to a collection of $19,300$ tweets. In order to have lexical diversity, we added 2500 randomly sampled tweets to our dataset. \n Question: How many tweets were collected?",
            "output": [
                "$19,300$ added 2500 randomly sampled tweets"
            ]
        },
        {
            "id": "task460-50c37fc6ed9046c087918733ae9cf9df",
            "input": "Three annotators, A1-A3, mark each tweet in the Dataset H as drunk or sober. \n Question: Is the data acquired under distant supervision verified by humans at any stage?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-8a81df383bbe4eebb1119a15c43212e7",
            "input": "ch As continuous word embedding spaces exhibit similar structures across (even distant) languages BIBREF35, we use a multilingual word representation which aims to learn a linear mapping from a source to a target embedding space. \n Question: What multilingual word representations are used?",
            "output": [
                " a multilingual word representation which aims to learn a linear mapping from a source to a target embedding space"
            ]
        },
        {
            "id": "task460-95ef61f3b6e142a9a2f98a8165f9d107",
            "input": "In this paper, we use three data sets from the literature to train and evaluate our own classifier. Data collected by BIBREF3 , which we term the Sexist/Racist (SR) data set, was collected using an initial Twitter search followed by analysis and filtering by the authors and their team who identified 17 common phrases, hashtags, and users that were indicative of abusive speech. BIBREF4 collected the HATE dataset by searching for tweets using a lexicon provided by Hatebase.org. The final data set we used, which we call HAR, was collected by BIBREF9 ; we removed all retweets reducing the dataset to 20,000 tweets. Tweets were labeled as “Harrassing” or “Non-Harrassing”; hate speech was not explicitly labeled, but treated as an unlabeled subset of the broader “Harrassing” category BIBREF9 . \n Question: Which publicly available datasets are used?",
            "output": [
                "BIBREF3 BIBREF4 BIBREF9"
            ]
        },
        {
            "id": "task460-f2096ee9f5fb47c8b6fda562c92e05f0",
            "input": "However, no prior work has explored the target of the offensive language, which is important in many scenarios, e.g., when studying hate speech with respect to a specific target. \n Question: What are the differences between this dataset and pre-existing ones?",
            "output": [
                "no prior work has explored the target of the offensive language"
            ]
        },
        {
            "id": "task460-d85680db97584a5d854d67ff9bc3bf7b",
            "input": "We compared several summarization methods which can be categorized into three groups: unsupervised, non-neural supervised, and neural supervised methods. For the unsupervised methods, we tested: SumBasic, which uses word frequency to rank sentences and selects top sentences as the summary BIBREF13 , BIBREF14 . Lsa, which uses latent semantic analysis (LSA) to decompose the term-by-sentence matrix of a document and extracts sentences based on the result. We experimented with the two approaches proposed in BIBREF15 and BIBREF16 respectively. LexRank, which constructs a graph representation of a document, where nodes are sentences and edges represent similarity between two sentences, and runs PageRank algorithm on that graph and extracts sentences based on the resulting PageRank values BIBREF17 . TextRank, which is very similar to LexRank but computes sentence similarity based on the number of common tokens BIBREF19 . Bayes, which represents each sentence as a feature vector and uses naive Bayes to classify them BIBREF5 . The original paper computes TF-IDF score on multi-word tokens that are identified automatically using mutual information. We did not do this identification, so our TF-IDF computation operates on word tokens. Hmm, which uses hidden Markov model where states correspond to whether the sentence should be extracted BIBREF20 . The original work uses QR decomposition for sentence selection but our implementation does not. We simply ranked the sentences by their scores and picked the top 3 as the summary. MaxEnt, which represents each sentence as a feature vector and leverages maximum entropy model to compute the probability of a sentence should be extracted BIBREF21 . The original approach puts a prior distribution over the labels but we put the prior on the weights instead. Our implementation still agrees with the original because we employed a bias feature which should be able to learn the prior label distribution. As for the neural supervised method, we evaluated NeuralSum BIBREF11 using the original implementation by the authors. We modified their implementation slightly to allow for evaluating the model with ROUGE. Note that all the methods are extractive. Our implementation code for all the methods above is available online. As a baseline, we used Lead-N which selects INLINEFORM0 leading sentences as the summary. For all methods, we extracted 3 sentences as the summary since it is the median number of sentences in the gold summaries that we found in our exploratory analysis. \n Question: Which approaches did they use?",
            "output": [
                "SumBasic Lsa LexRank TextRank Bayes Hmm MaxEnt NeuralSum Lead-N"
            ]
        },
        {
            "id": "task460-75003e393109499bb60471264a6b9cf4",
            "input": "We use three encoder-decoder neural architectures as baselines: (1) LSTM+attention as an LSTM BIBREF19 with attention mechanism BIBREF20; (2) Transformer BIBREF21 and (3) Universal Transformer BIBREF22. \n Question: What three machine architectures are analyzed?",
            "output": [
                "LSTM+attention Transformer  Universal Transformer"
            ]
        },
        {
            "id": "task460-9c37bc49baa54f5688b75b8a900de707",
            "input": "To capture this interesting property, we propose a new tagging scheme consisting of three tags, namely { INLINEFORM0 }.\n\nINLINEFORM0 tag indicates that the current word appears before the pun in the given context.\n\nINLINEFORM0 tag highlights the current word is a pun.\n\nINLINEFORM0 tag indicates that the current word appears after the pun. \n Question: What is the tagging scheme employed?",
            "output": [
                "A new tagging scheme that tags the words before and after the pun as well as the pun words."
            ]
        },
        {
            "id": "task460-a2de0d54ae0c43369a7fbbd5341f9775",
            "input": " We defined a sequence labeling task to extract custom entities from user input. We assumed seven (7) possible entities (see Table TABREF43) to be recognized by the model: topic, subtopic, examination mode and level, question number, intent, as well as the entity other for remaining words in the utterance.   We defined a classification problem to predict the system's next action according to the given user input. We assumed 13 custom actions (see Table TABREF42) that we considered being our labels. In the conversational dataset, each input was automatically labeled by the rule-based system with the corresponding next action and the dialogue-id. Thus, no additional post-labeling was required.  \n Question: How does the IPA label data after interacting with users?",
            "output": [
                "It defined a sequence labeling task to extract custom entities from user input and label the next action (out of 13  custom actions defined)."
            ]
        },
        {
            "id": "task460-a238c96cff7843299652ed07c2967567",
            "input": "We train a single layer LSTM with a 150-dimensional hidden state for hate / not hate classification. The input dimensionality is set to 100 and GloVe BIBREF26 embeddings are used as word input representations. \n Question: What unimodal detection models were used?",
            "output": [
                " single layer LSTM with a 150-dimensional hidden state for hate / not hate classification"
            ]
        },
        {
            "id": "task460-726700658f184d759b3ccd3d0c0e038e",
            "input": "GE-FL reduces the heavy load of instance annotation and performs well when we provide prior knowledge with no bias. In our experiments, we observe that comparable numbers of labeled features for each class have to be supplied. We randomly select $t \\in [1, 20]$ features from the feature pool for one class, and only one feature for the other. Our methods are also evaluated on datasets with different unbalanced class distributions. We manually construct several movie datasets with class distributions of 1:2, 1:3, 1:4 by randomly removing 50%, 67%, 75% positive documents. Incorporating KL divergence is robust enough to control unbalance both in the dataset and in labeled features while the other three methods are not so competitive. \n Question: How do they define robustness of a model?",
            "output": [
                "ability to accurately classify texts even when the amount of prior knowledge for different classes is unbalanced, and when the class distribution of the dataset is unbalanced"
            ]
        },
        {
            "id": "task460-e06c2781b8554a798166f918245bdd07",
            "input": "Given that non-content words are distinctive enough for a classifier to determine style, we propose a suite of low-level linguistic feature counts (henceforth, controls) as our formal, content-blind definition of style. The style of a sentence is represented as a vector of counts of closed word classes (like personal pronouns) as well as counts of syntactic features like the number of SBAR non-terminals in its constituency parse, since clause structure has been shown to be indicative of style BIBREF20. Controls are extracted heuristically, and almost all rely on counts of pre-defined word lists. For constituency parses we use the Stanford Parser BIBREF21. table:controlexamples lists all the controls along with examples. \n Question: How they model style as a suite of low-level linguistic controls, such as frequency of pronouns, prepositions, and subordinate clause constructions?",
            "output": [
                "style of a sentence is represented as a vector of counts of closed word classes (like personal pronouns) as well as counts of syntactic features like the number of SBAR non-terminals in its constituency parse, since clause structure has been shown to be indicative of style"
            ]
        },
        {
            "id": "task460-4deeadaf525b4b8fadd4cd969236ae7f",
            "input": "The task of Word Sense Induction (WSI) can be seen as an unsupervised version of WSD. WSI aims at clustering word senses and does not require to map each cluster to a predefined sense. Instead of that, word sense inventories are induced automatically from the clusters, treating each cluster as a single sense of a word. We suggest a more advanced procedure of graph construction that uses the interpretability of vector addition and subtraction operations in word embedding space BIBREF6 while the previous algorithm only relies on the list of nearest neighbours in word embedding space. The key innovation of our algorithm is the use of vector subtraction to find pairs of most dissimilar graph nodes and construct the graph only from the nodes included in such “anti-edges”. \n Question: Is the method described in this work a clustering-based method?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-2084355ca3d84788a685bd4a9bb1f49e",
            "input": "The dataset from Twitter that we are using in our work, consists of a train set, a validation set and a test set. It was published for the \"First workshop on categorizing different types of online harassment languages in social media\". The whole dataset is divided into two categories, which are harassment and non-harassment tweets. Moreover, considering the type of the harassment, the tweets are divided into three sub-categories which are indirect harassment, sexual and physical harassment. \n Question: What types of online harassment are studied?",
            "output": [
                "indirect harassment, sexual and physical harassment"
            ]
        },
        {
            "id": "task460-54e05b1089554fb59f9ba00b8841ede9",
            "input": " We apply an off-the-shelf tool for emotion recognition (the manufacturer cannot be disclosed due to licensing restrictions). It delivers frame-by-frame scores ($\\in [0;100]$) for discrete emotional states of joy, anger and fear. We extract the audio signal for the same sequence as described for facial expressions and apply an off-the-shelf tool for emotion recognition. The software delivers single classification scores for a set of 24 discrete emotions for the entire utterance. \n Question: What are the emotion detection tools used for audio and face input?",
            "output": [
                "We apply an off-the-shelf tool for emotion recognition (the manufacturer cannot be disclosed due to licensing restrictions)"
            ]
        },
        {
            "id": "task460-31c4e201d8f748c597072af0d99d12f5",
            "input": "In this section we describe how to evaluate and compare the outcomes of algorithms which assign relevance scores to words (such as LRP or SA) through intrinsic validation. Furthermore, we propose a measure of model explanatory power based on an extrinsic validation procedure. \n Question: Are the document vectors that the authors introduce evaluated in any way other than the new way the authors propose?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-6d55a5c4abdf49698993198f98c73de7",
            "input": "We use micro-averaged F1 score on all labels as the evaluation metric. \n Question: What metris are used for evaluation?",
            "output": [
                "micro-averaged F1 score"
            ]
        },
        {
            "id": "task460-59ba462091184a8baef1f7d0c904f112",
            "input": "Table TABREF10 reports our main results. Our models are built on top of prior works with the additional Attention Supervision Module as described in Section SECREF3 . Specifically, we denote by Attn-* our adaptation of the respective model by including our Attention Supervision Module. We highlight that MCB model is the winner of VQA challenge 2016 and MFH model is the best single model in VQA challenge 2017. In Table TABREF10 , we can observe that our proposed model achieves a significantly boost on rank-correlation with respect to human attention. Furthermore, our model outperforms alternative state-of-art techniques in terms of accuracy in answer prediction. Specifically, the rank-correlation for MFH model increases by 36.4% when is evaluated in VQA-HAT dataset and 7.7% when is evaluated in VQA-X. This indicates that our proposed methods enable VQA models to provide more meaningful and interpretable results by generating more accurate visual grounding. \n Question: By how much do they outperform existing state-of-the-art VQA models?",
            "output": [
                "the rank-correlation for MFH model increases by 36.4% when is evaluated in VQA-HAT dataset and 7.7% when is evaluated in VQA-X"
            ]
        },
        {
            "id": "task460-a0d956910c5641aca3b23c4df9cc9479",
            "input": "Therefore, we decided to compile our own corpora based on English documents, which we crawled from different publicly accessible sources. \n Question: Do they report results only on English data?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-2deb8e7b78c94e82a5cdd26ef9387000",
            "input": "In an offline step, we organize the content of each instance in a graph where each node represents a sentence from the passages and the question. Then, we add edges between nodes using the following topology:\n\nwe fully connect nodes that represent sentences from the same passage (dotted-black);\n\nwe fully connect nodes that represent the first sentence of each passage (dotted-red);\n\nwe add an edge between the question and every node for each passage (dotted-blue). \n Question: How are some nodes initially connected based on text structure?",
            "output": [
                "we fully connect nodes that represent sentences from the same passage we fully connect nodes that represent the first sentence of each passage we add an edge between the question and every node for each passage"
            ]
        },
        {
            "id": "task460-20b91d704bc34238b23dcf8c630da859",
            "input": "We compare our HR-VAE model with three strong baselines using VAE for text modelling:\n\nVAE-LSTM-base: A variational autoencoder model which uses LSTM for both encoder and decoder. KL annealing is used to tackled the latent variable collapse issue BIBREF0;\n\nVAE-CNN: A variational autoencoder model with a LSTM encoder and a dilated CNN decoder BIBREF7;\n\nvMF-VAE: A variational autoencoder model using LSTM for both encoder and decoder where the prior distribution is the von Mises-Fisher (vMF) distribution rather than a Gaussian distribution BIBREF5. \n Question: Do they compare against state of the art text generation?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-35fd4e5984cb4ff7b33c895b70cbb790",
            "input": "Although no perfect matches exist, we decided upon the Quora duplicate question dataset BIBREF22 as the best match. To study the embeddings, we computed the euclidean distance between the two questions using various embeddings, to study the distance between semantically similar and dissimilar questions. \n Question: What existing corpus is used for comparison in these experiments?",
            "output": [
                "Quora duplicate question dataset BIBREF22"
            ]
        },
        {
            "id": "task460-416748dbdfdc495cb524d5190a8e3c1c",
            "input": "We choose the following three models as the baselines:\n\nK-means is a well known data clustering algorithm, we implement the algorithm using sklearn toolbox, and represent documents using bag-of-words weighted by TF-IDF.\n\nLEM BIBREF13 is a Bayesian modeling approach for open-domain event extraction. It treats an event as a latent variable and models the generation of an event as a joint distribution of its individual event elements. We implement the algorithm with the default configuration.\n\nDPEMM BIBREF14 is a non-parametric mixture model for event extraction. It addresses the limitation of LEM that the number of events should be known beforehand. We implement the model with the default configuration. \n Question: What baseline approaches does this approach out-perform?",
            "output": [
                "K-means LEM BIBREF13 DPEMM BIBREF14"
            ]
        },
        {
            "id": "task460-7ff4559d15944860b4e3ddaa307dfaf5",
            "input": "Following BIBREF21, we sample $K$ captions for each image when applying REINFORCE: ${\\hat{c}}_1 \\ldots {\\hat{c}}_K$, ${\\hat{c}}_k \\sim p_{\\theta }(c|I)$,\n\nThe baseline for each sampled caption is defined as the average reward of the rest samples. \n Question: What baseline function is used in REINFORCE algorithm?",
            "output": [
                "baseline for each sampled caption is defined as the average reward of the rest samples"
            ]
        },
        {
            "id": "task460-6a9ba87d1041469cb1abb0fa9c58a256",
            "input": "Our systems are attentional encoder-decoder networks BIBREF0 . We base our implementation on the dl4mt-tutorial, which we enhanced with new features such as ensemble decoding and pervasive dropout. \n Question: what are the baseline systems?",
            "output": [
                "attentional encoder-decoder networks BIBREF0"
            ]
        },
        {
            "id": "task460-53076d2e9b2a440293773f4402256557",
            "input": "We test our proposed approach for binary classification on either sarcasm or irony, on seven benchmark datasets retrieved from different media sources. Below we describe each dataset, please see Table TABREF1 below for a summary. Reddit: BIBREF21 collected SARC, a corpus comprising of 600.000 sarcastic comments on Reddit. We use main subset, SARC 2.0, and the political subset, SARC 2.0 pol. Twitter: We use the Twitter dataset provided for the SemEval 2018 Task 3, Irony Detection in English Tweets BIBREF18 . The dataset was manually annotated using binary labels. We also use the dataset by BIBREF4 , which is manually annotated for sarcasm. Finally, we use the dataset by BIBREF20 , who collected a user self-annotated corpus of tweets with the #sarcasm hashtag. Online Dialogues: We utilize the Sarcasm Corpus V1 (SC-V1) and the Sarcasm Corpus V2 (SC-V2), which are subsets of the Internet Argument Corpus (IAC). Compared to other datasets in our selection, these differ mainly in text length and structure complexity BIBREF22 . \n Question: What are the three different sources of data?",
            "output": [
                "Twitter Reddit Online Dialogues"
            ]
        },
        {
            "id": "task460-2fe68031bb554699b94938ca39e5f8b9",
            "input": "The model is sequential in nature, consisting of the following steps: read text, process it into a dynamic relational memory and then attention conditioned on the question generates the answer. We model the dynamic memory in a fashion similar to Recurrent Entity Networks BIBREF17 and then equip it with an additional relational memory. \n Question: What is the architecture of the neural network?",
            "output": [
                "extends memory-augmented neural networks with a relational memory to reason about relationships between multiple entities present within the text.  The model is sequential in nature, consisting of the following steps: read text, process it into a dynamic relational memory and then attention conditioned on the question generates the answer. We model the dynamic memory in a fashion similar to Recurrent Entity Networks BIBREF17 and then equip it with an additional relational memory."
            ]
        },
        {
            "id": "task460-830cb53ed8e44c7687ec07b04cb8de47",
            "input": "We compare our system to a variety of baselines. (1) Rank by the number of times a citation is mentioned in the document. (2) Rank by the number of times the citation is cited in the literature (citation impact). (3) Rank using Google Scholar Related Articles. (4) Rank by the TF*IDF weighted cosine similarity. (5) Rank using a learning-to-rank model trained on text similarity rankings. The first two baseline systems are models where the values are ordered from highest to lowest to generate the ranking. The idea behind them is that the number of times a citation is mentioned in an article, or the citation impact may already be good indicators of their closeness. The text similarity model is trained using the same features and methods used by the annotation model, but trained using text similarity rankings instead of the author's judgments. \n Question: what were the baselines?",
            "output": [
                "Rank by the number of times a citation is mentioned in the document  Rank by the number of times the citation is cited in the literature (citation impact).  Rank using Google Scholar Related Articles. Rank by the TF*IDF weighted cosine similarity.  ank using a learning-to-rank model trained on text similarity rankings"
            ]
        },
        {
            "id": "task460-bdecfe0ffad840058dffd3ae9459378b",
            "input": "MTMSN BIBREF4 is the first, and only model so far, that specifically tried to tackle the multi-span questions of DROP.  \n Question: What is the previous model that attempted to tackle multi-span questions as a part of its design?",
            "output": [
                "MTMSN BIBREF4"
            ]
        },
        {
            "id": "task460-9f55055277a34f1db80505bb531964d5",
            "input": "We build a dataset of Twitter accounts based on two lists annotated in previous works. For the non-factual accounts, we rely on a list of 180 Twitter accounts from BIBREF1. On the other hand, for the factual accounts, we use a list with another 32 Twitter accounts from BIBREF19 that are considered trustworthy by independent third parties. \n Question: How did they obtain the dataset?",
            "output": [
                "For the non-factual accounts, we rely on a list of 180 Twitter accounts from BIBREF1 we use a list with another 32 Twitter accounts from BIBREF19 that are considered trustworthy"
            ]
        },
        {
            "id": "task460-398768d2e9c24bed903aa1acbb8eef07",
            "input": "We label each review vector with the car it reviews.  \n Question: What are labels in car speak language dataset?",
            "output": [
                "car "
            ]
        },
        {
            "id": "task460-b8b45ab69fbe484ea8b39e61978d6f8c",
            "input": "Where possible, we tried to restrict the content of each text to the abstract and conclusion of the original work. However, since in many cases these sections were too short, we also considered other parts of the original works such as introduction or discussion sections. To ensure that the extracted text portions are appropriate for the AV task, each original work was preprocessed manually. More precisely, we removed tables, formulas, citations, quotes and sentences that include non-language content such as mathematical constructs or specific names of researchers, systems or algorithms. \n Question: What is a self-compiled corpus?",
            "output": [
                " restrict the content of each text to the abstract and conclusion of the original work considered other parts of the original works such as introduction or discussion sections extracted text portions are appropriate for the AV task, each original work was preprocessed manually removed tables, formulas, citations, quotes and sentences that include non-language content such as mathematical constructs or specific names of researchers, systems or algorithms"
            ]
        },
        {
            "id": "task460-2cfd9aeaa3084782b49b926251e7dcfd",
            "input": "We used the relation classification dataset of the SemEval 2010 task 8 BIBREF8 . It consists of sentences which have been manually labeled with 19 relations (9 directed relations and one artificial class Other). 8,000 sentences have been distributed as training set and 2,717 sentences served as test set. \n Question: Which dataset do they train their models on?",
            "output": [
                "relation classification dataset of the SemEval 2010 task 8"
            ]
        },
        {
            "id": "task460-f3ddf375ebb94465a6770182d77d15f9",
            "input": "Prior to the SAOKE data set, an annotated data set for OIE tasks with 3,200 sentences in 2 domains was released in BIBREF20 to evaluate OIE algorithms, in which the data set was said BIBREF20 “13 times larger than the previous largest annotated Open IE corpus”. \n Question: What's the size of the previous largest OpenIE dataset?",
            "output": [
                "3,200 sentences"
            ]
        },
        {
            "id": "task460-4d6bc8b1db6c4c90b9a51421b67275de",
            "input": "We use the Yelp Challenge dataset BIBREF2 for our fake review generation.  \n Question: Which dataset do they use a starting point in generating fake reviews?",
            "output": [
                "the Yelp Challenge dataset"
            ]
        },
        {
            "id": "task460-7ab4e998bf1648ffb8fe3c4fa9495950",
            "input": "This study assumes that a robot does not have any vocabularies in advance but can recognize syllables or phonemes. \n Question: Does their model start with any prior knowledge of words?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-2039fe9991f9433dbf444bc57090665b",
            "input": "Next, we analyze why they do not perform well in this task and with this data:\n\n[noitemsep,leftmargin=*]\n\nNoisy data. A major challenge of this task is the discrepancy between annotations due to subjective judgement. Although this affects also detection using only text, its repercussion is bigger in more complex tasks, such as detection using images or multimodal detection.\n\nComplexity and diversity of multimodal relations. Hate speech multimodal publications employ a lot of background knowledge which makes the relations between visual and textual elements they use very complex and diverse, and therefore difficult to learn by a neural network.\n\nSmall set of multimodal examples. Fig. FIGREF5 shows some of the challenging multimodal hate examples that we aimed to detect. But although we have collected a big dataset of $150K$ tweets, the subset of multimodal hate there is still too small to learn the complex multimodal relations needed to identify multimodal hate. \n Question: What is author's opinion on why current multimodal models cannot outperform models analyzing only text?",
            "output": [
                "Noisy data Complexity and diversity of multimodal relations Small set of multimodal examples"
            ]
        },
        {
            "id": "task460-4d5051f9871f45418a8555379b3a2cb9",
            "input": "In Table TABREF26 we report the results of our model on the three datasets standardly used for the evaluation of emotion classification, which we have described in Section SECREF3 . \n Question: What was their performance on emotion detection?",
            "output": [
                "Answer with content missing: (Table 3) Best author's model B-M average micro f-score is 0.409, 0.459, 0.411 on Affective, Fairy Tales and ISEAR datasets respectively. "
            ]
        },
        {
            "id": "task460-ed952729d813414ca062b3ff54b0548f",
            "input": "We are using the dataset of the competition, which includes text from tweets having the aforementioned categories.  \n Question: What dataset is used for this work?",
            "output": [
                "Twitter dataset provided by the organizers"
            ]
        },
        {
            "id": "task460-c13d939a34ce4addaf36ae24d50cc19d",
            "input": "A frequency filter ratio INLINEFORM0 is set to filter out the low-frequency words (rare words) from the lookup table \n Question: how are rare words defined?",
            "output": [
                "low-frequency words"
            ]
        },
        {
            "id": "task460-a8853f95ef114a89b4527c3ed6520506",
            "input": "The Logoscope retrieves newspaper articles from several RSS feeds in French on a daily basis. \n Question: How often are the newspaper websites crawled daily?",
            "output": [
                "RSS feeds in French on a daily basis"
            ]
        },
        {
            "id": "task460-427fb54da8874e7fbc15a6eaefbbf657",
            "input": "Exemplars aim to provide appropriate context. To better understand the context, we experimented by analysing the questions generated through an exemplar. We observed that indeed a supporting exemplar could identify relevant tags (cows in Figure FIGREF3 ) for generating questions.\n\nWe improve use of exemplars by using a triplet network. This network ensures that the joint image-caption embedding for the supporting exemplar are closer to that of the target image-caption and vice-versa. \n Question: How do the authors define exemplars?",
            "output": [
                "Exemplars aim to provide appropriate context. joint image-caption embedding for the supporting exemplar are closer to that of the target image-caption"
            ]
        },
        {
            "id": "task460-49d7ff5e367046c48900e9fedb520485",
            "input": "o evaluate the quality of the generated sequences regarding both precision and recall, the evaluation metrics include BLEU and ROUGE (1, 2, L) scores with multiple references BIBREF22 . \n Question: What evaluation metrics are used?",
            "output": [
                "the evaluation metrics include BLEU and ROUGE (1, 2, L) scores"
            ]
        },
        {
            "id": "task460-2e3651edfbd8421485a74eda95daef4e",
            "input": "Results in Table TABREF13 show ten probes. Again, we see the performance of baseline ELMo-transformer and mSynC are similar, with mSynC doing slightly worse on 7 out of 9 tasks. \n Question: For how many probe tasks the shallow-syntax-aware contextual embedding perform better than ELMo’s embedding?",
            "output": [
                "performance of baseline ELMo-transformer and mSynC are similar, with mSynC doing slightly worse on 7 out of 9 tasks"
            ]
        },
        {
            "id": "task460-21ee09658bd0461fbbca114fa7230201",
            "input": "Considering the improvements over the majority baseline achieved by the RNN model for both non-English (on the average 22.76% relative improvement; 15.82% relative improvement on Spanish, 72.71% vs. 84.21%, 30.53% relative improvement on Turkish, 56.97% vs. 74.36%, 37.13% relative improvement on Dutch, 59.63% vs. 81.77%, and 7.55% relative improvement on Russian, 79.60% vs. 85.62%) and English test sets (27.34% relative improvement), we can draw the conclusion that our model is robust to handle multiple languages. \n Question: which non-english language had the best performance?",
            "output": [
                "Russian"
            ]
        },
        {
            "id": "task460-3797288ce6bf4800a68b483ffb22c6e3",
            "input": "Twitter provides well-documented API, which allows to request any information about Tweets, users and their profiles, with respect to rate limits. There is special type of API, called Streaming API, that provides a real-time stream of tweets. The key difference with regular API is that connection is kept alive as long as possible, and Tweets are sent in real-time to the client. There are three endpoints of Streaming API of our interest: “sample”, “filter” and “firehose”. The first one provides a sample (random subset) of the full Tweet stream. The second one allows to receive Tweets matching some search criteria: matching to one or more search keywords, produced by subset of users, or coming from certain geo location. The last one provides the full set of Tweets, although it is not available by default. In order to get Twitter “firehose” one can contact Twitter, or buy this stream from third-parties.\n\nIn our case the simplest approach would be to use “sample” endpoint, but it provides Tweets in all possible languages from all over the World, while we are concerned only about one language (Russian). In order to use this endpoint we implemented filtering based on language. The filter is simple: if Tweet does not contain a substring of 3 or more cyrillic symbols, it is considered non-Russian. Although this approach keeps Tweets in Mongolian, Ukrainian and other slavic languages (because they use cyrillic alphabet), the total amount of false-positives in this case is negligible. To demonstrate this we conducted simple experiment: on a random sample of 200 tweets only 5 were in a language different from Russian. In order not to rely on Twitter language detection, we chose to proceed with this method of language-based filtering.\n\nHowever, the amount of Tweets received through “sample” endpoint was not satisfying. This is probably because “sample” endpoint always streams the same content to all its clients, and small portion of it comes in Russian language. In order to force mining of Tweets in Russian language, we chose \"filter\" endpoint, which requires some search query. We constructed heuristic query, containing some auxiliary words, specific to Russian language: conjunctions, pronouns, prepositions. The full list is as follows:\n\nrussian я, у, к, в, по, на, ты, мы, до, на, она, он, и, да. \n Question: Which Twitter corpus was used to train the word vectors?",
            "output": [
                "They collected tweets in Russian language using a heuristic query specific to Russian"
            ]
        },
        {
            "id": "task460-b409fa1cf2d54631b12403854cd75205",
            "input": "The base sentence embedding model is a BiLSTM over all words in the respective sequences with randomly initialised word embeddings, following BIBREF30 . We opt for this strong baseline sentence encoding model, as opposed to engineering sentence embeddings that work particularly well for this dataset, to showcase the dataset. We would expect pre-trained contextual encoding models, e.g. ELMO BIBREF33 , ULMFit BIBREF34 , BERT BIBREF35 , to offer complementary performance gains, as has been shown for a few recent papers BIBREF36 , BIBREF37 . \n Question: What were the baselines?",
            "output": [
                "a BiLSTM over all words in the respective sequences with randomly initialised word embeddings, following BIBREF30"
            ]
        },
        {
            "id": "task460-6cfc05b188b846f5846ba3f6002e169d",
            "input": "To reduce variance and boost accuracy, we ensemble 10 CNNs and 10 LSTMs together through soft voting. The models ensembled have different random weight initializations, different number of epochs (from 4 to 20 in total), different set of filter sizes (either INLINEFORM0 , INLINEFORM1 or INLINEFORM2 ) and different embedding pre-training algorithms (either Word2vec or FastText). \n Question: How many CNNs and LSTMs were ensembled?",
            "output": [
                "10 CNNs and 10 LSTMs"
            ]
        },
        {
            "id": "task460-b18132f7682f4ef78a64677b1936c4c9",
            "input": "Defining Faithfulness ::: Assumption 1 (The Model Assumption).\nTwo models will make the same predictions if and only if they use the same reasoning process. Defining Faithfulness ::: Assumption 2 (The Prediction Assumption).\nOn similar inputs, the model makes similar decisions if and only if its reasoning is similar. Defining Faithfulness ::: Assumption 3 (The Linearity Assumption).\nCertain parts of the input are more important to the model reasoning than others. Moreover, the contributions of different parts of the input are independent from each other. \n Question: Which are three assumptions in current approaches for defining faithfulness?",
            "output": [
                "Two models will make the same predictions if and only if they use the same reasoning process. On similar inputs, the model makes similar decisions if and only if its reasoning is similar. Certain parts of the input are more important to the model reasoning than others. Moreover, the contributions of different parts of the input are independent from each other."
            ]
        },
        {
            "id": "task460-220bc6364d8647e8bc14521a50d06ae7",
            "input": "To account for this, we compute the balanced accuracy, i.e., the average of the three accuracies on each class. \n Question: Is model explanation output evaluated, what metric was used?",
            "output": [
                "balanced accuracy, i.e., the average of the three accuracies on each class"
            ]
        },
        {
            "id": "task460-41252f4ff7e947abbd77bc1032b5a05d",
            "input": "Noticeably, despite being both based on and integrated into a BERT$_\\text{base}$ model, our architecture even outperforms a standalone BERT$_\\text{large}$ model by a large margin. \n Question: What models other than standalone BERT is new model compared to?",
            "output": [
                "Only Bert base and Bert large are compared to proposed approach."
            ]
        },
        {
            "id": "task460-b606fd78302c46bb8293667a32a70e9a",
            "input": "To verify the effectiveness of our proposed model, we conduct multiple experiments on three Chinese Machine Reading Comprehension datasets, namely CMRC-2017 BIBREF17 , People's Daily (PD) and Children Fairy Tales (CFT) BIBREF2  Besides, we also use the Children's Book Test (CBT) dataset BIBREF1 to test the generalization ability in multi-lingual case. \n Question: which public datasets were used?",
            "output": [
                "CMRC-2017 People's Daily (PD) Children Fairy Tales (CFT)  Children's Book Test (CBT)"
            ]
        },
        {
            "id": "task460-2356c5f85cb64f96aa05b0b134be3031",
            "input": "We use the annotations from experts for an abstract if it exists otherwise use crowd annotations.  \n Question: How do they match annotators to instances?",
            "output": [
                "Annotations from experts are used if they have already been collected."
            ]
        },
        {
            "id": "task460-23bfc9e809a240bca24de55d01acd203",
            "input": "The SnapCaptions dataset is composed of 10K user-generated image (snap) and textual caption pairs where named entities in captions are manually labeled by expert human annotators (entity types: PER, LOC, ORG, MISC). \n Question: How large is their MNER SnapCaptions dataset?",
            "output": [
                "10K user-generated image (snap) and textual caption pairs"
            ]
        },
        {
            "id": "task460-bdf272cacbd641c3af6a29bf7f1b3498",
            "input": "Given the annotated tweets, we wanted to ascertain the distribution of: types of offensive language, genres where it is used, the dialects used, and the gender of users using such language. As the figure shows, sports and politics are most dominant for offensive language including vulgar and hate speech. As for dialect, we looked at MSA and four major dialects, namely Egyptian (EGY), Leventine (LEV), Maghrebi (MGR), and Gulf (GLF). Figure FIGREF14 shows that 71% of vulgar tweets were written in EGY followed by GLF, which accounted for 13% of vulgar tweets. MSA was not used in any of the vulgar tweets. As for offensive tweets in general, EGY and GLF were used in 36% and 35% of the offensive tweets respectively. Unlike the case of vulgar language where MSA was non-existent, 15% of the offensive tweets were in fact written in MSA. For hate speech, GLF and EGY were again dominant and MSA consistuted 21% of the tweets. This is consistent with findings for other languages such as English and Italian where vulgar language was more frequently associated with colloquial language BIBREF24, BIBREF25. Regarding the gender, Figure FIGREF15 shows that the vast majority of offensive tweets, including vulgar and hate speech, were authored by males. Female Twitter users accounted for 14% of offensive tweets in general and 6% and 9% of vulgar and hate speech respectively. Figure FIGREF16 shows a detailed categorization of hate speech types, where the top three include insulting groups based on their political ideology, origin, and sport affiliation. Religious hate speech appeared in only 15% of all hate speech tweets. \n Question: How did they analyze which topics, dialects and gender are most associated with tweets?",
            "output": [
                "ascertain the distribution of: types of offensive language, genres where it is used, the dialects used, and the gender of users using such language"
            ]
        },
        {
            "id": "task460-5f427df02cf647ce93c4e73725085157",
            "input": "We call these techniques Recurrence over BERT (RoBERT) and Transformer over BERT (ToBERT). We observed that ToBERT outperforms RoBERT on pre-trained BERT features and fine-tuned BERT features for all our tasks.  \n Question: On top of BERT does the RNN layer work better or the transformer layer?",
            "output": [
                "Transformer over BERT (ToBERT)"
            ]
        },
        {
            "id": "task460-38e3064c456342a995e4a715a9a64c42",
            "input": "In particular, we investigated attention to nouns, verbs, pronouns, subjects, objects, and negation words, and special BERT tokens across the tasks. \n Question: What handcrafter features-of-interest are used?",
            "output": [
                "nouns verbs pronouns subjects objects negation words special BERT tokens"
            ]
        },
        {
            "id": "task460-b729d98e5e474a639342644cd1d979e0",
            "input": "We approached the first and second challenges by using a Bayesian approach to learn which terms were associated with events, regardless of whether they are standard language, acronyms, or even a made-up word, so long as they match the events of interest. The third and fourth challenges are approached by using word-pairs, where we extract all the pairs of co-occurring words within each tweet. To find the words most associated with events, we search for the words that achieve the highest number of spikes matching the days of events.  \n Question: How are the keywords associated with events such as protests selected?",
            "output": [
                "By using a Bayesian approach  and by using word-pairs, where they extract all the pairs of co-occurring words within each tweet.  They search for the words that achieve the highest number of spikes matching the days of events."
            ]
        },
        {
            "id": "task460-b78c181f2fa54c9db873b7a3fd5e08b6",
            "input": " We use standard Precision, Recall and F1 at mention level (Micro) and at the document level (Macro) as measurements. We can see the average linking precision (Micro) of WW is lower than that of TAC2010, and NCEL outperforms all baseline methods in both easy and hard cases. \n Question: How do they verify generalization ability?",
            "output": [
                "By calculating Macro F1 metric at the document level."
            ]
        },
        {
            "id": "task460-da0aaabdeeb54cc18f0bfee94280b078",
            "input": "This process gives a total of INLINEFORM0 10700 training examples with roughly 2000 to 3000 examples per class, with each speaker having an average of 185 examples. \n Question: How many instances does their dataset have?",
            "output": [
                "10700"
            ]
        },
        {
            "id": "task460-8af5c293bb634528b88e477a0d95af6f",
            "input": "Our contribution leverages pre-trained word embeddings (GloVe, trained on wikipedia+gigaword corpus), the DepecheMood affective lexicon, and convolutional neural networks. \n Question: What is the source of their lexicon?",
            "output": [
                "DepecheMood"
            ]
        },
        {
            "id": "task460-700600a99b594372a1b2671d16393427",
            "input": "As before, the training and test sets contain some 30,000 and 5,000 sentence pairs, respectively \n Question: How many samples did they generate for the artificial language?",
            "output": [
                "70,000"
            ]
        },
        {
            "id": "task460-42678255fae84d98ac19be7fcd251f3a",
            "input": "For NMT, we used the KyotoNMT system BIBREF16 . The NMT training settings are the same as those of the best systems that participated in WAT 2016. The sizes of the source and target vocabularies, the source and target side embeddings, the hidden states, the attention mechanism hidden states, and the deep softmax output with a 2-maxout layer were set to 32,000, 620, 1000, 1000, and 500, respectively. We used 2-layer LSTMs for both the source and target sides. ADAM was used as the learning algorithm, with a dropout rate of 20% for the inter-layer dropout, and L2 regularization with a weight decay coefficient of 1e-6. The mini batch size was 64, and sentences longer than 80 tokens were discarded. \n Question: What kinds of neural networks did they use in this paper?",
            "output": [
                "LSTMs"
            ]
        },
        {
            "id": "task460-92e61a9221f1453c818fe42a04a8bda3",
            "input": "We build and test our MMT models on the Multi30K dataset BIBREF21 . Each image in Multi30K contains one English (EN) description taken from Flickr30K BIBREF22 and human translations into German (DE), French (FR) and Czech BIBREF23 , BIBREF24 , BIBREF25 . The dataset contains 29,000 instances for training, 1,014 for development, and 1,000 for test. We only experiment with German and French, which are languages for which we have in-house expertise for the type of analysis we present. In addition to the official Multi30K test set (test 2016), we also use the test set from the latest WMT evaluation competition, test 2018 BIBREF25 . \n Question: Do they report results only on English dataset?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-4eeb7049c25244bb828ed98833977830",
            "input": "To evaluate the effectiveness of the proposed framework for the ATE task, we conduct experiments over four benchmark datasets from the SemEval ABSA challenge BIBREF1 , BIBREF18 , BIBREF12 . Table TABREF24 shows their statistics. INLINEFORM0 (SemEval 2014) contains reviews of the laptop domain and those of INLINEFORM1 (SemEval 2014), INLINEFORM2 (SemEval 2015) and INLINEFORM3 (SemEval 2016) are for the restaurant domain. In these datasets, aspect terms have been labeled by the task organizer. \n Question: Which dataset(s) do they use to train the model?",
            "output": [
                "INLINEFORM0 (SemEval 2014) contains reviews of the laptop domain and those of INLINEFORM1 (SemEval 2014), INLINEFORM2 (SemEval 2015) and INLINEFORM3 (SemEval 2016) are for the restaurant domain."
            ]
        },
        {
            "id": "task460-8d1b78a234cc4fdfad22b05b7fd90d23",
            "input": "Crowd workers were asked to mark whether a triple was correct, namely, did the triple reflect the consequence of the sentence. \n Question: What is the role of crowd-sourcing?",
            "output": [
                "Crowd workers were asked to mark whether a triple was correct, namely, did the triple reflect the consequence of the sentence."
            ]
        },
        {
            "id": "task460-f0b5667a5c0f4fa5aa75fbdb043c21aa",
            "input": "Category II and III errors are harder to avoid and could be improved by applying reasoning BIBREF36 or irony detection methods BIBREF37. \n Question: What recommendations are made to improve the performance in future?",
            "output": [
                "applying reasoning BIBREF36 or irony detection methods BIBREF37"
            ]
        },
        {
            "id": "task460-476774172ce84bfea11090ed2e6d1cd7",
            "input": "Only the information concerned with the dictionary definitions are used there, discarding the polarity scores. However, when we utilise the supervised score (+1 or -1), words of opposite polarities (e.g. “happy\" and “unhappy\") get far away from each other as they are translated across coordinate regions. \n Question: How are the supervised scores of the words calculated?",
            "output": [
                "(+1 or -1), words of opposite polarities (e.g. “happy\" and “unhappy\") get far away from each other"
            ]
        },
        {
            "id": "task460-41aa1572d00c48c08d4f19071dab5d31",
            "input": "In the sentiment classification task, we tried our model on the following public datasets. MR Movie reviews with one sentence each. IMDB Movie reviews from IMDB website, where each movie review is labeled with binary classes, either positive or negative BIBREF19. SUBJ$^1$ Movie review labeled with subjective or objective BIBREF20. We also tested our CRU model in the cloze-style reading comprehension task. We carried out experiments on the public datasets: CBT NE/CN BIBREF25. \n Question: What datasets are used for testing sentiment classification and reading comprehension?",
            "output": [
                "CBT NE/CN MR Movie reviews IMDB Movie reviews SUBJ"
            ]
        },
        {
            "id": "task460-049633c5cd8447438778c0fae4b9c0aa",
            "input": "First, we can observe that the final model “Ours with Mask and Ordered Triplets” outperforms the Baseline and Ablation models on all metrics in previously seen environments. The difference in performance is particularly evident for the Exact Match and Goal Match metrics, with our model increasing accuracy by 35% and 25% in comparison to the Baseline and Ablation models, respectively. These results suggest that providing the behavioral navigation graph to the model and allowing it to process this information as a knowledge base in an end-to-end fashion is beneficial. Lastly, it is worth noting that our proposed model (last row of Table TABREF28 ) outperforms all other models in previously seen environments. In particular, we obtain over INLINEFORM0 increase in EM and GM between our model and the next best two models. \n Question: By how much did their model outperform the baseline?",
            "output": [
                "increasing accuracy by 35% and 25% in comparison to the Baseline and Ablation models, respectively over INLINEFORM0 increase in EM and GM between our model and the next best two models"
            ]
        },
        {
            "id": "task460-e4f905dda2d34f8184acc1796423e45f",
            "input": "Initially, we experimented with a graph-based community detection algorithm that maximizes cluster modularity BIBREF20 , but we found no simple way to constrain this method to produce a specific number of equally-sized clusters. The brute force approach of enumerating all possible cluster assignments is intractable given the large search space ( INLINEFORM0 possible assignments). We devise a simple clustering algorithm to approximate this process. First, we initialize with random cluster assignments and define “cluster strength” to be the relative difference between “intra-group” Euclidean distance and “inter-group” Euclidean distance. Then, we iteratively propose random exchanges of memberships, only accepting these proposals when the cluster strength increases, until convergence. To evaluate the quality of the computationally-derived clusters against those of Calvino, we measure cluster purity BIBREF21 : given a set of predicted clusters INLINEFORM1 and ground-truth clusters INLINEFORM2 that both partition a set of INLINEFORM3 data points, INLINEFORM4 \n Question: Which clustering method do they use to cluster city description embeddings?",
            "output": [
                " We devise a simple clustering algorithm to approximate this process. First, we initialize with random cluster assignments and define “cluster strength” to be the relative difference between “intra-group” Euclidean distance and “inter-group” Euclidean distance. Then, we iteratively propose random exchanges of memberships, only accepting these proposals when the cluster strength increases, until convergence. "
            ]
        },
        {
            "id": "task460-1882157949ee45f9b643e5a788510353",
            "input": " At the moment defender can draw on methods from image area to text for improving the robustness of DNNs, e.g. adversarial training BIBREF107 , adding extra layer BIBREF113 , optimizing cross-entropy function BIBREF114 , BIBREF115 or weakening the transferability of adversarial examples. \n Question: Which strategies show the most promise in deterring these attacks?",
            "output": [
                "At the moment defender can draw on methods from image area to text for improving the robustness of DNNs, e.g. adversarial training BIBREF107 , adding extra layer BIBREF113 , optimizing cross-entropy function BIBREF114 , BIBREF115 or weakening the transferability of adversarial examples."
            ]
        },
        {
            "id": "task460-654083a161e4433f905a758946c34797",
            "input": "As our models did not use RL refinement, these results constitute significant gains of 1.34 and 1.12 BLEU score on top of the strong baselines in BIBREF3 . The perplexity scores are also better. On the Google Production dataset, our model achieved 1.01 higher test BLEU score even after training for only one sixth of the time. \n Question: What improvement does the MOE model make over the SOTA on machine translation?",
            "output": [
                "1.34 and 1.12 BLEU score on top of the strong baselines in BIBREF3 perplexity scores are also better On the Google Production dataset, our model achieved 1.01 higher test BLEU score"
            ]
        },
        {
            "id": "task460-402569c2fd8144cd9c71d86aab97067e",
            "input": "the facts that are relevant for answering a particular question) are labeled during training. \n Question: What does supporting fact supervision mean?",
            "output": [
                " the facts that are relevant for answering a particular question) are labeled during training."
            ]
        },
        {
            "id": "task460-3cbcd1a79c9c4a65b013ddfcdad5757c",
            "input": "A base model, INLINEFORM0 , is trained on 190 hours ( INLINEFORM1 100K instances) of transcribed speech data. Then, it selects a subset of a 1,700-hour ( INLINEFORM2 1.1M instances) unlabeled dataset. We query labels for the selected subset and incorporate them into training. Learning rates are tuned on a small validation set of 2048 instances. The trained model is then tested on a 156-hour ( INLINEFORM3 100K instances) test set and we report CTC loss, Character Error Rate (CER) and Word Error Rate (WER). \n Question: Which dataset do they use?",
            "output": [
                "190 hours ( INLINEFORM1 100K instances)"
            ]
        },
        {
            "id": "task460-d4731f7b433046d7bb4a2c9685215ce0",
            "input": "It includes a total 708 hours of French (Fr), German (De), Dutch (Nl), Russian (Ru), Spanish (Es), Italian (It), Turkish (Tr), Persian (Fa), Swedish (Sv), Mongolian (Mn) and Chinese (Zh) speeches, with French and German ones having the largest durations among existing public corpora. \n Question: Which languages are part of the corpus?",
            "output": [
                "French (Fr), German (De), Dutch (Nl), Russian (Ru), Spanish (Es), Italian (It), Turkish (Tr), Persian (Fa), Swedish (Sv), Mongolian (Mn) and Chinese (Zh)"
            ]
        },
        {
            "id": "task460-88566b2169a24d0286d6c3650d51d7b1",
            "input": "As indicated in its name, Recurrent Deep Stacking Network stacks and concatenates the outputs of previous frames into the input features of the current frame. \n Question: What does recurrent deep stacking network do?",
            "output": [
                "Stacks and joins outputs of previous frames with inputs of the current frame"
            ]
        },
        {
            "id": "task460-fca0d61ffc344050af09811aeface19a",
            "input": "For sentence encoding models, we chose a simple one-layer bidirectional LSTM with max pooling (BiLSTM-max) with the hidden size of 600D per direction, used e.g. in InferSent BIBREF17 , and HBMP BIBREF18 . For the other models, we have chosen ESIM BIBREF19 , which includes cross-sentence attention, and KIM BIBREF2 , which has cross-sentence attention and utilizes external knowledge. We also selected two model involving a pre-trained language model, namely ESIM + ELMo BIBREF20 and BERT BIBREF0 . \n Question: Which models were compared?",
            "output": [
                "BiLSTM-max, HBMP, ESIM, KIM, ESIM + ELMo, and BERT"
            ]
        },
        {
            "id": "task460-f8ed68eb9c6840338d2092ff56c3d4e6",
            "input": "Guidelines for Evaluating Faithfulness\nWe propose the following guidelines for evaluating the faithfulness of explanations. These guidelines address common pitfalls and sub-optimal practices we observed in the literature.\n\nGuidelines for Evaluating Faithfulness ::: Be explicit in what you evaluate.\nConflating plausability and faithfulness is harmful. You should be explicit on which one of them you evaluate, and use suitable methodologies for each one. Of course, the same applies when designing interpretation techniques—be clear about which properties are being prioritized.\n\nGuidelines for Evaluating Faithfulness ::: Faithfulness evaluation should not involve human-judgement on the quality of interpretation.\nWe note that: (1) humans cannot judge if an interpretation is faithful or not: if they understood the model, interpretation would be unnecessary; (2) for similar reasons, we cannot obtain supervision for this problem, either. Therefore, human judgement should not be involved in evaluation for faithfulness, as human judgement measures plausability.\n\nGuidelines for Evaluating Faithfulness ::: Faithfulness evaluation should not involve human-provided gold labels.\nWe should be able to interpret incorrect model predictions, just the same as correct ones. Evaluation methods that rely on gold labels are influenced by human priors on what should the model do, and again push the evaluation in the direction of plausability.\n\nGuidelines for Evaluating Faithfulness ::: Do not trust “inherent interpretability” claims.\nInherent interpretability is a claim until proven otherwise. Explanations provided by “inherently interpretable” models must be held to the same standards as post-hoc interpretation methods, and be evaluated for faithfulness using the same set of evaluation techniques.\n\nGuidelines for Evaluating Faithfulness ::: Faithfulness evaluation of IUI systems should not rely on user performance.\nEnd-task user performance in HCI settings is merely indicative of correlation between plausibility and model performance, however small this correlation is. While important to evaluate the utility of the interpretations for some use-cases, it is unrelated to faithfulness. \n Question: Which are key points in guidelines for faithfulness evaluation?",
            "output": [
                "Be explicit in what you evaluate. Faithfulness evaluation should not involve human-judgement on the quality of interpretation. Faithfulness evaluation should not involve human-provided gold labels. Do not trust “inherent interpretability” claims. Faithfulness evaluation of IUI systems should not rely on user performance."
            ]
        },
        {
            "id": "task460-90321a75b2e049129bfe90bf8081901d",
            "input": "Figure FIGREF29 shows the $\\text{MPAs}$ of the proposed DP-LSTM and vanilla LSTM for comparison.  \n Question: Is the model compared against a linear regression baseline?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-cdd4d1d4a163422d8db770b5cb485d41",
            "input": "For the shared task, a balanced dataset of 2,396 ironic and 2,396 non-ironic tweets is provided. The ironic corpus was constructed by collecting self-annotated tweets with the hashtags #irony, #sarcasm and #not. The tweets were then cleaned and manually checked and labeled, using a fine-grained annotation scheme BIBREF3 .  \n Question: What is the size of the dataset?",
            "output": [
                "a balanced dataset of 2,396 ironic and 2,396 non-ironic tweets is provided"
            ]
        },
        {
            "id": "task460-6f4bbccbf09449cba313c0a0d87fbc42",
            "input": "We recruited 176 AMT workers to participate in our conceptualization task. \n Question: What crowdsourcing platform was used?",
            "output": [
                "AMT"
            ]
        },
        {
            "id": "task460-943f508fd1a240b7848af73bc06a0f00",
            "input": "We select a large-scale Chinese dataset BIBREF0 with millions of real comments and a human-annotated test set to evaluate our model. The dataset is collected from Tencent News, which is one of the most popular Chinese websites for news and opinion articles. The dataset consists of 198,112 news articles. Each piece of news contains a title, the content of the article, and a list of the users' comments. \n Question: What news comment dataset was used?",
            "output": [
                "Chinese dataset BIBREF0"
            ]
        },
        {
            "id": "task460-51054275a2e84ae09e7123f6823c879e",
            "input": "We trained word embeddings using either GloVe BIBREF11 or SGNS BIBREF12 on a small or a large corpus. \n Question: What types of word representations are they evaluating?",
            "output": [
                "GloVE; SGNS"
            ]
        },
        {
            "id": "task460-2d0b084c91c543868ad88098a79af8ba",
            "input": "The bossa-nova and jovem-guarda genres, which have few instances in the dataset, are among the most difficult ones to classify using the model. \n Question: what genre was the most difficult to classify?",
            "output": [
                " bossa-nova and jovem-guarda genres"
            ]
        },
        {
            "id": "task460-05cc484572af4f64b0191337127d5196",
            "input": "For each user, we calculate the proportion of tweets scored positively by each LIWC category.  \n Question: How is LIWC incorporated into this system?",
            "output": [
                " For each user, we calculate the proportion of tweets scored positively by each LIWC category."
            ]
        },
        {
            "id": "task460-47f63ea1ae724591b5ae10179b3a3ff2",
            "input": "For each object, $O_i$ in the input Open IE tuple $(S; P; O_1; O_2 \\ldots )$ , we add a triple $(S; P; O_i)$ to this table. \n Question: Are the OpenIE extractions all triples?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-6b78560efa3d406f88dea083b2f077f3",
            "input": "We use the additional INLINEFORM0 training articles labeled by publisher as an unsupervised data set to further train the BERT model. We first investigate the impact of pre-training on BERT-BASE's performance.  On the same computer, fine tuning the model on the small training set took only about 35 minutes for sequence length 100.  \n Question: How are the two different models trained?",
            "output": [
                "They pre-train the models using 600000 articles as an unsupervised dataset and then fine-tune the models on small training set."
            ]
        },
        {
            "id": "task460-f0d75734c5534742916e1627af13aab4",
            "input": "In our method, we take an image as input and generate a natural question as output. \n Question: What is the input to the differential network?",
            "output": [
                "image"
            ]
        },
        {
            "id": "task460-6124e6b8153d422785790bf666b556e7",
            "input": "We compare with three baseline methods. $({1})$ SC-LSTM BIBREF3 is a canonical model and a strong baseline that uses an additional dialog act vector and a reading gate to guide the utterance generation. $({2})$ GPT-2 BIBREF6 is used to directly fine-tune on the domain-specific labels, without pre-training on the large-scale corpus of (dialog act, response) pairs. $({3})$ HDSA BIBREF7 is a state-of-the-art model on MultiWOZ. It leverages dialog act structures to enable transfer in the multi-domain setting, showing superior performance than SC-LSTM. \n Question: What existing methods is SC-GPT compared to?",
            "output": [
                "$({1})$ SC-LSTM BIBREF3 $({2})$ GPT-2 BIBREF6  $({3})$ HDSA BIBREF7"
            ]
        },
        {
            "id": "task460-cabe827d00424b38b6077b3db0bb3823",
            "input": "Our prediction task is thus a straightforward binary classification task at the word level. We develop the following five groups of features to capture properties of how a word is used in the explanandum (see Table TABREF18 for the full list):\n\n[itemsep=0pt,leftmargin=*,topsep=0pt]\n\nNon-contextual properties of a word. These features are derived directly from the word and capture the general tendency of a word being echoed in explanations.\n\nWord usage in an OP or PC (two groups). These features capture how a word is used in an OP or PC. As a result, for each feature, we have two values for the OP and PC respectively.\n\nHow a word connects an OP and PC. These features look at the difference between word usage in the OP and PC. We expect this group to be the most important in our task.\n\nGeneral OP/PC properties. These features capture the general properties of a conversation. They can be used to characterize the background distribution of echoing.\n\nTable TABREF18 further shows the intuition for including each feature, and condensed $t$-test results after Bonferroni correction. Specifically, we test whether the words that were echoed in explanations have different feature values from those that were not echoed. In addition to considering all words, we also separately consider stopwords and content words in light of Figure FIGREF8. Here, we highlight a few observations:\n\n[itemsep=0pt,leftmargin=*,topsep=0pt] \n Question: What are their proposed features?",
            "output": [
                "Non-contextual properties of a word Word usage in an OP or PC (two groups) How a word connects an OP and PC. General OP/PC properties"
            ]
        },
        {
            "id": "task460-222bce4013ad4dd3bdebc8d030281538",
            "input": "In this paper, I introduce an interpretable word embedding model, and an associated topic model, which are designed to work well when trained on a small to medium-sized corpus of interest. I tested the performance of the representations as features for document categorization and regression tasks. The results are given in Table TABREF26 . For document categorization, I used three standard benchmark datasets: 20 Newsgroups (19,997 newsgroup posts), Reuters-150 newswire articles (15,500 articles and 150 classes), and Ohsumed medical abstracts on 23 cardiovascular diseases (20,000 articles). I I also analyzed the regression task of predicting the year of a state of the Union address based on its text information.  \n Question: What supervised learning tasks are attempted with these representations?",
            "output": [
                "document categorization regression tasks"
            ]
        },
        {
            "id": "task460-e1ab3fde3618498fbd0f9711a300d608",
            "input": "As is common in the literature BIBREF4 , BIBREF8 , we use 300-dimensional vectors for our embeddings and all word vectors are normalized to unit length prior to evaluation. \n Question: What dimensions of word embeddings do they produce using factorization?",
            "output": [
                "300-dimensional vectors"
            ]
        },
        {
            "id": "task460-8991630e093e4fcea44ee8d82b926ceb",
            "input": "The second algorithm incorporates recent ideas of data selection in machine translation BIBREF7 . We develop a novel re-weighting method to resolve these problems, using ideas inspired by data selection in machine translation BIBREF26 , BIBREF7 . \n Question: What is the data selection paper in machine translation",
            "output": [
                "BIBREF7 BIBREF26 "
            ]
        },
        {
            "id": "task460-f39ef80a7d2d4be6a30381320534f0bf",
            "input": "In multitask learning, a set of related tasks are learned (e.g., emotional activation), along with a primary task (e.g., emotional valence); both tasks share parts of the network topology and are hence jointly trained, as depicted in Figure FIGREF4 . \n Question: What are the tasks in the multitask learning setup?",
            "output": [
                "set of related tasks are learned (e.g., emotional activation) primary task (e.g., emotional valence)"
            ]
        },
        {
            "id": "task460-ca51ad0521d94787b33ed10aef351d31",
            "input": "The corpus used for sentiment analysis is the IMDb dataset of movie reviews by BIBREF11 while that for NER is Groningen Meaning Bank (GMB) by BIBREF12, containing 47,959 sentence samples. \n Question: What sentiment analysis dataset is used?",
            "output": [
                "IMDb dataset of movie reviews"
            ]
        },
        {
            "id": "task460-118485e832e7425fb8e602f1a2560144",
            "input": "Sentiment Classification\nWe first conduct a multi-task experiment on sentiment classification.\n\nWe use 16 different datasets from several popular review corpora used in BIBREF20 . These datasets consist of 14 product review datasets and two movie review datasets.\n\nAll the datasets in each task are partitioned randomly into training set, development set and testing set with the proportion of 70%, 10% and 20% respectively. Transferability of Shared Sentence Representation\nWith attention mechanism, the shared sentence encoder in our proposed models can generate more generic task-invariant representations, which can be considered as off-the-shelf knowledge and then be used for unseen new tasks. Introducing Sequence Labeling as Auxiliary Task\nA good sentence representation should include its linguistic information. Therefore, we incorporate sequence labeling task (such as POS Tagging and Chunking) as an auxiliary task into the multi-task learning framework, which is trained jointly with the primary tasks (the above 16 tasks of sentiment classification). \n Question: What tasks did they experiment with?",
            "output": [
                "Sentiment Classification Transferability of Shared Sentence Representation Introducing Sequence Labeling as Auxiliary Task"
            ]
        },
        {
            "id": "task460-625267d5cd7343688ca9e2d9a626e776",
            "input": "To select the most appropriate sentences in a large number of unlabeled corpora, we propose a scoring model based on information entropy and neural network as the sampling strategy of active learning, which is inspired by Cai and Zhao BIBREF32 . The score of a segmented sentence is computed as follows. First, mapping the segmented sentence to a sequence of candidate word embeddings. Then, the scoring model takes the word embedding sequence as input, scoring over each individual candidate word from two perspectives: (1) the possibility that the candidate word itself can be regarded as a legal word; (2) the rationality of the link that the candidate word directly follows previous segmentation history. Fig. FIGREF10 illustrates the entire scoring model. A gated neural network is employed over character embeddings to generate distributed representations of candidate words, which are sent to a LSTM model. \n Question: How does the scoring model work?",
            "output": [
                "First, mapping the segmented sentence to a sequence of candidate word embeddings. Then, the scoring model takes the word embedding sequence as input, scoring over each individual candidate word"
            ]
        },
        {
            "id": "task460-7bd1e88c29b640838bceeaa76b7e7d34",
            "input": "The model is composed of an encoder (§SECREF5) and a decoder with the attention mechanism (§SECREF7) that are both implemented using recurrent neural networks (RNNs); the encoder converts source words into a sequence of vectors, and the decoder generates target language words one-by-one with the attention mechanism based on the conditional probability shown in the equation DISPLAY_FORM2 and DISPLAY_FORM3. \n Question: Which model architecture do they use to build a model?",
            "output": [
                "model is composed of an encoder (§SECREF5) and a decoder with the attention mechanism (§SECREF7) that are both implemented using recurrent neural networks (RNNs)"
            ]
        },
        {
            "id": "task460-a7b950217a3e48b88a0c08c48366983a",
            "input": "We consider QA PGNet and Multi-decoder QA PGNet with lookup table embedding as baseline models and improve on the baselines with other variations described below. \n Question: What is the baseline?",
            "output": [
                "QA PGNet Multi-decoder QA PGNet with lookup table embedding"
            ]
        },
        {
            "id": "task460-032a36b64bc74ae691c45b171175b416",
            "input": "We consider two different models for each language pair: the Baseline and the Document model. We evaluate them on 3 test sets and report BLEU and TER scores. All experiments are run 8 times with different seeds, we report averaged results and p-values for each experiment. \n Question: What evaluation metrics did they use?",
            "output": [
                "BLEU and TER scores"
            ]
        },
        {
            "id": "task460-e669a0c4cd4e4f67ba43ba41272b9ad6",
            "input": "We published a survey on Reddit asking Danish speaking users to suggest offensive, sexist, and racist terms for a lexicon. Language and user behaviour varies between platforms, so the goal is to capture platform-specific terms. This gave 113 offensive and hateful terms which were used to find offensive comments. The remainder of comments in the corpus were shuffled and a subset of this corpus was then used to fill the remainder of the final dataset. The resulting dataset contains 3600 user-generated comments, 800 from Ekstra Bladet on Facebook, 1400 from r/DANMAG and 1400 from r/Denmark. \n Question: How large was the dataset of Danish comments?",
            "output": [
                "3600 user-generated comments"
            ]
        },
        {
            "id": "task460-b89d9cff651340048bad239d0f46f4e4",
            "input": "We use the pre-trained uncased BERT$_\\mathrm {BASE}$ model for fine-tuning, because we find that BERT$_\\mathrm {LARGE}$ model performs slightly worse than BERT$_\\mathrm {BASE}$ in this task.  \n Question: Do they use large or small BERT?",
            "output": [
                "small BERT"
            ]
        },
        {
            "id": "task460-cec10c18edf44b9db4b26b0449857129",
            "input": "Our experiments are run using the English$\\rightarrow $Italian/German portions of the MuST-C corpus BIBREF25, which is extracted from TED talks, using the same train/validation/test split as provided with the corpus (see Table TABREF18). As additional data, we use a mix of public and proprietary data for about 16 million sentence pairs for English-Italian (En-It) and $4.4$ million WMT14 sentence pairs for the English-German (En-De). \n Question: What dataset do they use for experiments?",
            "output": [
                "English$\\rightarrow $Italian/German portions of the MuST-C corpus As additional data, we use a mix of public and proprietary data for about 16 million sentence pairs for English-Italian (En-It) and $4.4$ million WMT14 sentence pairs for the English-German (En-De)"
            ]
        },
        {
            "id": "task460-8d24f05cd7d7431faf911e42400c4ef5",
            "input": "The training dataset contains 2,815 examples, where 1,940 (i.e., 69%) are fake news and 1,968 (i.e., 70%) are click-baits; we further have 761 testing examples. \n Question: what datasets were used?",
            "output": [
                " training dataset contains 2,815 examples 761 testing examples"
            ]
        },
        {
            "id": "task460-d93cdad93d464c178dc1f0df3f8bafb4",
            "input": "On the other hand, Go-Explore Seq2Seq shows promising results by solving almost half of the unseen games. Figure FIGREF62 (in Appendix SECREF60) shows that most of the lost games are in the hardest set, where a very long sequence of actions is required for winning the game. These results demonstrate both the relative effectiveness of training a Seq2Seq model on Go-Explore trajectories, but they also indicate that additional effort needed for designing reinforcement learning algorithms that effectively generalize to unseen games. \n Question: How do the authors show that their learned policy generalize better than existing solutions to unseen games?",
            "output": [
                "promising results by solving almost half of the unseen games most of the lost games are in the hardest set, where a very long sequence of actions is required for winning the game"
            ]
        },
        {
            "id": "task460-254aef7fc7f2440ebecf7d1ca38fb4e5",
            "input": "In order to understand what may be happening in the model, we used the body and punchline only datasets to see what part of the joke was most important for humor. We found that all of the models, including humans, relied more on the punchline of the joke in their predictions (Table 2). Thus, it seems that although both parts of the joke are needed for it to be humorous, the punchline carries higher weight than the body. We hypothesize that this is due to the variations found in the different joke bodies: some take paragraphs to set up the joke, while others are less than a sentence. \n Question: Which part of the joke is more important in humor?",
            "output": [
                "the punchline of the joke "
            ]
        },
        {
            "id": "task460-09292d88743d49f7a5aeb1b00ef46dd1",
            "input": "Comparing these topics with those that appeared on a Catholic women forum, it seems that both ISIS and non-violent groups use topics about motherhood, spousal relationship, and marriage/divorce when they address women. Moreover, we used Depechemood methods to analyze the emotions that these materials are likely to elicit in readers. The result of our emotion analysis suggests that both corpuses used words that aim to inspire readers while avoiding fear. However, the actual words that lead to these effects are very different in the two contexts. Overall, our findings indicate that, using proper methods, automated analysis of large bodies of textual data can provide novel insight insight into extremist propaganda that can assist the counterterrorism community. \n Question: What conclusions do the authors draw from their finding that the emotional appeal of ISIS and Catholic materials are similar?",
            "output": [
                "both corpuses used words that aim to inspire readers while avoiding fear actual words that lead to these effects are very different in the two contexts our findings indicate that, using proper methods, automated analysis of large bodies of textual data can provide novel insight insight into extremist propaganda"
            ]
        },
        {
            "id": "task460-d35970c4c58143ab87dc03431fa1ea01",
            "input": "The UCL team had access to micro-narratives, as well as context specific meta-data such as demographic information and project details. For a cross-national comparison for policy-makers, the team translated the responses in multiple languages into English using machine translation, in this case Translate API (Yandex Technologies). As a pre-processing step, words without functional meaning (e.g. `I'), rare words that occurred in only one narrative, numbers, and punctuation were all removed. The remaining words were stemmed to remove plural forms of nouns or conjugations of verbs. \n Question: What elements of natural language processing are proposed to analyze qualitative data?",
            "output": [
                "translated the responses in multiple languages into English using machine translation words without functional meaning (e.g. `I'), rare words that occurred in only one narrative, numbers, and punctuation were all removed remaining words were stemmed to remove plural forms of nouns or conjugations of verbs"
            ]
        },
        {
            "id": "task460-29419ccfff2f4880ac16938d2709f71d",
            "input": "We achieved first place for 8 directions: German$\\leftrightarrow $English, German$\\leftrightarrow $French, Chinese$\\leftrightarrow $English, English$\\rightarrow $Lithuanian, English$\\rightarrow $Finnish, and Russian$\\rightarrow $English, and three other directions were placed second (ranked by teams), which included Lithuanian$\\rightarrow $English, Finnish$\\rightarrow $English, and English$\\rightarrow $Kazakh. \n Question: Which language directions are machine translation systems of WMT evaluated on?",
            "output": [
                "German$\\leftrightarrow $English, German$\\leftrightarrow $French, Chinese$\\leftrightarrow $English, English$\\rightarrow $Lithuanian, English$\\rightarrow $Finnish, and Russian$\\rightarrow $English Lithuanian$\\rightarrow $English, Finnish$\\rightarrow $English, and English$\\rightarrow $Kazakh"
            ]
        },
        {
            "id": "task460-3bcd9d8eb2794ffe9acd470754f716af",
            "input": "Deeply moved readers shed tears or get chills and goosebumps even in lab settings BIBREF4. In cases like these, the emotional response actually implies an aesthetic evaluation: narratives that have the capacity to move readers are evaluated as good and powerful texts for this very reason. Similarly, feelings of suspense experienced in narratives not only respond to the trajectory of the plot's content, but are also directly predictive of aesthetic liking (or disliking). Emotions that exhibit this dual capacity have been defined as “aesthetic emotions” BIBREF2. \n Question: What are the aesthetic emotions formalized?",
            "output": [
                "feelings of suspense experienced in narratives not only respond to the trajectory of the plot's content, but are also directly predictive of aesthetic liking (or disliking) Emotions that exhibit this dual capacity have been defined as “aesthetic emotions”"
            ]
        },
        {
            "id": "task460-c4a8147c6165429d952b53d485840ccb",
            "input": "In this paper, we propose three different types of CRU models: shallow fusion, deep fusion and deep-enhanced fusion, from the most fundamental one to the most expressive one. The most simple one is to directly apply a CNN layer after the embedding layer to obtain blended contextual representations. Then a GRU layer is applied afterward. \n Question: How is CNN injected into recurent units?",
            "output": [
                "The most simple one is to directly apply a CNN layer after the embedding layer to obtain blended contextual representations. Then a GRU layer is applied afterward."
            ]
        },
        {
            "id": "task460-9287e35ba0db4dad85b8cce17f6e1637",
            "input": "Our experiments are performed on an actual endangered language, Mboshi (Bantu C25), a language spoken in Congo-Brazzaville, using the bilingual French-Mboshi 5K corpus of BIBREF17. \n Question: Which language family does Mboshi belong to?",
            "output": [
                "Bantu"
            ]
        },
        {
            "id": "task460-9c00fffda58b48aa872b83cfcc0160e0",
            "input": "We manually reviewed 1,177 pairs of entities and referring expressions generated by the system. Overall, 73.3% of the non-first mentions of entities were replaced with suitable shorter and more fluent expressions. \n Question: How is fluency of generated text evaluated?",
            "output": [
                "manually reviewed"
            ]
        },
        {
            "id": "task460-b0b7b012710c4ced9dec53e5c8d343f0",
            "input": "Recently bhat-EtAl:2017:EACLshort provided a CS dataset for the evaluation of their parsing models which they trained on the Hindi and English Universal Dependency (UD) treebanks. We extend this dataset by annotating 1,448 more sentences. \n Question: How big is the provided treebank?",
            "output": [
                "1448 sentences more than the dataset from Bhat et al., 2017"
            ]
        },
        {
            "id": "task460-7b1d391e33ba44afba64b5b51f1ae4fa",
            "input": "To find a large amount of hate speech on the refugee crisis, we used 10 hashtags that can be used in an insulting or offensive way. \n Question: How were potentially hateful messages identified?",
            "output": [
                "10 hashtags that can be used in an insulting or offensive way"
            ]
        },
        {
            "id": "task460-209fd6f012014d788371f58c15867f16",
            "input": "The ratio of correct `translations' (matches) was used as an evaluation measure. \n Question: What evaluation metric do they use?",
            "output": [
                "Accuracy"
            ]
        },
        {
            "id": "task460-af5b85e00b784be5b6a2a14232f3644d",
            "input": "We conducted our experiments on the CSJ BIBREF25, which is one of the most widely used evaluation sets for Japanese speech recognition. The CSJ consists of more than 600 hrs of Japanese recordings.\n\nWhile most of the content is lecture recordings by a single speaker, CSJ also contains 11.5 hrs of 54 dialogue recordings (average 12.8 min per recording) with two speakers, which were the main target of ASR and speaker diarization in this study. \n Question: How long are dialogue recordings used for evaluation?",
            "output": [
                "average 12.8 min per recording"
            ]
        },
        {
            "id": "task460-af4871b60c4b4ee5803fa580d441f09d",
            "input": "We evalute Bertram on the WNLaMPro dataset of BIBREF0. \n Question: What is dataset for word probing task?",
            "output": [
                "WNLaMPro dataset"
            ]
        },
        {
            "id": "task460-1566c4e4bd494243b67f7596b784f487",
            "input": "n this work, we apply an automated neural ensemble annotation process for dialogue act labeling. Several neural models are trained with the Switchboard Dialogue Act (SwDA) Corpus BIBREF9, BIBREF10 and used for inferring dialogue acts on the emotion datasets. We ensemble five model output labels by checking majority occurrences (most of the model labels are the same) and ranking confidence values of the models. We adopted the neural architectures based on Bothe et al. bothe2018discourse where two variants are: non-context model (classifying at utterance level) and context model (recognizing the dialogue act of the current utterance given a few preceding utterances). \n Question: How many models were used?",
            "output": [
                "five"
            ]
        },
        {
            "id": "task460-0b9e03e9fecd41538a5efeffe2811254",
            "input": "To the best of our knowledge, the only scientific summarization benchmark is from TAC 2014 summarization track. For evaluating the effectiveness of Rouge variants and our metric (Sera), we use this benchmark, which consists of 20 topics each with a biomedical journal article and 4 gold human written summaries. Consider the following example:\n\nEndogeneous small RNAs (miRNA) were genetically screened and studied to find the miRNAs which are related to tumorigenesis. \n Question: Do the authors report results only on English data?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-a451ef49930541fcb1b0853102c87f53",
            "input": "We train language models on two languages: One model is estimated on the German newscrawl distributed by WMT'18 comprising 260M sentences or 6B tokens. Another model is trained on the English newscrawl data comprising 193M sentences or 5B tokens. We learn a joint Byte-Pair-Encoding (BPE; Sennrich et al., 2016) vocabulary of 37K types on the German and English newscrawl and train the language models with this vocabulary. We consider two benchmarks: Most experiments are run on the WMT'18 English-German (en-de) news translation task and we validate our findings on the WMT'18 English-Turkish (en-tr) news task. For WMT'18 English-German, the training corpus consists of all available bitext excluding the ParaCrawl corpus and we remove sentences longer than 250 tokens as well as sentence-pairs with a source/target length ratio exceeding 1.5. This results in 5.18M sentence pairs. We tokenize all data with the Moses tokenizer BIBREF8 and apply the BPE vocabulary learned on the monolingual corpora. \n Question: What dataset do they use?",
            "output": [
                "German newscrawl distributed by WMT'18  English newscrawl data WMT'18 English-German (en-de) news translation task  WMT'18 English-Turkish (en-tr) news task"
            ]
        },
        {
            "id": "task460-462304d45f50463fbd5a4d7e21020479",
            "input": "In a nutshell, B4MSA starts by applying text-transformations to the messages, then transformed text is represented in a vector space model (see Subsection SECREF13 ), and finally, a Support Vector Machine (with linear kernel) is used as the classifier. \n Question: What are the components of the multilingual framework?",
            "output": [
                "text-transformations to the messages vector space model Support Vector Machine"
            ]
        },
        {
            "id": "task460-7ef9455ce0ec42a0b9d1b3433727138e",
            "input": "To increase the variety of corpus, we selected 4 English corpora and 4 Mandarin corpora in addition to the low resource language corpora. \n Question: Do they test their approach on large-resource tasks?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-8f3efd85be1243a59fa0fe1533cc6b0d",
            "input": "The ensembles were formed by simply averaging the predictions from the constituent single models. \n Question: How does their ensemble method work?",
            "output": [
                "simply averaging the predictions from the constituent single models"
            ]
        },
        {
            "id": "task460-a26275f5ad734ffd89c2d4bf3e20583b",
            "input": "The BLEU metric is adopted to evaluate the model performance during evaluation. \n Question: What evaluation metric is used?",
            "output": [
                "The BLEU metric "
            ]
        },
        {
            "id": "task460-4cc319c2eead45e18585c025b003feae",
            "input": "If we re-train the skip-gram model and update the emoji map periodically on new escort ads, when traffickers switch to new emojis, the map can link the new emojis to the old ones, assisting anti-trafficking experts in expanding the lexicon of trafficking flags. This approach also works for acronyms and deliberate typos. \n Question: How is the lexicon of trafficking flags expanded?",
            "output": [
                "re-train the skip-gram model and update the emoji map periodically on new escort ads, when traffickers switch to new emojis, the map can link the new emojis to the old ones"
            ]
        },
        {
            "id": "task460-652c08d8b02e4c7194e4fae26d6293a1",
            "input": "The neural machine translation was trained using Nematus. For the NMT system as well as for the PreMT system, we used the default configuration. \n Question: Which NMT architecture do they use?",
            "output": [
                "trained using Nematus default configuration"
            ]
        },
        {
            "id": "task460-2eb148ca4dda48db87d1344edb409412",
            "input": "We compare the performance of our model (Table 2 ) with traditional Bag of Words (BoW), TF-IDF, and n-grams features based classifiers. We also compare against averaged Skip-Gram BIBREF29 , Doc2Vec BIBREF30 , CNN BIBREF23 , Hierarchical Attention (HN-ATT) BIBREF24 and hierarchical network (HN) models. HN it is similar to our model HN-SA but without any self attention. We further investigated the performance on SWBD2 by examining the confusion matrix of the model. Figure 2 shows the heatmap of the normalized confusion matrix of the model on SWBD2. \n Question: Do the authors do manual evaluation?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-fcf67497b9804b9cb5899397407cefe9",
            "input": "In order to validate the proposed classification model in real crowdsourcing settings, Amazon Mechanical Turk (AMT) was used to obtain labels from multiple annotators for two popular datasets: Reuters-21578 BIBREF30 and LabelMe BIBREF31 . In order to first validate the proposed model for classification problems in a slightly more controlled environment, the well-known 20-Newsgroups benchmark corpus BIBREF29 was used by simulating multiple annotators with different levels of expertise.  \n Question: what datasets were used?",
            "output": [
                "Reuters-21578 BIBREF30  LabelMe BIBREF31 20-Newsgroups benchmark corpus BIBREF29 "
            ]
        },
        {
            "id": "task460-5d2760926d5e4e139bc019172d9fc651",
            "input": "We have downloaded 1,873 Twitter conversation threads, roughly 14k tweets, from a publicly available resource that were previously pre-processed and have conversation threads extracted. \n Question: How large is the Twitter dataset?",
            "output": [
                "1,873 Twitter conversation threads, roughly 14k tweets"
            ]
        },
        {
            "id": "task460-603708dce48c41139ea747510aebd79f",
            "input": "As a raw corpus, we used a Japanese web corpus that was compiled through the procedures proposed by BIBREF13.  From the corpus of about 100 million sentences, we obtained 1.4 millions event pairs for AL, 41 millions for CA, and 6 millions for CO. \n Question: How large is raw corpus used for training?",
            "output": [
                "100 million sentences"
            ]
        },
        {
            "id": "task460-010352fb5cae4efabdddd1a6b0c2b489",
            "input": "For the cross-domain polarity classification experiments, we use the second version of Multi-Domain Sentiment Dataset BIBREF0 . The data set contains Amazon product reviews of four different domains: Books (B), DVDs (D), Electronics (E) and Kitchen appliances (K).  \n Question: What domains are contained in the polarity classification dataset?",
            "output": [
                "Books DVDs Electronics Kitchen appliances"
            ]
        },
        {
            "id": "task460-10e47446390f4c25837afbb17b72f279",
            "input": "We use word2vec BIBREF0 to train the word embeddings. \n Question: What kind of model do they build to expand abbreviations?",
            "output": [
                "word2vec BIBREF0"
            ]
        },
        {
            "id": "task460-1ef3af80f3874b35b3cb4897583ef6d9",
            "input": "For the 2019 Workshop on Neural Generation of Text (WNGT) Efficiency shared task BIBREF0, the Notre Dame Natural Language Processing (NDNLP) group looked at a method of inducing sparsity in parameters called auto-sizing in order to reduce the number of parameters in the Transformer at the cost of a relatively minimal drop in performance. \n Question: What is WNGT 2019 shared task?",
            "output": [
                "efficiency task aimed  at reducing the number of parameters while minimizing drop in performance"
            ]
        },
        {
            "id": "task460-03fbd8ef6a634b1187d54431aeb5b5bc",
            "input": "The stance towards vaccination was categorized into `Negative’, `Neutral’, `Positive’ and `Not clear’. \n Question: Do they allow for messages with vaccination-related key terms to be of neutral stance?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-71d7f40715fd4ca9b3676fa4fb3ad1c6",
            "input": "BERT: We implement two BERT-based baselines BIBREF51 for evidence identification. First, we train BERT on each query-policy sentence pair as a binary classification task to identify if the sentence is evidence for the question or not (Bert). We also experiment with a two-stage classifier, where we separately train the model on questions only to predict answerability. At inference time, if the answerable classifier predicts the question is answerable, the evidence identification classifier produces a set of candidate sentences (Bert + Unanswerable).\n\n \n Question: What type of neural model was used?",
            "output": [
                "Bert + Unanswerable"
            ]
        },
        {
            "id": "task460-1dfd74686e4245ab91501dafd3495121",
            "input": "Although FAR is supposed to be favored as FAMs are already manually labeled and tell exactly if one sentence should be extracted (assuming our annotations are in agreement), to further verify that FAR correlates with human preference, we rank UnifiedSum(E), NeuSum, and Lead-3 in Table TABREF15. The order of the 1st rank in the human evaluation coincides with FAR. FAR also has higher Spearman's coefficient $\\rho $ than ROUGE (0.457 vs. 0.44, n=30, threshold=0.362 at 95% significance). \n Question: How do they evaluate their proposed metric?",
            "output": [
                "manually labeled and tell exactly if one sentence should be extracted (assuming our annotations are in agreement), to further verify that FAR correlates with human preference,"
            ]
        },
        {
            "id": "task460-d3e6f6c9ab3b421981ab03231319a778",
            "input": "The respondents in our MTurk survey had most difficulties recognizing reviews of category $(b=0.3, \\lambda=-5)$, where true positive rate was $40.4\\%$, while the true negative rate of the real class was $62.7\\%$. The precision were $16\\%$ and $86\\%$, respectively. The class-averaged F-score is $47.6\\%$, which is close to random. Detailed classification reports are shown in Table~\\ref{table:MTurk_sub} in Appendix. Our MTurk-study shows that \\emph{our NMT-Fake reviews pose a significant threat to review systems}, since \\emph{ordinary native English-speakers have very big difficulties in separating real reviews from fake reviews}. We use the review category $(b=0.3, \\lambda=-5)$ for future user tests in this paper, since MTurk participants had most difficulties detecting these reviews. We refer to this category as NMT-Fake* in this paper. The classifier is very effective in detecting reviews that humans have difficulties detecting. For example, the fake reviews MTurk users had most difficulty detecting ($b=0.3, \\lambda=-5$) are detected with an excellent 97\\% F-score. \n Question: Does their detection tool work better than human detection?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-fc9f3f5403c34b1682ce3ed8a758b32a",
            "input": "As a framework, we use the sentence classifier configuration of FLAIR BIBREF46 with a biLSTM encoder/classifier architecture fed by character and word level representations composed of a concatenation of fixed 300 dimensional GloVe embeddings BIBREF47, pre-trained contextualized FLAIR word embeddings, and pre-trained contextualized character embeddings from AllenNLP BIBREF48 with FLAIR's default hyperparameters. \n Question: Are some models evaluated using this metric, what are the findings?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-3b5c351d68c94005a686f72cbba82adc",
            "input": "Conditional Random Fields (CRF) BIBREF15 have been extensively used for tasks of sequential nature. In this paper, we propose as one of the competitive baselines a CRF classifier trained with sklearn-crfsuite for Python 3.5 and the following configuration: algorithm = lbfgs; maximum iterations = 100; c1 = c2 = 0.1; all transitions = true; optimise = false. spaCy is a widely used NLP library that implements state-of-the-art text processing pipelines, including a sequence-labelling pipeline similar to the one described by strubell2017fast. spaCy offers several pre-trained models in Spanish, which perform basic NLP tasks such as Named Entity Recognition (NER). In this paper, we have trained a new NER model to detect NUBes-PHI labels. As the simplest baseline, a sensitive data recogniser and classifier has been developed that consists of regular-expressions and dictionary look-ups. For each category to detect a specific method has been implemented. For instance, the Date, Age, Time and Doctor detectors are based on regular-expressions; Hospital, Sex, Kinship, Location, Patient and Job are looked up in dictionaries. The dictionaries are hand-crafted from the training data available, except for the Patient's case, for which the possible candidates considered are the 100 most common female and male names in Spain according to the Instituto Nacional de Estadística (INE; Spanish Statistical Office). \n Question: What are the other algorithms tested?",
            "output": [
                "NER model CRF classifier trained with sklearn-crfsuite classifier has been developed that consists of regular-expressions and dictionary look-up"
            ]
        },
        {
            "id": "task460-f8120b03aa8249b2a09ad95880ee8836",
            "input": "In this section, we describe the data augmentation methods we use to increase the amount of training data in order to make our NMT systems suffer less from the low-resourced situation in Japanese INLINEFORM0 Vietnamese translation. Back Translation\nOne of the approaches to leverage the monolingual data is to use a machine translation system to translate those data in order to create a synthetic parallel data. Normally, the monolingual data in the target language is translated, thus the name of the method: Back Translation BIBREF11 . Mix-Source Approach\nAnother data augmentation method considered useful in this low-resourced setting is the mix-source method BIBREF12 . \n Question: what methods were used to reduce data sparsity effects?",
            "output": [
                "Back Translation Mix-Source Approach"
            ]
        },
        {
            "id": "task460-6334af146e1546c0acdecafdcc31c0c5",
            "input": "We collected COVID-19 related Arabic tweets from January 1, 2020 until April 15, 2020, using Twitter streaming API and the Tweepy Python library.  \n Question: Over what period of time were the tweets collected?",
            "output": [
                "from January 1, 2020 until April 15, 2020"
            ]
        },
        {
            "id": "task460-bf1531a10ada472d83232cbbcd40bd30",
            "input": "Through the experiments, we empirically studied our analysis on DIRL and the effectiveness of our proposed solution in dealing with the problem it suffered from. In addition, we studied the impact of each step described in §SECREF10 and §SECREF14 to our proposed solution, respectively. To performe the study, we carried out performance comparison between the following models:\n\nSO: the source-only model trained using source domain labeled data without any domain adaptation.\n\nCMD: the centre-momentum-based domain adaptation model BIBREF3 of the original DIRL framework that implements $\\mathcal {L}_{inv}$ with $\\text{CMD}_K$.\n\nDANN: the adversarial-learning-based domain adaptation model BIBREF2 of the original DIRL framework that implements $\\mathcal {L}_{inv}$ with $\\text{JSD}(\\rm {P}_S, \\rm {P}_T)$.\n\n$\\text{CMD}^\\dagger $: the weighted version of the CMD model that only applies the first step (described in §SECREF10) of our proposed method.\n\n$\\text{DANN}^\\dagger $: the weighted version of the DANN model that only applies the first step of our proposed method.\n\n$\\text{CMD}^{\\dagger \\dagger }$: the weighted version of the CMD model that applies both the first and second (described in §SECREF14) steps of our proposed method.\n\n$\\text{DANN}^{\\dagger \\dagger }$: the weighted version of the DANN model that applies both the first and second steps of our proposed method.\n\n$\\text{CMD}^{*}$: a variant of $\\text{CMD}^{\\dagger \\dagger }$ that assigns $\\mathbf {w}^*$ (estimate from target labeled data) to $\\mathbf {w}$ and fixes this value during model training.\n\n$\\text{DANN}^{*}$: a variant of $\\text{DANN}^{\\dagger \\dagger }$ that assigns $\\mathbf {w}^*$ to $\\mathbf {w}$ and fixes this value during model training. We conducted experiments on the Amazon reviews dataset BIBREF9, which is a benchmark dataset in the cross-domain sentiment analysis field.  From this dataset, we constructed 12 binary-class cross-domain sentiment analysis tasks: B$\\rightarrow $D, B$\\rightarrow $E, B$\\rightarrow $K, D$\\rightarrow $B, D$\\rightarrow $E, D$\\rightarrow $K, E$\\rightarrow $B, E$\\rightarrow $D, E$\\rightarrow $K, K$\\rightarrow $B, K$\\rightarrow $D, K$\\rightarrow $E.  We additionally constructed 12 multi-class cross-domain sentiment classification tasks. Tasks were designed to distinguish reviews of 1 or 2 stars (class 1) from those of 4 stars (class 2) and those of 5 stars (class 3).  Table TABREF27 shows model performance on the 12 binary-class cross-domain tasks. From this table, we can obtain the following observations.  \n Question: How is DIRL evaluated?",
            "output": [
                "Through the experiments, we empirically studied our analysis on DIRL and the effectiveness of our proposed solution in dealing with the problem it suffered from."
            ]
        },
        {
            "id": "task460-18967104dba940bcab4f0af7df62f605",
            "input": "For MSA, we acquired the diacritized corpus that was used to train the RDI BIBREF7 diacritizer and the Farasa diacritizer BIBREF31. The corpus contains 9.7M tokens with approximately 194K unique surface forms (excluding numbers and punctuation marks). For testing, we used the freely available WikiNews test set BIBREF31, which is composed of 70 MSA WikiNews articles (18,300 tokens) and evenly covers a variety of genres including politics, economics, health, science and technology, sports, arts and culture. For CA, we obtained a large collection of fully diacritized classical texts (2.7M tokens) from a book publisher, and we held-out a small subset of 5,000 sentences (approximately 400k words) for testing. \n Question: what datasets were used?",
            "output": [
                "diacritized corpus that was used to train the RDI BIBREF7 diacritizer and the Farasa diacritizer BIBREF31 WikiNews test set BIBREF31  large collection of fully diacritized classical texts (2.7M tokens) from a book publisher"
            ]
        },
        {
            "id": "task460-ae17e17faaa84d009d044b05608fdfec",
            "input": "We design the word-level refine decoder because this process is similar to the cloze task in BERT's pre-train process, therefore by using the ability of the contextual language model the decoder can generate more fluent and natural sequences. \n Question: Why masking words in the decoder is helpful?",
            "output": [
                "ecause this process is similar to the cloze task in BERT's pre-train process, therefore by using the ability of the contextual language model the decoder can generate more fluent and natural sequences."
            ]
        },
        {
            "id": "task460-e9ac622719e4439aa79ae6997a5a564a",
            "input": "We quantify the efficiency-accuracy tradeoff compared to two rule-based baselines: Unif and Stopword.  \n Question: What are the baselines used?",
            "output": [
                "Unif and Stopword"
            ]
        },
        {
            "id": "task460-27d1659cb92f45738f39f5d8521d390d",
            "input": "Yet, it is possible that the precise interpretability scores that are measured here are biased by the dataset used. However, it should be noted that the change in the interpretability scores for different word coverages might be effected by non-ideal subsampling of category words. Although our word sampling method, based on words' distances to category centers, is expected to generate categories that are represented better compared to random sampling of category words, category representations might be suboptimal compared to human designed categories. \n Question: What are the weaknesses of their proposed interpretability quantification method?",
            "output": [
                "can be biased by dataset used and may generate categories which are suboptimal compared to human designed categories"
            ]
        },
        {
            "id": "task460-81798657f3c944b6881ac2365f7cf3a4",
            "input": "In order to represent individual sentences, we insert external [cls] tokens at the start of each sentence, and each [cls] symbol collects features for the sentence preceding it. We also use interval segment embeddings to distinguish multiple sentences within a document. This way, document representations are learned hierarchically where lower Transformer layers represent adjacent sentences, while higher layers, in combination with self-attention, represent multi-sentence discourse. Position embeddings in the original Bert model have a maximum length of 512; we overcome this limitation by adding more position embeddings that are initialized randomly and fine-tuned with other parameters in the encoder. \n Question: What is novel about their document-level encoder?",
            "output": [
                "Bert model have a maximum length of 512; we overcome this limitation by adding more position embeddings we insert external [cls] tokens at the start of each sentence, and each [cls] symbol collects features for the sentence preceding it document representations are learned hierarchically"
            ]
        },
        {
            "id": "task460-37ef8bc5b5084b73a2ee886f22a03db3",
            "input": "Experiments ::: Evaluation Metrics ::: Setup of Automatic Evaluation ::: Language Fluency\nWe fine-tuned the GPT-2 medium model BIBREF51 on our collected headlines and then used it to measure the perplexity (PPL) on the generated outputs. \n Question: How is fluency automatically evaluated?",
            "output": [
                "fine-tuned the GPT-2 medium model BIBREF51 on our collected headlines and then used it to measure the perplexity (PPL) on the generated outputs"
            ]
        },
        {
            "id": "task460-d0d5d3adf6af4acd8e7237c197345b77",
            "input": "From the description of music genres provided above emerges that there is a limited number of super-genres and derivation lines BIBREF19, BIBREF20, as shown in figure FIGREF1.\n\nFrom a computational perspective, genres are classes and, although can be treated by machine learning algorithms, they do not include information about the relations between them. In order to formalize the relations between genres for computing purposes, we define a continuous genre scale from the most experimental and introverted super-genre to the most euphoric and inclusive one. We selected from Wikipedia the 77 genres that we mentioned in bold in the previous paragraph and asked to two independent raters to read the Wikipedia pages of the genres, listen to samples or artists of the genres (if they did not know already) and then annotate the following dimensions: \n Question: How many genres did they collect from?",
            "output": [
                "77 genres"
            ]
        },
        {
            "id": "task460-32ffd2fa8a88418f835c0dbd507aa3b9",
            "input": "We found that the unsupervised morphological analyzer capable of using lattices improved the accuracy of phoneme recognition and word segmentation. Consequently, this result suggests that this word segmentation method considers the multiple hypothesis of speech recognition as a whole and reduces uncertainty such as variability in recognition by using the syllable recognition results in the lattice format. \n Question: How do they show that acquiring names of places helps self-localization?",
            "output": [
                "unsupervised morphological analyzer capable of using lattices improved the accuracy of phoneme recognition and word segmentation Consequently, this result suggests that this word segmentation method considers the multiple hypothesis of speech recognition as a whole and reduces uncertainty such as variability in recognition"
            ]
        },
        {
            "id": "task460-5b2fde8981d944aba222885a89211224",
            "input": "The challenge is addressed as follows: given a natural language input sequence describing the scene, such as a piece of a story coming from a transcript, the goal is to infer which action is most likely to happen next. \n Question: Do they literally just treat this as \"predict the next spell that appears in the text\"?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-7cb916b3a6ee42098cdbe17d3c1ce7a6",
            "input": "Several recent works have investigated jointly training the acoustic model with a masking speech enhancement model BIBREF11, BIBREF12, BIBREF13, but these works did not evaluate their system on speech enhancement metrics. Indeed, our internal experiments show that without access to the clean data, joint training severely harms performance on these metrics. \n Question: Which frozen acoustic model do they use?",
            "output": [
                "a masking speech enhancement model BIBREF11, BIBREF12, BIBREF13"
            ]
        },
        {
            "id": "task460-15b1bc3b066b4323b36eaf1eb74631f4",
            "input": "The best baseline model is NO-MOVE, reaching an accuracy of 30.3% on single sentences and 0.3 on complete paragraphs. \n Question: How well did the baseline perform?",
            "output": [
                "accuracy of 30.3% on single sentences and 0.3 on complete paragraphs"
            ]
        },
        {
            "id": "task460-45146e6058b2476f95e21e816b9bf523",
            "input": "The embeddings for words and POS tags were pre-trained on a large unannotated corpus consisting of the first 1 billion characters from Wikipedia. \n Question: Do they use pretrained models as part of their parser?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-44fb76f3f8e0479d9e3517c78c77fd07",
            "input": " Logistic regression model with character-level n-gram features is presented as a strong baseline for comparison since it was shown very effective. \n Question: What is their baseline?",
            "output": [
                "Logistic regression model with character-level n-gram features"
            ]
        },
        {
            "id": "task460-15f0ce89a8664b2a9d5613e465664c0c",
            "input": "In this work, we introduce IndoSum, a new benchmark dataset for Indonesian text summarization, and evaluated several well-known extractive single-document summarization methods on the dataset. The dataset consists of online news articles and has almost 200 times more documents than the next largest one of the same domain BIBREF2 We used a dataset provided by Shortir, an Indonesian news aggregator and summarizer company. The dataset contains roughly 20K news articles.  \n Question: What is the size of the dataset?",
            "output": [
                "20K"
            ]
        },
        {
            "id": "task460-0008f1639f5e46e6b135767e3b048f75",
            "input": "As a baseline for the RNN models, we apply a uni-directional RNN which predicts the relation after processing the whole sentence. \n Question: Which variant of the recurrent neural network do they use?",
            "output": [
                "uni-directional RNN"
            ]
        },
        {
            "id": "task460-6c4e62587a38483aa848b42bcb3303eb",
            "input": "In the future, we aim to use the corpus for annotation projects such as parts-of-speech tagging, named entity recognition. Furthermore, the generated word embeddings will be utilized for the automatic construction of Sindhi WordNet. \n Question: Are trained word embeddings used for any other NLP task?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-8465297d0ff4437ea58bc9992de50491",
            "input": "We evaluate our pointer-generator performance using BLEU score. The baseline language model is trained using RNNLM BIBREF23 . Perplexity measure is used in the evaluation.\n\n \n Question: Did they use other evaluation metrics?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-39b3c62dd7794878bf6807c9c5adb046",
            "input": "Drawing on the concept of variance in mathematics, local variance loss is defined as the reciprocal of its variance expecting the attention model to be able to focus on more salient parts. The standard variance calculation is based on the mean of the distribution. However, as previous work BIBREF15, BIBREF16 mentioned that the median value is more robust to outliers than the mean value, we use the median value to calculate the variance of the attention distribution. Thus, local variance loss can be calculated as:\n\nwhere $\\hat{\\cdot }$ is a median operator and $\\epsilon $ is utilized to avoid zero in the denominator. \n Question: How do they define local variance?",
            "output": [
                "The reciprocal of the variance of the attention distribution"
            ]
        },
        {
            "id": "task460-7613b6f9266d491b85eed924254427af",
            "input": " In almost all genres, DenseNMT models are significantly better than the baselines. \n Question: did they outperform previous methods?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-7107e6eb5ab2458fa952f28f6434e198",
            "input": "The De7 database used for this study was designed by Marc Schroeder as one of the first attempts of creating diphone databases for expressive speech synthesis BIBREF2. The database contains three voice qualities (modal, soft and loud) uttered by a German female speaker, with about 50 minutes of speech available for each voice quality. \n Question: What large corpus is used for experiments?",
            "output": [
                "The De7 database"
            ]
        },
        {
            "id": "task460-2fd0bf37b27645bfa9a5791827e82431",
            "input": "In BIBREF8 a refined collection of tweets gathered from twitter is presented. Their dataset which is labeled for named entity recognition task contains 8,257 tweets. There are 12,784 entities in total in this dataset. Table TABREF19 shows statistics related to each named entity in training, development and test sets.\n\n \n Question: What datasets did they use?",
            "output": [
                "BIBREF8 a refined collection of tweets gathered from twitter"
            ]
        },
        {
            "id": "task460-131f13313e0e4f968e54a8b61462ab22",
            "input": "The retrieved pairs are compared to the gold standard and evaluated using precision at $k$ (P@$k$, evaluating how often the correct translation is within the $k$ retrieved nearest neighbours of the query). Throughout this work we report P@1, which is equivalent to accuracy, but we also provide results with P@5 and P@10 in the Appendix. \n Question: What evaluation metrics did they use?",
            "output": [
                "we report P@1, which is equivalent to accuracy we also provide results with P@5 and P@10 in the Appendix"
            ]
        },
        {
            "id": "task460-ef1b8a4a676a4bb5ad3a702d8575cea9",
            "input": "To deal with this problem, BIBREF1 introduced a semantically conditioned generation model using Hierarchical Disentangled Self-Attention (HDSA) . \n Question: what semantically conditioned models did they compare with?",
            "output": [
                "Hierarchical Disentangled Self-Attention"
            ]
        },
        {
            "id": "task460-581e5699d34f4605ad295bfe2345945b",
            "input": "They annotated 7.8K verbs, reporting an average of 2.4 QA pairs per predicate. Even though multiple annotators were shown to produce greater coverage, their released dataset was produced using only a single annotator per verb. \n Question: How was coverage measured?",
            "output": [
                "QA pairs per predicate"
            ]
        },
        {
            "id": "task460-1b1b04b7d10d41cca191f69b612c4ce4",
            "input": "Input features are 40 mel-scaled log filterbank enegries (FBanks) computed every 10 ms with 25 ms window, concatenated with deltas and delta-deltas (120 features in vector). We also tried to use spectrogram and experimented with different normalization techniques. \n Question: What features do they experiment with?",
            "output": [
                "40 mel-scaled log filterbank enegries (FBanks) computed every 10 ms with 25 ms window deltas and delta-deltas (120 features in vector) spectrogram"
            ]
        },
        {
            "id": "task460-2b0f358099ea465a938b3c80ca3c57cc",
            "input": "We focus primarily on the word vector representations (word embeddings) created specifically using the twitter dataset. GloVe BIBREF13 is an unsupervised learning algorithm for obtaining vector representations for words. 200-dimensional GloVe embeddings trained on 2 Billion tweets are integrated. Edinburgh embeddings BIBREF14 are obtained by training skip-gram model on Edinburgh corpus BIBREF15 . Since tweets are abundant with emojis, Emoji embeddings BIBREF16 which are learned from the emoji descriptions have been used. Embeddings for each tweet are obtained by summing up individual word vectors and then dividing by the number of tokens in the tweet. \n Question: what pretrained word embeddings were used?",
            "output": [
                "Pretrained word embeddings  were not used"
            ]
        },
        {
            "id": "task460-5aff5a8775df4907bd9ef4fe83f91283",
            "input": "The Online Retail Data Set consists of a clean list of 25873 invoices, totaling 541909 rows and 8 columns. InvoiceNo, CustomerID and StockCode are mostly 5 or 6-digit integers with occasional letters. Quantity is mostly 1 to 3-digit integers, a part of them being negative, and UnitPrice is composed of 1 to 6 digits floating values. InvoiceDate are dates all in the same format, Country contains strings representing 38 countries and Description is 4224 strings representing names of products. We reconstruct text mails from this data, by separating each token with a blank space and stacking the lines for a given invoice, grouped by InvoiceNo. \n Question: What is the source of the tables?",
            "output": [
                "The Online Retail Data Set consists of a clean list of 25873 invoices, totaling 541909 rows and 8 columns."
            ]
        },
        {
            "id": "task460-f1be715119bc4031a35153775dd055e4",
            "input": " For PHASE-ONE, we randomly shuffled and divided the data (1913 signals from 14 individuals) into train (80%), development (10%) and test sets (10%). In PHASE-TWO, in order to perform a fair comparison with the previous methods reported on the same dataset, we perform a leave-one-subject out cross-validation experiment using the best settings we learn from PHASE-ONE. \n Question: How many electrodes were used on the subject in EEG sessions?",
            "output": [
                "1913 signals"
            ]
        },
        {
            "id": "task460-b70bff2fb43d4f3fa5335cc563bbd0ab",
            "input": "Contributors record voice clips by reading from a bank of donated sentences. \n Question: What domains are covered in the corpus?",
            "output": [
                "No specific domain is covered in the corpus."
            ]
        },
        {
            "id": "task460-5efb397a9c6e4535b6ba269a8c792fff",
            "input": "In the unsupervised scenario similarity is computed as the cosine of the produced $h_{L}$ and $h_{R}$ sentence/image representations. \n Question: How they compute similarity between the representations?",
            "output": [
                "similarity is computed as the cosine of the produced $h_{L}$ and $h_{R}$ sentence/image representations"
            ]
        },
        {
            "id": "task460-e8645b7e0a0c426997cd41cc527edae4",
            "input": "To make the annotated data publicly available, we selected 70 news articles from Arabic WikiNews site. These articles cover recent news from year 2013 to year 2015 in multiple genres (politics, economics, health, science and technology, sports, arts, and culture.) Articles contain 18,300 words, and they are evenly distributed among these 7 genres with 10 articles per each. \n Question: What is the size of the dataset?",
            "output": [
                "Articles contain 18,300 words, and they are evenly distributed among these 7 genres with 10 articles per each"
            ]
        },
        {
            "id": "task460-1d949f456bc14a28a3963225e8c61f43",
            "input": "To better exploit such existing data sources, we propose an end-to-end (E2E) model based on pointer networks with attention, which can be trained end-to-end on the input/output pairs of human IE tasks, without requiring token-level annotations. Since our model does not need token-level labels, we create an E2E version of each data set without token-level labels by chunking the BIO-labeled words and using the labels as fields to extract. \n Question: Do they assume sentence-level supervision?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-af50f0308792410e8910210f89d52485",
            "input": "We collect data by using the Twitter API for saved data, available for public use. For our experiments we collect 3200 tweets filtered by keywords like “fire”, “earthquake”, “theft”, “robbery”, “drunk driving”, “drunk driving accident” etc.  \n Question: Do the tweets come from any individual?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-d1fc4c059a724305b78fa4f787d70a25",
            "input": "The Title-to-Story system is a baseline, which generates directly from topic. \n Question: What are the baselines?",
            "output": [
                "Title-to-Story system"
            ]
        },
        {
            "id": "task460-bd7a7ffe279948e798b6665bbf8a409e",
            "input": "KryptoOracle has been built in the Apache ecosystem and uses Apache Spark. Data structures in Spark are based on resilient distributed datasets (RDD), a read only multi-set of data which can be distributed over a cluster of machines and is fault tolerant.  Spark RDD has the innate capability to recover itself because it stores all execution steps in a lineage graph. In case of any faults in the system, Spark redoes all the previous executions from the built DAG and recovers itself to the previous steady state from any fault such as memory overload. Spark RDDs lie in the core of KryptoOracle and therefore make it easier for it to recover from faults. Moreover, faults like memory overload or system crashes may require for the whole system to hard reboot. However, due to the duplicate copies of the RDDs in Apache Hive and the stored previous state of the machine learning model, KryptoOracle can easily recover to the previous steady state.\n\n \n Question: How is the architecture fault-tolerant?",
            "output": [
                "By using Apache Spark which stores all executions in a lineage graph and recovers to the previous steady state from any fault"
            ]
        },
        {
            "id": "task460-578e5aeb53564820a502d05fc9fa9759",
            "input": "The answer styles considered in the experiments corresponded to the two tasks. The NLG task requires a well-formed answer that is an abstractive summary of the question and ten passages, averaging 16.6 words. The Q&A task also requires an abstractive answer but prefers a more concise answer than the NLG task, averaging 13.1 words, where many of the answers do not contain the context of the question. \n Question: Is there exactly one \"answer style\" per dataset?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-0f7fa108d1bd4ae59c6e75da1c21fb8b",
            "input": "An overview of the provided dataset is given in Table TABREF16 . Notice that the available amount of data differs per language. \n Question: is the dataset balanced across the four languages?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-d5e32188fee245688d150628d8f4a009",
            "input": "The experimental results are shown in Table 1, where it can be seen that autoencoders outperformed MLP and CNN outperformed autoencoders with the highest achieved accuracy of 82.6%. \n Question: Which deep learning model performed better?",
            "output": [
                "autoencoders"
            ]
        },
        {
            "id": "task460-1564b495547448b7bb4806b287be316b",
            "input": "We would like to thank the organizers of the WASSA-2017 Shared Task on Emotion Intensity, for providing the data, the guidelines and timely support. \n Question: what dataset was used?",
            "output": [
                "WASSA-2017 Shared Task on Emotion Intensity"
            ]
        },
        {
            "id": "task460-1d874c956eed41f48656b9fbaf4b9a39",
            "input": "Experimental results show that our joint model outperforms the visual-only model in all cases, and the text-only model on Wikipedia and two subsets of arXiv. \n Question: Do the methods that work best on academic papers also work best on Wikipedia?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-32cde5a006b54aa9a006f18dcf1b5330",
            "input": "Unfortunately, $\\mathit {PMI}(w,c)$ goes to negative infinity when the word-context pair $(w,c)$ does not appear in the training corpus. Due to unreliable statistics, this happens very frequently in finite corpora.  \n Question: Why are statistics from finite corpora unreliable?",
            "output": [
                "$\\mathit {PMI}(w,c)$ goes to negative infinity when the word-context pair $(w,c)$ does not appear in the training corpus"
            ]
        },
        {
            "id": "task460-5d3fa8a4bd1a4257b9253c5d34ecd894",
            "input": "(1) AutoJudge consistently and significantly outperforms all the baselines, including RC models and other neural text classification models, which shows the effectiveness and robustness of our model. (2) RC models achieve better performance than most text classification models (excluding GRU+Attention), which indicates that reading mechanism is a better way to integrate information from heterogeneous yet complementary inputs. (3) Comparing with conventional RC models, AutoJudge achieves significant improvement with the consideration of additional law articles. \n Question: what are their results on the constructed dataset?",
            "output": [
                "AutoJudge consistently and significantly outperforms all the baselines RC models achieve better performance than most text classification models (excluding GRU+Attention) Comparing with conventional RC models, AutoJudge achieves significant improvement"
            ]
        },
        {
            "id": "task460-29e8c27df9ff4bd28b6ee16d969e221d",
            "input": "Intuitively, under-translated input words should contribute little to the NMT outputs, yielding much smaller word importance.  By exploiting the word importance calculated by Attribution method, we can identify the under-translation errors automatically without the involvement of human interpreters.  \n Question: How do they measure which words are under-translated by NMT models?",
            "output": [
                "They measured the under-translated words with low word importance score as calculated by Attribution.\nmethod"
            ]
        },
        {
            "id": "task460-24139fd4a15e4212a4d473a45f19fc59",
            "input": "For the competition, we provided 1,000 labeled dialogues from each dataset for training, and 240 unlabeled dialogues from each dataset for evaluation.  \n Question: How large is the first dataset?",
            "output": [
                "1 000 labeled dialogues for training and 240 unlabeled dialogues for evaluation"
            ]
        },
        {
            "id": "task460-ba7cd79a7481483e88e56fd74496bdc5",
            "input": "Most importantly, all three languages have error-corrected corpora for testing purposes, though work on their automatic grammatical error correction is extremely limited (see Section SECREF3 ). \n Question: Do they introduce errors in the data or does the data already contain them?",
            "output": [
                " all three languages have error-corrected corpora for testing purposes"
            ]
        },
        {
            "id": "task460-ec2ac5f9347f41c8b96bf35ab1b77161",
            "input": "Finally, we combine our CNN and RNN models using a voting process. For each sentence in the test set, we apply several CNN and RNN models presented in Tables TABREF12 and TABREF14 and predict the class with the most votes. In case of a tie, we pick one of the most frequent classes randomly. \n Question: How does their simple voting scheme work?",
            "output": [
                "we apply several CNN and RNN models presented in Tables TABREF12 and TABREF14 and predict the class with the most votes In case of a tie, we pick one of the most frequent classes randomly"
            ]
        },
        {
            "id": "task460-352bda007f664494b19ff3fc5b80fc13",
            "input": "Intuitively, when the Ranker makes more accurate passage selections, the Reasoner will work with less noisy data and thus is easier to succeed. Specifically, the Reasoner learns to extract the linking entity from chains selected by a well-trained Ranker, and it benefits the Ranker training by providing extra rewards. \n Question: How do two models cooperate to select the most confident chains?",
            "output": [
                "Reasoner learns to extract the linking entity from chains selected by a well-trained Ranker, and it benefits the Ranker training by providing extra rewards"
            ]
        },
        {
            "id": "task460-388e234b9b6449babd2c5c93ff0a37ac",
            "input": "To test the robustness of the taggers against the OOV problem, we also conduct experiments using artificially constructed unnormalized text by corrupting words in the normal dev set. Again, the CNN tagger outperforms the two baselines by a very large margin. \n Question: How do they confirm their model working well on out-of-vocabulary problems?",
            "output": [
                "conduct experiments using artificially constructed unnormalized text by corrupting words in the normal dev set"
            ]
        },
        {
            "id": "task460-126126bce1344df89abbdd21b5d14ca5",
            "input": "Baselines We thoroughly compare our approaches to the following baselines:\n\nDirect source$\\rightarrow $target: A standard NMT model trained on given source$\\rightarrow $target parallel data.\n\nMultilingual: A single, shared NMT model for multiple translation directions BIBREF6.\n\nMany-to-many: Trained for all possible directions among source, target, and pivot languages.\n\nMany-to-one: Trained for only the directions to target language, i.e., source$\\rightarrow $target and pivot$\\rightarrow $target, which tends to work better than many-to-many systems BIBREF27. \n Question: What are multilingual models that were outperformed in performed experiment?",
            "output": [
                "Direct source$\\rightarrow $target: A standard NMT model trained on given source$\\rightarrow $target Multilingual: A single, shared NMT model for multiple translation directions Many-to-many: Trained for all possible directions among source, target, and pivot languages Many-to-one: Trained for only the directions to target language"
            ]
        },
        {
            "id": "task460-84da074535aa43359a2bb1f65b6cd0b2",
            "input": "Different from scaled dot-product attention, Gaussian-masked directional attention expects to pay attention to the adjacent characters of each positions and cast the localness relationship between characters as a fix Gaussian weight for attention. We assume that the Gaussian weight only relys on the distance between characters. \n Question: How does Gaussian-masked directional multi-head attention works?",
            "output": [
                "pays attentions to adjacent characters and casts a localness relationship between the characters as a fixed Gaussian weight assuming the weight relies on the distance between characters"
            ]
        },
        {
            "id": "task460-a81dfd8f32e34744acfbe1cdbfa2f4ef",
            "input": "We test our proposed approach on three public short text datasets.  SearchSnippets. This dataset was selected from the results of web search transaction using predefined phrases of 8 different domains by Phan et al. BIBREF41 . StackOverflow. We use the challenge data published in Kaggle.com.  Biomedical. We use the challenge data published in BioASQ's official website.  \n Question: What datasets did they use?",
            "output": [
                "SearchSnippets StackOverflow Biomedical"
            ]
        },
        {
            "id": "task460-7dbd5b81abac4f68868471db8a7cae0a",
            "input": "We ran UTD over all 104 telephone calls, which pair 11 hours of audio with Spanish transcripts and their crowdsourced English translations. The transcripts contain 168,195 Spanish word tokens (10,674 types), and the translations contain 159,777 English word tokens (6,723 types). \n Question: what is the size of the speech corpus?",
            "output": [
                "104 telephone calls transcripts contain 168,195 Spanish word tokens  translations contain 159,777 English word tokens"
            ]
        },
        {
            "id": "task460-4b128f1f675f4e63be3d751f5f582c3a",
            "input": "The two popular segmentation methods are morpheme segmentation BIBREF4 and Byte Pair Encoding (BPE) BIBREF5. After word segmentation, we additionally add an specific symbol behind each separated subword unit, which aims to assist the NMT model to identify the morpheme boundaries and capture the semantic information effectively.  We utilize the Zemberek with a morphological disambiguation tool to segment the Turkish words into morpheme units, and utilize the morphology analysis tool BIBREF12 to segment the Uyghur words into morpheme units.  \n Question: How does the word segmentation method work?",
            "output": [
                "morpheme segmentation BIBREF4 and Byte Pair Encoding (BPE) BIBREF5 Zemberek BIBREF12"
            ]
        },
        {
            "id": "task460-4163425a2a9e49c2bbeaebea765d9057",
            "input": "Looking into the future, an excellent extension from the works surveyed in this paper would be to give more independence to the several learning methods (e.g. less human intervention) involved in the studies as well as increasing the size of the output images. \n Question: What challenges remain unresolved?",
            "output": [
                "give more independence to the several learning methods (e.g. less human intervention) involved in the studies increasing the size of the output images"
            ]
        },
        {
            "id": "task460-61ddf15bcbf64805ba45f8d33fb8b350",
            "input": "We first use state-of-the-art PDTB taggers for our baseline BIBREF13 , BIBREF12 for the evaluation of the causality prediction of our models ( BIBREF12 requires sentences extracted from the text as its input, so we used our parser to extract sentences from the message). \n Question: What baselines did they consider?",
            "output": [
                "state-of-the-art PDTB taggers"
            ]
        },
        {
            "id": "task460-5d991b2fd42a4741989809c82e2bf3ec",
            "input": "We report the experimental results for our two tasks in terms of the correct classification rate (CCR). For sentiment analysis, we have a three-class problem (positive, negative, and neutral), where the classes are mutually exclusive. The CCR, averaged for a set of tweets, is defined to be the number of correctly-predicted sentiments over the number of groundtruth sentiments in these tweets. For NER, we consider that each tweet may reference up to four candidates, i.e., targeted entities. The CCR, averaged for a set of tweets, is the number of correctly predicted entities (candidates) over the number of groundtruth entities (candidates) in this set. \n Question: What measures are used for evaluation?",
            "output": [
                "correct classification rate (CCR)"
            ]
        },
        {
            "id": "task460-31e73b6e6f3645858efcf3095f42b34b",
            "input": "To further improve the performance, we adopt system combination on the decoding lattice level. By combining systems, we can take advantage of the strength of each model that is optimized for different domains.  The best result for vlsp2018 of 4.85% WER is obtained by the combination weights 0.6:0.4 where 0.6 is given to the general language model and 0.4 is given to the conversation one. On the vlsp2019 set, the ratio is change slightly by 0.7:0.3 to deliver the best result of 15.09%. \n Question: What is the language model combination technique used in the paper?",
            "output": [
                "system combination on the decoding lattice level combination weights"
            ]
        },
        {
            "id": "task460-fd985625203d48f88c6fbcde46288d4c",
            "input": "We create a new manually annotated multimodal hate speech dataset formed by $150,000$ tweets, each one of them containing text and an image.  \n Question: How large is the dataset?",
            "output": [
                " $150,000$ tweets"
            ]
        },
        {
            "id": "task460-5fbfe4373b8d477a9666430f1d7a1d45",
            "input": "We compare our approach to two other, the first one uses almost the same tweet data as we use for training, and the second one is the CrowdFlower dataset annotated for emotions.\n\nIn the first one Wang et al. BIBREF21 downloaded over 5M tweets which included one of 131 emotional hashtags based on Parrott's three-level categorization of emotions in seven categories: joy, sadness, anger, love, fear, thankfulness, surprise.  In the second one, the reported results are from a paper by BIBREF33 in which they used maximum entropy classifier with bag of words model to classify various emotional datasets. Here we only report part of their result for CrowdFlower dataset that can be mapped to one of our seven labels. \n Question: What baseline is used?",
            "output": [
                " Wang et al. BIBREF21 paper by BIBREF33 in which they used maximum entropy classifier with bag of words model to classify various emotional datasets"
            ]
        },
        {
            "id": "task460-76ecd0bf475f422ab0159c34a7201ec9",
            "input": "The focus of this section is on recently published datasets and LID research applicable to the South African context. An in depth survey of algorithms, features, datasets, shared tasks and evaluation methods may be found in BIBREF0.\n\nThe datasets for the DSL 2015 & DSL 2017 shared tasks BIBREF1 are often used in LID benchmarks and also available on Kaggle . The recently published JW300 parallel corpus BIBREF2 covers over 300 languages with around 100 thousand parallel sentences per language pair on average. The WiLI-2018 benchmark dataset BIBREF4 for monolingual written natural language identification includes around 1000 paragraphs of 235 languages. The NCHLT text corpora BIBREF7 is likely a good starting point for a shared LID task dataset for the South African languages BIBREF8. \n Question: Which datasets are employed for South African languages LID?",
            "output": [
                "DSL 2015 DSL 2017 JW300 parallel corpus  NCHLT text corpora"
            ]
        },
        {
            "id": "task460-04f67f00caf44557823505c538e6cbed",
            "input": "MIMIC-III is a freely available, deidentified database containing electronic health records of patients admitted to an Intensive Care Unit (ICU) at Beth Israel Deaconess Medical Center between 2001 and 2012 \n Question: what datasets were used?",
            "output": [
                "MIMIC-III"
            ]
        },
        {
            "id": "task460-141486a18004455580c69ba039b1eb2b",
            "input": "The overall Total Accuracy score reported in table TABREF19 using the entire feature set is 549.  \n Question: Do they experiment with the dataset?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-0cfedbf1bc0f485788bfcdee5622142b",
            "input": "Two metrics, the accuracy (ACC) and the normalized mutual information metric (NMI), are used to measure the clustering performance BIBREF38 , BIBREF48 .  \n Question: What were the evaluation metrics used?",
            "output": [
                "accuracy normalized mutual information"
            ]
        },
        {
            "id": "task460-d2868db8b899479a9639b7b608312174",
            "input": "Our baseline is TransE since that the score function of our models is based on TransE. \n Question: What baselines are used for comparison?",
            "output": [
                "TransE"
            ]
        },
        {
            "id": "task460-5a49cf15b73b4ff18e88cc7cf3a6759e",
            "input": "Three clear domains can be noticed in the behavior of $\\langle cc \\rangle $ versus $\\wp $, at $t_f$, as shown in Fig. FIGREF15 (blue squares). Phase I: $\\langle cc \\rangle $ increases smoothly for $\\wp < 0.4$, indicating that for this domain there is a small correlation between word neighborhoods. Full vocabularies are attained also for $\\wp < 0.4$; Phase II: a drastic transition appears at the critical domain $\\wp ^* \\in (0.4,0.6)$, in which $\\langle cc \\rangle $ shifts abruptly towards 1. An abrupt change in $V(t_f)$ versus $\\wp $ is also found (Fig. FIGREF16) for $\\wp ^*$; Phase III: single-word languages dominate for $\\wp > 0.6$. The maximum value of $\\langle cc \\rangle $ indicate that word neighborhoods are completely correlated. \n Question: What are three possible phases for language formation?",
            "output": [
                "Phase I: $\\langle cc \\rangle $ increases smoothly for $\\wp < 0.4$, indicating that for this domain there is a small correlation between word neighborhoods. Full vocabularies are attained also for $\\wp < 0.4$ Phase II: a drastic transition appears at the critical domain $\\wp ^* \\in (0.4,0.6)$, in which $\\langle cc \\rangle $ shifts abruptly towards 1. An abrupt change in $V(t_f)$ versus $\\wp $ is also found (Fig. FIGREF16) for $\\wp ^*$ Phase III: single-word languages dominate for $\\wp > 0.6$. The maximum value of $\\langle cc \\rangle $ indicate that word neighborhoods are completely correlated"
            ]
        },
        {
            "id": "task460-b3e25c1f70984c528520d66584f240c0",
            "input": "We mix standard linguistic features, such as Part-Of-Speech (POS) and chunk tag, together with several gazetteers specifically built for classical music, and a series of features representing tokens' left and right context. \n Question: What kind of corpus-based features are taken into account?",
            "output": [
                "standard linguistic features, such as Part-Of-Speech (POS) and chunk tag series of features representing tokens' left and right context"
            ]
        },
        {
            "id": "task460-d011678fc66a4c32858cac75d971d5d5",
            "input": "300 test queries are randomly selected out, and annotators are asked to independently score the results of these queries with different points in terms of their quality: (1) Good (3 points): The response is grammatical, semantically relevant to the query, and more importantly informative and interesting; (2) Acceptable (2 points): The response is grammatical, semantically relevant to the query, but too trivial or generic (e.g.,“我不知道(I don't know)\", “我也是(Me too)”, “我喜欢(I like it)\" etc.); (3) Failed (1 point): The response has grammar mistakes or irrelevant to the query. \n Question: How is human evaluation performed, what were the criteria?",
            "output": [
                "(1) Good (3 points): The response is grammatical, semantically relevant to the query, and more importantly informative and interesting (2) Acceptable (2 points): The response is grammatical, semantically relevant to the query, but too trivial or generic (3) Failed (1 point): The response has grammar mistakes or irrelevant to the query"
            ]
        },
        {
            "id": "task460-3bf62057b7484abea7469dc80eda198e",
            "input": "We employ a huge dataset of 45,000 self-reported depressed users introduced in BIBREF46 where a lexicon of depression symptoms consisting of 1500 depression-indicative terms was created with the help of psychologist clinician and employed for collecting self-declared depressed individual's profiles. A subset of 8,770 users (24 million time-stamped tweets) containing 3981 depressed and 4789 control users (that do not show any depressive behavior) were verified by two human judges BIBREF46 . \n Question: How is the data annotated?",
            "output": [
                "The data are self-reported by Twitter users and then verified by two human experts."
            ]
        },
        {
            "id": "task460-799737c27e124c79bde8e5e3ab1e3e94",
            "input": "We created eight different classifiers, each of which used one of the following eight features available from a tweet as retrieved from a stream of the Twitter API:\n\nUser location (uloc): This is the location the user specifies in their profile. While this feature might seem a priori useful, it is somewhat limited as this is a free text field that users can leave empty, input a location name that is ambiguous or has typos, or a string that does not match with any specific locations (e.g., “at home”). Looking at users' self-reported locations, Hecht et al. BIBREF49 found that 66% report information that can be translated, accurately or inaccurately, to a geographic location, with the other 34% being either empty or not geolocalisable.\n\nUser language (ulang): This is the user's self-declared user interface language. The interface language might be indicative of the user's country of origin; however, they might also have set up the interface in a different language, such as English, because it was the default language when they signed up or because the language of their choice is not available.\n\nTimezone (tz): This indicates the time zone that the user has specified in their settings, e.g., “Pacific Time (US & Canada)”. When the user has specified an accurate time zone in their settings, it can be indicative of their country of origin; however, some users may have the default time zone in their settings, or they may use an equivalent time zone belonging to a different location (e.g., “Europe/London” for a user in Portugal). Also, Twitter's list of time zones does not include all countries.\n\nTweet language (tlang): The language in which a tweet is believed to be written is automatically detected by Twitter. It has been found to be accurate for major languages, but it leaves much to be desired for less widely used languages. Twitter's language identifier has also been found to struggle with multilingual tweets, where parts of a tweet are written in different languages BIBREF50 .\n\nOffset (offset): This is the offset, with respect to UTC/GMT, that the user has specified in their settings. It is similar to the time zone, albeit more limited as it is shared with a number of countries.\n\nUser name (name): This is the name that the user specifies in their settings, which can be their real name, or an alternative name they choose to use. The name of a user can reveal, in some cases, their country of origin.\n\nUser description (description): This is a free text where a user can describe themselves, their interests, etc.\n\nTweet content (content): The text that forms the actual content of the tweet. The use of content has a number of caveats. One is that content might change over time, and therefore new tweets might discuss new topics that the classifiers have not seen before. Another caveat is that the content of the tweet might not be location-specific; in a previous study, Rakesh et al. BIBREF51 found that the content of only 289 out of 10,000 tweets was location-specific. \n Question: What are the eight features mentioned?",
            "output": [
                "User location (uloc) User language (ulang) Timezone (tz) Tweet language (tlang) Offset (offset) User name (name) User description (description) Tweet content (content)"
            ]
        },
        {
            "id": "task460-4762417678af41a4a427f379b4159d3b",
            "input": "Combining the modulus part and the phase part, HAKE maps entities into the polar coordinate system, where the radial coordinate and the angular coordinates correspond to the modulus part and the phase part, respectively. \n Question: How are entities mapped onto polar coordinate system?",
            "output": [
                "radial coordinate and the angular coordinates correspond to the modulus part and the phase part, respectively"
            ]
        },
        {
            "id": "task460-87606206fe104a81a26592bd04d1c883",
            "input": "In this work, we introduce a methodology that provides VQA algorithms with the ability to generate human interpretable attention maps which effectively ground the answer to the relevant image regions. We accomplish this by leveraging region descriptions and object annotations available in the Visual Genome dataset, and using these to automatically construct attention maps that can be used for attention supervision, instead of requiring human annotators to manually provide grounding labels. \n Question: How do they obtain region descriptions and object annotations?",
            "output": [
                "they are available in the Visual Genome dataset"
            ]
        },
        {
            "id": "task460-b0ac93d5a64743c88d3cef83f9cfa428",
            "input": "We began with evaluating standard MT paradigms, i.e., PBSMT BIBREF3 and NMT BIBREF1 . As for PBSMT, we also examined two advanced methods: pivot-based translation relying on a helping language BIBREF10 and induction of phrase tables from monolingual data BIBREF14 .\n\nAs for NMT, we compared two types of encoder-decoder architectures: attentional RNN-based model (RNMT) BIBREF2 and the Transformer model BIBREF18 . In addition to standard uni-directional modeling, to cope with the low-resource problem, we examined two multi-directional models: bi-directional model BIBREF11 and multi-to-multi (M2M) model BIBREF8 .\n\nAfter identifying the best model, we also examined the usefulness of a data augmentation method based on back-translation BIBREF17 . \n Question: what was the baseline?",
            "output": [
                "pivot-based translation relying on a helping language BIBREF10 nduction of phrase tables from monolingual data BIBREF14  attentional RNN-based model (RNMT) BIBREF2 Transformer model BIBREF18 bi-directional model BIBREF11 multi-to-multi (M2M) model BIBREF8 back-translation BIBREF17"
            ]
        },
        {
            "id": "task460-00fbb023104e4e9a86aa0b42f8ea2fea",
            "input": "We use a standard set of supervised as well as unsupervised benchmark tasks from the literature to evaluate our trained models, following BIBREF16 Sentence embeddings are evaluated for various supervised classification tasks as follows. The predefined training split is used to tune the L2 penalty parameter using cross-validation and the accuracy and F1 scores are computed on the test set.  We perform unsupervised evaluation of the learnt sentence embeddings using the sentence cosine similarity, on the STS 2014 BIBREF31 and SICK 2014 BIBREF32 datasets. These similarity scores are compared to the gold-standard human judgements using Pearson's INLINEFORM0 BIBREF33 and Spearman's INLINEFORM1 BIBREF34 correlation scores.  \n Question: What metric is used to measure performance?",
            "output": [
                "Accuracy and F1 score for supervised tasks, Pearson's and Spearman's correlation for unsupervised tasks"
            ]
        },
        {
            "id": "task460-63876eb44ba2457e9664b310f4090cbe",
            "input": "On the same architecture, our RCRN outperforms ablative baselines BiLSTM by INLINEFORM2 and 3L-BiLSTM by INLINEFORM3 on average across 16 datasets. \n Question: By how much do they outperform BiLSTMs in Sentiment Analysis?",
            "output": [
                "Proposed RCRN outperforms ablative baselines BiLSTM by +2.9% and 3L-BiLSTM by +1.1% on average across 16 datasets."
            ]
        },
        {
            "id": "task460-9c9d0000f57f40808898bc046ec51641",
            "input": "Our data consists of two sets used to train and evaluate our automatic speech recognition system. Four major evaluation campaigns have enabled the creation of wide corpora of French broadcast speech: ESTER1 BIBREF13, ESTER2 BIBREF14, ETAPE BIBREF15 and REPERE BIBREF16. \n Question: Which corpora does this paper analyse?",
            "output": [
                "ESTER1 ESTER2 ETAPE REPERE"
            ]
        },
        {
            "id": "task460-bf7eea62a07b475f9d7773495fc77676",
            "input": "We consider pre-training on the audio data (without labels) of WSJ, part of clean Librispeech (about 80h) and full Librispeech as well as a combination of all datasets (§ SECREF7 ).  Our experimental results on the WSJ benchmark demonstrate that pre-trained representations estimated on about 1,000 hours of unlabeled speech can substantially improve a character-based ASR system and outperform the best character-based result in the literature, Deep Speech 2.  \n Question: Which unlabeled data do they pretrain with?",
            "output": [
                "1000 hours of WSJ audio data"
            ]
        },
        {
            "id": "task460-851ca540a4f24d389f2eb82c974bef4d",
            "input": "To conclude this section, our model reliable corrects grammatical, spelling and word order errors on , with more mixed performance on lexical choice errors and some unnecessary paraphrasing of the input. \n Question: What error types is their model more reliable for?",
            "output": [
                "grammatical, spelling and word order errors"
            ]
        },
        {
            "id": "task460-0e3b9b37f8054b88bdb39c8573d54049",
            "input": "Data-driven models for morphological analysis are constructed using training data INLINEFORM0 consisting of INLINEFORM1 training examples. The baseline model BIBREF5 we compare with regards the output space of the model as a subset INLINEFORM2 where INLINEFORM3 is the set of all tag sets seen in this training data. Specifically, they solve the task as a multi-class classification problem where the classes are individual tag sets. In low-resource scenarios, this indicates that INLINEFORM4 and even for those tag sets existing in INLINEFORM5 we may have seen very few training examples. The conditional probability of a sequence of tag sets given the sentence is formulated as a 0th order CRF. DISPLAYFORM0 \n Question: What other cross-lingual approaches is the model compared to?",
            "output": [
                "The baseline model BIBREF5 we compare with regards the output space of the model as a subset INLINEFORM2 where INLINEFORM3 is the set of all tag sets seen in this training data."
            ]
        },
        {
            "id": "task460-a74ce1e0110049a68d0b198e388d80cb",
            "input": "The dataset contains a total of 9710 passages, with an average of 6.24 sentences per passage, 16.16 words per sentence, and an average length of 86 words. \n Question: What is the size of this dataset?",
            "output": [
                "9710 passages, with an average of 6.24 sentences per passage, 16.16 words per sentence, and an average length of 86 words"
            ]
        },
        {
            "id": "task460-d60d9838e78f40e3af211ab3acf91497",
            "input": "We collect a novel dataset of 230K+ recipe texts and 1M+ user interactions (reviews) over 18 years (2000-2018) from Food.com. \n Question: Where do they get the recipes from?",
            "output": [
                "from Food.com"
            ]
        },
        {
            "id": "task460-a88e83a16b7644b094de4b0d02b7dba6",
            "input": "We use OpenNMT BIBREF24 as the implementation of the NMT system for all experiments BIBREF5 . PBMT-R is a phrase-based method with a reranking post-processing step BIBREF18 . Hybrid performs sentence splitting and deletion operations based on discourse representation structures, and then simplifies sentences with PBMT-R BIBREF25 . SBMT-SARI BIBREF19 is syntax-based translation model using PPDB paraphrase database BIBREF26 and modifies tuning function (using SARI). Dress is an encoder-decoder model coupled with a deep reinforcement learning framework, and the parameters are chosen according to the original paper BIBREF20 . \n Question: what state of the art methods did they compare with?",
            "output": [
                "OpenNMT PBMT-R Hybrid SBMT-SARI Dress"
            ]
        },
        {
            "id": "task460-159d7adc110f418e8d270421be1005e3",
            "input": ". We address various different challenges: dialogue act annotated data is not available for customer service on Twitter, the task of dialogue act annotation is subjective, existing taxonomies do not capture the fine-grained information we believe is valuable to our task, and tweets, although concise in nature, often consist of overlapping dialogue acts to characterize their full intent. \n Question: Which dialogue acts are more suited to the twitter domain?",
            "output": [
                "overlapping dialogue acts"
            ]
        },
        {
            "id": "task460-a0c7c94f9a8645c48af7e55c7d53583d",
            "input": "Given a multiple-choice question $qa$ with question text $q$ and answer choices A= $\\lbrace a_i\\rbrace $ , we select the most relevant tuples from $T$ and $S$ as follows. Selecting from Tuple KB: We use an inverted index to find the 1,000 tuples that have the most overlapping tokens with question tokens $tok(qa).$ . \n Question: Is an entity linking process used?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-d6178767bbcf4446a54ea45a6a3c15aa",
            "input": "As shown in Table , the proposed PS-rnn-elmo shows a significant MAP performance improvement compared to the previous best model, CompClip-LM (0.696 to 0.734 absolute). \n Question: How much better performance of proposed model compared to answer-selection models?",
            "output": [
                "significant MAP performance improvement compared to the previous best model, CompClip-LM (0.696 to 0.734 absolute)"
            ]
        },
        {
            "id": "task460-17a4cfce3849458c8ebfa25b788fc9d0",
            "input": "We compared our multi-turn emotionally engaging dialog model (denoted as MEED) with two baselines—the vanilla sequence-to-sequence model (denoted as S2S) and HRAN. \n Question: What two baseline models are used?",
            "output": [
                " sequence-to-sequence model (denoted as S2S) HRAN"
            ]
        },
        {
            "id": "task460-699b733330c24eb09a7c9b6bbba9e2f8",
            "input": " the TransE model Embedding models The Unstructured model BIBREF22 The TransH model BIBREF26 DISTMULT BIBREF45 ConvE BIBREF51 and ConvKB BIBREF52 The Path Ranking Algorithm (PRA) BIBREF21 \n Question: What models does this overview cover?",
            "output": [
                "This article presented a brief overview of embedding models of entity and relationships for KB completion. "
            ]
        },
        {
            "id": "task460-70df9fa21c0f4715b59625acba10465c",
            "input": "Performing the Welch's t-test, both changes after weGAN training are statistically significant at a INLINEFORM0 significance level.  \n Question: Do they evaluate grammaticality of generated text?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-47b560d4ebea43af8291277f01539a12",
            "input": "We compare the performance of the following models:\n\n- IMG-only: This is a simple baseline where we just pass the image through a VGG19 and use the embedding of the image to predict the answer from a fixed vocabulary.\n\n- QUES-only: This is a simple baseline where we just pass the question through a LSTM and use the embedding of the question to predict the answer from a fixed vocabulary.\n\n- SANBIBREF2: This is a state of the art VQA model which is an encoder-decoder model with a multi-layer stacked attention BIBREF26 mechanism. It obtains a representation for the image using a deep CNN and a representation for the query using LSTM. It then uses the query representation to locate relevant regions in the image and uses this to pick an answer from a fixed vocabulary.\n\n- SANDYBIBREF1: This is the best performing model on the DVQA dataset and is a variant of SAN. Unfortunately, the code for this model is not available and the description in the paper was not detailed enough for us to reimplement it. Hence, we report the numbers for this model only on DVQA (from the original paper).\n\n- VOES: This is our model as described in section SECREF3 which is specifically designed for questions which do not have answers from a fixed vocabulary.\n\n- VOES-Oracle: blackThis is our model where the first three stages of VOES are replaced by an Oracle, i.e., the QA model answers questions on a table that has been generated using the ground truth annotations of the plot. With this we can evaluate the performance of the WikiTableQA model when it is not affected by the VED model's errors.\n\n- SAN-VOES: Given the complementary strengths of SAN-VQA and VOES, we train a hybrid model with a binary classifier which given a question decides whether to use the SAN or the VOES model. The data for training this binary classifier is generated by comparing the predictions of a trained SAN model and a trained VOES model on the training dataset. For a given question, the label is set to 1 (pick SAN) if the performance of SAN was better than that of VOES. We ignore questions where there is a tie. The classifier is a simple LSTM based model which computes a representation for the question using an LSTM and uses this representation to predict 1/0. At test time, we first pass the question through this model and depending on the output of this model use SAN or VOES. \n Question: What models other than SAN-VOES are trained on new PlotQA dataset?",
            "output": [
                "IMG-only QUES-only SAN SANDY  VOES-Oracle VOES"
            ]
        },
        {
            "id": "task460-3aba94a5e095461f8c61cc4d2144edb3",
            "input": "The evaluation results are quite favorable for both targets and particularly higher for Target-1, considering the fact that they are the initial experiments on the data set. \n Question: Which SVM approach resulted in the best performance?",
            "output": [
                "Target-1"
            ]
        },
        {
            "id": "task460-d302c710c597459bab94361817981f15",
            "input": "We first experiment with a suite of non-neural models, including Support Vector Machines (SVMs), logistic regression, Naïve Bayes, Perceptron, and decision trees.  We finally experiment with neural models, although our dataset is relatively small. We train both a two-layer bidirectional Gated Recurrent Neural Network (GRNN) BIBREF20 and Convolutional Neural Network (CNN) (as designed in BIBREF21) with parallel filters of size 2 and 3, as these have been shown to be effective in the literature on emotion detection in text (e.g., BIBREF22, BIBREF23).  \n Question: What supervised methods are used?",
            "output": [
                "Support Vector Machines (SVMs), logistic regression, Naïve Bayes, Perceptron, and decision trees a two-layer bidirectional Gated Recurrent Neural Network (GRNN) BIBREF20 and Convolutional Neural Network (CNN) (as designed in BIBREF21)"
            ]
        },
        {
            "id": "task460-a96f4ffdc4fc4fa8bdb55c805d859b28",
            "input": "Among all available parameters to tune the D2V algorithm released by Gensim, six of them were selected for optimisation BIBREF14. The window_size parameter affects the size of the sliding window used to parse texts. The alpha parameter represents the learning rate of the network. The sample setting allows the model to reduce the importance given to high-frequency words. The dm parameter defines the training used architecture (PV-DM or PV-DBOW). The hs option defines whether hierarchical softmax or negative sampling is used during the training. Finally, the vector_size parameter affects the number of dimensions composing the resulting vector. \n Question: What six parameters were optimized with grid search?",
            "output": [
                "window_size alpha sample dm hs vector_size"
            ]
        },
        {
            "id": "task460-01d070628346488180163cf4e5543044",
            "input": "We compare our method with the models trained using Adobe internal NLU tool, Pytext BIBREF18 and Rasa BIBREF19 NLU tools. \n Question: What are the baselines?",
            "output": [
                "Adobe internal NLU tool Pytext Rasa"
            ]
        },
        {
            "id": "task460-c20b8a9a56544e548d2b960ad177d297",
            "input": "From the results which we get, it is evident that the transformer model achieves higher BLEU score than both Attention encoder-decoder and sequence-sequence model. \n Question: How were their results compared to state-of-the-art?",
            "output": [
                "transformer model achieves higher BLEU score than both Attention encoder-decoder and sequence-sequence model"
            ]
        },
        {
            "id": "task460-a1fc396bc3664e33aa0724e0c45ddfd4",
            "input": "CFQ contains the most query patterns by an order of magnitude and also contains significantly more queries and questions than the other datasets.  \n Question: How authors justify that question answering dataset presented is realistic?",
            "output": [
                "CFQ contains the most query patterns by an order of magnitude and also contains significantly more queries and questions than the other datasets"
            ]
        },
        {
            "id": "task460-3b75e14e6d5d43318869ea9dee2084a3",
            "input": "The most intuitive way to evaluate the text answer is to directly compute the Exact Match (EM) and Macro-averaged F1 scores (F1) between the predicted text answer and the ground-truth text answer. \n Question: What evaluation metrics were used?",
            "output": [
                "Exact Match (EM) Macro-averaged F1 scores (F1)"
            ]
        },
        {
            "id": "task460-ff55d340086c46b0a388b92bcb02ca49",
            "input": "However, we found that explicitly initializing these embeddings with vectors pre-trained over a large collection of unlabeled data significantly improved performance (see Section \"Effects of initialized embeddings and corrupt-sampling schemes\" ). \n Question: What is the new initialization method proposed in this paper?",
            "output": [
                "They initialize their word and entity embeddings with vectors pre-trained over a large corpus of unlabeled data."
            ]
        },
        {
            "id": "task460-973ff7cef14946a9a726ad0500cc89b0",
            "input": "Manual verification of Twitter profiles: We verified each profile discovered manually by examining the profile picture, profile background image, recent tweets, and recent pictures posted by a user. \n Question: How is gang membership verified?",
            "output": [
                "Manual verification"
            ]
        },
        {
            "id": "task460-c0904c93fcc44776bf3dda8e832b1ffa",
            "input": "However, adversarial misspellings constitute a longstanding real-world problem. Spammers continually bombard email servers, subtly misspelling words in efforts to evade spam detection while preserving the emails' intended meaning BIBREF1 , BIBREF2 . \n Question: Why is the adversarial setting appropriate for misspelling recognition?",
            "output": [
                "Adversarial misspellings are a real-world problem"
            ]
        },
        {
            "id": "task460-52f23ed02a554e96a97750cba36fa7f1",
            "input": "To tackle this issue, we investigate how to improve the generalization of BERT-based MCQA models with the constraint of limited training data using four representative MCQA datasets: DREAM, MCTest, TOEFL, and SemEval-2018 Task 11. \n Question: What four representative datasets are used for bechmark?",
            "output": [
                "DREAM, MCTest, TOEFL, and SemEval-2018 Task 11"
            ]
        },
        {
            "id": "task460-0eb5c0569373492ba16d87e6043058e3",
            "input": "At the end of the annotation process, we have annotated 700 tweets, where 175 tweets are in favor of and 175 tweets are against Target-1, and similarly 175 tweets are in favor of and 175 are against Target-2.  \n Question: How many tweets did they collect?",
            "output": [
                "700 "
            ]
        },
        {
            "id": "task460-9b1af6cd47df4bac9f16d0659c7fc513",
            "input": "Similar equations were discovered using Euclidean distance computed between the context vector representations of the equations ( INLINEFORM2 ). We give additional example results in Appendix B. \n Question: How do they define similar equations?",
            "output": [
                "By using Euclidean distance computed between the context vector representations of the equations"
            ]
        },
        {
            "id": "task460-44e1358761d84f8bb7001bd497a44c47",
            "input": "We train three systems (S1, S2 and S3) with the corpora summarised in Table TABREF5. The first two systems are transformer models trained on different amounts of data (6M vs. 18M parallel sentences as seen in the Table). The third system includes a modification to consider the information of full coreference chains throughout a document augmenting the sentence to be translated with this information and it is trained with the same amount of sentence pairs as S1. \n Question: Which three neural machine translation systems are analyzed?",
            "output": [
                "first two systems are transformer models trained on different amounts of data The third system includes a modification to consider the information of full coreference chains"
            ]
        },
        {
            "id": "task460-8c2b277fb492404c86207aef3bcd592b",
            "input": "Note that the question answering tasks we consider here are multimodal in that while the context is a procedural text, the question and the multiple choice answers are composed of images. \n Question: What multimodality is available in the dataset?",
            "output": [
                "context is a procedural text, the question and the multiple choice answers are composed of images"
            ]
        },
        {
            "id": "task460-817ff7b0a71e465eb294fea7dfdaedb0",
            "input": "We therefore use reduction in softmax probability of the correct relation as our signaling strength metric for the model. We call this metric ${\\Delta }_s$ (for delta-softmax), which can be written as:\n\nwhere $rel$ is the true relation of the EDU pair, $t_i$ represents the token at index $i$ of $N$ tokens, and $X_{mask=i}$ represents the input sequence with the masked position $i$ (for $i \\in 1 \\ldots N$ ignoring separators, or $\\phi $, the empty set). \n Question: How is the delta-softmax calculated?",
            "output": [
                "Answer with content missing: (Formula) Formula is the answer."
            ]
        },
        {
            "id": "task460-24dd1669885b418bba1aec3373985f89",
            "input": "What similarities and/or differences do these topics have with non-violent, non-Islamic religious material addressed specifically to women? As these questions suggest, to understand what, if anything, makes extremist appeals distinctive, we need a point of comparison in terms of the outreach efforts to women from a mainstream, non-violent religious group. For this purpose, we rely on an online Catholic women's forum. Comparison between Catholic material and the content of ISIS' online magazines allows for novel insight into the distinctiveness of extremist rhetoric when targeted towards the female population. To accomplish this task, we employ topic modeling and an unsupervised emotion detection method. \n Question: How are similarities and differences between the texts from violent and non-violent religious groups analyzed?",
            "output": [
                "By using topic modeling and unsupervised emotion detection on ISIS materials and articles from Catholic women forum"
            ]
        },
        {
            "id": "task460-23f1b1dda10a46f4a32ed178c4e1b45e",
            "input": "In contrast, in order to exploit section information, in this paper we propose to capture a distributed representation of both the global (the whole document) and the local context (e.g., the section/topic) when deciding if a sentence should be included in the summary \n Question: What do they mean by global and local context?",
            "output": [
                "global (the whole document) local context (e.g., the section/topic)"
            ]
        },
        {
            "id": "task460-8068d46bd8824123a37235bede38f931",
            "input": "A “coding scheme” is defined, which is a set of labels, annotations, or codes that items in the corpus may have. Schemes include formal definitions or procedures, and often include examples, particularly for borderline cases. Next, coders are trained with the coding scheme, which typically involves interactive feedback. Training sometimes results in changes to the coding scheme, in which the first round becomes a pilot test. Then, annotators independently review at least a portion of the same items throughout the entire process, with a calculation of “inter-annotator agreement” or “inter-rater reliability.” Finally, there is a process of “reconciliation” for disagreements, which is sometimes by majority vote without discussion and other times discussion-based. \n Question: What are the core best practices of structured content analysis?",
            "output": [
                "“coding scheme” is defined coders are trained with the coding scheme Training sometimes results in changes to the coding scheme calculation of “inter-annotator agreement” or “inter-rater reliability.” there is a process of “reconciliation” for disagreements"
            ]
        },
        {
            "id": "task460-039b2912a200423bb5d009530b731e8a",
            "input": "Corpus utilizados ::: Corpus 5KL\nEste corpus fue constituido con aproximadamente 5 000 documentos (en su mayor parte libros) en español. Los documentos originales, en formatos heterogéneos, fueron procesados para crear un único documento codificado en utf8. Las frases fueron segmentadas automáticamente, usando un programa en PERL 5.0 y expresiones regulares, para obtener una frase por línea.\n\nLas características del corpus 5KL se encuentran en la Tabla TABREF4. Este corpus es empleado para el entrenamiento de los modelos de aprendizaje profundo (Deep Learning, Sección SECREF4).\n\nEl corpus literario 5KL posee la ventaja de ser muy extenso y adecuado para el aprendizaje automático. Tiene sin embargo, la desventaja de que no todas las frases son necesariamente “frases literarias”. Muchas de ellas son frases de lengua general: estas frases a menudo otorgan una fluidez a la lectura y proporcionan los enlaces necesarios a las ideas expresadas en las frases literarias.\n\nOtra desventaja de este corpus es el ruido que contiene. El proceso de segmentación puede producir errores en la detección de fronteras de frases. También los números de página, capítulos, secciones o índices producen errores. No se realizó ningún proceso manual de verificación, por lo que a veces se introducen informaciones indeseables: copyrights, datos de la edición u otros. Estas son, sin embargo, las condiciones que presenta un corpus literario real.\n\nCorpus utilizados ::: Corpus 8KF\nUn corpus heterogéneo de casi 8 000 frases literarias fue constituido manualmente a partir de poemas, discursos, citas, cuentos y otras obras. Se evitaron cuidadosamente las frases de lengua general, y también aquellas demasiado cortas ($N \\le 3$ palabras) o demasiado largas ($N \\ge 30$ palabras). El vocabulario empleado es complejo y estético, además que el uso de ciertas figuras literarias como la rima, la anáfora, la metáfora y otras pueden ser observadas en estas frases.\n\nLas características del corpus 8KF se muestran en la Tabla TABREF6. Este corpus fue utilizado principalmente en los dos modelos generativos: modelo basado en cadenas de Markov (Sección SECREF13) y modelo basado en la generación de Texto enlatado (Canned Text, Sección SECREF15). \n Question: What datasets are used?",
            "output": [
                "Corpus 5KL Corpus 8KF"
            ]
        },
        {
            "id": "task460-d88bc97f52df490f96f66c4fefec25f3",
            "input": "We extend the Amazon Conversational Bot Toolkit (CoBot) BIBREF6 which is a flexible event-driven framework. CoBot provides ASR results and natural language processing pipelines through the Alexa Skills Kit (ASK) BIBREF7. Gunrock corrects ASR according to the context (asr) and creates a natural language understanding (NLU) (nlu) module where multiple components analyze the user utterances. A dialog manager (DM) (dm) uses features from NLU to select topic dialog modules and defines an individual dialog flow. Each dialog module leverages several knowledge bases (knowledge). Then a natural language generation (NLG) (nlg) module generates a corresponding response. Finally, we markup the synthesized responses and return to the users through text to speech (TTS) (tts). \n Question: What the system designs introduced?",
            "output": [
                "Amazon Conversational Bot Toolkit natural language understanding (NLU) (nlu) module dialog manager knowledge bases natural language generation (NLG) (nlg) module text to speech (TTS) (tts)"
            ]
        },
        {
            "id": "task460-afc6694e017541d1af37066e9a43493d",
            "input": "We implement three baseline models for comparison: Noun WordNet Semantic Text Exchange Model (NWN-STEM), General WordNet Semantic Text Exchange Model (GWN-STEM), and Word2Vec Semantic Text Exchange Model (W2V-STEM). \n Question: What are the baseline models mentioned in the paper?",
            "output": [
                "Noun WordNet Semantic Text Exchange Model (NWN-STEM) General WordNet Semantic Text Exchange Model (GWN-STEM) Word2Vec Semantic Text Exchange Model (W2V-STEM)"
            ]
        },
        {
            "id": "task460-49ca16732f14421bb40b5c61f33b737a",
            "input": "We assess the optimal number of topics that need to be specified for the STM analysis. We follow the recommendations of the original STM paper and focus on exclusivity and semantic coherence measures. Highly frequent words in a given topic that do not appear very often in other topics are viewed as making that topic exclusive.  Following BIBREF8 we generate a set of candidate models ranging between 3 and 50 topics. We then plot the exclusivity and semantic coherence (numbers closer to 0 indicate higher coherence), with a linear regression overlaid (Figure FIGREF3 ). Models above the regression line have a “better” exclusivity-semantic coherence trade off. We select the 16-topic model, which has the largest positive residual in the regression fit, and provides higher exclusivity at the same level of semantic coherence. The topic quality is usually evaluated by highest probability words, which is presented in Figure FIGREF4 . \n Question: How are the main international development topics that states raise identified?",
            "output": [
                " They focus on exclusivity and semantic coherence measures: Highly frequent words in a given topic that do not appear very often in other topics are viewed as making that topic exclusive. They select select the 16-topic model, which has the largest positive residual in the regression fit, and provides higher exclusivity at the same level of semantic coherence."
            ]
        },
        {
            "id": "task460-4cf1e2efd51e4fe7aa638c9a9b445ebe",
            "input": "holistic: The baseline model which maps the holistic image feature and LSTM-encoded question feature to a common space and perform element-wise multiplication between them.\r\n\r\nTraAtt: The traditional attention model, implementation of WTL model BIBREF9 using the same $3\\times 3$ regions in SalAtt model.\r\n\r\nRegAtt: The region attention model which employs our novel attention method, same as the SalAtt model but without region pre-selection.\r\n\r\nConAtt: The convolutional region pre-selection attention model which replaces the BiLSTM in SalAtt model with a weight-sharing linear mapping, implemented by a convolutional layer.\r\n\r\nBesides, we also compare our SalAtt model with the popular baseline models i.e. iBOWIMG BIBREF4 , VQA BIBREF1 , and the state-of-the-art attention-based models i.e. WTL BIBREF9 , NMN BIBREF21 , SAN BIBREF14 , AMA BIBREF33 , FDA BIBREF34 , D-NMN BIBREF35 , DMN+ BIBREF8 on two tasks of COCO-VQA. \n Question: To which previous papers does this work compare its results?",
            "output": [
                "holistic TraAtt RegAtt ConAtt ConAtt iBOWIMG  VQA VQA WTL  NMN  SAN  AMA  FDA  D-NMN DMN+"
            ]
        },
        {
            "id": "task460-aa156eefd34943b3a4cb46c5a70691bd",
            "input": "Our analysis shows that attention models traditional alignment in some cases more closely while it captures information beyond alignment in others. For instance, attention agrees with traditional alignments to a high degree in the case of nouns. However, it captures other information rather than only the translational equivalent in the case of verbs. One can see that attention loss varies substantially across different POS tags. In particular, we focus on the cases of NOUN and VERB which are the most frequent POS tags in the dataset. As shown, the attention of NOUN is the closest to alignments on average. But the average attention loss for VERB is almost two times larger than the loss for NOUN. \n Question: What useful information does attention capture?",
            "output": [
                "it captures other information rather than only the translational equivalent in the case of verbs"
            ]
        },
        {
            "id": "task460-05cd136479cc413da5505797ff3c1ae3",
            "input": "Our method is tested on Twitter datasets. This microblogging platform has been widely used to analyze discussions and polarization BIBREF12, BIBREF13, BIBREF14, BIBREF15, BIBREF2. It is a natural choice for these kind of problems, as it represents one of the main fora for public debate in online social media BIBREF15, it is a common destination for affiliative expressions BIBREF16 and is often used to report and read news about current events BIBREF17. An extra advantage of Twitter for this kind of studies is the availability of real-time data generated by millions of users. Other social media platforms offer similar data-sharing services, but few can match the amount of data and the accompanied documentation provided by Twitter. One last asset of Twitter for our work is given by retweets, whom typically indicate endorsement BIBREF18 and hence become a useful concept to model discussions as we can set “who is with who\". However, our method has a general approach and it could be used a priori in any social network. In this work we report excellent result tested on Twitter but in future work we are going to test it in other social networks. \n Question: What social media platform is observed?",
            "output": [
                "Twitter"
            ]
        },
        {
            "id": "task460-b4bafdb6dad14db6a02ec69e3b62fb5f",
            "input": " Last, we evaluate our approaches in 9 commonly used text classification datasets. We evaluate our methods on several commonly used datasets whose themes range from sentiment, web-page, science to medical and healthcare. \n Question: What NLP tasks do they consider?",
            "output": [
                "text classification for themes including sentiment, web-page, science, medical and healthcare"
            ]
        },
        {
            "id": "task460-5a224b58197140dfba93b4a52a8da3a6",
            "input": "In natural language processing (NLP), taking into account the morphological complexity inherent to each language could be important for improving or adapting the existing methods, since the amount of semantic and grammatical information encoded at the word level, may vary significantly from language to language. This information could be useful for linguistic analysis and for measuring the impact of different word form normalization tools depending of the language. \n Question: what is the practical application for this paper?",
            "output": [
                "Improve existing NLP methods. Improve linguistic analysis. Measure impact of word normalization tools."
            ]
        },
        {
            "id": "task460-499a9b64434e4ee5939504616b989fb7",
            "input": "In our profile verification process, we observed that most gang member profiles portray a context representative of gang culture. Some examples of these profile pictures are shown in Figure FIGREF32 , where the user holds or points weapons, is seen in a group fashion which displays a gangster culture, or is showing off graffiti, hand signs, tattoos and bulk cash. \n Question: What are the differences in the use of images between gang member and the rest of the Twitter population?",
            "output": [
                "user holds or points weapons, is seen in a group fashion which displays a gangster culture, or is showing off graffiti, hand signs, tattoos and bulk cash"
            ]
        },
        {
            "id": "task460-989a45694ec94444b16d211f919e0808",
            "input": "We leveraged an existing, annotated Twitter dataset that was constructed based on a hierarchical model of depression-related symptoms BIBREF12 , BIBREF13 . The dataset contains 9,473 annotations for 9,300 tweets. Each tweet is annotated as no evidence of depression (e.g., “Citizens fear an economic depression\") or evidence of depression (e.g., “depressed over disappointment\").  \n Question: Do they evaluate only on English datasets?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-0d11daf809884431ae5d4f9288cc455b",
            "input": "We then train models on WMT English-German (EnDe) without BT noise, and instead explicitly tag the synthetic data with a reserved token. We repeat these experiments with WMT English-Romanian (EnRo), where NoisedBT underperforms standard BT and TaggedBT improves over both techniques. We perform our experiments on WMT18 EnDe bitext, WMT16 EnRo bitext, and WMT15 EnFr bitext respectively. We use WMT Newscrawl for monolingual data (2007-2017 for De, 2016 for Ro, 2007-2013 for En, and 2007-2014 for Fr). For bitext, we filter out empty sentences and sentences longer than 250 subwords. We remove pairs whose whitespace-tokenized length ratio is greater than 2. This results in about 5.0M pairs for EnDe, and 0.6M pairs for EnRo. We do not filter the EnFr bitext, resulting in 41M sentence pairs. \n Question: What datasets was the method evaluated on?",
            "output": [
                "WMT18 EnDe bitext WMT16 EnRo bitext WMT15 EnFr bitext We perform our experiments on WMT18 EnDe bitext, WMT16 EnRo bitext, and WMT15 EnFr bitext respectively. We use WMT Newscrawl for monolingual data (2007-2017 for De, 2016 for Ro, 2007-2013 for En, and 2007-2014 for Fr). For bitext, we filter out empty sentences and sentences longer than 250 subwords. We remove pairs whose whitespace-tokenized length ratio is greater than 2. This results in about 5.0M pairs for EnDe, and 0.6M pairs for EnRo. We do not filter the EnFr bitext, resulting in 41M sentence pairs."
            ]
        },
        {
            "id": "task460-e890873ecf7a460b96f2c156517b343b",
            "input": "In phase 1 (also referred to as the “exploration” phase) the algorithm explores the state space through keeping track of previously visited states by maintaining an archive. During this phase, instead of resuming the exploration from scratch, the algorithm starts exploring from promising states in the archive to find high performing trajectories. \n Question: How is trajectory with how rewards extracted?",
            "output": [
                "explores the state space through keeping track of previously visited states by maintaining an archive"
            ]
        },
        {
            "id": "task460-a7ca074ad9584f449334f98ac8b5a5d8",
            "input": "We use two datasets, NELL-One and Wiki-One which are constructed by BIBREF11 . \n Question: What datasets are used to evaluate the approach?",
            "output": [
                "NELL-One, Wiki-One"
            ]
        },
        {
            "id": "task460-c352eb71868e4aea8a0fe2a64575fc35",
            "input": "We notice that the concatenation consistently outperforms the gated-attention mechanism for both training and testing instructions. We suspect that the gated-attention is useful in the scenarios where objects are described in terms of multiple attributes, but it has no to harming effect when it comes to the order connectors. \n Question: Why they conclude that the usage of Gated-Attention provides no competitive advantage against concatenation in this setting?",
            "output": [
                "concatenation consistently outperforms the gated-attention mechanism for both training and testing instructions"
            ]
        },
        {
            "id": "task460-47f9da60af3c4f5aa1461db8380d3ea8",
            "input": "Experiments were conducted using the AMI IHM meeting corpus BIBREF18 to evaluated the speech recognition performance of various language models.  \n Question: Which dataset do they use?",
            "output": [
                " AMI IHM meeting corpus"
            ]
        },
        {
            "id": "task460-261e8e968bb2414c85ff0b6c0a8c7cf0",
            "input": "In this work, we analyze a set of tweets related to a specific classical music radio channel, BBC Radio 3, interested in detecting two types of musical named entities, Contributor and Musical Work. \n Question: What language is the Twitter content in?",
            "output": [
                "English"
            ]
        },
        {
            "id": "task460-f3efbb00107b4ef88fda6ae3e7a61f32",
            "input": "As previous systems collect relevant data from knowledge bases after observing questions during evaluation BIBREF24 , BIBREF25 , we also explore using this option. Namely, we build a customized text corpus based on questions in commonsense reasoning tasks.  \n Question: Do they fine-tune their model on the end task?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-0c90e2106780499b9fba2df0fbc67734",
            "input": "We apply the Generative Pre-trained Transformer (GPT) BIBREF2 as our pre-trained language model.  \n Question: What pretrained LM is used?",
            "output": [
                "Generative Pre-trained Transformer (GPT)"
            ]
        },
        {
            "id": "task460-35133bb80b1d4df5a63e9bb93f93b5f3",
            "input": "Compared to the in-context-parsing-based system, the combination with exact string matching yields a gain in recall of over 6%, and the combination with inflectional string matching yields an even bigger gain of almost 8%, at precision losses of 0.6% and 0.8%, respectively. \n Question: What compleentary PIE extraction methods are used to increase reliability further?",
            "output": [
                "exact string matching inflectional string matching"
            ]
        },
        {
            "id": "task460-1806f02a595a4f16b65ef2469c355642",
            "input": " News Articles\nOur dataset source of news articles is described in BIBREF2. This dataset was built from two different sources, for the trusted news (real news) they sampled news articles from the English Gigaword corpus. For the false news, they collected articles from seven different unreliable news sites. Twitter\nFor this dataset, we rely on a list of several Twitter accounts for each type of false information from BIBREF6. This list was created based on public resources that annotated suspicious Twitter accounts. The authors in BIBREF6 have built a dataset by collecting tweets from these accounts and they made it available.  \n Question: What datasets did they use?",
            "output": [
                "News Articles Twitter"
            ]
        },
        {
            "id": "task460-0b653f77562244ca8577deb0f58c0c2f",
            "input": "BIBREF14 introduce EMPATHETICDIALOGUES dataset, a novel dataset containing 25k conversations include emotional contexts information to facilitate training and evaluating the textual conversational system. Then, work from BIBREF2 produce a dataset containing 1.5 million Twitter conversation, gathered by using Twitter API from customer care account of 62 brands across several industries. This dataset was used to build tone-aware customer care chatbot. Finally, BIBREF29 tried to enhance SEMAINE corpus BIBREF30 by using crowdsourcing scenario to obtain a human judgement for deciding which response that elicits positive emotion.  \n Question: What are the currently available datasets for EAC?",
            "output": [
                "EMPATHETICDIALOGUES dataset a dataset containing 1.5 million Twitter conversation, gathered by using Twitter API from customer care account of 62 brands across several industries SEMAINE corpus BIBREF30"
            ]
        },
        {
            "id": "task460-65dc68cfae3640f3a09d7d9141b32b49",
            "input": "The full dataset comprises both the sample mentioned above and the 819 pairs from our preliminary work, totalling 2677 pairs. We split the vSTS dataset into training, validation and test partitions sampling at random and preserving the overall score distributions. In total, we use 1338 pairs for training, 669 for validation, and the rest of the 670 pairs were used for the final testing. \n Question: How big is vSTS training data?",
            "output": [
                "1338 pairs for training"
            ]
        },
        {
            "id": "task460-566f623c795b491aa1c62d5c434bc8a9",
            "input": "Puns are a form of wordplay jokes in which one sign (e.g. a word or a phrase) suggests two or more meanings by exploiting polysemy, homonymy, or phonological similarity to another sign, for an intended humorous or rhetorical effect BIBREF3 . \n Question: What are puns?",
            "output": [
                "a form of wordplay jokes in which one sign (e.g. a word or a phrase) suggests two or more meanings by exploiting polysemy, homonymy, or phonological similarity to another sign, for an intended humorous or rhetorical effect"
            ]
        },
        {
            "id": "task460-4bc3f5be54da44b08545695e5511626d",
            "input": " Note that we adopt a parsimonious design principle in our modelling: both Centroid and Naïve Bayes are parameter-free models, $k$NN only depends on the choice of $k$, and KDE uses a single bandwidth parameter $h$. A Centroid model summarizes each set of seed words by its expected vector in embedding space, and classifies concepts into the class of closest expected embedding in Euclidean distance following a softmax rule; A Naïve Bayes model considers both mean and variance, under the assumption of independence among embedding dimensions, by fitting a normal distribution with mean vector and diagonal covariance matrix to the set of seed words of each class; \n Question: How does the parameter-free model work?",
            "output": [
                "A Centroid model summarizes each set of seed words by its expected vector in embedding space, and classifies concepts into the class of closest expected embedding in Euclidean distance following a softmax rule; A Naïve Bayes model considers both mean and variance, under the assumption of independence among embedding dimensions, by fitting a normal distribution with mean vector and diagonal covariance matrix to the set of seed words of each class;"
            ]
        },
        {
            "id": "task460-9cbc0555dc1742a7802fa7aa47723c23",
            "input": "We conducted the human evaluation using Amazon Mechanical Turk to assess subjective quality. We recruit master level workers (who have good prior approval rates) to perform a human comparison between generated responses from two systems (which are randomly sampled from comparison systems). The workers are required to judge each utterance from 1 (bad) to 3 (good) in terms of informativeness and naturalness. Informativeness indicates the extent to which generated utterance contains all the information specified in the dialog act. Naturalness denotes whether the utterance is as natural as a human does. To reduce judgement bias, we distribute each question to three different workers. Finally, we collected in total of 5800 judges. \n Question: What was the criteria for human evaluation?",
            "output": [
                "to judge each utterance from 1 (bad) to 3 (good) in terms of informativeness and naturalness"
            ]
        },
        {
            "id": "task460-8a7f702780b144d18a14711f1d5cd0a5",
            "input": "In other words, there exist a few directions in the embedding space which disproportionately explain the variance. It is clear that the visual features in the case of How2 dataset are much more dominated by the \"common\" dimensions, when compared to the Multi30k dataset. Further, this analysis is still at the sentence level, i.e. the visual features are much less discriminative among individual sentences, further aggravating the problem at the token level. This suggests that the existing visual features aren't sufficient enough to expect benefits from the visual modality in NMT, since they won't provide discriminativeness among the vocabulary elements at the token level during prediction. \n Question: What is result of their Principal Component Analysis?",
            "output": [
                "existing visual features aren't sufficient enough to expect benefits from the visual modality in NMT"
            ]
        },
        {
            "id": "task460-ef124cd4478d4ecab1c96ca5c37290e4",
            "input": "In this section, we consider two kinds of relational classification tasks: (1) relation prediction and (2) relation extraction.  We hope to design a simple and clear experiment setup to conduct error analysis for relational prediction. Therefore, we consider a typical method TransE BIBREF3 as the subject as well as FB15K BIBREF3 as the dataset. For relation extraction, we consider the supervised relation extraction setting and TACRED dataset BIBREF10 . As for the subject model, we use the best model on TACRED dataset — position-aware neural sequence model.  \n Question: Which competitive relational classification models do they test?",
            "output": [
                "For relation prediction they test TransE and for relation extraction they test position aware neural sequence model"
            ]
        },
        {
            "id": "task460-ef7c18b16eb64aea98f77cc14419cb3e",
            "input": "Second is our first random seed selected for the classifier which produces a 0.8083 result. While better than the NBSVM solution, we pick the best validation F1 from the 20 seeds we tried. This produced our final submission of 0.8099. Our best model achieved an five-fold average F1 of 0.8254 on the validation set shown in Figure FIGREF27 but a test set F1 of 0.8099 - a drop of 0.0155 in F1 for the true out-of-sample data. \n Question: What were their results on the classification and regression tasks",
            "output": [
                "F1 of 0.8099"
            ]
        },
        {
            "id": "task460-686db312c9844ecea203335daa2de761",
            "input": "For this reason as a baseline algorithm for English dataset we refer to results from BIBREF0, and as for Russian dataset, we used the probabilistic language model, described in BIBREF8. \n Question: Is the RNN model evaluated against any baseline?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-15e4b7a3e4b94f5785e6385ad001607f",
            "input": "For POS tagging, we observe error reductions of respectively 0.71% for GSD, 0.81% for Sequoia, 0.7% for Spoken and 0.28% for ParTUT. For parsing, we observe error reductions in LAS of 2.96% for GSD, 3.33% for Sequoia, 1.70% for Spoken and 1.65% for ParTUT. On the XNLI benchmark, CamemBERT obtains improved performance over multilingual language models on the TRANSLATE-TRAIN setting (81.2 vs. 80.2 for XLM) while using less than half the parameters (110M vs. 250M). However, its performance still lags behind models trained on the original English training set in the TRANSLATE-TEST setting, 81.2 vs. 82.91 for RoBERTa. Both improvements result in a 2.36 point increase in the F1 score with respect to the best SEM architecture (BiLSTM-CRF), giving CamemBERT the state of the art for NER on the FTB. \n Question: How much better was results of CamemBERT than previous results on these tasks?",
            "output": [
                "2.36 point increase in the F1 score with respect to the best SEM architecture on the TRANSLATE-TRAIN setting (81.2 vs. 80.2 for XLM) lags behind models trained on the original English training set in the TRANSLATE-TEST setting, 81.2 vs. 82.91 for RoBERTa For POS tagging, we observe error reductions of respectively 0.71% for GSD, 0.81% for Sequoia, 0.7% for Spoken and 0.28% for ParTUT For parsing, we observe error reductions in LAS of 2.96% for GSD, 3.33% for Sequoia, 1.70% for Spoken and 1.65% for ParTUT"
            ]
        },
        {
            "id": "task460-640e0f8842944c2ba22db351f99ba1f1",
            "input": "A prototypical implementation, in which the words are assumed to be in the fundamental representation of the special orthogonal group, INLINEFORM0 , and are conditioned on losses sensitive to the relative actions of words, is the subject of another manuscript presently in preparation. \n Question: Is there a formal proof that the RNNs form a representation of the group?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-771ffac3ee87452d89a7dd874e878d89",
            "input": "he first dataset (WIKI) was the same set of 200 sentences from Wikipedia used in BIBREF7 .  The second dataset (SCI) was a set of 220 sentences from the scientific literature. We sourced the sentences from the OA-STM corpus. In total 2247 triples were extracted. In total, 11262 judgements were obtained after running the annotation process. \n Question: What is the size of the released dataset?",
            "output": [
                "440 sentences, 2247 triples extracted from those sentences, and 11262 judgements on those triples."
            ]
        },
        {
            "id": "task460-a2de522fac574937a63320e6c3a5f953",
            "input": "We use the Ultrax Typically Developing dataset (UXTD) from the publicly available UltraSuite repository BIBREF19 . This dataset contains synchronized acoustic and ultrasound data from 58 typically developing children, aged 5-12 years old (31 female, 27 male). The data was aligned at the phone-level, according to the methods described in BIBREF19 , BIBREF25 . The data was recorded using an Ultrasonix SonixRP machine using Articulate Assistant Advanced (AAA) software at INLINEFORM0 121fps with a 135 field of view. A single ultrasound frame consists of 412 echo returns from each of the 63 scan lines (63x412 raw frames). \n Question: What are the characteristics of the dataset?",
            "output": [
                "synchronized acoustic and ultrasound data from 58 typically developing children, aged 5-12 years old (31 female, 27 male) data was aligned at the phone-level 121fps with a 135 field of view single ultrasound frame consists of 412 echo returns from each of the 63 scan lines (63x412 raw frames)"
            ]
        },
        {
            "id": "task460-03361e46bcb14f97831d36e2a618273e",
            "input": "Instead, our analysis reveals that this behavior is caused by subtle artifacts arising from the translation process itself. In particular, we show that translating different parts of each instance separately (e.g. the premise and the hypothesis in NLI) can alter superficial patterns in the data (e.g. the degree of lexical overlap between them), which severely affects the generalization ability of current models. Based on the gained insights, we improve the state-of-the-art in XNLI, and show that some previous findings need to be reconsidered in the light of this phenomenon. The benchmark consists of a competence test, which evaluates the ability to understand antonymy relation and perform numerical reasoning, a distraction test, which evaluates the robustness to shallow patterns like lexical overlap and the presence of negation words, and a noise test, which evaluates robustness to spelling errors. Just as with previous experiments, we report results for the best epoch checkpoint in each test set. \n Question: What are examples of these artificats?",
            "output": [
                "the degree of lexical overlap between them presence of negation words"
            ]
        },
        {
            "id": "task460-ee213be813c5425693ce181e10df7b2f",
            "input": "Training was accomplished using the Kaldi Babel receipe using 198 hours of data in 10 languages (Bulgarian, Czech, French, German, Korean, Polish, Portuguese, Russian, Thai, Vietnamese) from GlobalPhone. \n Question: What languages are considered?",
            "output": [
                "Bulgarian, Czech, French, German, Korean, Polish, Portuguese, Russian, Thai, Vietnamese"
            ]
        },
        {
            "id": "task460-3f5825c60aa748aabd7603b01aad231d",
            "input": "EmotionLines BIBREF6 is a dialogue dataset composed of two subsets, Friends and EmotionPush, according to the source of the dialogues. \n Question: what datasets were used?",
            "output": [
                "Friends EmotionPush"
            ]
        },
        {
            "id": "task460-f09d948538b34ddc8d4b3d4bdbba2e40",
            "input": "We adopt several standard metrics widely used in existing works to measure the performance of dialogue generation models, including BLEU BIBREF24, embedding-based metrics (Average, Extrema, Greedy and Coherence) BIBREF25, BIBREF26, entropy-based metrics (Ent-{1,2}) BIBREF0 and distinct metrics (Dist-{1,2,3} and Intra-{1,2,3}) BIBREF1, BIBREF6. \n Question: What automatic evaluation metrics are used?",
            "output": [
                "BLEU embedding-based metrics (Average, Extrema, Greedy and Coherence) , entropy-based metrics (Ent-{1,2}) distinct metrics (Dist-{1,2,3} and Intra-{1,2,3})"
            ]
        },
        {
            "id": "task460-b23a6f232e1544bf8cd25371cd318d3b",
            "input": "Baseline Experiments\nIn this section, we briefly describe a baseline and evaluation scripts that we release, with a detailed documentation, along with the corpus. \n Question: What baseline system is proposed?",
            "output": [
                "Answer with content missing: (Baseline Method section) We implemented a simple approach inspired by previous work on concept map generation and keyphrase extraction."
            ]
        },
        {
            "id": "task460-36400a836cac4802888fdc356a4bbc03",
            "input": "What is the impact of pre-trained representations with less transcribed data? In order to get a better understanding of this, we train acoustic models with different amounts of labeled training data and measure accuracy with and without pre-trained representations (log-mel filterbanks). The pre-trained representations are trained on the full Librispeech corpus and we measure accuracy in terms of WER when decoding with a 4-gram language model. Figure shows that pre-training reduces WER by 32% on nov93dev when only about eight hours of transcribed data is available. Pre-training only on the audio data of WSJ ( WSJ) performs worse compared to the much larger Librispeech ( Libri). This further confirms that pre-training on more data is crucial to good performance. \n Question: Do they explore how much traning data is needed for which magnitude of improvement for WER? ",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-ec85e31f02984711be29d859a588ca92",
            "input": "For each task, the training data that was made available by the organizers is used, which is a selection of tweets with for each tweet a label describing the intensity of the emotion or sentiment BIBREF1 .  Since the present study focuses on Spanish tweets, all tweets from the English datasets were translated into Spanish. This new set of “Spanish” data was then added to our original training set. Again, the machine translation platform Apertium BIBREF5 was used for the translation of the datasets. \n Question: What dataset did they use?",
            "output": [
                " Selection of tweets with for each tweet a label describing the intensity of the emotion or sentiment provided by organizers and  tweets translated form English to Spanish."
            ]
        },
        {
            "id": "task460-cbb2774bf0d64756943f8d366892e4fc",
            "input": "First, we simply calculate the average unique predictions produced by both INLINEFORM0 and INLINEFORM1 in experiments shown in Section SECREF36 . \n Question: What two metrics are proposed?",
            "output": [
                "average unique predictions randomly sample 2000 decoder hidden states at INLINEFORM4 steps following a delimiter ( INLINEFORM5 ) and apply an unsupervised clustering method (t-SNE BIBREF35 )"
            ]
        },
        {
            "id": "task460-c57b02e466ac4480b2d2476b04147679",
            "input": "In this paper, we describe and evaluate Nefnir BIBREF0 , a new open source lemmatizer for Icelandic. Nefnir uses suffix substitution rules derived (learned) from the Database of Modern Icelandic Inflection (DMII) BIBREF1 , which contains over 5.8 million inflectional forms. \n Question: How are the substitution rules built?",
            "output": [
                "from the Database of Modern Icelandic Inflection (DMII) BIBREF1"
            ]
        },
        {
            "id": "task460-8ecb20d327cc4c5aa915a036a5787ca0",
            "input": "To establish a baseline, we consider a random method that predicts the positive label with 0.15 probability (the base rate of positive instances). Overall performance (Figure FIGREF28). Although our word-level task is heavily imbalanced, all of our models outperform the random baseline by a wide margin. As expected, content words are much more difficult to predict than stopwords, but the best F1 score in content words more than doubles that of the random baseline (0.286 vs. 0.116). Notably, although we strongly improve on our random baseline, even our best F1 scores are relatively low, and this holds true regardless of the model used. Despite involving more tokens than standard tagging tasks (e.g., BIBREF41 and BIBREF42), predicting whether a word is going to be echoed in explanations remains a challenging problem. \n Question: What are overall baseline results on new this new task?",
            "output": [
                "all of our models outperform the random baseline by a wide margin he best F1 score in content words more than doubles that of the random baseline (0.286 vs. 0.116)"
            ]
        },
        {
            "id": "task460-0318cb7fb67e48fa9d3a0eec40d32ee3",
            "input": " We use hyperopt library to select the hyper-parameters on the following values: LSTM layer size (16, 32, 64), dropout ($0.0-0.9$), activation function ($relu$, $selu$, $tanh$), optimizer ($sgd$, $adam$, $rmsprop$) with varying the value of the learning rate (1e-1,..,-5), and batch size (4, 8, 16) \n Question: What activation function do they use in their model?",
            "output": [
                "relu selu tanh"
            ]
        },
        {
            "id": "task460-8950cfb098bc455fb1b08ecacea8760f",
            "input": "Given a pair of ultrasound and audio segments we can calculate the distance between them using our model. To predict the synchronisation offset for an utterance, we consider a discretised set of candidate offsets, calculate the average distance for each across utterance segments, and select the one with the minimum average distance.  \n Question: Does their neural network predict a single offset in a recording?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-bcc42e14dc94410bafd3ebdd17a5e9c3",
            "input": "The dataset at HASOC 2019 were given in three languages: Hindi, English, and German. Dataset in Hindi and English had three subtasks each, while German had only two subtasks. We participated in all the tasks provided by the organisers and decided to develop a single model that would be language agnostic. We used the same model architecture for all the three languages. The training dataset of Hindi dataset was more balanced than English or German dataset. Hence, the results were around 0.78. As the dataset in German language was highly imbalanced, the results drops to 0.62. \n Question: What are the languages used to test the model?",
            "output": [
                "Hindi, English and German (German task won)"
            ]
        },
        {
            "id": "task460-07ede71d330446beaffa224f1f9c9d73",
            "input": "Our data set contains $3,209$ reviews about 553 different cars from 49 different car manufacturers. \n Question: How big is dataset of car-speak language?",
            "output": [
                "$3,209$ reviews "
            ]
        },
        {
            "id": "task460-be18b85e919e4e85bac8c5c9643964f9",
            "input": "Therefore, we implement reinforcement learning and elaborately design two rewards to describe the irony accuracy and sentiment preservation, respectively. \n Question: What is the combination of rewards for reinforcement learning?",
            "output": [
                "irony accuracy sentiment preservation"
            ]
        },
        {
            "id": "task460-0c84f4d9c1f741e3b4855a56661aba4d",
            "input": "In this research, we briefly discuss the steps of QA system and compare the performance of seven machine learning based classifiers (Multi-Layer Perceptron (MLP), Naive Bayes Classifier (NBC), Support Vector Machine (SVM), Gradient Boosting Classifier (GBC), Stochastic Gradient Descent (SGD), K Nearest Neighbour (K-NN) and Random Forest (RF)) in classifying Bengali questions to classes based on their anticipated answers. \n Question: what ml based approaches were compared?",
            "output": [
                "Multi-Layer Perceptron (MLP), Naive Bayes Classifier (NBC), Support Vector Machine (SVM), Gradient Boosting Classifier (GBC), Stochastic Gradient Descent (SGD), K Nearest Neighbour (K-NN) and Random Forest (RF)"
            ]
        },
        {
            "id": "task460-ede3e67b96744d5e8c4797de348d0996",
            "input": "To achieve this, a method is proposed that transforms the goal-labels of the used dataset (DSTC2) into labels whose behaviour can be replicated during deployment. \n Question: what corpus is used to learn behavior?",
            "output": [
                "DSTC2"
            ]
        },
        {
            "id": "task460-c9a61c463f9e471fb0f8c2901ad29acc",
            "input": "The regressor and classifier used Support Vector Regression (SVR) and Support Vector Classification (SVC) respectively. The bottom section of Table TABREF26 shows the results of several variants of the neural architecture. The table includes a neural regressor (NNR) and a neural classifier (NNC).  \n Question: What approaches without reinforcement learning have been tried?",
            "output": [
                "classification, regression, neural methods"
            ]
        },
        {
            "id": "task460-61737f7929a74ff08384f6b8fa7f7baf",
            "input": "For the overlapped speech recognition problem, the conditional independence assumption in the output label streams is still made as in Equation ( 5 ). Then the cross-entropy based PIT can be transformed to sequence discriminative criterion based PIT as below,\n\n$$\\begin{split} \\mathcal {J}_{\\text{SEQ-PIT}}=\\sum _u \\min _{s^{\\prime }\\in \\mathbf {S}} \\frac{1}{N} \\sum _{n\\in [1,N]}-\\mathcal {J}_{\\text{SEQ}}(\\mathbf {L}_{un}^{(s^{\\prime })},\\mathbf {L}_{un}^{(r)}) \\end{split}$$ (Eq. 44)\n\nDifferent from Equation ( 7 ), the best permutation is decided by $\\mathcal {J}_{\\text{SEQ}}(\\mathbf {L}_{un}^{(s^{\\prime })},\\mathbf {L}_{un}^{(r)})$ , which is the sequence discriminative criterion of taking the $s^{\\prime }$ -th permutation in $n$ -th output inference stream at utterance $u$ . Similar to CE-PIT, $\\mathcal {J}_{\\text{SEQ}}$ of all the permutations are calculated and the minimum permutation is taken to do the optimization. \n Question: How is the discriminative training formulation different from the standard ones?",
            "output": [
                "the best permutation is decided by $\\mathcal {J}_{\\text{SEQ}}(\\mathbf {L}_{un}^{(s^{\\prime })},\\mathbf {L}_{un}^{(r)})$ , which is the sequence discriminative criterion of taking the $s^{\\prime }$ -th permutation in $n$ -th output inference stream at utterance $u$"
            ]
        },
        {
            "id": "task460-188ab5e07457477ca308d20ad05e67b6",
            "input": "We calculated inter-annotator agreement by Krippendorff's Alpha BIBREF23, which accounts for different annotator pairs and empty values. To also zoom in on the particular agreement by category, we calculated mutual F-scores for each of the categories. This metric is typically used to evaluate system performance by category on gold standard data, but could also be applied to annotation pairs by alternating the roles of the two annotators between classifier and ground truth. While both the Relevance and Subject categorizations are annotated at a percent agreement of $0.71$ and $0.70$, their agreement scores are only fair, at $\\alpha =0.27$ and $\\alpha =0.29$. The percent agreement on Stance and Sentiment, which carry more categories than the former two, is $0.54$ for both. Their agreement scores are also fair, at $\\alpha =0.35$ and $\\alpha =0.34$. The mutual F-scores show marked differences in agreement by category, where the categories that were annotated most often typically yield a higher score. This holds for the Relevant category ($0.81$), the Vaccine category ($0.79$) and the Positive category ($0.64$). The Negative category yields a mutual F-score of $0.42$, which is higher than the more frequently annotated categories Neutral ($0.23$) and Not clear ($0.31$). We found that these categories are often confused. After combining the annotations of the two, the stance agreement would be increased to $\\alpha =0.43$. \n Question: What is the agreement score of their annotated dataset?",
            "output": [
                " Relevance and Subject categorizations are annotated at a percent agreement of $0.71$ and $0.70$, their agreement scores are only fair, at $\\alpha =0.27$ and $\\alpha =0.29$ Stance and Sentiment, which carry more categories than the former two, is $0.54$ for both. Their agreement scores are also fair, at $\\alpha =0.35$ and $\\alpha =0.34$ This holds for the Relevant category ($0.81$), the Vaccine category ($0.79$) and the Positive category ($0.64$)  The Negative category yields a mutual F-score of $0.42$, which is higher than the more frequently annotated categories Neutral ($0.23$) and Not clear ($0.31$)."
            ]
        },
        {
            "id": "task460-6e79853bbea44eefbf93f46babb057ee",
            "input": "During inference, we adopt beam search with a beam size of 5 for De-En, En-Fr and En-Vi translation tasks. \n Question: What are three main machine translation tasks?",
            "output": [
                "De-En, En-Fr and En-Vi translation tasks"
            ]
        },
        {
            "id": "task460-556a083cc8d143b9bec06e6b7832210b",
            "input": "We attribute the significant performance gain of our model over the baseline to a combination of several factors as described below: \n Question: Do they train a different training method except from scheduled sampling?",
            "output": [
                "Answer with content missing: (list missing) \nScheduled sampling: In our experiments, we found that models trained with scheduled sampling performed better (about 0.004 BLEU-4 on validation set) than the ones trained using teacher-forcing for the AVSD dataset. Hence, we use scheduled sampling for all the results we report in this paper.\n\nYes."
            ]
        },
        {
            "id": "task460-74beeb3bb0704703bd0f7952e758a74f",
            "input": "As a result, transfer training with only 1000 hours data can match equivalent performance for full training with 7300 hours data. \n Question: how small of a dataset did they train on?",
            "output": [
                "1000 hours data"
            ]
        },
        {
            "id": "task460-9c59fac072fb4f3c9ae59e396fbda607",
            "input": "BIBREF4 contains a variety of news-related information such as images, captions, geo-location information and comments which could be used as a proxy for article popularity. The articles in this dataset were collected between January and December 2014. Although we tried to retrieve the entire dataset, we were able to download only 38,182 articles due to the dead links published in the dataset. The retrieved articles were published in main news channels, such as Yahoo News, The Guardian or The Washington Post. Similarly, to The NowThisNews dataset we normalize the data by grouping articles per publisher, and classifying them as popular, when the number of comments exceeds the median value for given publisher. \n Question: What is the source of the news articles?",
            "output": [
                "main news channels, such as Yahoo News, The Guardian or The Washington Post"
            ]
        },
        {
            "id": "task460-f3f5fbad28ad43ee957f7e00939ae83a",
            "input": "We focus on three main issues, including, how to incorporate affective information into chatbots, what are resources that available and can be used to build EAC, and how to evaluate EAC performance. \n Question: What are the research questions posed in the paper regarding EAC studies?",
            "output": [
                "how to incorporate affective information into chatbots, what are resources that available and can be used to build EAC, and how to evaluate EAC performance"
            ]
        },
        {
            "id": "task460-4af010a7e8ec43dabd3e6f600df94274",
            "input": "Also, we compared our models with other existing works on this dataset including OpATT BIBREF6 and Neural Content Planning with conditional copy (NCP+CC) BIBREF4. \n Question: What is the state-of-the-art model for the task?",
            "output": [
                "OpATT BIBREF6 Neural Content Planning with conditional copy (NCP+CC) BIBREF4"
            ]
        },
        {
            "id": "task460-69a0d202d380476c94c3a445dd29c16a",
            "input": "We aim to find such content in the social media focusing on the tweets. \n Question: Does the dataset contain content from various social media platforms?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-89c306d57006416e969e70ae95ab1111",
            "input": "We compare our model with other strong competitive methods on the SQuAD leaderboard and TriviaQA leaderboard. \n Question: What other solutions do they compare to?",
            "output": [
                " strong competitive methods on the SQuAD leaderboard and TriviaQA leaderboard"
            ]
        },
        {
            "id": "task460-50a750ae86f74b918119e91b47c09676",
            "input": "While each of the city descriptions is relatively short, Calvino's writing is filled with rare words, complex syntactic structures, and figurative language. Capturing the essential components of each city in a single vector is thus not as simple as it is with more standard forms of text. Nevertheless, we hope that representations from language models trained over billions of words of text can extract some meaningful semantics from these descriptions. We experiment with three different pretrained representations: ELMo BIBREF5 , BERT BIBREF6 , and GloVe BIBREF18 . To produce a single city embedding, we compute the TF-IDF weighted element-wise mean of the token-level representations. For all pretrained methods, we additionally reduce the dimensionality of the city embeddings to 40 using PCA for increased compatibility with our clustering algorithm. \n Question: How do they model a city description using embeddings?",
            "output": [
                "We experiment with three different pretrained representations: ELMo BIBREF5 , BERT BIBREF6 , and GloVe BIBREF18 . To produce a single city embedding, we compute the TF-IDF weighted element-wise mean of the token-level representations. For all pretrained methods, we additionally reduce the dimensionality of the city embeddings to 40 using PCA for increased compatibility with our clustering algorithm."
            ]
        },
        {
            "id": "task460-3e54750958cd430d8ad0f5427df43e7e",
            "input": "Table TABREF14 presents the distribution of the tweets by country before and after the filtering process. A large portion of the samples is from India because the MeToo movement has peaked towards the end of 2018 in India. There are very few samples from Russia likely because of content moderation and regulations on social media usage in the country. \n Question: Do the tweets come from a specific region?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-253efd7be8d2452bb6b05923b48ab15b",
            "input": "As of January 2020, the latest CoVo 2019-06-12 release includes 29 languages. CoVoST is currently built on that release and covers the following 11 languages: French, German, Dutch, Russian, Spanish, Italian, Turkish, Persian, Swedish, Mongolian and Chinese. Common Voice BIBREF10 is a crowdsourcing speech recognition corpus with an open CC0 license. Contributors record voice clips by reading from a bank of donated sentences \n Question: How was the dataset collected?",
            "output": [
                "Contributors record voice clips by reading from a bank of donated sentences."
            ]
        },
        {
            "id": "task460-486d81e2fdfc4b388dfcbdb81cfbb489",
            "input": "We provide three simple baselines for the RUN task: (1) NO-MOVE: the only position considered is the starting point; (2) RANDOM: As in BIBREF4, turn to a randomly selected heading, then execute a number of *WALK actions of an average route; (3) JUMP: at each sentence, extract entities from the map and move between them in the order they appear.  \n Question: What is the baseline?",
            "output": [
                "NO-MOVE RANDOM JUMP"
            ]
        },
        {
            "id": "task460-56ad419a970c4a599b3899de71c529ed",
            "input": "This dataset is aimed to contribute to developing a partisan news detector. There are several ways that the dataset can be used to devise the system. For example, it is possible to train the detector using publisher-level labels and test with article-level labels. It is also possible to use semi-supervised learning and treat the publisher-level part as unsupervised, or use only the article-level part. We also released the raw survey data so that new mechanisms to decide the article-level labels can be devised. \n Question: What examples of applications are mentioned?",
            "output": [
                "partisan news detector"
            ]
        },
        {
            "id": "task460-0674b36429d44a32bb5f2314c8559515",
            "input": "The first data batch consists of tweets relevant to blizzards, hurricanes, and wildfires, under the constraint that they are tweeted by “influential\" tweeters, who we define as individuals certain to have a classifiable sentiment regarding the topic at hand. For example, we assume that any tweet composed by Al Gore regarding climate change is a positive sample, whereas any tweet from conspiracy account @ClimateHiJinx is a negative sample. The assumption we make in ensuing methods (confirmed as reasonable in Section SECREF2 ) is that influential tweeters can be used to label tweets in bulk in the absence of manually-labeled tweets.  \n Question: What methodology is used to compensate for limited labelled data?",
            "output": [
                "Influential tweeters ( who they define as individuals certain to have a classifiable sentiment regarding the topic at hand) is used to label tweets in bulk in the absence of manually-labeled tweets."
            ]
        },
        {
            "id": "task460-681a771bb5294d6fa026833d4f4dcedd",
            "input": "we use a common sense definition of racist language, including all negative utterances, negative generalizations and insults concerning ethnicity, nationality, religion and culture. \n Question: how did they ask if a tweet was racist?",
            "output": [
                "if it includes  negative utterances, negative generalizations and insults concerning ethnicity, nationality, religion and culture."
            ]
        },
        {
            "id": "task460-c60dc056618d4fe0b583e477f6dee142",
            "input": "Across all languages, 145 human annotators were asked to score all 1,888 pairs (in their given language). We finally collect at least ten valid annotations for each word pair in each language. All annotators were required to abide by the following instructions:\n\n1. Each annotator must assign an integer score between 0 and 6 (inclusive) indicating how semantically similar the two words in a given pair are. A score of 6 indicates very high similarity (i.e., perfect synonymy), while zero indicates no similarity.\n\n2. Each annotator must score the entire set of 1,888 pairs in the dataset. The pairs must not be shared between different annotators.\n\n3. Annotators are able to break the workload over a period of approximately 2-3 weeks, and are able to use external sources (e.g. dictionaries, thesauri, WordNet) if required.\n\n4. Annotators are kept anonymous, and are not able to communicate with each other during the annotation process. \n Question: How were the datasets annotated?",
            "output": [
                "1. Each annotator must assign an integer score between 0 and 6 (inclusive) indicating how semantically similar the two words in a given pair are. A score of 6 indicates very high similarity (i.e., perfect synonymy), while zero indicates no similarity. 2. Each annotator must score the entire set of 1,888 pairs in the dataset.  able to use external sources (e.g. dictionaries, thesauri, WordNet) if required not able to communicate with each other during the annotation process"
            ]
        },
        {
            "id": "task460-3ec5134de91e45f9973080a749a3ae3b",
            "input": "We use Waseem and Hovy BIBREF5, Davidson et al. BIBREF9, and Waseem et al. BIBREF10 as baselines and compare the results with our different fine-tuning strategies using pre-trained BERTbase model.  \n Question: What existing approaches do they compare to?",
            "output": [
                "Waseem and Hovy BIBREF5, Davidson et al. BIBREF9, and Waseem et al. BIBREF10"
            ]
        },
        {
            "id": "task460-cea189cd395d4882bf1fe91a0b3bb5fd",
            "input": "Suppose there are $M$ total layers, and define a subset of these layers at which to apply the loss function: $K = \\lbrace k_1, k_2, ..., k_L\\rbrace \\subseteq \\lbrace 1,..,M-1\\rbrace $. The total objective function is then defined as\n\nwhere $Z_{k_l}$ is the $k_l$-th Transformer layer activations, $Y$ is the ground-truth transcription for CTC and context dependent states for hybrid ASR, and $Loss(P, Y)$ can be defined as CTC objective BIBREF11 or cross entropy for hybrid ASR. T \n Question: Do they just sum up all the loses the calculate to end up with one single loss?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-4da9695a95af4c12984290b1717d533e",
            "input": "We first expand upon previous work and generic dialogue act taxonomies, developing a fine-grained set of dialogue acts for customer service, and conducting a systematic user study to identify these acts in a dataset of 800 conversations from four Twitter customer service accounts (i.e. four different companies in the telecommunication, electronics, and insurance industries). \n Question: Which Twitter customer service industries are investigated?",
            "output": [
                " four different companies in the telecommunication, electronics, and insurance industries"
            ]
        },
        {
            "id": "task460-2b7fada7496542afa7fa23159d5c458d",
            "input": "The dataset consists of 5000 reviews in bahasa Indonesia. \n Question: Does the dataset contain non-English reviews?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-d24e30ea6ecc4b3381c0f71b4c4579ef",
            "input": "Mathur et al. in their paper for detecting offensive tweets proposed a Ternary Trans-CNN model where they train a model architecture comprising of 3 layers of Convolution 1D having filter sizes of 15, 12 and 10 and kernel size of 3 followed by 2 dense fully connected layer of size 64 and 3. The first dense FC layer has ReLU activation while the last Dense layer had Softmax activation. They were able to train this network on a parallel English dataset provided by Davidson et al. The authors were able to achieve Accuracy of 83.9%, Precision of 80.2%, Recall of 69.8%.\n\nThe approach looked promising given that the dataset was merely 3189 sentences divided into three categories and thus we replicated the experiment but failed to replicate the results. The results were poor than what the original authors achieved. But, most of the model hyper-parameter choices where inspired from this work. \n Question: What is the previous work's model?",
            "output": [
                "Ternary Trans-CNN"
            ]
        },
        {
            "id": "task460-510579dfca9747b3968556d51d62c229",
            "input": "We find that the visual attention is very sparse, in that just one source encoding is attended to (the maximum visual attention over source encodings, across the test set, has mean 0.99 and standard deviation 0.015), thereby limiting the use of modulation. In both cases, we find that the visual component of the attention hasn't learnt any variation over the source encodings, again suggesting that the visual embeddings do not lend themselves to enhancing token-level discriminativess during prediction. We find this to be consistent across sentences of different lengths. \n Question: What is result of their attention distribution analysis?",
            "output": [
                "visual attention is very sparse  visual component of the attention hasn't learnt any variation over the source encodings"
            ]
        },
        {
            "id": "task460-fe05515b9b07456ca9e2eb2789b8a73e",
            "input": "We observe an abundance of adjective and adverb patterns for the sarcastic class, although we do not use adjective and adverb patterns in our regex retrieval method.  Many of our sarcastic questions focus specifically on attacks on the mental abilities of the addressee. This generalization is made clear when we extract and analyze the verb, subject, and object arguments using the Stanford dependency parser BIBREF32 for the questions in the RQ dataset.  One common pattern for hyperbole involves adverbs and adjectives, as noted above. We did not use this pattern to retrieve hyperbole, but because each hyperbolic sarcastic utterance contains multiple cues, we learn an expanded class of patterns for hyperbole.  We learn a number of verbal patterns that we had not previously associated with hyperbole, as shown in Table TABREF34 .  \n Question: What lexico-syntactic cues are used to retrieve sarcastic utterances?",
            "output": [
                "adjective and adverb patterns verb, subject, and object arguments verbal patterns"
            ]
        },
        {
            "id": "task460-8357ee9b28e8402f9dd625f0c25d27be",
            "input": "We develop a null model to determine how much variation in binomial orderings we might expect across communities and across time, if binomial orderings were randomly ordered according to global asymmetry values.  \n Question: What new model is proposed for binomial lists?",
            "output": [
                "null model "
            ]
        },
        {
            "id": "task460-bc875ac5c27b4754a709d040de0b8951",
            "input": "We propose the following challenge to the community: We must develop formal definition and evaluation for faithfulness that allows us the freedom to say when a method is sufficiently faithful to be useful in practice.\n\nWe note two possible approaches to this end:\n\nAcross models and tasks: The degree (as grayscale) of faithfulness at the level of specific models and tasks. Perhaps some models or tasks allow sufficiently faithful interpretation, even if the same is not true for others.\n\nFor example, the method may not be faithful for some question-answering task, but faithful for movie review sentiment, perhaps based on various syntactic and semantic attributes of those tasks.\n\nAcross input space: The degree of faithfulness at the level of subspaces of the input space, such as neighborhoods of similar inputs, or singular inputs themselves. If we are able to say with some degree of confidence whether a specific decision's explanation is faithful to the model, even if the interpretation method is not considered universally faithful, it can be used with respect to those specific areas or instances only. \n Question: What approaches they propose?",
            "output": [
                "Across models and tasks: The degree (as grayscale) of faithfulness at the level of specific models and tasks. Across input space: The degree of faithfulness at the level of subspaces of the input space, such as neighborhoods of similar inputs, or singular inputs themselves."
            ]
        },
        {
            "id": "task460-e946972efae84d189f42a6cc70fb1633",
            "input": "We present two use cases on which Seshat was developped: clinical interviews, and daylong child-centered recordings. \n Question: Did they experiment with the tool?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-3513ace8e1fe4d62a67dad175b76c098",
            "input": "Table 1 shows the official leaderboard on SQuAD test set when we submitted our system. Our model achieves a 68.73% EM score and 77.39% F1 score, which is ranked among the state of the art single models (without model ensembling). \n Question: What is the exact performance on SQUAD?",
            "output": [
                "Our model achieves a 68.73% EM score and 77.39% F1 score"
            ]
        },
        {
            "id": "task460-256de7cc8229410883094ce54533afa5",
            "input": "Given that words from texts of the same class belong to the same context, it is possible to model word vectors of each class as word subspaces and efficiently compare them in terms of similarity by using canonical angles between the word subspaces.  \n Question: What can word subspace represent?",
            "output": [
                "Word vectors, usually in the context of others within the same class"
            ]
        },
        {
            "id": "task460-49b41b6610b740afabb938f396f99d99",
            "input": "word embedding, input encoder, alignment, aggregation, and prediction \n Question: what is the architecture of the baseline model?",
            "output": [
                "word embedding, input encoder, alignment, aggregation, and prediction."
            ]
        },
        {
            "id": "task460-bf2aca26ed064043963e67e9ee43cb76",
            "input": "First we are comparing BiLSTMBIBREF14, BiLSTM+CNNBIBREF20, BiLSTM+CRFBIBREF1, BiLSTM+CNN+CRFBIBREF2 models with CNN modelBIBREF0 and Stanford CRF modelBIBREF21. \n Question: What is the baseline?",
            "output": [
                "CNN modelBIBREF0 Stanford CRF modelBIBREF21"
            ]
        },
        {
            "id": "task460-f18b30c69af24e038ba63e6b718c18ea",
            "input": "To build the classifiers we used three different learning algorithms, namely Logistic Regression (LR), Random Forest (RF), and Support Vector Machines (SVM). \n Question: Which supervised learning algorithms are used in the experiments?",
            "output": [
                "Logistic Regression (LR) Random Forest (RF) Support Vector Machines (SVM)"
            ]
        },
        {
            "id": "task460-f96d6115bbfe48f7ac1bc05e8bb22b78",
            "input": "We first use predictors based on rules that have previously been proposed in the literature: word length, number of phonemes, number of syllables, alphabetical order, and frequency. We collect all binomials but make predictions only on binomials appearing at least 30 times total, stratified by subreddit. However, none of these features appear to be particularly predictive across the board (Table TABREF15). A simple linear regression model predicts close to random, which bolsters the evidence that these classical rules for frozen binomials are not predictive for general binomials. \n Question: How was performance of previously proposed rules at very large scale?",
            "output": [
                " close to random,"
            ]
        },
        {
            "id": "task460-d6bd53fe2ffc423ea7dbbc5eb441b9f8",
            "input": "We evaluate the TN topic model quantitatively with standard topic model measures such as test-set perplexity, likelihood convergence and clustering measures. Qualitatively, we evaluate the model by visualizing the topic summaries, authors' topic distributions and by performing an automatic labeling task. \n Question: What are the measures of \"performance\" used in this paper?",
            "output": [
                "test-set perplexity, likelihood convergence and clustering measures visualizing the topic summaries, authors' topic distributions and by performing an automatic labeling task"
            ]
        },
        {
            "id": "task460-bab6bab93e424663b7adfbce101aa8b6",
            "input": "jiant provides support for cutting-edge sentence encoder models, including support for Huggingface's Transformers. Supported models include: BERT BIBREF17, RoBERTa BIBREF27, XLNet BIBREF28, XLM BIBREF29, GPT BIBREF30, GPT-2 BIBREF31, ALBERT BIBREF32 and ELMo BIBREF33. jiant also supports the from-scratch training of (bidirectional) LSTMs BIBREF34 and deep bag of words models BIBREF35, as well as syntax-aware models such as PRPN BIBREF36 and ON-LSTM BIBREF37. jiant also supports word embeddings such as GloVe BIBREF38. \n Question: Is jiant compatible with models in any programming language?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-0b7d70c860bc49b88ae3eeae5e4c3456",
            "input": "The first step is to pre-train the model on one MCQA dataset referred to as the source task, which usually contains abundant training data. The second step is to fine-tune the same model on the other MCQA dataset, which is referred to as the target task, that we actually care about, but that usually contains much less training data.  \n Question: How different is the dataset size of source and target?",
            "output": [
                "the training dataset is large while the target dataset is usually much smaller"
            ]
        },
        {
            "id": "task460-90e05ff1653e4982b064d893a32164be",
            "input": "We find that communities that are characterized by specialized, constantly-updating content have higher user retention rates, but also exhibit larger linguistic gaps that separate newcomers from established members. More closely examining factors that could contribute to this linguistic gap, we find that especially within distinctive communities, established users have an increased propensity to engage with the community's specialized content, compared to newcomers (Section SECREF5 ).  \n Question: What patterns do they observe about how user engagement varies with the characteristics of a community?",
            "output": [
                "communities that are characterized by specialized, constantly-updating content have higher user retention rates, but also exhibit larger linguistic gaps that separate newcomers from established members within distinctive communities, established users have an increased propensity to engage with the community's specialized content, compared to newcomers "
            ]
        },
        {
            "id": "task460-1156212ae7d1479a99d044e0b385e0ec",
            "input": "Furthermore, we employ two different parsers for comparison: 1) the PCFGLA-based parser, viz. Berkeley parser BIBREF5 , and 2) a minimal span-based neural parser BIBREF6 . \n Question: What is the baseline model for the agreement-based mode?",
            "output": [
                "PCFGLA-based parser, viz. Berkeley parser BIBREF5 minimal span-based neural parser BIBREF6"
            ]
        },
        {
            "id": "task460-2d907fd283ba4fa38f100ba4f3ae9b88",
            "input": "Table TABREF46 shows that our Open model achieves more than 3 points of f1-score than the state-of-the-art result, and RelAwe with DepPath&RelPath achieves the best in both Closed and Open settings. \n Question: How big is improvement over the old  state-of-the-art performance on CoNLL-2009 dataset?",
            "output": [
                "our Open model achieves more than 3 points of f1-score than the state-of-the-art result"
            ]
        },
        {
            "id": "task460-0ccf519fe66f4f1ca980a89ebf8b220a",
            "input": "E2E NLG challenge Dataset: The training set of the E2E challenge dataset which consists of 42K samples was partitioned into a 10K paired and 32K unpaired datasets by a random process.  The Wikipedia Company Dataset: The Wikipedia company dataset presented in Section SECREF18 was filtered to contain only companies having abstracts of at least 7 words and at most 105 words. The dataset was then divided into: a training set (35K), a development set (4.3K) and a test set (4.3K). The training set was also partitioned in order to obtain the paired and unpaired datasets.  \n Question: What non-annotated datasets are considered?",
            "output": [
                "E2E NLG challenge Dataset The Wikipedia Company Dataset"
            ]
        },
        {
            "id": "task460-2af7d5ebe44e489fb4575ff4b4be9f48",
            "input": "As the baseline model (BASE-4L) for IWSLT14 German-English and Turkish-English, we use a 4-layer encoder, 4-layer decoder, residual-connected model, with embedding and hidden size set as 256 by default. \n Question: what are the baselines?",
            "output": [
                " 4-layer encoder, 4-layer decoder, residual-connected model, with embedding and hidden size set as 256"
            ]
        },
        {
            "id": "task460-4c136936788b4531ad114cca88dfb20a",
            "input": "We have also performed preliminary experiments on the Arabic portion of the SemEval-2016 cQA task.  \n Question: Did they experimnet in other languages?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-cae7e9fe077f457f980af1c40b52c16a",
            "input": "To compare our neural models with the traditional approaches, we experimented with a number of existing models including: Support Vector Machine (SVM), a discriminative max-margin model; Logistic Regression (LR), a discriminative probabilistic model; and Random Forest (RF), an ensemble model of decision trees. \n Question: what was their baseline comparison?",
            "output": [
                "Support Vector Machine (SVM) Logistic Regression (LR) Random Forest (RF)"
            ]
        },
        {
            "id": "task460-3f2586c0347c41a3950615c7940a987b",
            "input": "CIC is a suitable complementary metric to ROUGE because it accounts for the most important information within each dialog domain. CIC can be applied to any summarization task with predefined essential entities. \n Question: Is new evaluation metric extension of ROGUE?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-e3d07a827314436382951f5001de761b",
            "input": "In total around 500 different workers were involved in the annotation. about 500 \n Question: How many people participated in their evaluation study of table-to-text models?",
            "output": [
                "about 500"
            ]
        },
        {
            "id": "task460-4462b6ee1a41439eac8f22548e3e63b3",
            "input": "We validate our approach on the Wikipedia toxic comments dataset BIBREF18 . \n Question: Which datasets do they use?",
            "output": [
                " Wikipedia toxic comments"
            ]
        },
        {
            "id": "task460-fe9889fa63994d22aef2efb89831c446",
            "input": "Meanwhile, attending colleges in the Northeast, West and South regions increases the possibility of posting about sexual harassment (positive coefficients), over the Midwest region.  \n Question: Which geographical regions correlate to the trend?",
            "output": [
                "Northeast U.S., West U.S. and South U.S."
            ]
        },
        {
            "id": "task460-d091f9ddd4c34d9b9203693db598cbd7",
            "input": "Baselines. We compare our approach (FacTweet) to the following set of baselines:\n\n[leftmargin=4mm]\n\nLR + Bag-of-words: We aggregate the tweets of a feed and we use a bag-of-words representation with a logistic regression (LR) classifier.\n\nTweet2vec: We use the Bidirectional Gated recurrent neural network model proposed in BIBREF20. We keep the default parameters that were provided with the implementation. To represent the tweets, we use the decoded embedding produced by the model. With this baseline we aim at assessing if the tweets' hashtags may help detecting the non-factual accounts.\n\nLR + All Features (tweet-level): We extract all our features from each tweet and feed them into a LR classifier. Here, we do not aggregate over tweets and thus view each tweet independently.\n\nLR + All Features (chunk-level): We concatenate the features' vectors of the tweets in a chunk and feed them into a LR classifier.\n\nFacTweet (tweet-level): Similar to the FacTweet approach, but at tweet-level; the sequential flow of the tweets is not utilized. We aim at investigating the importance of the sequential flow of tweets.\n\nTop-$k$ replies, likes, or re-tweets: Some approaches in rumors detection use the number of replies, likes, and re-tweets to detect rumors BIBREF21. Thus, we extract top $k$ replied, liked or re-tweeted tweets from each account to assess the accounts factuality. We tested different $k$ values between 10 tweets to the max number of tweets from each account. Figure FIGREF24 shows the macro-F1 values for different $k$ values. It seems that $k=500$ for the top replied tweets achieves the highest result. Therefore, we consider this as a baseline. \n Question: What baselines do they compare to?",
            "output": [
                "LR + Bag-of-words Tweet2vec LR + All Features (tweet-level) LR + All Features (chunk-level) FacTweet (tweet-level) Top-$k$ replies, likes, or re-tweets"
            ]
        },
        {
            "id": "task460-39b76d2f436b490aae638d52fec07ac5",
            "input": "We focus on the data shared by BIBREF3, and empirically test to what extent changes in the evaluation design affect the outcome of the human evaluation. In this study, we address three aspects that we consider to be particularly relevant for human evaluation of MT, with a special focus on testing human–machine parity: the choice of raters, the use of linguistic context, and the construction of reference translations. We empirically test and discuss the impact of these factors on human evaluation of MT in Sections SECREF3–SECREF5.  \n Question: What empricial investigations do they reference?",
            "output": [
                "empirically test to what extent changes in the evaluation design affect the outcome of the human evaluation"
            ]
        },
        {
            "id": "task460-15ac4819883f4e729252554dd5e5584f",
            "input": "The goal here being to assess if D2V could effectively replace the related-document function on PubMed, five different document similarity evaluations were designed as seen on figure FIGREF9.  Methods ::: Evaluation ::: String length\nTo assess whether a similar length could lead to convergence of two documents, the size of the query document $D_{x}$ has been compared with the top-close document $C_{x}$ for 10,000 document randomly selected from the TeS after some pre-processing steps (stopwords and spaces were removed from both documents). Methods ::: Evaluation ::: Words co-occurrences\nA matrix of words co-occurrence was constructed on the total corpus from PubMed. Briefly, each document was lowered and tokenized. A matrix was filled with the number of times that two words co-occur in a single document. Then, for 5,000 documents $D_{x}$ from the TeS, all models were queried for the top-close document $C_{x}$. All possible combinations between all words $WD_{x} \\in D_{x}$ and all words $WC_{x} \\in C_{x}$ (excluding stopwords) were extracted, 500 couples were randomly selected and the number of times each of them was co-occurring was extracted from the matrix. The average value of this list was calculated, reflecting the proximity between D and C regarding their words content. This score was also calculated between each $D_{x}$ and the top-close document $C_{x}$ returned by the pmra algorithm. Methods ::: Evaluation ::: Stems co-occurrences\nThe evaluation task explained above was also applied on 10,000 stemmed texts (using the Gensim’s PorterStemmer to only keep word’s roots). The influence of the conjugation form or other suffixes can be assessed. Methods ::: Evaluation ::: MeSH similarity\nIt is possible to compare the ability of both pmra and D2V to bring closer articles which were indexed with common labels. To do so, 5,000 documents $D_{x}$ randomly selected from the TeS were sent to both pmra and D2V architectures, and the top-five closer articles $C_{x}$ were extracted. The following rules were then applied to each MeSH found associated with $D_{x}$ for each document $C_{x_i}$ : add 1 to the score if this MeSH term is found in both $D_{x}$ and $C_{x_i}$, add 3 if this MeSH is defined as major topic and add 1 for each qualifier in common between $D_{x}$ and Cxi regarding this particular MeSH term. Then, the mean of these five scores was calculated for both pmra and D2V. \n Question: What four evaluation tasks are defined to determine what influences proximity?",
            "output": [
                "String length Words co-occurrences Stems co-occurrences MeSH similarity"
            ]
        },
        {
            "id": "task460-61a5666c491a43e39682dea0b7eeb36e",
            "input": "We map each relation type $R(x,y)$ to at least one parametrized natural-language question $q_x$ whose answer is $y$ . \n Question: How is the input triple translated to a slot-filling task?",
            "output": [
                "The relation R(x,y) is mapped onto a question q whose answer is y"
            ]
        },
        {
            "id": "task460-496b530b69cd4ca087067e33e8360425",
            "input": "Task: given a caption and a paired image (if used), the goal is to label every token in a caption in BIO scheme (B: beginning, I: inside, O: outside) BIBREF27 .  \n Question: Can named entities in SnapCaptions be discontigious?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-8ac803f4d3dc4480b90921d68e915dbb",
            "input": "System description ::: Architecture The components involved in the process are: QnAMaker Portal: This is the Graphical User Interface (GUI) for using QnAMaker.  QnaMaker Management APIs: This is used for the extraction of Question-Answer (QA) pairs from semi-structured content.  Azure Search Index: Stores the KB with questions and answers as indexable columns, thus acting as a retrieval layer. QnaMaker WebApp: Acts as a layer between the Bot, Management APIs, and Azure Search Index.  Bot: Calls the WebApp with the User's query to get results. \n Question: What components is the QnAMaker composed of?",
            "output": [
                "QnAMaker Portal QnaMaker Management APIs Azure Search Index QnaMaker WebApp Bot"
            ]
        },
        {
            "id": "task460-fb857ee9878f43c9ae13c1582399368d",
            "input": "We adopt a DNN-based acoustic model BIBREF0 with 11 hidden layers and the alignment used to train the model is derived from a HMM-GMM model trained with SAT criterion. \n Question: What are the deep learning architectures used in the task?",
            "output": [
                "DNN-based acoustic model BIBREF0"
            ]
        },
        {
            "id": "task460-d306d7e0215b4131a478c510646ef48c",
            "input": "Training Dataset UM Inventory BIBREF5 is a public dataset created by researchers from the University of Minnesota, containing about 37,500 training samples with 75 abbreviation terms. Existing work reports abbreviation disambiguation results on 50 abbreviation terms BIBREF6, BIBREF5, BIBREF17. However, after carefully reviewing this dataset, we found that it contains many samples where medical professionals disagree: wrong samples and uncategorized samples. Due to these mistakes and flaws of this dataset, we removed the erroneous samples and eventually selected 30 abbreviation terms as our training dataset that can be made public. \n Question: What existing dataset is re-examined and corrected for training?",
            "output": [
                " UM Inventory "
            ]
        },
        {
            "id": "task460-5189ac579bb94c8c95d94beab5c92df9",
            "input": "Although cursing is frequent in tweets, they represent just 1.15% of all words used BIBREF21 . In contrast, we found 5.72% of all words posted by gang member accounts to be classified as a curse word, which is nearly five times more than the average curse word usage on Twitter. The clouds also reflect the fact that gang members often talk about drugs and money with terms such as smoke, high, hit, and money, while ordinary users hardly speak about finances and drugs. We also noticed that gang members talk about material things with terms such as got, money, make, real, need whereas ordinary users tend to vocalize their feelings with terms such as new, like, love, know, want, look, make, us. \n Question: What are the differences in language use between gang member and the rest of the Twitter population?",
            "output": [
                "Although cursing is frequent in tweets, they represent just 1.15% of all words used BIBREF21 . In contrast, we found 5.72% of all words posted by gang member accounts to be classified as a curse word gang members talk about material things with terms such as got, money, make, real, need whereas ordinary users tend to vocalize their feelings with terms such as new, like, love, know, want, look, make, us"
            ]
        },
        {
            "id": "task460-e78a00ff9a8d4f14a833bd9ac6702c56",
            "input": "The dataset consists of more than $0.3$ million records and has been made available for future research. \n Question: What is the size of the dataset?",
            "output": [
                "$0.3$ million records"
            ]
        },
        {
            "id": "task460-8b5a9261fa7c4ff6ae8b9813c8ea7a86",
            "input": "We have shown that transformer-based encoders together with GRU models provide the best performance for question representation. Notably, we demonstrated that using pre-trained text representations provide consistent performance improvements across several hyper-parameter configurations. We have also shown that using an object detector fine-tuned with external data provides large improvements in accuracy. Moreover, we have shown that attention mechanisms are paramount for learning top performing networks, once they allow producing question-aware image representations that are capable of encoding spatial relations. It became clear that Top-Down is the preferred attention method, given its results with ReLU activation. \n Question: What components are identified as core components for training VQA models?",
            "output": [
                "pre-trained text representations transformer-based encoders together with GRU models attention mechanisms are paramount for learning top performing networks Top-Down is the preferred attention method"
            ]
        },
        {
            "id": "task460-79c57bfdb49f4a5999de1414981a6fa4",
            "input": "Trinomials are likely to appear in exactly one order, and when they appear in more than one order the last word is almost always the same across all instances.  \n Question: Are there any new finding in analasys of trinomials that was not present binomials?",
            "output": [
                "Trinomials are likely to appear in exactly one order"
            ]
        },
        {
            "id": "task460-6f173ece95544cce80128d1ea33b82fc",
            "input": "We evaluate the QRNN architecture on a popular document-level sentiment classification benchmark, the IMDb movie review dataset BIBREF17 . The dataset consists of a balanced sample of 25,000 positive and 25,000 negative reviews, divided into equal-size train and test sets, with an average document length of 231 words BIBREF18 . We compare only to other results that do not make use of additional unlabeled data (thus excluding e.g., BIBREF19 ). \n Question: What sentiment classification dataset is used?",
            "output": [
                "the IMDb movie review dataset BIBREF17"
            ]
        },
        {
            "id": "task460-8aa61d688ad4476bbbf10afb867c6df5",
            "input": "The learning model for retrieval is trained by an oracle constructed using distant supervision. Using the answer labels in the training set, we can find appropriate articles that include the information requested in the question. First, the question and article are embedded into vector sequences, using the same method as the comprehension model. We do not use anonymization here, to retain simplicity. Otherwise, the anonymization procedure would have to be repeated several times for a potentially large collection of documents. These vector sequences are next fed to a Bi-GRU, to produce the outputs $v$ (for the question) and $H_c$ (for the document) similar to the previous section. \n Question: How can a neural model be used for a retrieval if the input is the entire Wikipedia?",
            "output": [
                "Using the answer labels in the training set, we can find appropriate articles that include the information requested in the question."
            ]
        },
        {
            "id": "task460-3855b064fc6b4b4aaf40b8c95238fd01",
            "input": " We refer the reader to BIBREF10 for more details on the propaganda techniques; below we report the list of techniques:\n\nPropaganda Techniques ::: 1. Loaded language.\nUsing words/phrases with strong emotional implications (positive or negative) to influence an audience BIBREF11.\n\nPropaganda Techniques ::: 2. Name calling or labeling.\nLabeling the object of the propaganda as something the target audience fears, hates, finds undesirable or otherwise loves or praises BIBREF12.\n\nPropaganda Techniques ::: 3. Repetition.\nRepeating the same message over and over again, so that the audience will eventually accept it BIBREF13, BIBREF12.\n\nPropaganda Techniques ::: 4. Exaggeration or minimization.\nEither representing something in an excessive manner: making things larger, better, worse, or making something seem less important or smaller than it actually is BIBREF14, e.g., saying that an insult was just a joke.\n\nPropaganda Techniques ::: 5. Doubt.\nQuestioning the credibility of someone or something.\n\nPropaganda Techniques ::: 6. Appeal to fear/prejudice.\nSeeking to build support for an idea by instilling anxiety and/or panic in the population towards an alternative, possibly based on preconceived judgments.\n\nPropaganda Techniques ::: 7. Flag-waving.\nPlaying on strong national feeling (or with respect to a group, e.g., race, gender, political preference) to justify or promote an action or idea BIBREF15.\n\nPropaganda Techniques ::: 8. Causal oversimplification.\nAssuming one cause when there are multiple causes behind an issue. We include scapegoating as well: the transfer of the blame to one person or group of people without investigating the complexities of an issue.\n\nPropaganda Techniques ::: 9. Slogans.\nA brief and striking phrase that may include labeling and stereotyping. Slogans tend to act as emotional appeals BIBREF16.\n\nPropaganda Techniques ::: 10. Appeal to authority.\nStating that a claim is true simply because a valid authority/expert on the issue supports it, without any other supporting evidence BIBREF17. We include the special case where the reference is not an authority/expert, although it is referred to as testimonial in the literature BIBREF14.\n\nPropaganda Techniques ::: 11. Black-and-white fallacy, dictatorship.\nPresenting two alternative options as the only possibilities, when in fact more possibilities exist BIBREF13. As an extreme case, telling the audience exactly what actions to take, eliminating any other possible choice (dictatorship).\n\nPropaganda Techniques ::: 12. Thought-terminating cliché.\nWords or phrases that discourage critical thought and meaningful discussion about a given topic. They are typically short and generic sentences that offer seemingly simple answers to complex questions or that distract attention away from other lines of thought BIBREF18.\n\nPropaganda Techniques ::: 13. Whataboutism.\nDiscredit an opponent's position by charging them with hypocrisy without directly disproving their argument BIBREF19.\n\nPropaganda Techniques ::: 14. Reductio ad Hitlerum.\nPersuading an audience to disapprove an action or idea by suggesting that the idea is popular with groups hated in contempt by the target audience. It can refer to any person or concept with a negative connotation BIBREF20.\n\nPropaganda Techniques ::: 15. Red herring.\nIntroducing irrelevant material to the issue being discussed, so that everyone's attention is diverted away from the points made BIBREF11. Those subjected to a red herring argument are led away from the issue that had been the focus of the discussion and urged to follow an observation or claim that may be associated with the original claim, but is not highly relevant to the issue in dispute BIBREF20.\n\nPropaganda Techniques ::: 16. Bandwagon.\nAttempting to persuade the target audience to join in and take the course of action because “everyone else is taking the same action” BIBREF15.\n\nPropaganda Techniques ::: 17. Obfuscation, intentional vagueness, confusion.\nUsing deliberately unclear words, to let the audience have its own interpretation BIBREF21, BIBREF11. For instance, when an unclear phrase with multiple possible meanings is used within the argument and, therefore, it does not really support the conclusion.\n\nPropaganda Techniques ::: 18. Straw man.\nWhen an opponent's proposition is substituted with a similar one which is then refuted in place of the original BIBREF22. \n Question: What are the 18 propaganda techniques?",
            "output": [
                "Loaded language Name calling or labeling Repetition Exaggeration or minimization Doubt Appeal to fear/prejudice Flag-waving Causal oversimplification Slogans  Appeal to authority Black-and-white fallacy, dictatorship Thought-terminating cliché Whataboutism Reductio ad Hitlerum Red herring Bandwagon Obfuscation, intentional vagueness, confusion Straw man"
            ]
        },
        {
            "id": "task460-c5b6e1f4ad7048ad83ae33e3b59f5f1b",
            "input": "There are almost 1.1 million sentences in the collection. There are 119 different relation types (unique predicates), having from just a few relations to a few million relations. \n Question: How big is their dataset?",
            "output": [
                "1.1 million sentences 119 different relation types (unique predicates)"
            ]
        },
        {
            "id": "task460-317a06232e0a4d11b8cc1f3707c2a1ef",
            "input": "This work also contributes a new dataset of INLINEFORM0 pairs of free-form natural language instructions and high-level navigation plans. This dataset was collected through Mechanical Turk using 100 simulated environments with a corresponding topological map and, to the best of our knowledge, it is the first of its kind for behavioral navigation. The dataset opens up opportunities to explore data-driven methods for grounding navigation commands into high-level motion plans. While the dataset was collected with simulated environments, no structure was imposed on the navigation instructions while crowd-sourcing data. Thus, many instructions in our dataset are ambiguous. Moreover, the order of the behaviors in the instructions is not always the same. For instance, a person said “turn right and advance” to describe part of a route, while another person said “go straight after turning right” in a similar situation. The high variability present in the natural language descriptions of our dataset makes the problem of decoding instructions into behaviors not trivial. See Appendix A of the supplementary material for additional details on our data collection effort. \n Question: Did the collection process use a WoZ method?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-85ff97eb00e449dc9cec3c0b3a9275e3",
            "input": "For the CNN-Dailymail dataset, the Lead-3 model is considered a strong baseline; both the abstractive BIBREF16 and extractive BIBREF14 state-of-the art methods on this dataset beat this baseline only marginally. For the proxy report section of the AMR bank, we consider the Lead-1-AMR model as the baseline. \n Question: Which other methods do they compare with?",
            "output": [
                "Lead-3 Lead-1-AMR"
            ]
        },
        {
            "id": "task460-301cbec022d94d29a91ea67d83ef38c9",
            "input": "In tab:blueperpl we report BLEU scores for the reconstruction of test set sentences from their content and feature representations, as well as the model perplexities of the reconstruction. For both models, we use beam decoding with a beam size of eight. Note that for both models, the all and top classification accuracy tends to be quite similar, though for the Baseline they are often almost exactly the same when the Baseline has little to no diversity in the outputs. \n Question: What metrics are used for automatic evaluation?",
            "output": [
                "classification accuracy BLEU scores model perplexities of the reconstruction"
            ]
        },
        {
            "id": "task460-e429da1c3dfc46d4b0140c9f8a29de68",
            "input": "In this chapter, we apply an ensemble of deep learning and linguistics to tackle both the problem of aspect extraction and subjectivity detection. \n Question: How are aspects identified in aspect extraction?",
            "output": [
                "apply an ensemble of deep learning and linguistics t"
            ]
        },
        {
            "id": "task460-27531740c66b4f8bac2ce152f2b84510",
            "input": "Lastly, the attention model, MDREA, also outperforms the best existing research results (WAP 0.690 to 0.688) BIBREF19 . \n Question: By how much does their model outperform the state of the art results?",
            "output": [
                "the attention model, MDREA, also outperforms the best existing research results (WAP 0.690 to 0.688)"
            ]
        },
        {
            "id": "task460-f256a39e3e5e4af99ada4a181b1f2373",
            "input": "Four factual attributes pertaining to the 200 colleges are extracted from the U.S. News Statistics, which consists of Undergraduate Enrollment, Male/Female Ratio, Private/Public, and Region (Northeast, South, West, and Midwest).  \n Question: Which major geographical regions are studied?",
            "output": [
                "Northeast U.S, South U.S., West U.S. and Midwest U.S."
            ]
        },
        {
            "id": "task460-aa9bc9ab747b4b7dab1e43228c236344",
            "input": "Recommendation diversity. As defined in BIBREF18 , we calculate recommendation diversity as the average dissimilarity of all pairs of tags in the list of recommended tags. \n Question: how is diversity measured?",
            "output": [
                "average dissimilarity of all pairs of tags in the list of recommended tags"
            ]
        },
        {
            "id": "task460-38ade7c2f4fa497297d59617c21b6658",
            "input": "We tested the natural language representation against the visual-based and feature representations on several tasks, with varying difficulty. The tasks included a basic scenario, a health gathering scenario, a scenario in which the agent must take cover from fireballs, a scenario in which the agent must defend itself from charging enemies, and a super scenario, where a mixture of the above scenarios was designed to challenge the agent. \n Question: What experiments authors perform?",
            "output": [
                "a basic scenario, a health gathering scenario, a scenario in which the agent must take cover from fireballs, a scenario in which the agent must defend itself from charging enemies, and a super scenario, where a mixture of the above scenarios"
            ]
        },
        {
            "id": "task460-cb30bcf590684021b0d4aceb0bf04ac8",
            "input": " In this scenario we have a paired dataset that contains article-comment parallel contents INLINEFORM0 , and an unpaired dataset that contains the documents (articles or comments) INLINEFORM1 . The supervised model is trained on INLINEFORM2 so that we can learn the matching or mapping between articles and comments. By sharing the encoder of the supervised model and the unsupervised model, we can jointly train both the models with a joint objective function: DISPLAYFORM0 \n Question: Which paired corpora did they use in the other experiment?",
            "output": [
                "dataset that contains article-comment parallel contents INLINEFORM0 , and an unpaired dataset that contains the documents (articles or comments) INLINEFORM1"
            ]
        },
        {
            "id": "task460-d32bad7dcc4e4bac90826eceee740bd9",
            "input": "In the experiments, we consider four data sets, two of them newly created and the remaining two already public: CNN, TIME, 20 Newsgroups, and Reuters-21578. The code and the two new data sets are available at github.com/baiyangwang/emgan. For the pre-processing of all the documents, we transformed all characters to lower case, stemmed the documents, and ran the word2vec model on each corpora to obtain word embeddings with a size of 300. In all subsequent models, we only consider the most frequent INLINEFORM0 words across all corpora in a data set. \n Question: Which corpora do they use?",
            "output": [
                "CNN, TIME, 20 Newsgroups, and Reuters-21578"
            ]
        },
        {
            "id": "task460-8453f1c26a66410d9e3147107e11e1fc",
            "input": "For the NER task, we consider both Chinese datasets, i.e., OntoNotes4.0 BIBREF34 and MSRA BIBREF35, and English datasets, i.e., CoNLL2003 BIBREF36 and OntoNotes5.0 BIBREF37. Table shows experimental results on NER datasets. For English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively. We observe huge performance boosts on Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively. As far as we are concerned, we are setting new SOTA performances on all of the four NER datasets. \n Question: What are method's improvements of F1 for NER task for English and Chinese datasets?",
            "output": [
                "English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively"
            ]
        },
        {
            "id": "task460-cb6ef4bf3298443992ad5112d9c727a6",
            "input": "In this section, we evaluate our method and compare its performance against the competitive approaches. We use INLINEFORM0 -fold evaluation protocol with INLINEFORM1 with random dataset split. We measure the performance using standard accuracy metric which we define as a ratio between correctly classified data samples from test dataset and all test samples. \n Question: What evaluation metrics are used?",
            "output": [
                "standard accuracy metric"
            ]
        },
        {
            "id": "task460-30cde64faea746f9a4a866316cc9c02e",
            "input": "We recruited 100 crowdworkers on Amazon Mechanical Turk (AMT) and measured completion times and accuracies for typing randomly sampled sentences from the Yelp corpus.  \n Question: How many participants were trying this communication game?",
            "output": [
                "100 "
            ]
        },
        {
            "id": "task460-05c70265d0d2421081c4d7035d0e652a",
            "input": "Our features consisted of direct similarity over ConceptNet Numberbatch embeddings, the relationships inferred over ConceptNet by SME, features that compose ConceptNet with other resources (WordNet and Wikipedia), and a purely corpus-based feature that looks up two-word phrases in the Google Books dataset. \n Question: What features did they train on?",
            "output": [
                "direct similarity over ConceptNet Numberbatch embeddings, the relationships inferred over ConceptNet by SME, features that compose ConceptNet with other resources (WordNet and Wikipedia), and a purely corpus-based feature that looks up two-word phrases in the Google Books dataset"
            ]
        },
        {
            "id": "task460-3618c1fef18548eb8c79bc904a90df02",
            "input": "The final result submitted in public leaderboard is 0.73019 and in private leaderboard is 0.58455. It is quite different in bad way. That maybe is the result of the model too overfit on train set tuning on public test set. \n Question: What is private dashboard?",
            "output": [
                "Private dashboard is leaderboard where competitors can see results after competition is finished - on hidden part of test set (private test set)."
            ]
        },
        {
            "id": "task460-e6c9309c42e049e6bbefa0805f9508ad",
            "input": "The key idea behind our proposed interactive MRC (iMRC) is to restrict the document context that a model observes at one time. Concretely, we split a supporting document into its component sentences and withhold these sentences from the model. Given a question, the model must issue commands to observe sentences in the withheld set; we equip models with actions such as Ctrl+F (search for token) and stop for searching through partially observed documents. A model searches iteratively, conditioning each command on the input question and the sentences it has observed previously. Thus, our task requires models to `feed themselves' rather than spoon-feeding them with information. This casts MRC as a sequential decision-making problem amenable to reinforcement learning (RL). \n Question: How do they train models in this setup?",
            "output": [
                "Thus, our task requires models to `feed themselves' rather than spoon-feeding them with information. This casts MRC as a sequential decision-making problem amenable to reinforcement learning (RL)."
            ]
        },
        {
            "id": "task460-3a0ade1938ef4dde8880c18cff7eec59",
            "input": "We propose a natural language generation model based on BERT, making good use of the pre-trained language model in the encoder and decoder process, and the model can be trained end-to-end without handcrafted features. During model training, the objective of our model is sum of the two processes, jointly trained using \"teacher-forcing\" algorithm. During training we feed the ground-truth summary to each decoder and minimize the objective.\n\n$$L_{model} = \\hat{L}_{dec} + \\hat{L}_{refine}$$ (Eq. 23)\n\nAt test time, each time step we choose the predicted word by $\\hat{y} = argmax_{y^{\\prime }} P(y^{\\prime }|x)$ , use beam search to generate the draft summaries, and use greedy search to generate the refined summaries. \n Question: How are the different components of the model trained? Is it trained end-to-end?",
            "output": [
                "the objective of our model is sum of the two processes, jointly trained using \"teacher-forcing\" algorithm we feed the ground-truth summary to each decoder and minimize the objective At test time, each time step we choose the predicted word by $\\hat{y} = argmax_{y^{\\prime }} P(y^{\\prime }|x)$ , use beam search to generate the draft summaries, and use greedy search to generate the refined summaries. the model can be trained end-to-end"
            ]
        },
        {
            "id": "task460-3c12285a80894ebb9f2d1d19e0ccb529",
            "input": "With regards to complexity and intangibility of ethics and morals, we restrict ourselves to a rather basic implementation of this construct, following the theories of deontological ethics. These ask which choices are morally required, forbidden, or permitted instead of asking which kind of a person we should be or which consequences of our actions are to be preferred. Thus, norms are understood as universal rules of what to do and what not to do. Therefore, we focus on the valuation of social acceptance in single verbs and single verbs with surrounding context information —e.g. trust my friend or trust a machine— to figure out which of them represent a Do and which tend to be a Don't.  \n Question: How do the authors define deontological ethical reasoning?",
            "output": [
                "These ask which choices are morally required, forbidden, or permitted norms are understood as universal rules of what to do and what not to do"
            ]
        },
        {
            "id": "task460-1599256453824472a2eb6a5b000ed400",
            "input": "We introduce a surrogate training objective that avoids these problems and as a result is fully continuous.  Specifically, we form a continuous function softLB that seeks to approximate the result of running our decoder on input INLINEFORM0 and then evaluating the result against INLINEFORM1 using INLINEFORM2 . \n Question: Do they provide a framework for building a sub-differentiable for any final loss metric?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-ecccd11f13e24c80b965f6071d50ca69",
            "input": "We then propose a group of new interesting tasks, such as analogy query and analogy browsing, and discuss how they can be used in modern digital libraries. \n Question: What new interesting tasks can be solved based on the uncanny semantic structures of the embedding space?",
            "output": [
                " analogy query analogy browsing"
            ]
        },
        {
            "id": "task460-a3f8babcdcb542b6b5e93d47d56ab279",
            "input": "For every gendered character in the dataset, we ask annotators to create a new character with a persona of the opposite gender that is otherwise identical except for referring nouns or pronouns. \n Question: In the targeted data collection approach, what type of data is targetted?",
            "output": [
                "Gendered characters in the dataset"
            ]
        },
        {
            "id": "task460-e877bd6e8abc45dda799bd3c34a0883b",
            "input": "Attention-base translation model We use the system of BIBREF6 , a convolutional sequence to sequence model. \n Question: Which translation system do they use to translate to English?",
            "output": [
                "Attention-based translation model with convolution sequence to sequence model"
            ]
        },
        {
            "id": "task460-4823c93c4d1a45e8b3c1e71f5cd0a013",
            "input": "In Spanish, serigos2017using extracted anglicisms from a corpus of Argentinian newspapers by combining dictionary lookup (aided by TreeTagger and the NLTK lemmatizer) with automatic filtering of capitalized words and manual inspection. In serigos2017applying, a character n-gram module was added to estimate the probabilities of a word being English or Spanish. moreno2018configuracion used different pattern-matching filters and lexicon lookup to extract anglicism cadidates from a corpus of tweets in US Spanish. \n Question: Does the paper mention other works proposing methods to detect anglicisms in Spanish?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-759e1af75097488cbacafb2f37a02e15",
            "input": " To remain consistent with experiments performed with LSTM's we use the morfessor for the subword tokenization in the Finnish Language. \n Question: Is the LSTM baseline a sub-word model?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-1eb7561a85d24473964748d79a15a028",
            "input": "In this study we use transcripts and results of Oxford-style debates from the public debate series “Intelligence Squared Debates” (IQ2 for short). \n Question: what debates dataset was used?",
            "output": [
                "Intelligence Squared Debates"
            ]
        },
        {
            "id": "task460-82067b319045410c899df84749dbb754",
            "input": "We introduce our proposed diversity, density, and homogeneity metrics with their detailed formulations and key intuitions. \n Question: Did they propose other metrics?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-4f068a9dc93b4e87a581d3eab0116729",
            "input": "In this experiment, we compare our Enitity-GCN against recent prior work on the same task. We present test and development results (when present) for both versions of the dataset in Table 2 . From BIBREF0 , we list an oracle based on human performance as well as two standard reading comprehension models, namely BiDAF BIBREF3 and FastQA BIBREF6 . We also compare against Coref-GRU BIBREF12 , MHPGM BIBREF11 , and Weaver BIBREF10 . Additionally, we include results of MHQA-GRN BIBREF23 , from a recent arXiv preprint describing concurrent work. They jointly train graph neural networks and recurrent encoders. We report single runs of our two best single models and an ensemble one on the unmasked test set (recall that the test set is not publicly available and the task organizers only report unmasked results) as well as both versions of the validation set. \n Question: What baseline did they compare Entity-GCN to?",
            "output": [
                "Human, FastQA, BiDAF, Coref-GRU, MHPGM, Weaver / Jenga, MHQA-GRN"
            ]
        },
        {
            "id": "task460-9cd817506b5f4281a3b7f2dd30f2ce42",
            "input": "The baseline model for our experiments is explained in the paper by Alec Go [1]. The model uses the Naive Bayes, SVM, and the Maximum Entropy classifiers for their experiment. \n Question: What previously proposed methods is this method compared against?",
            "output": [
                "Naive Bayes SVM Maximum Entropy classifiers"
            ]
        },
        {
            "id": "task460-a1c24a408d384b1795f5154fde963d50",
            "input": "In a cooperation with researchers from the German Institute for International Educational Research we identified the following current controversial topics in education in English-speaking countries: (1) homeschooling, (2) public versus private schools, (3) redshirting — intentionally delaying the entry of an age-eligible child into kindergarten, allowing their child more time to mature emotionally and physically BIBREF51 , (4) prayer in schools — whether prayer in schools should be allowed and taken as a part of education or banned completely, (5) single-sex education — single-sex classes (males and females separate) versus mixed-sex classes (“co-ed”), and (6) mainstreaming — including children with special needs into regular classes. \n Question: Do they report results only on English data?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-821ce2f2f05c4bf0b1d19a1f994227df",
            "input": "Secondly, a well-known problem of uneven yet unknown distribution of word senses is alleviated by modifying a naïve Bayesian classifier. Thanks to this correction, the classifier is no longer biased towards senses that have more training data. Finally, the aggregated feature values corresponding to target word senses are used to build a naïve Bayesian classifier adjusted to a situation of unknown a priori probabilities. \n Question: How do they deal with unknown distribution senses?",
            "output": [
                "The Näive-Bayes classifier is corrected so it is not biased to most frequent classes"
            ]
        },
        {
            "id": "task460-7d3d57bc82c74a76ad02919acd7f809f",
            "input": "To derive contextualized representations from textual features, the book title and blurb are concatenated and then fed through BERT The non-text features are generated in a separate preprocessing step. The metadata features are represented as a ten-dimensional vector (two dimensions for gender, see Section SECREF10). Author embedding vectors have a length of 200 (see Section SECREF22). In the next step, all three representations are concatenated and passed into a MLP with two layers, 1024 units each and ReLu activation function. \n Question: How do they combine text representations with the knowledge graph embeddings?",
            "output": [
                "all three representations are concatenated and passed into a MLP"
            ]
        },
        {
            "id": "task460-886aba48a15a40a7bdd5546b08477826",
            "input": "We test the character and word-level variants by predicting hashtags for a held-out test set of posts. \n Question: What other tasks do they test their method on?",
            "output": [
                "None"
            ]
        },
        {
            "id": "task460-2002f573289a454bbcecccc573213760",
            "input": "These recordings contain background noise (as is expected in a mall) and have different languages (Indians speak a mixture of English and Hindi). \n Question: What languages are considered?",
            "output": [
                "English Hindi"
            ]
        },
        {
            "id": "task460-3e20d9c6f8e2481b95e0aa8599b859d7",
            "input": "We compare the performance of a standard Transformer Base model and our semi-parametric NMT approach on an English-French translation task. \n Question: To which systems do they compare their results against?",
            "output": [
                "standard Transformer Base model"
            ]
        },
        {
            "id": "task460-334765bf253243bf898f363a3a731a21",
            "input": " In particular, we use Semeval 2014 BIBREF34 Twitter Sentiment Analysis Dataset for the training Sarcasm Datasets Used in the Experiment\nThis dataset was created by BIBREF8 . The tweets were downloaded from Twitter using #sarcasm as a marker for sarcastic tweets. It is a monolingual English dataset which consists of a balanced distribution of 50,000 sarcastic tweets and 50,000 non-sarcastic tweets.\n\nSince sarcastic tweets are less frequently used BIBREF8 , we also need to investigate the robustness of the selected features and the model trained on these features on an imbalanced dataset. To this end, we used another English dataset from BIBREF8 . It consists of 25,000 sarcastic tweets and 75,000 non-sarcastic tweets.\n\nWe have obtained this dataset from The Sarcasm Detector. It contains 120,000 tweets, out of which 20,000 are sarcastic and 100,000 are non-sarcastic. We randomly sampled 10,000 sarcastic and 20,000 non-sarcastic tweets from the dataset. Visualization of both the original and subset data show similar characteristics. \n Question: Which benchmark datasets are used?",
            "output": [
                "Semeval 2014 BIBREF34 Twitter Sentiment Analysis Dataset   dataset was created by BIBREF8  English dataset from BIBREF8  dataset from The Sarcasm Detector"
            ]
        },
        {
            "id": "task460-4e24f7590b6a451e81ba68a2e1a33d2e",
            "input": "The final collection of Facebook pages for the experiments described in this paper is as follows: FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney. \n Question: Which Facebook pages did they look at?",
            "output": [
                "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney"
            ]
        },
        {
            "id": "task460-3f095250269040fc886705169267f63d",
            "input": "We plotted the distribution of harassment incidents in each categorization dimension (Figure FIGREF19). It displays statistics that provide important evidence as to the scale of harassment and that can serve as the basis for more effective interventions to be developed by authorities ranging from advocacy organizations to policy makers. It provides evidence to support some commonly assumed factors about harassment: First, we demonstrate that harassment occurred more frequently during the night time than the day time. Second, it shows that besides unspecified strangers (not shown in the figure), conductors and drivers are top the list of identified types of harassers, followed by friends and relatives. Furthermore, we uncovered that there exist strong correlations between the age of perpetrators and the location of harassment, between the single/multiple harasser(s) and location, and between age and single/multiple harasser(s) (Figure FIGREF20).  We also found that the majority of young perpetrators engaged in harassment behaviors on the streets. These findings suggest that interventions with young men and boys, who are readily influenced by peers, might be most effective when education is done peer-to-peer. It also points to the locations where such efforts could be made, including both in schools and on the streets.  In contrast, we found that adult perpetrators of sexual harassment are more likely to act alone. Most of the adult harassers engaged in harassment on public transportation. These differences in adult harassment activities and locations, mean that interventions should be responsive to these factors. For example, increasing the security measures on transit at key times and locations. In addition, we also found that the correlations between the forms of harassment with the age, single/multiple harasser, type of harasser, and location (Figure FIGREF21). For example, young harassers are more likely to engage in behaviors of verbal harassment, rather than physical harassment as compared to adults. It was a single perpetrator that engaged in touching or groping more often, rather than groups of perpetrators. In contrast, commenting happened more frequently when harassers were in groups. Last but not least, public transportation is where people got indecently touched most frequently both by fellow passengers and by conductors and drivers. The nature and location of the harassment are particularly significant in developing strategies for those who are harassed or who witness the harassment to respond and manage the everyday threat of harassment. For example, some strategies will work best on public transport, a particular closed, shared space setting, while other strategies might be more effective on the open space of the street. \n Question: What patterns were discovered from the stories?",
            "output": [
                "we demonstrate that harassment occurred more frequently during the night time than the day time it shows that besides unspecified strangers (not shown in the figure), conductors and drivers are top the list of identified types of harassers, followed by friends and relatives we uncovered that there exist strong correlations between the age of perpetrators and the location of harassment, between the single/multiple harasser(s) and location, and between age and single/multiple harasser(s)  We also found that the majority of young perpetrators engaged in harassment behaviors on the streets we found that adult perpetrators of sexual harassment are more likely to act alone we also found that the correlations between the forms of harassment with the age, single/multiple harasser, type of harasser, and location  commenting happened more frequently when harassers were in groups. Last but not least, public transportation is where people got indecently touched most frequently both by fellow passengers and by conductors and drivers."
            ]
        },
        {
            "id": "task460-9c110f9ae90c4bb5a6a9bca6ebb5fb18",
            "input": "Experiment 1: scene variation in modified referring expressions We recruited 58 pairs of participants (116 participants total) over Amazon's Mechanical Turk who were each paid $1.75 for their participation. \n Question: Does the paper describe experiments with real humans?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-6adaa0964a584997a01790e05cccbff7",
            "input": "To simplify the task, we set up a binary classification: candidates who have been liked or shortlisted are considered part of the hirable class and others part of the not hirable class. \n Question: How is \"hirability\" defined?",
            "output": [
                "candidates who have been liked or shortlisted are considered part of the hirable class"
            ]
        },
        {
            "id": "task460-dbba5c307aec4ed4bf3ead4122cef5bc",
            "input": "The dataset consists of queries and the corresponding image search results. Each token in each query is given a language tag based on the user-set home language of the user making the search on Google Images. \n Question: Do the images have multilingual annotations or monolingual ones?",
            "output": [
                "monolingual"
            ]
        },
        {
            "id": "task460-ed9ab066ded34b76a6edc804dc86fc8c",
            "input": "irremediable annotation discrepancies Some shortcomings of recall come from irremediable annotation discrepancies. Largely, we are hamstrung by differences in choice of attributes to annotate. When one resource marks gender and the other marks case, we can't infer the gender of the word purely from its surface form. The resources themselves would need updating to encode the relevant morphosyntactic information. Some languages had a very low number of overlapping forms, and no tag matches or near-matches between them: Arabic, Hindi, Lithuanian, Persian, and Russian. A full list of observed, irremediable discrepancies is presented alongside the codebase. We were able to find specific instances of incorrectly applied UniMorph annotation, as well as specific instances of cross-lingual inconsistency in both resources.  \n Question: What are the main sources of recall errors in the mapping?",
            "output": [
                "irremediable annotation discrepancies differences in choice of attributes to annotate The resources themselves would need updating to encode the relevant morphosyntactic information. Some languages had a very low number of overlapping forms, and no tag matches or near-matches between them the two annotations encode distinct information incorrectly applied UniMorph annotation cross-lingual inconsistency in both resources"
            ]
        },
        {
            "id": "task460-2bccf525ff1f42cd8c795788b27c1ee1",
            "input": "Fortunately, 180221 of 231162 author names could be matched successfully. There are many reasons for the remaining uncovered cases. 9073 Latin names could not be found in the name dictionary ENAMDICT and 14827 name matchings between the names' Latin and kanji representations did not succeed. These names might be missing at all in the dictionary, delivered in a very unusual format that the tool does not cover, or might not be Japanese or human names at all. Of course, Japanese computer scientists sometimes also cooperate with foreign colleagues but our tool expects Japanese names and is optimized for them. \n Question: How successful are they at matching names of authors in Japanese and English?",
            "output": [
                "180221 of 231162 author names could be matched successfully"
            ]
        },
        {
            "id": "task460-88f648964c0e4d928deeceb883abd037",
            "input": "Baselines. We use one strong non-DNN baseline, NBSVM (with unigrams or bigrams features) BIBREF23 and six DNN baselines. The first DNN baseline is CNN BIBREF25, which does not handle noisy labels. The other five were designed to handle noisy labels.\n\nThe comparison results are shown in Table TABREF12. From the results, we can make the following observations. (1) Our NetAb model achieves the best ACC and F1 on all datasets except for F1 of negative class on Laptop. \n Question: Is the model evaluated against a CNN baseline?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-fdecaa64139643d8a8edab9072cde388",
            "input": " The dataset was split timewise into training/validation/test sets, and hyperparameters were optimized to maximize the AUC-ROC on the validation set.  \n Question: What evaluation metrics were used?",
            "output": [
                "AUC-ROC"
            ]
        },
        {
            "id": "task460-a71f7c650d8d48c9942a75da2cc1ba52",
            "input": "Especially for Davidson-dataset, some tweets with specific language (written within the African American Vernacular English) and geographic restriction (United States of America) are oversampled such as tweets containing disparage words “nigga\", “faggot\", “coon\", or “queer\", result in high rates of misclassification. However, these misclassifications do not confirm the low performance of our classifier because annotators tended to annotate many samples containing disrespectful words as hate or offensive without any presumption about the social context of tweeters such as the speaker’s identity or dialect, whereas they were just offensive or even neither tweets.  \n Question: What biases does their model capture?",
            "output": [
                "Data annotation biases where tweet containing disrespectful words are annotated as hate or offensive without any presumption about the social context of tweeters"
            ]
        },
        {
            "id": "task460-2b58ba15cfeb4c72ac0916ecc54685e0",
            "input": "As a result, the neutral class in SNLI-VE has substantial labelling errors. Vu BIBREF3 estimated ${\\sim }31\\%$ errors in this class, and ${\\sim }1\\%$ for the contradiction and entailment classes. \n Question: What is the class with highest error rate in SNLI-VE?",
            "output": [
                "neutral class"
            ]
        },
        {
            "id": "task460-3bd9b9e21a6a41a0b986523f1fcfe789",
            "input": "Our full dataset consists of all subreddits on Reddit from January 2013 to December 2014, for which there are at least 500 words in the vocabulary used to estimate our measures, in at least 4 months of the subreddit's history. We compute our measures over the comments written by users in a community in time windows of months, for each sufficiently active month, and manually remove communities where the bulk of the contributions are in a foreign language.  \n Question: How did the select the 300 Reddit communities for comparison?",
            "output": [
                "They selected all the subreddits from January 2013 to December 2014 with at least 500 words in the vocabulary and at least 4 months of the subreddit's history. They also removed communities with the bulk of the contributions are in foreign language."
            ]
        },
        {
            "id": "task460-c6c359d38d8e4a2c84c658abc2540239",
            "input": "The answer styles considered in the experiments corresponded to the two tasks. The NLG task requires a well-formed answer that is an abstractive summary of the question and ten passages, averaging 16.6 words. The Q&A task also requires an abstractive answer but prefers a more concise answer than the NLG task, averaging 13.1 words, where many of the answers do not contain the context of the question. \n Question: What is an \"answer style\"?",
            "output": [
                "well-formed sentences vs concise answers"
            ]
        },
        {
            "id": "task460-64a1191897984d298f00f25057008426",
            "input": "In our experiment, we use a conversational Mandarin-English code-switching speech corpus called SEAME Phase II (South East Asia Mandarin-English).  \n Question: What languages are explored in this paper?",
            "output": [
                "Mandarin English"
            ]
        },
        {
            "id": "task460-fa94ca84ab57429093c6b1c4c81da76d",
            "input": "Thus, SIM has many fewer parameters than existing dialogue state tracking models. To compensate for the exclusion of slot-specific parameters, we incorporate better feature representation of user utterance and dialogue states using syntactic information and convolutional neural networks (CNN).  \n Question: How do they prevent the model complexity increasing with the increased number of slots?",
            "output": [
                "They exclude slot-specific parameters and incorporate better feature representation of user utterance and dialogue states using syntactic information and convolutional neural networks (CNN)."
            ]
        },
        {
            "id": "task460-2351cdbe21344eeca75e695d090b0bf8",
            "input": "We created this CORD-19-NER dataset with comprehensive named entity annotation on the CORD-19 corpus (2020-03-13).  The corpus is generated from the 29,500 documents in the CORD-19 corpus (2020-03-13).  \n Question: What is the size of this dataset?",
            "output": [
                "29,500 documents"
            ]
        },
        {
            "id": "task460-7efe9eb0fd0842a8905a7bc168fdcec4",
            "input": "To collect article-level labels, we utilized a platform in the company that has been used by the market research team to collect surveys from the subscribers of different news publishers. The survey works as follows: The user is first presented with a set of selected pages (usually 4 pages and around 20 articles) from the print paper the day before. The user can select an article each time that he or she has read, and answer some questions about it. We added 3 questions to the existing survey that asked the level of partisanship, the polarity of partisanship, and which pro- or anti- entities the article presents. We also asked the political standpoint of the user. The complete survey can be found in Appendices. \n Question: Did they crowdsource the annotations?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-e259de77d11d42ad873f5af0691270df",
            "input": "But we also showed that in some cases the saliency maps seem to not capture the important input features.  The second observation we can make is that the saliency map doesn't seem to highlight the right things in the input for the summary it generates \n Question: Is the explanation from saliency map correct?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-10ebd548192d4449b4320f2d2c52ffa6",
            "input": "In this work, we explore the applicability of our method to three popular architectures of this layer: the LSTM-based, the CNN-based, and the transformer-based. Table TABREF46 shows performance of our method with different sequence modeling architectures. From the table, we can first see that the LSTM-based architecture performed better than the CNN- and transformer- based architectures.  \n Question: Which are the sequence model architectures this method can be transferred across?",
            "output": [
                "The sequence model architectures which this method is transferred to are: LSTM and Transformer-based models"
            ]
        },
        {
            "id": "task460-f4443c63b31449cba0ee5ad58b7385cf",
            "input": "We perform experiments on document-level variants of the SQuAD dataset BIBREF1 . \n Question: What datasets have this method been evaluated on?",
            "output": [
                "document-level variants of the SQuAD dataset "
            ]
        },
        {
            "id": "task460-ab71d95f3cba43a0822146f01622e17e",
            "input": "We evaluate our method on IWSLT16 German-English (DE-EN) translation in both directions, WMT15 English-German (EN-DE) translation in both directions, and NIST Chinese-to-English (ZH$\\rightarrow $EN) translation.  \n Question: What corpora is used?",
            "output": [
                "IWSLT16 WMT15 NIST"
            ]
        },
        {
            "id": "task460-36bc3bf6766e4eb9b6154de1f8cf8639",
            "input": "We call this lifelong interactive learning and inference (LiLi). Lifelong learning is reflected by the facts that the newly acquired facts are retained in the KB and used in inference for future queries, and that the accumulated knowledge in addition to the updated KB including past inference performances are leveraged to guide future interaction and learning. \n Question: In what way does LiLi imitate how humans acquire knowledge and perform inference during an interactive conversation?",
            "output": [
                "newly acquired facts are retained in the KB and used in inference for future queries, and that the accumulated knowledge in addition to the updated KB including past inference performances are leveraged to guide future interaction and learning"
            ]
        },
        {
            "id": "task460-e482a76b446f40318a7b2ec0640fbe55",
            "input": "For training and testing we use the train-validation-test split of WikiTableQuestions BIBREF0 , a dataset containing 22,033 pairs of questions and answers based on 2,108 Wikipedia tables. This dataset is also used by our baselines, BIBREF0 , BIBREF3 . \n Question: Does the dataset they use differ from the one used by Pasupat and Liang, 2015?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-4f984a1125a24f6cb4a20d02472aa37a",
            "input": "We compute our measures over the comments written by users in a community in time windows of months, for each sufficiently active month, and manually remove communities where the bulk of the contributions are in a foreign language.  \n Question: Do they report results only on English data?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-8c15297feacd407ea41600e4c30e26e6",
            "input": "We adopt the following two datasets:\n\nQuora BIBREF1: The Quora Question Pairs dataset contains question pairs annotated with labels indicating whether the two questions are paraphrases. We use the same dataset partition as BIBREF5, with 384,348/10,000/10,000 pairs in the training/development/test set respectively.\n\nMRPC BIBREF34: The Microsoft Research Paraphrase Corpus consists of sentence pairs collected from online news. Each pair is annotated with a label indicating whether the two sentences are semantically equivalent. There are 4,076/1,725 pairs in the training/test set respectively. \n Question: What are benhmark datasets for paraphrase identification?",
            "output": [
                "Quora MRPC"
            ]
        },
        {
            "id": "task460-3a4e98daf96844bebb1b49aafbb9068b",
            "input": "To enlarge our experiment base we added new debates, more detailed information about each one is shown in Table TABREF24 in UNKREF6. To select new discussions and to determine if they are controversial or not we looked for topics widely covered by mainstream media, and that have generated ample discussion, both online and offline. For non-controversy discussions we focused on “soft news\" and entertainment, but also to events that, while being impactful and/or dramatic, did not generate large controversies. To validate that intuition, we manually checked a sample of tweets, being unable to identify any clear instance of controversy On the other side, for controversial debates we focused on political events such as elections, corruption cases or justice decisions. \n Question: What controversial topics are experimented with?",
            "output": [
                "political events such as elections, corruption cases or justice decisions"
            ]
        },
        {
            "id": "task460-ee5bdc57e031470ca0bbd2dd16341721",
            "input": "We collected a corpus of poems and a corpus of vernacular literature from online resources. The poem corpus contains 163K quatrain poems from Tang Poems and Song Poems, the vernacular literature corpus contains 337K short paragraphs from 281 famous books, the corpus covers various literary forms including prose, fiction and essay. Note that our poem corpus and a vernacular corpus are not aligned. We further split the two corpora into a training set and a validation set. \n Question: What dataset is used for training?",
            "output": [
                "We collected a corpus of poems and a corpus of vernacular literature from online resources"
            ]
        },
        {
            "id": "task460-08b76e981fd54c12b92e30fd86b8d90a",
            "input": "We explored three different aspects of inconsistency and designed metrics for their measurements.  A word level feature using tf-idf BIBREF22 is added for robustness. \n Question: What features do they extract?",
            "output": [
                "Inconsistency in Noun Phrase Structures  Inconsistency Between Clauses Inconsistency Between Named Entities and Noun Phrases Word Level Feature Using TF-IDF"
            ]
        },
        {
            "id": "task460-29f8f0dddc024b24b6b0d8d5acd8db80",
            "input": "In our experiments, we used TF.IDF-based features over the title and over the content of the article we wanted to classify. We had these features twice – once for the title and once for the the content of the article, as we wanted to have two different representations of the same article. Thus, we used a total of 1,100 TF.IDF-weighted features (800 content + 300 title), limiting the vocabulary to the top 800 and 300 words, respectively (which occurred in more than five articles). \n Question: what lexical features did they experiment with?",
            "output": [
                "TF.IDF-based features"
            ]
        },
        {
            "id": "task460-885fc447cbfe45bf8914113abc8b1516",
            "input": "We explore an architecture based on a stack of dilated convolution layers, effectively operating on a broader scale than with standard convolutions while limiting model size. Standard convolutional networks cannot capture long temporal patterns with reasonably small models due to the increase in computational cost yielded by larger receptive fields. Dilated convolutions skip some input values so that the convolution kernel is applied over a larger area than its own. The network therefore operates on a larger scale, without the downside of increasing the number of parameters. \n Question: What are dilated convolutions?",
            "output": [
                "Similar to standard convolutional networks but instead they skip some input values effectively operating on a broader scale."
            ]
        },
        {
            "id": "task460-bb0a72d29e994f3785c5536779a111c4",
            "input": "This allows Macaw to support multi-modal interactions, such as text, speech, image, click, etc. \n Question: What modalities are supported by Macaw?",
            "output": [
                "text, speech, image, click, etc"
            ]
        },
        {
            "id": "task460-346776d1fbc14acab85c906a12716605",
            "input": "A simple, yet recurrent text model is the well-known word co-occurrence network. After optional textual pre-processing steps, in a co-occurrence network each different word becomes a node and edges are established via co-occurrence in a desired window. A common strategy connects only adjacent words in the so called word adjacency networks. \n Question: On what model architectures are previous co-occurence networks based?",
            "output": [
                "in a co-occurrence network each different word becomes a node and edges are established via co-occurrence in a desired window connects only adjacent words in the so called word adjacency networks"
            ]
        },
        {
            "id": "task460-312320cb097d400698543a378145d51a",
            "input": "The extensive use of emojis has drawn a growing attention from researchers BIBREF4 , BIBREF5 because the emojis convey fruitful semantical and sentimental information to visually complement the textual information which is significantly useful in understanding the embedded emotional signals in texts BIBREF6  However, the previous literatures lack in considerations of the linguistic complexities and diversity of emoji. Therefore, previous emoji embedding methods fail to handle the situation when the semantics or sentiments of the learned emoji embeddings contradict the information from the corresponding contexts BIBREF5 , or when the emojis convey multiple senses of semantics and sentiments such as ( and ). In practice, emojis can either summarize and emphasis the original tune of their contexts, or express more complex semantics such as irony and sarcasm by being combined with contexts of contradictory semantics or sentiments. Conventional emoji analysis can only extract single embedding of each emoji, and such embeddings will confuse the following sentiment analysis model by inconsistent sentiment signals from the input texts and emojis \n Question: What is the motivation for training bi-sense embeddings?",
            "output": [
                " previous emoji embedding methods fail to handle the situation when the semantics or sentiments of the learned emoji embeddings contradict the information from the corresponding contexts BIBREF5 , or when the emojis convey multiple senses of semantics and sentiments "
            ]
        },
        {
            "id": "task460-6136374419554895aebf83992b9e068d",
            "input": "Car-speak is abstract language that pertains to a car's physical attribute(s). In this instance the physical attributes that the term “fast” pertains to could be the horsepower, or it could be the car's form factor (how the car looks). However, we do not know exactly which attributes the term “fast” refers to. We train a series of classifiers in order to classify car-speak. We train three classifiers on the review vectors that we prepared in Section SECREF8. The classifiers we use are K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), and Multi-layer Perceptron (MLP) BIBREF13. \n Question: Is car-speak language collection of abstract features that classifier is later trained on?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-70e8cdc22d2d4cff8e00148063c57180",
            "input": "Level B categorizes the type of offense and two labels are used: targeted (TIN) and untargeted (INT) insults and threats.\n\nTargeted Insult (TIN): Posts which contain an insult/threat to an individual, group, or others (see next layer);\n\nUntargeted (UNT): Posts containing non-targeted profanity and swearing. Posts with general profanity are not targeted, but they contain non-acceptable language. Level C categorizes the targets of insults and threats as individual (IND), group (GRP), and other (OTH).\n\nIndividual (IND): Posts targeting an individual. It can be a a famous person, a named individual or an unnamed participant in the conversation. Insults and threats targeted at individuals are often defined as cyberbulling.\n\nGroup (GRP): The target of these offensive posts is a group of people considered as a unity due to the same ethnicity, gender or sexual orientation, political affiliation, religious belief, or other common characteristic. Many of the insults and threats targeted at a group correspond to what is commonly understood as hate speech.\n\nOther (OTH): The target of these offensive posts does not belong to any of the previous two categories (e.g. an organization, a situation, an event, or an issue). \n Question: What kinds of offensive content are explored?",
            "output": [
                "non-targeted profanity and swearing, targeted insults such as cyberbullying, offensive content related to ethnicity, gender or sexual orientation, political affiliation, religious belief, and anything belonging to hate speech"
            ]
        },
        {
            "id": "task460-ad1dd8deee5a48d0b81fe79fa16220b4",
            "input": "We cluster the embeddings with INLINEFORM0 -Means.  \n Question: How were the cluster extracted? ",
            "output": [
                "Word clusters are extracted using k-means on word embeddings"
            ]
        },
        {
            "id": "task460-857c959c26204157b3568cf1e5714adb",
            "input": "As the gold standard sentiment lexica, we chose manually created lexicon in Czech BIBREF11 , German BIBREF12 , French BIBREF13 , Macedonian BIBREF14 , and Spanish BIBREF15 . \n Question: what sentiment sources do they compare with?",
            "output": [
                "manually created lexicon in Czech BIBREF11 , German BIBREF12 , French BIBREF13 , Macedonian BIBREF14 , and Spanish BIBREF15"
            ]
        },
        {
            "id": "task460-d5095102b31d4140a1014c2bfa83cd39",
            "input": "Among the GLUE tasks, these state-of-the-art systems make their greatest gains on the acceptability task with the Corpus of Linguistic Acceptability BIBREF0 . CoLA contains example sentences from linguistics publications labeled by experts for grammatical acceptability, and written to show subtle grammatical features. Because minimal syntactic differences can separate acceptable sentences from unacceptable ones (What did Bo write a book about? / *What was a book about written by Bo?), and acceptability classifiers are more reliable when trained on GPT and BERT than on recurrent models, it stands to reason that GPT and BERT have better implicit knowledge of syntactic features relevant to acceptability. \n Question: How is the CoLA grammatically annotated?",
            "output": [
                "labeled by experts"
            ]
        },
        {
            "id": "task460-e14db1ce1dab44fcb5fdc9a3d1f71276",
            "input": "In CBOW architecture the task is predicting the word given its context and in SG the task in predicting the context given the word. We built 16 models of word embeddings using the implementation of CBOW and Skip-gram methods in the FastText tool BIBREF9 . \n Question: What is specific about the specific embeddings?",
            "output": [
                "predicting the word given its context"
            ]
        },
        {
            "id": "task460-fcf01bd328a94c2297c3e1d4888ce0d6",
            "input": "Based on the gained insights, we have improved the state-of-the-art in XNLI for the Translate-Test and Zero-Shot approaches by a substantial margin. \n Question: Is the improvement over state-of-the-art statistically significant?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-ae190007b84d480da6fdee276e46d46f",
            "input": " Inspired by the line of work reported in these studies, we propose to use modified objective functions for a different purpose: learning more interpretable dense word embeddings. By doing this, we aim to incorporate semantic information from an external lexical resource into the word embedding so that the embedding dimensions are aligned along predefined concepts. This alignment is achieved by introducing a modification to the embedding learning process. In our proposed method, which is built on top of the GloVe algorithm BIBREF2 , the cost function for any one of the words of concept word-groups is modified by the introduction of an additive term to the cost function.  \n Question: What is the additive modification to the objective function?",
            "output": [
                "The cost function for any one of the words of concept word-groups is modified by the introduction of an additive term to the cost function. . Each embedding vector dimension is first associated with a concept. For a word belonging to any one of the word-groups representing these concepts, the modified cost term favors an increase for the value of this word's embedding vector dimension corresponding to the concept that the particular word belongs to,"
            ]
        },
        {
            "id": "task460-9561684ea3c4462a99b23d834480f474",
            "input": "Our end-to-end method reads text, and writes to both memory slots and edges between them. Intuitively, the memory slots correspond to entities and the edges correspond to relationships between entities, each represented as a vector. \n Question: How is knowledge retrieved in the memory?",
            "output": [
                "the memory slots correspond to entities and the edges correspond to relationships between entities, each represented as a vector."
            ]
        },
        {
            "id": "task460-92ba72bb32394529a5c13bf93d47ed1a",
            "input": "In this section, we first describe a baseline method inspired by the “align to segment” of BIBREF12, BIBREF13. We then propose two extensions providing the model with a signal relevant to the segmentation process, so as to move towards a joint learning of segmentation and alignment. \n Question: Does the paper report any alignment-only baseline?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-e64c2f9810b24124bc0b665f1e03cf02",
            "input": "In task 1, there are two top categories, namely, chit-chat and task-oriented dialogue. \n Question: How many intents were classified?",
            "output": [
                "two"
            ]
        },
        {
            "id": "task460-cee693ee65844cdb9a7fd470f435fdc9",
            "input": "Generally, adversarial training improves the accuracy across all languages, and the improvement is sometimes dramatic versus the BERT non-adversarial baseline. When combined with adversarial learning, the BERT cross-lingual F1 scores increased for German over the non-adversarial baseline, and the scores remained largely the same for Spanish and Dutch. \n Question: Does adversarial learning have stronger performance gains for text classification, or for NER?",
            "output": [
                "classification"
            ]
        },
        {
            "id": "task460-f286c133da2a41508fc3dff7e58caf6c",
            "input": "We evaluate the quality of the document embeddings learned by the different variants of CAHAN and the HAN baseline on three of the large-scale document classification datasets introduced by BIBREF14 and used in the original HAN paper BIBREF5. They fall into two categories: topic classification (Yahoo) and fine-grained sentiment analysis (Amazon, Yelp). \n Question: What are the datasets used",
            "output": [
                "large-scale document classification datasets introduced by BIBREF14"
            ]
        },
        {
            "id": "task460-e1b29358d0704988bb805d15a768b091",
            "input": "For the MCTest dataset, Fig. FIGREF30 compares our proposed models with the current state-of-art ensemble of hand-crafted syntactic and frame-semantic features BIBREF16 , as well as past neural models from the literature, all using attention mechanisms — the Attentive Reader of BIBREF26 , Neural Reasoner of BIBREF27 and the HABCNN model family of BIBREF17 .  \n Question: what is the state of the art for ranking mc test answers?",
            "output": [
                "ensemble of hand-crafted syntactic and frame-semantic features BIBREF16"
            ]
        },
        {
            "id": "task460-362c76156074454b94be23e94161c48e",
            "input": "We demonstrate our approach on the task of answering open-domain fill-in-the-blank natural language questions. \n Question: What task do they evaluate on?",
            "output": [
                "Fill-in-the-blank natural language questions"
            ]
        },
        {
            "id": "task460-9d103965ba674956bbe6cb3c4998a898",
            "input": "In this section, we evaluate the proposed method intrinsically in terms of whether the co-occurrence matrix after the low-rank approximation is able to capture similar concepts on student response data sets, and also extrinsically in terms of the end task of summarization on all corpora. In the following experiments, summary length is set to be the average number of words in human summaries or less. An alternative way to evaluate the hypothesis is to let humans judge whether two bigrams are similar or not, which we leave for future work. \n Question: Do they quantitavely or qualitatively evalute the output of their low-rank approximation to verify the grouping of lexical items?",
            "output": [
                "They evaluate quantitatively."
            ]
        },
        {
            "id": "task460-42b3f75efe6642acb45de23a50780317",
            "input": "We used four classification algorithms: 1) Logistic regression, which is conventionally used in sentiment classification. Other three algorithms which are relatively new and has shown great results on sentiment classification types of problems are: 2) Naïve Bayes with SVM (NBSVM), 3) Extreme Gradient Boosting (XGBoost) and 4) FastText algorithm with Bidirectional LSTM (FastText-BiLSTM). \n Question: What state of the art models are used in the experiments?",
            "output": [
                "2) Naïve Bayes with SVM (NBSVM) 3) Extreme Gradient Boosting (XGBoost) 4) FastText algorithm with Bidirectional LSTM (FastText-BiLSTM)"
            ]
        },
        {
            "id": "task460-5fcdc4a086c744ec81b314871309e4ae",
            "input": "Similarly the agents utterances can be clustered to identify system responses. However, we argue that rather than treating user utterances and agents responses in an isolated manner, there is merit in jointly clustering them. There is adjacency information of these utterances that can be utilized to identify better user intents and system responses. \n Question: Do they study frequent user responses to help automate modelling of those?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-9c213c8d9b0f405793c1eaa1f8e7e982",
            "input": "We collected Japanese-Vietnamese parallel data from TED talks extracted from WIT3's corpus BIBREF15 .  \n Question: what japanese-vietnamese dataset do they use?",
            "output": [
                "WIT3's corpus"
            ]
        },
        {
            "id": "task460-212877680d9d4942b272dcba4a1c2b3c",
            "input": ". We could observe that compared with the base model, we have an improvement of 7.36% on accuracy and 9.69% on F1 score.  \n Question: How big are improvements of small-scale unbalanced datasets when sentence representation is enhanced with topic information?",
            "output": [
                "7.36% on accuracy and 9.69% on F1 score"
            ]
        },
        {
            "id": "task460-f5a66228a55c402d8f5cf921544bb090",
            "input": "For evaluation metrics, we used BLEU BIBREF8 and RIBES BIBREF9 to measure translation accuracy, and token-level delay to measure latency. We used Kytea BIBREF10 as a tokenize method for evaluations of Japanese translation accuracy. \n Question: Which metrics do they use to evaluate simultaneous translation?",
            "output": [
                "BLEU BIBREF8 RIBES BIBREF9 token-level delay"
            ]
        },
        {
            "id": "task460-746c3655719948b4bf9d9a987e0a8207",
            "input": "Task 1: Quora Duplicate Question Pair Detection Task 2: Ranking questions in Bing's People Also Ask \n Question: On which tasks do they test their conflict method?",
            "output": [
                "Task 1: Quora Duplicate Question Pair Detection Task 2: Ranking questions"
            ]
        },
        {
            "id": "task460-c32cb4119f9a4e3db53215ea2060e9b3",
            "input": "The first type of classifier we evaluate in this work are feedforward neural networks (DNNs) consisting of 3 hidden layers, each with 512 rectified linear units (ReLUs) with a softmax activation function. As a second classifier to evaluate, we use convolutional neural networks (CNNs) with 2 convolutional and max pooling layers, followed by 2 fully-connected ReLU layers with 512 nodes. \n Question: What type of models are used for classification?",
            "output": [
                "feedforward neural networks (DNNs) convolutional neural networks (CNNs)"
            ]
        },
        {
            "id": "task460-a360ae5d50b34e83b0a5319c7d759d1b",
            "input": "The DDI corpus contains thousands of XML files, each of which are constructed by several records. For a sentence containing INLINEFORM0 drugs, there are INLINEFORM1 drug pairs. \n Question: How big is the evaluated dataset?",
            "output": [
                "contains thousands of XML files, each of which are constructed by several records"
            ]
        },
        {
            "id": "task460-2c5dc87979a143e99d5c012926f0f058",
            "input": "We empirically test our approach on a series of experiments on WikiTableQuestions, to our knowledge the only dataset designed for this task. In Algorithm 1 we describe how logical forms are transformed into interpretable textual representations called \"paraphrases\". \n Question: What is the source of the paraphrases of the questions?",
            "output": [
                "WikiTableQuestions"
            ]
        },
        {
            "id": "task460-d3bceb4136f24c9dba237a675d945405",
            "input": "Dataset Probes and Construction\nOur probing methodology starts by constructing challenge datasets (Figure FIGREF1, yellow box) from a target set of knowledge resources. Each of our probing datasets consists of multiple-choice questions that include a question $\\textbf {q}$ and a set of answer choices or candidates $\\lbrace a_{1},...a_{N}\\rbrace $. This section describes in detail the 5 different datasets we build, which are drawn from two sources of expert knowledge, namely WordNet BIBREF35 and the GNU Collaborative International Dictionary of English (GCIDE). We describe each resource in turn, and explain how the resulting dataset probes, which we call WordNetQA and DictionaryQA, are constructed.\n\nFor convenience, we will describe each source of expert knowledge as a directed, edge-labeled graph $G$. The nodes of this graph are $\\mathcal {V} = \\mathcal {C} \\cup \\mathcal {W} \\cup \\mathcal {S} \\cup \\mathcal {D}$, where $\\mathcal {C}$ is a set of atomic concepts, $\\mathcal {W}$ a set of words, $\\mathcal {S}$ a set of sentences, and $\\mathcal {D}$ a set of definitions (see Table TABREF4 for details for WordNet and GCIDE). Each edge of $G$ is directed from an atomic concept in $\\mathcal {C}$ to another node in $V$, and is labeled with a relation, such as hypernym or isa$^\\uparrow $, from a set of relations $\\mathcal {R}$ (see Table TABREF4).\n\nWhen defining our probe question templates, it will be useful to view $G$ as a set of (relation, source, target) triples $\\mathcal {T} \\subseteq \\mathcal {R} \\times \\mathcal {C} \\times \\mathcal {V}$. Due to their origin in an expert knowledge source, such triples preserve semantic consistency. For instance, when the relation in a triple is def, the corresponding edge maps a concept in $\\mathcal {C}$ to a definition in $\\mathcal {D}$.\n\nTo construct probe datasets, we rely on two heuristic functions, defined below for each individual probe: $\\textsc {gen}_{\\mathcal {Q}}(\\tau )$, which generates gold question-answer pairs $(\\textbf {q},\\textbf {a})$ from a set of triples $\\tau \\subseteq \\mathcal {T}$ and question templates $\\mathcal {Q}$, and $\\textsc {distr}(\\tau ^{\\prime })$, which generates distractor answers choices $\\lbrace a^{\\prime }_{1},...a^{\\prime }_{N-1} \\rbrace $ based on another set of triples $\\tau ^{\\prime }$ (where usually $\\tau \\subset \\tau ^{\\prime }$). For brevity, we will use $\\textsc {gen}(\\tau )$ to denote $\\textsc {gen}_{\\mathcal {Q}}(\\tau )$, leaving question templates $\\mathcal {Q}$ implicit.\n\nDataset Probes and Construction ::: WordNetQA\nWordNet is an English lexical database consisting of around 117k concepts, which are organized into groups of synsets that each contain a gloss (i.e., a definition of the target concept), a set of representative English words (called lemmas), and, in around 33k synsets, example sentences. In addition, many synsets have ISA links to other synsets that express complex taxonomic relations. Figure FIGREF6 shows an example and Table TABREF4 summarizes how we formulate WordNet as a set of triples $\\mathcal {T}$ of various types. These triples together represent a directed, edge-labeled graph $G$. Our main motivation for using WordNet, as opposed to a resource such as ConceptNet BIBREF36, is the availability of glosses ($\\mathcal {D}$) and example sentences ($\\mathcal {S}$), which allows us to construct natural language questions that contextualize the types of concepts we want to probe.\n\nDataset Probes and Construction ::: WordNetQA ::: Example Generation @!START@$\\textsc {gen}(\\tau )$@!END@.\nWe build 4 individual datasets based on semantic relations native to WordNet (see BIBREF37): hypernymy (i.e., generalization or ISA reasoning up a taxonomy, ISA$^\\uparrow $), hyponymy (ISA$^{\\downarrow }$), synonymy, and definitions. To generate a set of questions in each case, we employ a number of rule templates $\\mathcal {Q}$ that operate over tuples. A subset of such templates is shown in Table TABREF8. The templates were designed to mimic naturalistic questions we observed in our science benchmarks.\n\nFor example, suppose we wish to create a question $\\textbf {q}$ about the definition of a target concept $c \\in \\mathcal {C}$. We first select a question template from $\\mathcal {Q}$ that first introduces the concept $c$ and its lemma $l \\in \\mathcal {W}$ in context using the example sentence $s \\in \\mathcal {S}$, and then asks to identify the corresponding WordNet gloss $d \\in \\mathcal {D}$, which serves as the gold answer $\\textbf {a}$. The same is done for ISA reasoning; each question about a hypernym/hyponym relation between two concepts $c \\rightarrow ^{\\uparrow /\\downarrow } c^{\\prime } \\in \\mathcal {T}_{i}$ (e.g., $\\texttt {dog} \\rightarrow ^{\\uparrow /\\downarrow } \\texttt {animal/terrier}$) first introduces a context for $c$ and then asks for an answer that identifies $c^{\\prime }$ (which is also provided with a gloss so as to contain all available context).\n\nIn the latter case, the rules $(\\texttt {isa}^{r},c,c^{\\prime }) \\in \\mathcal {T}_i$ in Table TABREF8 cover only direct ISA links from $c$ in direction $r \\in \\lbrace \\uparrow ,\\downarrow \\rbrace $. In practice, for each $c$ and direction $r$, we construct tests that cover the set HOPS$(c,r)$ of all direct as well as derived ISA relations of $c$:\n\nThis allows us to evaluate the extent to which models are able to handle complex forms of reasoning that require several inferential steps or hops.\n\nDataset Probes and Construction ::: WordNetQA ::: Distractor Generation: @!START@$\\textsc {distr}(\\tau ^{\\prime })$@!END@.\nAn example of how distractors are generated is shown in Figure FIGREF6, which relies on similar principles as above. For each concept $c$, we choose 4 distractor answers that are close in the WordNet semantic space. For example, when constructing hypernymy tests for $c$ from the set hops$(c,\\uparrow )$, we build distractors by drawing from $\\textsc {hops}(c,\\downarrow )$ (and vice versa), as well as from the $\\ell $-deep sister family of $c$, defined as follows. The 1-deep sister family is simply $c$'s siblings or sisters, i.e., the other children $\\tilde{c} \\ne c$ of the parent node $c^{\\prime }$ of $c$. For $\\ell > 1$, the $\\ell $-deep sister family also includes all descendants of each $\\tilde{c}$ up to $\\ell -1$ levels deep, denoted $\\textsc {hops}_{\\ell -1}(\\tilde{c},\\downarrow )$. Formally:\n\nFor definitions and synonyms we build distractors from all of these sets (with a similar restriction on the depth of sister distractors as noted above). In doing this, we can systematically investigate model performance on a wide range of distractor sets.\n\nDataset Probes and Construction ::: WordNetQA ::: Perturbations and Semantic Clusters\nBased on how we generate data, for each concept $c$ (i.e., atomic WordNet synset) and probe type (i.e., definitions, hypernymy, etc.), we have a wide variety of questions related to $c$ that manipulate 1) the complexity of reasoning that is involved (e.g., the number of inferential hops) and; 2) the types of distractors (or distractor perturbations) that are employed. We call such sets semantic clusters. As we describe in the next section, semantic clusters allow us to devise new types of evaluation that reveal whether models have comprehensive and consistent knowledge of target concepts (e.g., evaluating whether a model can correctly answer several questions associated with a concept, as opposed to a few disjoint instances).\n\nDetails of the individual datasets are shown in Table TABREF12. From these sets, we follow BIBREF22 in allocating a maximum of 3k examples for training and reserve the rest for development and testing. Since we are interested in probing, having large held-out sets allows us to do detailed analysis and cluster-based evaluation.\n\nDataset Probes and Construction ::: DictionaryQA\nThe DictionaryQA dataset is created from the GCIDE dictionary, which is a comprehensive open-source English dictionary built largely from the Webster's Revised Unabridged Dictionary BIBREF38. Each entry consists of a word, its part-of-speech, its definition, and an optional example sentence (see Table TABREF14). Overall, 33k entries (out of a total of 155k) contain example sentences/usages. As with the WordNet probes, we focus on this subset so as to contextualize each word being probed. In contrast to WordNet, GCIDE does not have ISA relations or explicit synsets, so we take each unique entry to be a distinct sense. We then use the dictionary entries to create a probe that centers around word-sense disambiguation, as described below.\n\nDataset Probes and Construction ::: DictionaryQA ::: Example and Distractor Generation.\nTo generate gold questions and answers, we use the same generation templates for definitions exemplified in Figure TABREF8 for WordNetQA. To generate distractors, we simply take alternative definitions for the target words that represent a different word sense (e.g., the alternative definitions of gift shown in Table TABREF14), as well as randomly chosen definitions if needed to create a 5-way multiple choice question. As above, we reserve a maximum of 3k examples for training. Since we have only 9k examples in total in this dataset (see WordSense in Table TABREF12), we also reserve 3k each for development and testing.\n\nWe note that initial attempts to build this dataset through standard random splitting gave rise to certain systematic biases that were exploited by the choice-only baseline models described in the next section, and hence inflated overall model scores. After several efforts at filtering we found that, among other factors, using definitions from entries without example sentences as distractors (e.g., the first two entries in Table TABREF14) had a surprising correlation with such biases. This suggests that possible biases involving differences between dictionary entries with and without examples can taint the resulting automatically generated MCQA dataset (for more discussion on the pitfalls involved with automatic dataset construction, see Section SECREF5). \n Question: Are the automatically constructed datasets subject to quality control?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-56b080c585b3483eb221f5640232339e",
            "input": "First, we compare its performance with other BERT-models and state-of-the-art systems in sentiment analysis, to show its performance for classification tasks.  Second, we compare its performance in a recent Dutch language task, namely the disambiguation of demonstrative pronouns, which allows us to additionally compare the zero-shot performance of our and other BERT models, i.e. using only the pre-trained model without any fine-tuning. \n Question: What language tasks did they experiment on?",
            "output": [
                "sentiment analysis the disambiguation of demonstrative pronouns,"
            ]
        },
        {
            "id": "task460-83e4e7f25b7445ffbb33bce6bad09fdd",
            "input": "We find that while optimizing its reformulations to adapt to the language of the QA system, AQA diverges from well structured language in favour of less fluent, but more effective, classic information retrieval (IR) query operations.  \n Question: What is the difference in findings of Buck et al? It looks like the same conclusion was mentioned in Buck et al..",
            "output": [
                "AQA diverges from well structured language in favour of less fluent, but more effective, classic information retrieval (IR) query operations"
            ]
        },
        {
            "id": "task460-5b15375c221146d98c48d2c7c1020e48",
            "input": "Besides, we also use two of its adversarial sets, namely AddSent and AddOneSent BIBREF6 , to evaluate the robustness to noise of MRC models. The passages in the adversarial sets contain misleading sentences, which are aimed at distracting MRC models.  Specifically, we not only evaluate the performance of KAR on the development set and the test set, but also do this on the adversarial sets.  \n Question: How do the authors examine whether a model is robust to noise or not?",
            "output": [
                "By evaluating their model on adversarial sets containing misleading sentences"
            ]
        },
        {
            "id": "task460-45b352ee805b4a269e281d6a7694f988",
            "input": "In test batch 4, our system (called FACTOIDS) achieved highest recall score of ‘0.7033’ but low precision of 0.1119, leaving open the question of how could we have better balanced the two measures. \n Question: What was their highest recall score?",
            "output": [
                "0.7033"
            ]
        },
        {
            "id": "task460-d3c4d4ef9b53446dae05fe570815d507",
            "input": "Embedding: We developed different variations of our models with a simple lookup table embeddings learned from scratch and using high-performance contextual embeddings, which are ELMo BIBREF11, BERT BIBREF16 and ClinicalBERT BIBREF13 (trained and provided by the authors). \n Question: What embeddings are used?",
            "output": [
                " simple lookup table embeddings learned from scratch using high-performance contextual embeddings, which are ELMo BIBREF11, BERT BIBREF16 and ClinicalBERT BIBREF13"
            ]
        },
        {
            "id": "task460-e8485906dab64cf68202a8d4889f6f98",
            "input": "With this approach we improve the standard BERT models by up to four percentage points in accuracy. \n Question: By how much do they outperform standard BERT?",
            "output": [
                "up to four percentage points in accuracy"
            ]
        },
        {
            "id": "task460-61e93c0ebde64dc189dbd37c9706d3e2",
            "input": "Of the 144 schemas in the collection at WinogradSchemas there are 33 that can plausibly be translated this way. \n Question: Did they collect their own datasets?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-694662e7115246919f75b4a4f478f08b",
            "input": "DNN based models coupled with transfer learning beat the best-known results for all three datasets. Previous best F1 scores for Wikipedia BIBREF4 and Twitter BIBREF8 datasets were 0.68 and 0.93 respectively. We achieve F1 scores of 0.94 for both these datasets using BLSTM with attention and feature level transfer learning (Table TABREF25 ). For Formspring dataset, authors have not reported F1 score. Their method has accuracy score of 78.5% BIBREF2 . We achieve F1 score of 0.95 with accuracy score of 98% for the same dataset. \n Question: What were their performance results?",
            "output": [
                "best model achieves 0.94 F1 score for Wikipedia and Twitter datasets and 0.95 F1 on Formspring dataset"
            ]
        },
        {
            "id": "task460-2f9acf0f584f4f14bb0da09b656ce666",
            "input": "Hate memes were retrieved from Google Images with a downloading tool. We used the following queries to collect a total of 1,695 hate memes: racist meme (643 memes), jew meme (551 memes), and muslim meme (501 Memes). Non-hate memes were obtained from the Reddit Memes Dataset . \n Question: What is the source of memes?",
            "output": [
                "Google Images Reddit Memes Dataset"
            ]
        },
        {
            "id": "task460-17e4f7166948487e9e67548bd0eac671",
            "input": "The baseline classifier uses a linear Support Vector Machine BIBREF7 , which is suited for a high number of features.  \n Question: What baseline is used?",
            "output": [
                "SVM"
            ]
        },
        {
            "id": "task460-2e785b13054d4b0d80823283f6ce0d5e",
            "input": "We see that parent quality is a simple yet effective feature and SVM model with this feature can achieve significantly higher ($p<0.001$) F1 score ($46.61\\%$) than distance from the thesis and linguistic features. Although the BiLSTM model with attention and FastText baselines performs better than the SVM with distance from the thesis and linguistic features, it has similar performance to the parent quality baseline. We find that the flat representation of the context achieves the highest F1 score. It may be more difficult for the models with a larger number of parameters to perform better than the flat representation since the dataset is small. We also observe that modeling 3 claims on the argument path before the target claim achieves the best F1 score ($55.98\\%$). \n Question: How better are results compared to baseline models?",
            "output": [
                "F1 score of best authors' model is 55.98 compared to BiLSTM and FastText that have F1 score slighlty higher than 46.61."
            ]
        },
        {
            "id": "task460-bcff4ce3107e41f79dc077ea6ba41a7f",
            "input": "This version highly surpasses the previous state of the art while still having fewer parameters than previous work. \n Question: Do they compare results against state-of-the-art language models?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-b79f4a6116dd47d5b46c8ab8b4dc3fe0",
            "input": "Table TABREF16 shows the recognition performance of naive multilingual approach using BLSTMP and VGG model against a monolingual model trained with BLSTMP. The results clearly indicate that having a better architecture such as VGG-BLSTM helps in improving multilingual performance. We used a character-level RNNLM, which was trained with 2-layer LSTM on character sequences. \n Question: What architectures are explored to improve the seq2seq model?",
            "output": [
                "VGG-BLSTM character-level RNNLM"
            ]
        },
        {
            "id": "task460-6032a6cbf65f49569febc9914325ff06",
            "input": "In this method, we simply concatenate the corpora of multiple domains with two small modifications: a. Appending the domain tag “<2domain>\" to the source sentences of the respective corpora.  \n Question: How did they use the domain tags?",
            "output": [
                "Appending the domain tag “<2domain>\" to the source sentences of the respective corpora"
            ]
        },
        {
            "id": "task460-4c81ec0bf8ed43df85482c2bdec5d03e",
            "input": "To fine-tune BERT, we used a proprietary corpus that consists of hundreds of thousands of legal agreements. \n Question: How big is dataset used for fine-tuning BERT?",
            "output": [
                "hundreds of thousands of legal agreements"
            ]
        },
        {
            "id": "task460-22637a29daa9447895dc0b1a795597d9",
            "input": "However, as shown in Figure 1 , to assess the similarity between two research papers, a more effective strategy would compare and align (via local-weighting) individual important words (keywords) within a pair of abstracts, while information from other words (e.g., stop words) that tend to be less relevant can be effectively ignored (down-weighted).  We propose to learn a semantic-aware Network Embedding (NE) that incorporates word-level alignment features abstracted from text sequences associated with vertex pairs. Given a pair of sentences, our model first aligns each word within one sentence with keywords from the other sentence (adaptively up-weighted via an attention mechanism), producing a set of fine-grained matching vectors \n Question: What text sequences are associated with each vertex?",
            "output": [
                "abstracts sentences"
            ]
        },
        {
            "id": "task460-c9cf398f1c194f03b2a65c1b397ff79a",
            "input": "RNN-based NMT model\nWe first briefly introduce the RNN based Neural Machine Translation (RNN-based NMT) model. Transformer-NMT\nRecently, the Transformer model BIBREF4 has made remarkable progress in machine translation. This model contains a multi-head self-attention encoder and a multi-head self-attention decoder. \n Question: what NMT models did they compare with?",
            "output": [
                "RNN-based NMT model Transformer-NMT"
            ]
        },
        {
            "id": "task460-c7f62cb0a0364461b7d8128f9a45c3c6",
            "input": "Given raw audio samples INLINEFORM0 , we apply the encoder network INLINEFORM1 which we parameterize as a five-layer convolutional network similar to BIBREF15 . Next, we apply the context network INLINEFORM0 to the output of the encoder network to mix multiple latent representations INLINEFORM1 into a single contextualized tensor INLINEFORM2 for a receptive field size INLINEFORM3 . The context network has seven layers and each layer has kernel size three and stride one.  \n Question: How many convolutional layers does their model have?",
            "output": [
                "wav2vec has 12 convolutional layers"
            ]
        },
        {
            "id": "task460-0445c903ce5f4467acaffa392fab377f",
            "input": "image feature pre-selection part which models the tendency where people focus to ask questions We propose to perform saliency-like pre-selection operation to alleviate the problems and model the RoI patterns. The image is first divided into $g\\times g$ grids as illustrated in Figure. 2 . Taking $m\\times m$ grids as a region, with $s$ grids as the stride, we obtain $n\\times n$ regions, where $n=\\left\\lfloor \\frac{g-m}{s}\\right\\rfloor +1$ . We then feed the regions to a pre-trained ResNet BIBREF24 deep convolutional neural network to produce $n\\times n\\times d_I$ -dimensional region features, where $d_I$ is the dimension of feature from the layer before the last fully-connected layer. \n Question: Does the new system utilize pre-extracted bounding boxes and/or features?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-04d85a165dbb4802ad78dd7f6020c500",
            "input": " We use three creative English datasets with distinct linguistic characteristics: (1) A corpus of 740 classical and contemporary English poems, (2) a corpus of 14950 metaphor sentences retrieved from a metaphor database website and (3) a corpus of 1500 song lyrics ranging across genres.  \n Question: Do they report results only on English data?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-fe25721d34084bcd8d6e636e99dde9a2",
            "input": "We test our proposed approach for binary classification on either sarcasm or irony, on seven benchmark datasets retrieved from different media sources. Below we describe each dataset, please see Table TABREF1 below for a summary. Twitter: We use the Twitter dataset provided for the SemEval 2018 Task 3, Irony Detection in English Tweets BIBREF18  Finally, we use the dataset by BIBREF20 , who collected a user self-annotated corpus of tweets with the #sarcasm hashtag. Reddit: BIBREF21 collected SARC, a corpus comprising of 600.000 sarcastic comments on Reddit. We use main subset, SARC 2.0, and the political subset, SARC 2.0 pol. Online Dialogues: We utilize the Sarcasm Corpus V1 (SC-V1) and the Sarcasm Corpus V2 (SC-V2), which are subsets of the Internet Argument Corpus (IAC). Compared to other datasets in our selection, these differ mainly in text length and structure complexity BIBREF22 .  To augment each dataset with our external data, we first filter out tweets that are not in English using language guessing systems. \n Question: Do they evaluate only on English?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-2257b7a8fc26468c8fe0e66773aee232",
            "input": "In this paper, we propose an improved RNN-T model with language bias to alleviate the problem. The model is trained to predict language IDs as well as the subwords. To ensure the model can learn CS information, we add language IDs in the CS point of transcription, as illustrated in Fig. 1. \n Question: How do they obtain language identities?",
            "output": [
                "model is trained to predict language IDs as well as the subwords we add language IDs in the CS point of transcriptio"
            ]
        },
        {
            "id": "task460-9fb591f952484024b2dcd74df105dc3a",
            "input": "Our in-house dataset includes manually annotated RE data for 6 languages: English, German, Spanish, Italian, Japanese and Portuguese. It defines 56 entity types (e.g., Person, Organization, Geo-Political Entity, Location, Facility, Time, Event_Violence, etc.) and 53 relation types between the entities (e.g., AgentOf, LocatedAt, PartOf, TimeOf, AffectedBy, etc.). The ACE05 dataset includes manually annotated RE data for 3 languages: English, Arabic and Chinese. It defines 7 entity types (Person, Organization, Geo-Political Entity, Location, Facility, Weapon, Vehicle) and 6 relation types between the entities (Agent-Artifact, General-Affiliation, ORG-Affiliation, Part-Whole, Personal-Social, Physical). the ACE (Automatic Content Extraction) 2005 multilingual dataset BIBREF11. \n Question: What datasets are used?",
            "output": [
                "in-house dataset ACE05 dataset "
            ]
        },
        {
            "id": "task460-d0b768c276dd428d83151b85b4919cc0",
            "input": "CNN can also be employed on the sarcasm datasets in order to identify sarcastic and non-sarcastic tweets. We term the features extracted from this network baseline features, the method as baseline method and the CNN architecture used in this baseline method as baseline CNN. Since the fully-connected layer has 100 neurons, we have 100 baseline features in our experiment.  \n Question: What are the network's baseline features?",
            "output": [
                " The features extracted from CNN."
            ]
        },
        {
            "id": "task460-11708912adbe4d2182319df557640dfa",
            "input": "The reports are published by Météo France and the Met Office, its British counterpart. They are publicly available on the respective websites of the organizations. Both corpora span on the same period as the corresponding time series and given their daily nature, it yields a total of 4,261 and 4,748 documents respectively. \n Question: How big is dataset used for training/testing?",
            "output": [
                "4,261  days for France and 4,748 for the UK"
            ]
        },
        {
            "id": "task460-a030ebeb728a4cd282489a12a3fa556a",
            "input": "For testing, we also applied the TongueNet on the UBC database BIBREF14 without any training to see the generalization ability of the model. \n Question: What previously annotated databases are available?",
            "output": [
                "the UBC database BIBREF14"
            ]
        },
        {
            "id": "task460-5027c4c04df9448f81941af99b044931",
            "input": "Strategy-based methods depart from the pre-training stage, seeking to take advantage of the pre-trained models during the process of target task learning. The approaches include fine-tuning schedules that elaborately design the control of learning rates for optimization, proxy tasks that leverage labeled data to help the pre-trained model better fit the target data distribution, and knowledge distillation approaches that ditch the paradigm of initialization with pre-trained parameters by adopting the pre-trained model as a teacher network. \n Question: How strategy-based methods handle obstacles in NLG?",
            "output": [
                "fine-tuning schedules that elaborately design the control of learning rates for optimization proxy tasks that leverage labeled data to help the pre-trained model better fit the target data distribution knowledge distillation approaches that ditch the paradigm of initialization with pre-trained parameters by adopting the pre-trained model as a teacher network"
            ]
        },
        {
            "id": "task460-95de970f71be46808e8d744d0f83be45",
            "input": "We present in this section the baseline model from See et al. See2017 trained on the CNN/Daily Mail dataset. The baseline model is a deep sequence-to-sequence encoder/decoder model with attention. The encoder is a bidirectional Long-Short Term Memory(LSTM) cell BIBREF14 and the decoder a single LSTM cell with attention mechanism. The attention mechanism is computed as in BIBREF9 and we use a greedy search for decoding. We train end-to-end including the words embeddings. The embedding size used is of 128 and the hidden state size of the LSTM cells is of 254. \n Question: Which baselines did they compare?",
            "output": [
                "The baseline model is a deep sequence-to-sequence encoder/decoder model with attention. The encoder is a bidirectional Long-Short Term Memory(LSTM) cell BIBREF14 and the decoder a single LSTM cell with attention mechanism. The attention mechanism is computed as in BIBREF9 and we use a greedy search for decoding. We train end-to-end including the words embeddings. The embedding size used is of 128 and the hidden state size of the LSTM cells is of 254."
            ]
        },
        {
            "id": "task460-c441159d734c416e98146d5700633666",
            "input": "As mentioned in subsec:datasets, all the word-similarity datasets contain pairs of words annotated with similarity or relatedness scores, although this difference is not always explicit. Below we provide some details for each.\n\nMEN contains 3000 annotated word pairs with integer scores ranging from 0 to 50. Words correspond to image labels appearing in the ESP-Game and MIRFLICKR-1M image datasets.\n\nMTurk287 contains 287 annotated pairs with scores ranging from 1.0 to 5.0. It was created from words appearing in both DBpedia and in news articles from The New York Times.\n\nMTurk771 contains 771 annotated pairs with scores ranging from 1.0 to 5.0, with words having synonymy, holonymy or meronymy relationships sampled from WordNet BIBREF56 .\n\nRG contains 65 annotated pairs with scores ranging from 0.0 to 4.0 representing “similarity of meaning”.\n\nRW contains 2034 pairs of words annotated with similarity scores in a scale from 0 to 10. The words included in this dataset were obtained from Wikipedia based on their frequency, and later filtered depending on their WordNet synsets, including synonymy, hyperonymy, hyponymy, holonymy and meronymy. This dataset was created with the purpose of testing how well models can represent rare and complex words.\n\nSimLex999 contains 999 word pairs annotated with similarity scores ranging from 0 to 10. In this case the authors explicitly considered similarity and not relatedness, addressing the shortcomings of datasets that do not, such as MEN and WS353. Words include nouns, adjectives and verbs.\n\nSimVerb3500 contains 3500 verb pairs annotated with similarity scores ranging from 0 to 10. Verbs were obtained from the USF free association database BIBREF66 , and VerbNet BIBREF63 . This dataset was created to address the lack of representativity of verbs in SimLex999, and the fact that, at the time of creation, the best performing models had already surpassed inter-annotator agreement in verb similarity evaluation resources. Like SimLex999, this dataset also explicitly considers similarity as opposed to relatedness.\n\nWS353 contains 353 word pairs annotated with similarity scores from 0 to 10.\n\nWS353R is a subset of WS353 containing 252 word pairs annotated with relatedness scores. This dataset was created by asking humans to classify each WS353 word pair into one of the following classes: synonyms, antonyms, identical, hyperonym-hyponym, hyponym-hyperonym, holonym-meronym, meronym-holonym, and none-of-the-above. These annotations were later used to group the pairs into: similar pairs (synonyms, antonyms, identical, hyperonym-hyponym, and hyponym-hyperonym), related pairs (holonym-meronym, meronym-holonym, and none-of-the-above with a human similarity score greater than 5), and unrelated pairs (classified as none-of-the-above with a similarity score less than or equal to 5). This dataset is composed by the union of related and unrelated pairs.\n\nWS353S is another subset of WS353 containing 203 word pairs annotated with similarity scores. This dataset is composed by the union of similar and unrelated pairs, as described previously. \n Question: Which similarity datasets do they use?",
            "output": [
                "MEN MTurk287 MTurk771 RG RW SimLex999 SimVerb3500 WS353 WS353R WS353S"
            ]
        },
        {
            "id": "task460-4b542277c18e4248911e84c0f0917277",
            "input": "With an intention to exploit the contribution of hashtag use to stance detection, we have also used the existence of hashtags in tweets as an additional feature to unigrams. \n Question: What are hashtag features?",
            "output": [
                "hashtag features contain whether there is any hashtag in the tweet"
            ]
        },
        {
            "id": "task460-d3ab9feb3c0c42139ae98af6b9863347",
            "input": "Because our approach is specifically intended to yield sentences that are grammatical, we additionally consider the following log ratio (i.e., the grammatical phrase over the ungrammatical phrase): \n Question: How do they measure grammaticality?",
            "output": [
                "by calculating log ratio of grammatical phrase over ungrammatical phrase"
            ]
        },
        {
            "id": "task460-7cb8d8655a67483db59be2d81960df18",
            "input": "We first conduct experiments to compare the performance of FlowSeq with strong baseline models, including NAT w/ Fertility BIBREF6, NAT-IR BIBREF7, NAT-REG BIBREF25, LV NAR BIBREF26, CTC Loss BIBREF27, and CMLM BIBREF8. \n Question: What non autoregressive NMT models are used for comparison?",
            "output": [
                "NAT w/ Fertility NAT-IR NAT-REG LV NAR CTC Loss CMLM"
            ]
        },
        {
            "id": "task460-64b09a62e69f4514bc8bdc895303c3f8",
            "input": "We evaluated the proposed model using the publicly available LibriSpeech ASR corpus BIBREF23. The LibriSpeech dataset consists of 970 hours of audio data with corresponding text transcripts (around 10M word tokens) and an additional 800M word token text only dataset. \n Question: How big is LibriSpeech dataset?",
            "output": [
                "970 hours of audio data with corresponding text transcripts (around 10M word tokens) and an additional 800M word token text only dataset"
            ]
        },
        {
            "id": "task460-92280383bf5d4df7aeb43ddce33e3f59",
            "input": "We used the Levenshtein distance metric BIBREF8 implemented in Apache Lucene library BIBREF9 . Another simple approach is the aforementioned diacritical swapping, which is a term that we introduce here for referring to a solution inspired by the work of BIBREF4 . A promising method, adapted from work on correcting texts by English language learners BIBREF11 , expands on the concept of selecting a correction nearest to the spelling error according to some notion of distance. Here, the Levenshtein distance is used in a weighted sum to cosine distance between word vectors. Our ELMo-augmented LSTM is bidirectional. \n Question: What methods are tested in PIEWi?",
            "output": [
                "Levenshtein distance metric BIBREF8 diacritical swapping Levenshtein distance is used in a weighted sum to cosine distance between word vectors ELMo-augmented LSTM"
            ]
        },
        {
            "id": "task460-479947b26d6746229f7806a68b746f80",
            "input": "Knowledge Base Question Answering (KBQA) systems answer questions by obtaining information from KB tuples BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 . \n Question: What is te core component for KBQA?",
            "output": [
                "answer questions by obtaining information from KB tuples "
            ]
        },
        {
            "id": "task460-179a911f448245a085b5708074a4c7ac",
            "input": "Outlier Detection. The Outlier Detection task BIBREF0 is to determine which word in a list INLINEFORM0 of INLINEFORM1 words is unrelated to the other INLINEFORM2 which were chosen to be related. For each INLINEFORM3 , one can compute its compactness score INLINEFORM4 , which is the compactness of INLINEFORM5 . Sentiment analysis. We also consider sentiment analysis as described by BIBREF31 . \n Question: Do they test their word embeddings on downstream tasks?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-b5a3e76c46644c61ade085f8a73b8367",
            "input": "Other Timex3 mentions are more ambiguous so we use a distant supervision approach. Phrases like “currently\" and “today's\" tend to occur near Events that overlap the current document creation time, while “ago\" or “ INLINEFORM0 -years\" indicate past events. These dominant temporal associations can be learned from training data and then used to label Timex3s. Finally, we define a logistic regression rule to predict entity DocRelTime values as well as specify a linear skip-chain factor over Event mentions and their nearest Timex3 neighbor, encoding the baseline system heuristic directly as an inference rule. \n Question: How do they obtain distant supervision rules for predicting relations?",
            "output": [
                "dominant temporal associations can be learned from training data"
            ]
        },
        {
            "id": "task460-b206f4519c8848f2952cbf7b914b7af3",
            "input": "We would expect that explanation performance should correlate with prediction performance. Since Possible-answer knowledge is primarily needed to decide if the net has enough information to answer the challenge question without guessing and relevant-variable knowledge is needed for the net to know what to query, we analyzed the network's performance on querying and answering separately. The memory network has particular difficulty learning to query relevant variables, reaching only about .5 accuracy when querying. At the same time, it learns to answer very well, reaching over .9 accuracy there. Since these two parts of the interaction are what we ask it to explain in the two modes, we find that the quality of the explanations strongly correlates with the quality of the algorithm executed by the network. \n Question: How do they measure correlation between the prediction and explanation quality?",
            "output": [
                "They look at the performance accuracy of explanation and the prediction performance"
            ]
        },
        {
            "id": "task460-fd52a2719da84a6f9bab6738dd44d7b8",
            "input": "We address the robustness problem on top of GE-FL BIBREF0 , a GE method which leverages labeled features as prior knowledge. \n Question: What background knowledge do they leverage?",
            "output": [
                "labeled features"
            ]
        },
        {
            "id": "task460-a83c3615536e490aa7d06dd9faf6f3ef",
            "input": "The greatest challenges with the competition arise directly from the potential for ongoing mixed-initiative multi-turn dialogues, which do not follow a particular plan or pursue a particular fixed information need.  This paper describes some of the lessons we learned building SlugBot for the 2017 Alexa Prize, particularly focusing on the challenges of integrating content found via search with content from structured data in order to carry on an ongoing, coherent, open-domain, mixed-initiative conversation More challenging is that at each system turn, there are a large number of conversational moves that are possible.  Finally, most other domains do not have such high quality structured data available, leaving us to develop or try to rely on more general models of discourse coherence.  Search cannot be used effectively here without constructing an appropriate query, or knowing in advance where plot information might be available. In a real-time system, live search may not be able to achieve the required speed and efficiency, so preprocessing or caching of relevant information may be necessary.  \n Question: Why mixed initiative multi-turn dialogs are the greatest challenge in building open-domain conversational agents?",
            "output": [
                "do not follow a particular plan or pursue a particular fixed information need  integrating content found via search with content from structured data at each system turn, there are a large number of conversational moves that are possible most other domains do not have such high quality structured data available live search may not be able to achieve the required speed and efficiency"
            ]
        },
        {
            "id": "task460-241de33da00a499d989369fcf8d7839b",
            "input": "The result on original WikiQA indicates that all three transfer learning methods not only do not improve the results but also hurt the F1-score. These are because other datasets could not add new information to the original dataset or they apparently add some redundant information which are dissimilar to the target dataset. \n Question: Do transferring hurt the performance is the corpora are not related?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-e272c3ead63f4fb6b12854d91595faac",
            "input": "We consider 5 major profile attributes for further analysis. These attributes are username, display name, profile image, location and description respectively. \n Question: What profile metadata is used for this analysis?",
            "output": [
                "username display name profile image location description"
            ]
        },
        {
            "id": "task460-41cae39abcce4f18a43666cee5a4a929",
            "input": "In order to evaluate the models trained on generated data, we manually annotated a named entities dataset comprising 53453 tokens and 2566 sentences selected from over 250 news texts from ilur.am. During annotation, we generally relied on categories and guidelines assembled by BBN Technologies for TREC 2002 question answering track. \n Question: did they use a crowdsourcing platform for manual annotations?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-fb3b4d393784460e81dfc7b16ce1dd03",
            "input": "Furthermore, the basic seq2seq assumes a strict order between generated tokens, but in reality, we should not severely punish the model when it predicts the correct tokens in the wrong order. \n Question: Do they impose any grammatical constraints over the generated output?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-1e183aa86ead4bdc90a4b1b8594f47a1",
            "input": "While the dataset was collected with simulated environments, no structure was imposed on the navigation instructions while crowd-sourcing data. Thus, many instructions in our dataset are ambiguous. Moreover, the order of the behaviors in the instructions is not always the same. For instance, a person said “turn right and advance” to describe part of a route, while another person said “go straight after turning right” in a similar situation. The high variability present in the natural language descriptions of our dataset makes the problem of decoding instructions into behaviors not trivial. See Appendix A of the supplementary material for additional details on our data collection effort. \n Question: What language is the experiment done in?",
            "output": [
                "english language"
            ]
        },
        {
            "id": "task460-bb2fdbe91b1e4726874c9862672d5c67",
            "input": "The crowdworkers were located in the US and hired on the BIBREF22 platform. \n Question: Who are the crowdworkers?",
            "output": [
                "people in the US that use Amazon Mechanical Turk"
            ]
        },
        {
            "id": "task460-b7c4ecaffff7477aa768b9b6c51c1b89",
            "input": "TF-IDF (Lexical, Non-Neural) is an important unsupervised baseline. NVDM (Lexical, Neural) is a VAE-based approach for document modeling BIBREF10 . \n Question: Which lexicon-based models did they compare with?",
            "output": [
                "TF-IDF NVDM"
            ]
        },
        {
            "id": "task460-ad5a1f9039404df7ac4ac95328961699",
            "input": "In this paper, we use three encoders (NBOW, LSTM and attentive LSTM) to model the text descriptions. \n Question: What neural models are used to encode the text?",
            "output": [
                "NBOW, LSTM, attentive LSTM"
            ]
        },
        {
            "id": "task460-5ff09fc05f7147c48550851a7cfa4e1f",
            "input": "Ethnicity/race\nOne interesting pattern is that the ethnicity/race of babies doesn't seem to be mentioned unless the baby is black or asian. In other words: white seems to be the default, and others seem to be marked. How can we tell whether or not the data is actually biased?\n\n The numbers in Table TABREF22 are striking: there seems to be a real, systematic difference in ethnicity marking between the groups. We can take one step further and look at all the 697 pictures with the word `baby' in it. If there turn out to be disproportionately many white babies, this strengthens the conclusion that the dataset is biased. \n Question: What biases are found in the dataset?",
            "output": [
                "Ethnic bias"
            ]
        },
        {
            "id": "task460-d970535db8624a5082313c6bf9e8aa88",
            "input": "We use MS COCO, Bing and Flickr datasets from BIBREF26 to train the model that generates questions. These datasets contain natural questions about images with the purpose of knowing more about the picture. As can be seen in the Figure FIGREF8, questions cannot be answered by only looking at the image. Each source contains 5,000 images with 5 questions per image, adding a total of 15,000 images with 75,000 questions. \n Question: How many questions per image on average are available in dataset?",
            "output": [
                "5 questions per image"
            ]
        },
        {
            "id": "task460-04ba5f13065d4603a21fff64f1c579ba",
            "input": "The performance on discriminating between offensive (OFF) and non-offensive (NOT) posts is reported in Table TABREF18 . We can see that all systems perform significantly better than chance, with the neural models being substantially better than the SVM. The CNN outperforms the RNN model, achieving a macro-F1 score of 0.80. The CNN system achieved higher performance in this experiment compared to the BiLSTM, with a macro-F1 score of 0.69. All systems performed better at identifying target and threats (TIN) than untargeted offenses (UNT). \n Question: What is the best performing model?",
            "output": [
                "CNN "
            ]
        },
        {
            "id": "task460-29d1344a81d740868006793da85e2353",
            "input": "For DQA four participants answered each question, therefore we took the average INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 values over the four evaluators as the result per question. The detailed answers by the participants and available online. To assess the correctness of the answers given both by participants in the DQA experiments, and by the QALD system, we use the classic information retrieval metrics of precision (P), recall (R), and F1. INLINEFORM0 measures the fraction of relevant (correct) answer (items) given versus all answers (answer items) given. \n Question: Do they test performance of their approaches using human judgements?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-cd51b3b371794174ace84e83497e845a",
            "input": "To further improve the performance of our models, we introduce in-house labeled data that we use to fine-tune BERT. For the gender classification task, we manually label an in-house dataset of 1,100 users with gender tags, including 550 female users, 550 male users. We obtain 162,829 tweets by crawling the 1,100 users' timelines. For the dialect identification task, we randomly sample 20,000 tweets for each class from an in-house dataset gold labeled with the same 15 classes as the shared task. \n Question: What are the in-house data employed?",
            "output": [
                "we manually label an in-house dataset of 1,100 users with gender tags we randomly sample 20,000 tweets for each class from an in-house dataset gold labeled with the same 15 classes as the shared task"
            ]
        },
        {
            "id": "task460-19ff24280a324d85a1f4b28fe3c029f7",
            "input": "Since the training data consists only of utterance-denotation pairs, the ranker is trained to maximize the log-likelihood of the correct answer $z$ by treating logical forms as a latent variable The role of the ranker is to score the candidate logical forms generated by the parser; at test time, the logical form receiving the highest score will be used for execution. The ranker is a discriminative log-linear model over logical form $y$ given utterance $x$ :  \n Question: How does the model compute the likelihood of executing to the correction semantic denotation?",
            "output": [
                "By treating logical forms as a latent variable and training a discriminative log-linear model over logical form y given x."
            ]
        },
        {
            "id": "task460-5e0558a38c2b4fd7b697f310588597ba",
            "input": "We validate the performance of the proposed s2sL by providing the preliminary results obtained on two different tasks namely, Speech/Music discrimination and emotion classification. We considered the GTZAN Music-Speech dataset [17], consisting of 120 audio files (60 speech and 60 music), for task of classifying speech and music. Each audio file (of 2 seconds duration) is represented using a 13-dimensional mel-frequency cepstral coefficient (MFCC) vector, where each MFCC vector is the average of all the frame level (frame size of 30 msec and an overlap of 10 msec) MFCC vectors. It is to be noted that our main intention for this task is not better feature selection, but to demonstrate the effectiveness of our approach, in particular for low data scenarios. The standard Berlin speech emotion database (EMO-DB) [18] consisting of 535 utterances corresponding to 7 different emotions is considered for the task of emotion classification.  \n Question: Up to how many samples do they experiment with?",
            "output": [
                "535"
            ]
        },
        {
            "id": "task460-1f9aaae91ac04306a943bce36d325023",
            "input": "The task, as framed above, requires to detect the semantic change between two corpora. The two corpora used in the shared task correspond to the diachronic corpus pair from BIBREF0: DTA18 and DTA19. They consist of subparts of DTA corpus BIBREF11 which is a freely available lemmatized, POS-tagged and spelling-normalized diachronic corpus of German containing texts from the 16th to the 20th century. DTA18 contains 26 million sentences published between 1750-1799 and DTA19 40 million between 1850-1899. \n Question: What is the corpus used for the task?",
            "output": [
                "DTA18 DTA19"
            ]
        },
        {
            "id": "task460-927b946af52f43cda4fe0c7dd18fd170",
            "input": "We compared several summarization methods which can be categorized into three groups: unsupervised, non-neural supervised, and neural supervised methods.  As a baseline, we used Lead-N which selects INLINEFORM0 leading sentences as the summary. For all methods, we extracted 3 sentences as the summary since it is the median number of sentences in the gold summaries that we found in our exploratory analysis. Lead-3 baseline performs really well and outperforms almost all the other models, which is not surprising and even consistent with other work that for news summarization, Lead-N baseline is surprisingly hard to beat. \n Question: What was the best performing baseline?",
            "output": [
                "Lead-3"
            ]
        },
        {
            "id": "task460-22b1da5f49974c03abc87137fae910fc",
            "input": "The log-likelihood of a Plackett-Luce model is not a strict upper bound of the BLEU score, however, it correlates with BLEU well in the case of rich features. The concept of “rich” is actually qualitative, and obscure to define in different applications. We empirically provide a formula to measure the richness in the scenario of machine translation. The greater, the richer. In practice, we find a rough threshold of r is 5 \n Question: How they measure robustness in experiments?",
            "output": [
                "We empirically provide a formula to measure the richness in the scenario of machine translation."
            ]
        },
        {
            "id": "task460-ce475288cb7e4bb59a5591e4b6e30459",
            "input": "Table TABREF14 shows the results of our main experiments on the 2016 and 2018 test sets for French and German. We use Meteor BIBREF31 as the main metric, as in the WMT tasks BIBREF25 . We compare our transformer baseline to transformer models enriched with image information, as well as to the deliberation models, with or without image information.\n\nWe first note that our multimodal models achieve the state of the art performance for transformer networks (constrained models) on the English-German dataset, as compared to BIBREF30 . Second, our deliberation models lead to significant improvements over this baseline across test sets (average INLINEFORM0 , INLINEFORM1 ). \n Question: What dataset does this approach achieve state of the art results on?",
            "output": [
                "the English-German dataset"
            ]
        },
        {
            "id": "task460-b9784884216e45c5ababbc74dbd41d96",
            "input": "To counter that, we use a left-to-right attention mask, similar to the one employed in the original Transformer decoder BIBREF1. For the input tokens in $X$, we apply such mask to all the target tokens $Y$ that were concatenated to $X$, so that input tokens can only attend to the other input tokens. Conversely, for target tokens $y_t$, we put an attention mask on all tokens $y_{>t}$, allowing target tokens $y_t$ to attend only to the input tokens and the already generated target tokens. \n Question: What is different in BERT-gen from standard BERT?",
            "output": [
                "They use a left-to-right attention mask so that the input tokens can only attend to other input tokens, and the target tokens can only attend to the input tokens and already generated target tokens."
            ]
        },
        {
            "id": "task460-5ca6328e92b4481391b0c0a769026562",
            "input": "We recruited 100 crowdworkers on Amazon Mechanical Turk (AMT) and measured completion times and accuracies for typing randomly sampled sentences from the Yelp corpus.  \n Question: What user variations have been tested?",
            "output": [
                "completion times and accuracies "
            ]
        },
        {
            "id": "task460-a092e9c354cb45b8b441e230f2ba8d25",
            "input": "With this challenge in mind, we introduce Torch-Struct with three specific contributions:\n\nModularity: models are represented as distributions with a standard flexible API integrated into a deep learning framework.\n\nCompleteness: a broad array of classical algorithms are implemented and new models can easily be added in Python.\n\nEfficiency: implementations target computational/memory efficiency for GPUs and the backend includes extensions for optimization. \n Question: Is this library implemented into Torch or is framework agnostic?",
            "output": [
                "It uses deep learning framework (pytorch)"
            ]
        },
        {
            "id": "task460-7f30be8dd8e247089a6febff22693a2b",
            "input": "Using our annotation software, we automatically extracted landmarks of 2000 images from the UOttawa database BIBREF14 had been annotated for image segmentation tasks.  \n Question: How big are datasets used in experiments?",
            "output": [
                "2000 images"
            ]
        },
        {
            "id": "task460-e38cee8627294a11858d1bc36df070c3",
            "input": "We compare the predictions of logistic regression models based on unigram bag-of-words features (BOW), sentiment signals (SENT), the linguistic features from our earlier analyses (LING), and combinations of these features. \n Question: What predictive model do they build?",
            "output": [
                "logistic regression models"
            ]
        },
        {
            "id": "task460-3a4a39dd5fb0430e83d11109a296469f",
            "input": "In this work, we evaluate three publicly-available off-the-shelf coreference resolution systems, representing three different machine learning paradigms: rule-based systems, feature-driven statistical systems, and neural systems. We evaluate examples of each of the three coreference system architectures described in \"Coreference Systems\" : the BIBREF5 sieve system from the rule-based paradigm (referred to as RULE), BIBREF6 from the statistical paradigm (STAT), and the BIBREF11 deep reinforcement system from the neural paradigm (NEURAL). \n Question: Which coreference resolution systems are tested?",
            "output": [
                "the BIBREF5 sieve system from the rule-based paradigm (referred to as RULE), BIBREF6 from the statistical paradigm (STAT), and the BIBREF11 deep reinforcement system from the neural paradigm (NEURAL)."
            ]
        },
        {
            "id": "task460-9cfcdc1a0a3d45f7ad081fa9e68b83d5",
            "input": "Finally, our combined model which uses both Latent and Explicit structure performs the best with a strong improvement of 1.08 points in ROUGE-L over our base pointer-generator model and 0.6 points in ROUGE-1.  \n Question: By how much they improve over the previous state-of-the-art?",
            "output": [
                "1.08 points in ROUGE-L over our base pointer-generator model  0.6 points in ROUGE-1"
            ]
        },
        {
            "id": "task460-b4ce672a29eb4c229c58e071ba575113",
            "input": "For our single-label topic classification experiments, we use the Switchboard Telephone Speech Corpus BIBREF21 , a collection of two-sided telephone conversations. We further evaluate our topic ID performance on the speech corpora of three languages released by the DARPA LORELEI (Low Resource Languages for Emergent Incidents) Program. \n Question: What datasets are used to assess the performance of the system?",
            "output": [
                "Switchboard Telephone Speech Corpus BIBREF21 LORELEI (Low Resource Languages for Emergent Incidents) Program"
            ]
        },
        {
            "id": "task460-fdd22e92e6a14c60aafb37f39cc3de7b",
            "input": "In addition, we evaluate the effectiveness of pre-training by comparing it with a jointly-trained model of forward translation and back-translation. Experimental results show that the encoder-decoder-reconstructor offers significant improvement in BLEU scores and alleviates the problem of repeating and missing words in the translation on English-Japanese translation task, and the encoder-decoder-reconstructor can not be trained well without pre-training, so it proves that we have to train the forward translation model in a manner similar to the conventional attention-based NMT as pre-training. \n Question: Is pre-training effective in their evaluation?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-64c3e512299347b4985777e469a0bace",
            "input": "We apply the same data split in BIBREF31, BIBREF30, BIBREF6, which uses news (the union of bn and nw) as the training set, a half of bc as the development set and the remaining data as the test set.\n\nWe learn the model parameters using Adam BIBREF32. We apply dropout BIBREF33 to the hidden layers to reduce overfitting. The development set is used for tuning the model hyperparameters and for early stopping. We train 5 Bi-LSTM English RE models initiated with different random seeds, apply the 5 models on the target languages, and combine the outputs by selecting the relation type labels with the highest probabilities among the 5 models. \n Question: Do they train their own RE model?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-5d38e8353e5e4c96989c48873aefc3a0",
            "input": "I first measured the effectiveness of the embeddings at the skip-gram's training task, predicting context words INLINEFORM0 given input words INLINEFORM1 . This task measures the methods' performance for predictive language modeling. I used four datasets of sociopolitical, scientific, and literary interest: the corpus of NIPS articles from 1987 – 1999 ( INLINEFORM2 million), the U.S. presidential state of the Union addresses from 1790 – 2015 ( INLINEFORM3 ), the complete works of Shakespeare ( INLINEFORM4 ; this version did not contain the Sonnets), and the writings of black scholar and activist W.E.B. Du Bois, as digitized by Project Gutenberg ( INLINEFORM5 ). For each dataset, I held out 10,000 INLINEFORM6 pairs uniformly at random, where INLINEFORM7 , and aimed to predict INLINEFORM8 given INLINEFORM9 (and optionally, INLINEFORM10 ). Since there are a large number of classes, I treat this as a ranking problem, and report the mean reciprocal rank. The experiments were repeated and averaged over 5 train/test splits. \n Question: What is MRR?",
            "output": [
                "mean reciprocal rank"
            ]
        },
        {
            "id": "task460-b244233ec5d146d4b018c4738c291ad6",
            "input": "Machine-machine Interaction A related line of work explores simulation-based dialogue generation, where the user and system roles are simulated to generate a complete conversation flow, which can then be converted to natural language using crowd workers BIBREF1. Such a framework may be cost-effective and error-resistant since the underlying crowd worker task is simpler, and semantic annotations are obtained automatically. It is often argued that simulation-based data collection does not yield natural dialogues or sufficient coverage, when compared to other approaches such as Wizard-of-Oz. We argue that simulation-based collection is a better alternative for collecting datasets like this owing to the factors below. \n Question: How did they gather the data?",
            "output": [
                "simulation-based dialogue generation, where the user and system roles are simulated to generate a complete conversation flow, which can then be converted to natural language using crowd workers "
            ]
        },
        {
            "id": "task460-8d466889a4ff4e768a641fbf0a1cf7c5",
            "input": "In this section we describe a number of experiments targeted to compare the performance of popular named entity recognition algorithms on our data. We trained and evaluated Stanford NER, spaCy 2.0, and a recurrent model similar to BIBREF13 , BIBREF14 that uses bidirectional LSTM cells for character-based feature extraction and CRF, described in Guillaume Genthial's Sequence Tagging with Tensorflow blog post BIBREF15 . Stanford NER is conditional random fields (CRF) classifier based on lexical and contextual features such as the current word, character-level n-grams of up to length 6 at its beginning and the end, previous and next words, word shape and sequence features BIBREF16 . spaCy 2.0 uses a CNN-based transition system for named entity recognition. The main model that we focused on was the recurrent model with a CRF top layer, and the above-mentioned methods served mostly as baselines.  \n Question: what ner models were evaluated?",
            "output": [
                "Stanford NER spaCy 2.0  recurrent model with a CRF top layer"
            ]
        },
        {
            "id": "task460-71731ca020644ac691782c6b03af5e83",
            "input": "We suspect the absence of decoding in maximum likelihood estimation as a cause behind this inconsistency, and suggest investigating sequence-level learning as an alternative in the future. Inconsistency may arise from the lack of decoding in solving this optimization problem. Maximum likelihood learning fits the model $p_{\\theta }$ using the data distribution, whereas a decoded sequence from the trained model follows the distribution $q_{\\mathcal {F}}$ induced by a decoding algorithm. Based on this discrepancy, we make a strong conjecture: we cannot be guaranteed to obtain a good consistent sequence generator using maximum likelihood learning and greedy decoding. \n Question: Is infinite-length sequence generation a result of training with maximum likelihood?",
            "output": [
                "There are is a strong conjecture that it might be the reason but it is not proven."
            ]
        },
        {
            "id": "task460-ce04d66a4fd045e3b0cc7bcc11018f79",
            "input": "In order to evaluate the precision of the retrieved documents in each experiment, we used \"TREC_Eval\" tool [3]. TREC_Eval is a standard tool for evaluation of IR tasks and its name is a short form of Text REtrieval Conference (TREC) Evaluation tool. The Mean Average Precision (MAP) reported by TREC_Eval was 27.99% without query expansion and 37.10% with query expansion which shows more than 9 percent improvement. \n Question: Which evaluation metric has been measured?",
            "output": [
                "Mean Average Precision"
            ]
        },
        {
            "id": "task460-4c4834bde0c4424da125f859bcbe52b3",
            "input": "We evaluate our model on two public datasets, namely, Penn Treebank (PTB) BIBREF9 and the end-to-end (E2E) text generation corpus BIBREF10, which have been used in a number of previous works for text generation BIBREF0, BIBREF5, BIBREF11, BIBREF12. PTB consists of more than 40,000 sentences from Wall Street Journal articles whereas the E2E dataset contains over 50,000 sentences of restaurant reviews. The statistics of these two datasets are summarised in Table TABREF11. \n Question: Which dataset do they use for text modelling?",
            "output": [
                "Penn Treebank (PTB) end-to-end (E2E) text generation corpus"
            ]
        },
        {
            "id": "task460-b78c22aec3e343989d32a4c6a0efc796",
            "input": "With the goal of building a generalizable sentiment analysis model, we used three different training sets as provided in Table TABREF5 . One of these three datasets (Amazon reviews BIBREF23 , BIBREF24 ) is larger and has product reviews from several different categories including book reviews, electronics products reviews, and application reviews. The other two datasets are to make the model more specialized in the domain. In this paper we focus on restaurant reviews as our domain and use Yelp restaurant reviews dataset extracted from Yelp Dataset Challenge BIBREF25 and restaurant reviews dataset as part of a Kaggle competition BIBREF26 . \n Question: what dataset was used for training?",
            "output": [
                "Amazon reviews Yelp restaurant reviews restaurant reviews"
            ]
        },
        {
            "id": "task460-863514aa89eb4669b97ef80eb5ab8135",
            "input": "We asked medical doctors experienced in extracting knowledge related to medical entities from texts to annotate the entities described above. Initially, we asked four annotators to test our guidelines on two texts. Subsequently, identified issues were discussed and resolved. Following this pilot annotation phase, we asked two different annotators to annotate two case reports according to our guidelines. The same annotators annotated an overall collection of 53 case reports. The annotation was performed using WebAnno BIBREF7, a web-based tool for linguistic annotation. The annotators could choose between a pre-annotated version or a blank version of each text. The pre-annotated versions contained suggested entity spans based on string matches from lists of conditions and findings synonym lists. \n Question: How was annotation performed?",
            "output": [
                "Experienced medical doctors used a linguistic annotation tool to annotate entities."
            ]
        },
        {
            "id": "task460-031111ca037d49f5aa6945e03e7795ac",
            "input": "Experimental results on public datasets show that our CRU model could substantially outperform various systems by a large margin, and set up new state-of-the-art performances on related datasets. \n Question: Are there some results better than state of the art on these tasks?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-d8cb72fad1a6419ab5ff1ae6b9ecb74d",
            "input": "Our model adopts the recently introduced biaffine attention BIBREF14 to enhance our role scorer.  \n Question: What is the biaffine scorer?",
            "output": [
                "biaffine attention BIBREF14"
            ]
        },
        {
            "id": "task460-19b6b3ce3ef44e6fa2e89a2db5313a3d",
            "input": "In this paper, we provide a brief survey of existing datasets and describe the CRWIZ framework for pairing crowdworkers and having half of them acting as Wizards by limiting their dialogue options only to relevant and plausible ones, at any one point in the interaction Dialogue structure: we introduced structured dialogues through a Finite State Machine (FSM) that controls the current dialogue state and offers multiple suitable and relevant state transitions (actions) to the Wizard depending on the point in the interaction, the state of the world and the history. A graph of dialogue states, transitions and utterances is loaded when the system is initialised, and each chat room has its own dialogue state, which changes through actions. \n Question: How is dialogue guided to avoid interactions that breach procedures and processes only known to experts?",
            "output": [
                "pairing crowdworkers and having half of them acting as Wizards by limiting their dialogue options only to relevant and plausible ones, at any one point in the interaction"
            ]
        },
        {
            "id": "task460-7dcef33ad007469b89025e97787f4569",
            "input": "We also evaluate all five models on downstream tasks from the VecEval suite BIBREF13 , using only the tasks for which training and evaluation data is freely available: chunking, sentiment and question classification, and natural language identification (NLI). The default settings from the suite are used, but we run only the fixed settings, where the embeddings themselves are not tunable parameters of the models, forcing the system to use only the information already in the embeddings.\n\nFinally, we use LV-N, LV-M, and FT to generate OOV word representations for the following words: 1) “hellooo”: a greeting commonly used in instant messaging which emphasizes a syllable. 2) “marvelicious”: a made-up word obtained by merging “marvelous” and “delicious”. 3) “louisana”: a misspelling of the proper name “Louisiana”. 4) “rereread”: recursive use of prefix “re”. 5) “tuzread”: made-up prefix “tuz”. \n Question: How do they evaluate their resulting word embeddings?",
            "output": [
                "We also evaluate all five models on downstream tasks from the VecEval suite BIBREF13 , using only the tasks for which training and evaluation data is freely available: chunking, sentiment and question classification, and natural language identification (NLI). The default settings from the suite are used, but we run only the fixed settings, where the embeddings themselves are not tunable parameters of the models, forcing the system to use only the information already in the embeddings."
            ]
        },
        {
            "id": "task460-2c178930d4804f28af5580e6b3ea6021",
            "input": "In this work, the experiments are conducted using the BABEL speech corpus collected from the IARPA babel program.  \n Question: What data do they train the language models on?",
            "output": [
                " BABEL speech corpus "
            ]
        },
        {
            "id": "task460-2e75f7771c3d44cab0e16f6b4157be85",
            "input": "The corpus of supervisor assessment has 26972 sentences. The summary statistics about the number of words in a sentence is: min:4 max:217 average:15.5 STDEV:9.2 Q1:9 Q2:14 Q3:19. \n Question: What is the average length of the sentences?",
            "output": [
                "15.5"
            ]
        },
        {
            "id": "task460-45f10f51b1ad4bc996bf78a8de66324e",
            "input": "For our study, we consider that a tweet went viral if it was retweeted more than 1000 times. \n Question: What is their definition of tweets going viral?",
            "output": [
                "Viral tweets are the ones that are retweeted more than 1000 times"
            ]
        },
        {
            "id": "task460-575d8af9ca7a448bb16c22f23cbce28e",
            "input": "There are several caveats in our work: first, tweet sentiment is rarely binary (this work could be extended to a multinomial or continuous model). Second, our results are constrained to Twitter users, who are known to be more negative than the general U.S. population BIBREF9 . Third, we do not take into account the aggregate effects of continued natural disasters over time. Going forward, there is clear demand in discovering whether social networks can indicate environmental metrics in a “nowcasting\" fashion. As climate change becomes more extreme, it remains to be seen what degree of predictive power exists in our current model regarding climate change sentiments with regards to natural disasters. \n Question: Do the authors mention any confounds to their study?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-164dc44d5e8a4c2ea8877fe0eed4f860",
            "input": "To test the applicability of the proposed MWA attention, we choose three publicly available Chinese pre-trained models as the basic encoder: BERT, ERNIE, and BERT-wwm. In order to make a fair comparison, we keep the same hyper-parameters (such maximum length, warm-up steps, initial learning rate, etc) as suggested in BERT-wwm BIBREF13 for both baselines and our method on each dataset. \n Question: What pre-trained models did they compare to?",
            "output": [
                "BERT, ERNIE, and BERT-wwm"
            ]
        },
        {
            "id": "task460-cbe70ea267204e4290ed4b53d6b16520",
            "input": "We first evaluate our converted embedding models on multi-language lexical similarity and relatedness tasks, as a sanity check, to make sure the word sense induction process did not hurt the general performance of the embeddings. Then, we test the sense embeddings on WSD task. \n Question: Was any extrinsic evaluation carried out?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-95af9c26a94d4c72b290cd139e03125a",
            "input": "We treat each individual language as a task and train a joint model for all the tasks. We seek to learn a set of shared character embeddings for taggers in both languages together through optimization of a joint loss function that combines the high-resource tagger and the low-resource one. \n Question: How are character representations from various languages joint?",
            "output": [
                "shared character embeddings for taggers in both languages together through optimization of a joint loss function"
            ]
        },
        {
            "id": "task460-e309dbe3c346473ebe4ac03a906039d0",
            "input": "We have evaluated our framework using a synthetic literal set (S-Lite) and a real literal set (R-Lite) from DBpedia BIBREF0 .  \n Question: What KB is used in this work?",
            "output": [
                "DBpedia"
            ]
        },
        {
            "id": "task460-fa036151a6cc40eabf35149a1e381b86",
            "input": "On Figure FIGREF1 one can see that the outcomes for every single rerun differ significantly. Namely, accuracy can change up to 5 percentage points, whereas BLEU can vary up to 8 points. \n Question: How much does the standard metrics for style accuracy vary on different re-runs?",
            "output": [
                "accuracy can change up to 5 percentage points, whereas BLEU can vary up to 8 points"
            ]
        },
        {
            "id": "task460-38b053dc9c474ac0aa2cecb32875f08a",
            "input": "To have an estimation about human performance in each metric, we iteratively treat every reference sentence in dev/test data as the prediction to be compared with all references (including itself). That is, if a model has the same reasoning ability with average performance of our crowd workers, its results should exceed this “human bound”. \n Question: What measures were used for human evaluation?",
            "output": [
                "To have an estimation about human performance in each metric, we iteratively treat every reference sentence in dev/test data as the prediction to be compared with all references (including itself)."
            ]
        },
        {
            "id": "task460-aca31bb87a834dd8bd7538265e9c7315",
            "input": "Our Pointer-Gen+ARL-SEN model achieves the best performance of 60.8%. This is an absolute improvement of 18.2% over the Pointer-Gen baseline. \n Question: What is this method improvement over the best performing state-of-the-art?",
            "output": [
                "absolute improvement of 18.2% over the Pointer-Gen baseline"
            ]
        },
        {
            "id": "task460-b10989e515e7408da8c21c6d7610235c",
            "input": "To demonstrate the generality of NSA, we further present video captioning, machine translation, and visual question answering experiments on the VATEX, WMT 2014 English-to-German, and VQA-v2 datasets, respectively.  \n Question: What datasets are used for experiments on three other tasks?",
            "output": [
                "VATEX, WMT 2014 English-to-German, and VQA-v2 datasets"
            ]
        },
        {
            "id": "task460-e28ae900cdc147779fc69cf4449712fa",
            "input": "A part of the ILCI English-Hindi Tourism parallel corpus (1500 sentences) was used for training the classifiers.  \n Question: What is the size of the parallel corpus used to train the model constraints?",
            "output": [
                "1500 sentences"
            ]
        },
        {
            "id": "task460-114c5551048c4687a11aeae1a37f5635",
            "input": "Table 2 lists the accuracies of the all methods on two classifier architectures. The results show that, for various datasets on different classifier architectures, our conditional BERT contextual augmentation improves the model performances most. BERT can also augments sentences to some extent, but not as much as conditional BERT does. \n Question: Do the authors report performance of conditional bert on tasks without data augmentation?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-ab42ff7566cb43b0a4488f6a1aeac637",
            "input": "In this paper, we analyze the task of automatically correcting run-on sentences. We develop two methods: a conditional random field model (roCRF) and a Seq2Seq attention model (roS2S) and show that they outperform models from the sister tasks of punctuation restoration and whole-sentence grammatical error correction. \n Question: Which machine learning models do they use to correct run-on sentences?",
            "output": [
                "conditional random field model Seq2Seq attention model"
            ]
        },
        {
            "id": "task460-c43a97a4779140eeaedfa58f64ab0b05",
            "input": "We initialized the embeddings of these words with 300 dimensional Glove embeddings BIBREF31 .  \n Question: Do they use pretrained embeddings?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-0345483551da40ce8851cbb0a411515d",
            "input": "We took inspiration from these works to design our experiments to solve the CSKS task. \n Question: What problem do they apply transfer learning to?",
            "output": [
                "CSKS task"
            ]
        },
        {
            "id": "task460-b80801c1906e4341abffb5cb06a68723",
            "input": "In this section, we describe our data collection for these 3 components. We first describe a series of pilot studies that we conducted in order to collect commonsense inference questions (Section SECREF4 ). In Section SECREF5 , we discuss the resulting data collection of questions, texts and answers via crowdsourcing on Amazon Mechanical Turk (henceforth MTurk). Section SECREF17 gives information about some necessary postprocessing steps and the dataset validation.  \n Question: how was the data collected?",
            "output": [
                "The data was collected using 3 components: describe a series of pilot studies that were conducted to collect commonsense inference questions, then discuss the resulting data collection of questions, texts and answers via crowdsourcing on Amazon Mechanical Turk and gives information about some necessary postprocessing steps and the dataset validation."
            ]
        },
        {
            "id": "task460-8774fac0ae5f4db4a47be04a4d8aafa1",
            "input": "On Twitter we can see results that are consistent with the RCV results for the left-to-center political spectrum. The exception, which clearly stands out, is the right-wing groups ENL and EFDD that seem to be the most cohesive ones. This is the direct opposite of what was observed in the RCV data. We speculate that this phenomenon can be attributed to the fact that European right-wing groups, on a European but also on a national level, rely to a large degree on social media to spread their narratives critical of European integration. \n Question: Do the authors mention any possible confounds in their study?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-8841a1c40b504bd3ba485e2cd8820d3f",
            "input": "It is worth mentioning that the collected texts contain a large quantity of errors of several types: orthographic, syntactic, code-switched words (i.e. words not in the required language), jokes, etc. Hence, the original written sentences have been processed in order to produce “cleaner” versions, in order to make the data usable for some research purposes (e.g. to train language models, to extract features for proficiency assessment, ...). \n Question: Are any of the utterances ungrammatical?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-40a598836a074973a37a623ecaabedd5",
            "input": "After averaging over all concepts, we lose information on the lexical variation that each concept presents but on the other hand one can now investigate which regions show similar geolectal variation, yielding well defined linguistic varieties. Those cells that have similar colors in either figure FIGREF16 or figure FIGREF17 are expected to be ascribed to the same dialect zone. Thus, we can distinguish two main regions or clusters in the maps. The purple background covers most of the map and represents rural regions with small, scattered population. Our analysis shows that this group of cells possesses more specific words in their lexicon. In contrast, the green and yellow cells form a second cluster that is largely concentrated on the center and along the coastline, which correspond to big cities and industrialized areas. In these cells, the use of standard Spanish language is widespread due probably to school education, media, travelers, etc. The character of its vocabulary is more uniform as compared with the purple group. While the purple cluster prefer particular utterances, the lexicon of the urban group includes most of the keywords.  \n Question: What are the characteristics of the rural dialect?",
            "output": [
                "It uses particular forms of a concept rather than all of them uniformly"
            ]
        },
        {
            "id": "task460-f3a25ade202c4527b550f267d96894ca",
            "input": "Finally, the decoder task, which predicts the target sequence probability at time INLINEFORM3 based on previous output and context information INLINEFORM4 , can be formulated as: DISPLAYFORM0 \n Question: How does their decoder generate text?",
            "output": [
                "decoder task, which predicts the target sequence probability at time INLINEFORM3 based on previous output and context information"
            ]
        },
        {
            "id": "task460-b21001f069ac40df8d51d182f4fa46a5",
            "input": "A growing body of evidence shows that state-of-the-art models learn to exploit spurious statistical patterns in datasets BIBREF12, BIBREF13, BIBREF14, BIBREF15, BIBREF16, BIBREF17, instead of learning meaning in the flexible and generalizable way that humans do. Given this, human annotators—be they seasoned NLP researchers or non-experts—might easily be able to construct examples that expose model brittleness. \n Question: What are the weaknesses found by non-expert annotators of current state-of-the-art NLI models?",
            "output": [
                "state-of-the-art models learn to exploit spurious statistical patterns in datasets human annotators—be they seasoned NLP researchers or non-experts—might easily be able to construct examples that expose model brittleness"
            ]
        },
        {
            "id": "task460-e33e18f00f0d445c93cf7003b1fda70b",
            "input": "In this study, we investigate the performance of our proposed models on WSJ BIBREF5 .  \n Question: Which dataset do they use?",
            "output": [
                "WSJ"
            ]
        },
        {
            "id": "task460-06d7d6f7092741308f47023d2797f7d1",
            "input": "We feed the above-described hand-crafted features together with the task-specific embeddings learned by the deep neural neural network (a total of 1,892 attributes combined) into a Support Vector Machines (SVM) classifier BIBREF37 . \n Question: what classifiers were used in this paper?",
            "output": [
                "Support Vector Machines (SVM) classifier"
            ]
        },
        {
            "id": "task460-a9f44d122b7844c0839c0d46f3e3d10d",
            "input": "When we annotate dialogues, we should read dialogues from begin to the end. For each utterance, we should find its one parent node at least from all its previous utterances. We assume that the discourse structure is a connected graph and no utterance is isolated. We propose three questions for eache dialogue and annotate the span of answers in the input dialogue. As we know, our dataset is the first corpus for multi-party dialogues reading comprehension.\n\nWe construct following questions and answers for the dialogue in Example 1:\n\nQ1: When does Bdale leave?\n\nA1: Fri morning\n\nQ2: How to get people love Mark in Mjg59's opinion.\n\nA2: Hire people to work on reverse-engineering closed drivers.\n\nOn the other hand, to improve the difficulty of the task, we propose $ \\frac{1}{6}$ to $ \\frac{1}{3}$ unanswerable questions in our dataset. We annotate unanswerable questions and their plausible answers (PA). Each plausible answer comes from the input dialogue, but is not the answer for the plausible question.\n\nQ1: Whis is the email of daniels?\n\nPA: +61 403 505 896 \n Question: Is annotation done manually?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-c1b187060c6c43399b5255eec009056e",
            "input": "Baselines. As none of the existing KBC methods can solve the OKBC problem, we choose various versions of LiLi as baselines.\n\nSingle: Version of LiLi where we train a single prediction model INLINEFORM0 for all test relations.\n\nSep: We do not transfer (past learned) weights for initializing INLINEFORM0 , i.e., we disable LL.\n\nF-th): Here, we use a fixed prediction threshold 0.5 instead of relation-specific threshold INLINEFORM0 .\n\nBG: The missing or connecting links (when the user does not respond) are filled with “@-RelatedTo-@\" blindly, no guessing mechanism.\n\nw/o PTS: LiLi does not ask for additional clues via past task selection for skillset improvement. \n Question: What baseline is used in the experiments?",
            "output": [
                "versions of LiLi"
            ]
        },
        {
            "id": "task460-80038e73477343ef94cb1876753c7d0d",
            "input": "Filter 1: The string match filter deletes all KB triples where the correct answer (e.g., Apple) is a case-insensitive substring of the subject entity name (e.g., Apple Watch). Filter 2: Of course, entity names can be revealing in ways that are more subtle. As illustrated by our French actor example, a person's name can be a useful prior for guessing their native language and by extension, their nationality, place of birth, etc. Our person name filter uses cloze-style questions to elicit name associations inherent in BERT, and deletes KB triples that correlate with them. \n Question: How is it determined that a fact is easy-to-guess?",
            "output": [
                " filter deletes all KB triples where the correct answer (e.g., Apple) is a case-insensitive substring of the subject entity name (e.g., Apple Watch) person name filter uses cloze-style questions to elicit name associations inherent in BERT, and deletes KB triples that correlate with them"
            ]
        },
        {
            "id": "task460-617792401acb41809ec2d788dc416c98",
            "input": "Participants who were shown the definition were more likely to suggest to ban the tweet. In fact, participants in group one very rarely gave different answers to questions one and two (18 of 500 instances or 3.6%). This suggests that participants in that group aligned their own opinion with the definition. \n Question: How did the authors demonstrate that showing a hate speech definition caused annotators to partially align their own opinion with the definition?",
            "output": [
                "participants in group one very rarely gave different answers to questions one and two (18 of 500 instances or 3.6%)"
            ]
        },
        {
            "id": "task460-712a274628e0474ea9ce95fe0e2cfb3d",
            "input": "It is an order of magnitude more efficient in terms of training time. The model is complex, both in terms of implementation and run-time. Indeed, this model requires pre-training and mutual-learning and requires days of training time, whereas the simple architecture we propose requires on the order of an hour (and is easy to implement). MGNC-CNN is usually better than MG-CNN. And on the Subj dataset, MG-CNN actually achieves slightly better results than BIBREF11 , with far less complexity and required training time (MGNC-CNN performs comparably, although no better, here). \n Question: How much faster is training time for MGNC-CNN over the baselines?",
            "output": [
                "It is an order of magnitude more efficient in terms of training time. his model requires pre-training and mutual-learning and requires days of training time, whereas the simple architecture we propose requires on the order of an hour"
            ]
        },
        {
            "id": "task460-0db117908def4066b25ef069c63c9f69",
            "input": "Human Judgments\nFollowing BIBREF11 , BIBREF12 and the vast amount of previous work on semantic similarity, we ask nine undergraduate subjects to assess the similarity of 360 pairs of relations from a subset of Wikidata BIBREF8 that are chosen to cover from high to low levels of similarity. In our experiment, subjects were asked to rate an integer similarity score from 0 (no similarity) to 4 (perfectly the same) for each pair.  \n Question: How do they gather human judgements for similarity between relations?",
            "output": [
                "By assessing similarity of 360 pairs of relations from a subset of Wikidata using an integer similarity score from 0 to 4"
            ]
        },
        {
            "id": "task460-da5daa1c7024435a9c794ea67309afbf",
            "input": "UTD aims to automatically identify and cluster repeated terms (e.g. words or phrases) from speech. To circumvent the exhaustive DTW-based search limited by INLINEFORM0 time BIBREF6 , we exploit the scalable UTD framework in the Zero Resource Toolkit (ZRTools) BIBREF7 , which permits search in INLINEFORM1 time. \n Question: How is the vocabulary of word-like or phoneme-like units automatically discovered?",
            "output": [
                "Zero Resource Toolkit (ZRTools) BIBREF7"
            ]
        },
        {
            "id": "task460-d331100fc6b74c9a9276d7eb0e70ecff",
            "input": "These powerful ideas from the NLP community have been adapted by researchers from other domains to tasks beyond representation of words, including relational entities BIBREF1 , BIBREF2 , general text-based attributes BIBREF3 , descriptive text of images BIBREF4 , nodes in graph structure of networks BIBREF5 , and queries BIBREF6 , to name a few. \n Question: What domains are considered that have such large vocabularies?",
            "output": [
                "relational entities general text-based attributes descriptive text of images nodes in graph structure of networks queries"
            ]
        },
        {
            "id": "task460-eb23e257b56544759dcd753d9ea857a5",
            "input": "We considered UD1.2 corpora for the following 16 languages: Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish. \n Question: which languages are explored?",
            "output": [
                "Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish"
            ]
        },
        {
            "id": "task460-3dea3238d3b84ac59c9a6e8cd6eb111a",
            "input": "o sufficiently utilize the large dataset $\\mathcal {A}$ and $\\mathcal {M}$, the model is pre-trained on CTC-based ASR task and MT task in the pre-training stage. \n Question: What is the attention module pretrained on?",
            "output": [
                "the model is pre-trained on CTC-based ASR task and MT task in the pre-training stage."
            ]
        },
        {
            "id": "task460-c690163dbf8e4430a91fe3bff61b45ee",
            "input": "Looking to other target tasks, the grammar-related CoLA task benefits dramatically from ELMo pretraining: The best result without language model pretraining is less than half the result achieved with such pretraining. In contrast, the meaning-oriented textual similarity benchmark STS sees good results with several kinds of pretraining, but does not benefit substantially from the use of ELMo. \n Question: Do some pretraining objectives perform better than others for sentence level understanding tasks?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-44b6989c96c7436abe07c2ff02d9dc9d",
            "input": "Methods ::: Length Token Method\nOur first approach to control the length is inspired by target forcing in multilingual NMT BIBREF15, BIBREF16. We first split the training sentence pairs into three groups according to the target/source length ratio (in terms of characters). Ideally, we want a group where the target is shorter than the source (short), one where they are equally-sized (normal) and a last group where the target is longer than the source (long). In practice, we select two thresholds $t_\\text{min}$ and $t_\\text{max}$ according to the length ratio distribution. All the sentence pairs with length ratio between $t_\\text{min}$ and $t_\\text{max}$ are in the normal group, the ones with ratio below $t_\\text{min}$ in short and the remaining in long. At training time we prepend a length token to each source sentence according to its group ($<$short$>$, $<$normal$>$, or $<$long$>$), in order to let a single network to discriminate between the groups (see Figure FIGREF2). At inference time, the length token is used to bias the network to generate a translation that belongs to the desired length group. \n Question: How do they condition the output to a given target-source class?",
            "output": [
                "They use three groups short/normal/long translation classes to learn length token, which is in inference used to bias network to generate desired length group."
            ]
        },
        {
            "id": "task460-c9161d1ee37d4d80b19cd0ee2e748cda",
            "input": "It was developed by querying the Followerwonk Web service API with location-neutral seed words known to be used by gang members across the U.S. in their Twitter profiles. \n Question: Do the authors report on English datasets only?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-6c263e1b550f4d3e82e940aaec54512b",
            "input": "We evaluate the quality of the document embeddings learned by MPAD on 10 document classification datasets, covering the topic identification, coarse and fine sentiment analysis and opinion mining, and subjectivity detection tasks. We briefly introduce the datasets next. Their statistics are reported in Table TABREF21.\n\n(1) Reuters. This dataset contains stories collected from the Reuters news agency in 1987. Following common practice, we used the ModApte split and considered only the 10 classes with the highest number of positive training examples. We also removed documents belonging to more than one class and then classes left with no document (2 classes).\n\n(2) BBCSport BIBREF30 contains documents from the BBC Sport website corresponding to 2004-2005 sports news articles.\n\n(3) Polarity BIBREF31 features positive and negative labeled snippets from Rotten Tomatoes.\n\n(4) Subjectivity BIBREF32 contains movie review snippets from Rotten Tomatoes (subjective sentences), and Internet Movie Database plot summaries (objective sentences).\n\n(5) MPQA BIBREF33 is made of positive and negative phrases, annotated as part of the summer 2002 NRRC Workshop on Multi-Perspective Question Answering.\n\n(6) IMDB BIBREF34 is a collection of highly polarized movie reviews from IMDB (positive and negative). There are at most 30 reviews for each movie.\n\n(7) TREC BIBREF35 consists of questions that are classified into 6 different categories.\n\n(8) SST-1 BIBREF36 contains the same snippets as Polarity. The authors used the Stanford Parser to parse the snippets and split them into multiple sentences. They then used Amazon Mechanical Turk to annotate the resulting phrases according to their polarity (very negative, negative, neutral, positive, very positive).\n\n(9) SST-2 BIBREF36 is the same as SST-1 but with neutral reviews removed and snippets classified as positive or negative.\n\n(10) Yelp2013 BIBREF26 features reviews obtained from the 2013 Yelp Dataset Challenge. \n Question: Which datasets are used?",
            "output": [
                "Reuters  BBCSport Polarity Subjectivity MPQA IMDB TREC SST-1 SST-2 Yelp2013"
            ]
        },
        {
            "id": "task460-66d410fd6e004c1f86cd3c747c5b694d",
            "input": "For the human evaluation, we follow the standard approach in evaluating machine translation systems BIBREF23 , as used for question generation by BIBREF9 . We asked three workers to rate 300 generated questions between 1 (poor) and 5 (good) on two separate criteria: the fluency of the language used, and the relevance of the question to the context document and answer. \n Question: What human evaluation metrics were used in the paper?",
            "output": [
                "rating questions on a scale of 1-5 based on fluency of language used and relevance of the question to the context"
            ]
        },
        {
            "id": "task460-2364a35041334d0f9bd8ffabd9efe4df",
            "input": "The annotation projection method, which we follow in this work, is one way to address this problem. It was introduced for POS tagging, base noun phrase bracketing, NER tagging, and inflectional morphological analysis BIBREF29 but it has also been used for dependency parsing BIBREF30 , role labeling BIBREF31 , BIBREF32 and semantic parsing BIBREF26 . \n Question: Do the authors test their annotation projection techniques on tasks other than AMR?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-c3f935547b5b43a8ad141d8eb9a3981e",
            "input": "However, recent work has found that many NLI datasets contain biases, or annotation artifacts, i.e., features present in hypotheses that enable models to perform surprisingly well using only the hypothesis, without learning the relationship between two texts BIBREF2 , BIBREF3 , BIBREF4 . For instance, in some datasets, negation words like “not” and “nobody” are often associated with a relationship of contradiction. As a ramification of such biases, models may not generalize well to other datasets that contain different or no such biases. \n Question: Is such bias caused by bad annotation?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-b92e4b1ca04c4c2fa53ba523a5df9a63",
            "input": "To study the lexical and semantic diversities of responses, we performed three analyses. First, we aggregated all worker responses to a particular question into a single list corresponding to that question. Second, we compared the diversity of individual responses between Control and AUI for each question. To measure diversity for a question, we computed the number of responses divided by the number of unique responses to that question. We call this the response density.  \n Question: How was lexical diversity measured?",
            "output": [
                "By computing number of unique responses and number of responses divided by the number of unique responses to that question for each of the questions"
            ]
        },
        {
            "id": "task460-d5734605f5ed491f9d4cab2b52e5ecb1",
            "input": "This work also contributes a new dataset of INLINEFORM0 pairs of free-form natural language instructions and high-level navigation plans. This dataset was collected through Mechanical Turk using 100 simulated environments with a corresponding topological map and, to the best of our knowledge, it is the first of its kind for behavioral navigation.  \n Question: Did the authors use a crowdsourcing platform?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-2492dc2c80284815928f934ecf39ad94",
            "input": "Our dataset is constructed via distant supervision from Twitter. \n Question: Where did they get the data for this project?",
            "output": [
                "Twitter"
            ]
        },
        {
            "id": "task460-fd9d66a8a49147a0bc59c71b717fd534",
            "input": "In our experiments, we compared about 60 elementary metrics, which can be organised as follows:\n\nMT metrics\n\nBLEU, ROUGE, METEOR, TERp\n\nVariants of BLEU: BLEU_1gram, BLEU_2gram, BLEU_3gram, BLEU_4gram and seven smoothing methods from NLTK BIBREF32 .\n\nIntermediate components of TERp inspired by BIBREF18 : e.g. number of insertions, deletions, shifts...\n\nReadability metrics and other sentence-level features: FKGL and FRE, numbers of words, characters, syllables...\n\nMetrics based on the baseline QuEst features (17 features) BIBREF28 , such as statistics on the number of words, word lengths, language model probability and INLINEFORM0 -gram frequency.\n\nMetrics based on other features: frequency table position, concreteness as extracted from BIBREF33 's BIBREF33 list, language model probability of words using a convolutional sequence to sequence model from BIBREF34 , comparison methods using pre-trained fastText word embeddings BIBREF35 or Skip-thought sentence embeddings BIBREF36 . \n Question: what approaches are compared?",
            "output": [
                "MT metrics Readability metrics and other sentence-level features Metrics based on the baseline QuEst features Metrics based on other features"
            ]
        },
        {
            "id": "task460-f60af89e05f84da0b519b3968b8d1e47",
            "input": "In order to verify the reliability of our technique in coverage expansion for infrequent words we did a set of experiments on the Rare Word similarity dataset BIBREF6 . The dataset comprises 2034 pairs of rare words, such as ulcerate-change and nurturance-care, judged by 10 raters on a [0,10] scale. Table TABREF15 shows the results on the dataset for three pre-trained word embeddings (cf. § SECREF2 ), in their initial form as well as when enriched with additional words from WordNet. \n Question: How are rare words defined?",
            "output": [
                "judged by 10 raters on a [0,10] scale"
            ]
        },
        {
            "id": "task460-bbf33236eae54213a60f45403bd7fa92",
            "input": "The ICSI Meeting Corpus BIBREF11 is a collection of meeting recordings that has been thoroughly annotated, including annotations for involvement hot spots BIBREF12, linguistic utterance units, and word time boundaries based on forced alignment. The dataset is comprised of 75 meetings and about 70 hours of real-time audio duration, with 6 speakers per meeting on average. Most of the participants are well-acquainted and friendly with each other. Hot spots were originally annotated with 8 levels and degrees, ranging from `not hot' to `luke warm' to `hot +'. Every utterance was labeled with one of these discrete labels by a single annotator. Hightened involvement is rare, being marked on only 1% of utterances. \n Question: How big is ICSI meeting corpus?",
            "output": [
                " 75 meetings and about 70 hours of real-time audio duration"
            ]
        },
        {
            "id": "task460-d15210f6f0744367879bbb0e03915fd5",
            "input": "Turning to semantics, using visualizations of the activations created by different pieces of text, we show suggestive evidence that BERT distinguishes word senses at a very fine level.  We apply attention probes to the task of identifying the existence and type of dependency relation between two words. \n Question: How were the feature representations evaluated?",
            "output": [
                "attention probes using visualizations of the activations created by different pieces of text"
            ]
        },
        {
            "id": "task460-f6459e50fa3a4b7bb404bfaec191f74b",
            "input": "Table TABREF44 shows average results of our automatic and human evaluations. \n Question: How big is the difference in performance between proposed model and baselines?",
            "output": [
                "Metric difference between Aloha and best baseline score:\nHits@1/20: +0.061 (0.3642 vs 0.3032)\nMRR: +0.0572(0.5114 vs 0.4542)\nF1: -0.0484 (0.3901 vs 0.4385)\nBLEU: +0.0474 (0.2867 vs 0.2393)"
            ]
        },
        {
            "id": "task460-8047d5ed1a884adaac0d7645ea672935",
            "input": "Following lebret2016neural, we used BLEU-4, NIST-4 and ROUGE-4 as the evaluation metrics. \n Question: What metrics are used for evaluation?",
            "output": [
                "BLEU-4 NIST-4 ROUGE-4"
            ]
        },
        {
            "id": "task460-83992ba37a5e4280bc468cd49137ca54",
            "input": "Similar trends are seen in the performance of other two state-of-the-art approaches BIBREF9 , BIBREF8 . \n Question: What are the state of the art models?",
            "output": [
                "BIBREF9  BIBREF8 "
            ]
        },
        {
            "id": "task460-193e6b2bd4304c39957d1b7338d741e3",
            "input": " We use the $\\textsc {BERT}_{\\textsc {BASE}}$ ensemble from BIBREF3 as the single-hop QA model.  \n Question: What off-the-shelf QA model was used to answer sub-questions?",
            "output": [
                "$\\textsc {BERT}_{\\textsc {BASE}}$ ensemble from BIBREF3"
            ]
        },
        {
            "id": "task460-594d2737a4d44d9aac2def8664f463e4",
            "input": "As a first experiment, we compare the quality of fastText embeddings trained on (high-quality) curated data and (low-quality) massively extracted data for Twi and Yorùbá languages. The huge ambiguity in the written Twi language motivates the exploration of different approaches to word embedding estimations. In this work, we compare the standard fastText methodology to include sub-word information with the character-enhanced approach with position-based clustered embeddings (CWE-LP as introduced in Section SECREF17). \n Question: What two architectures are used?",
            "output": [
                "fastText CWE-LP"
            ]
        },
        {
            "id": "task460-959c1ea91d0b450f9b9e00bb60a6f8b6",
            "input": "Although techniques for constructing this knowledge base are outside the scope of this paper, we briefly mention them. Tables were constructed using a mixture of manual and semi-automatic techniques \n Question: How is the semi-structured knowledge base created?",
            "output": [
                "using a mixture of manual and semi-automatic techniques"
            ]
        },
        {
            "id": "task460-b9dcfe1f2a0d4da0924ff73bcf507674",
            "input": "We constructed our seed lexicon consisting of 15 positive words and 15 negative words, as shown in Section SECREF27.  \n Question: How big is seed lexicon used for training?",
            "output": [
                "30 words"
            ]
        },
        {
            "id": "task460-ac90aa55563a419d8a9ee350a9d4a4ab",
            "input": "The character sets of these 7 languages have little overlap except that (i) they all include common basic Latin alphabet, and (ii) both Hindi and Marathi use Devanagari script. We took the union of 7 character sets therein as the multilingual grapheme set (Section SECREF2), which contained 432 characters. \n Question: How much of the ASR grapheme set is shared between languages?",
            "output": [
                "Little overlap except common basic Latin alphabet and that Hindi and Marathi languages use same script."
            ]
        },
        {
            "id": "task460-e13322ea42004fcb86e74d438df0377b",
            "input": "Last but not least, ethics and fairness are important considerations, that deserve to be studied. In that sense, detection of individual and global bias should be prioritized in order to give useful feedbacks to practitioners. Furthermore we are considering using adversarial learning as in BIBREF33 in order to ensure fairness during the training process. \n Question: Do they analyze if their system has any bias?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-c614b775431147f28f384be564726328",
            "input": "We consider the following metrics for automatic evaluation of different submissions. Joint goal accuracy has been used as the primary metric to rank the submissions.\n\nActive Intent Accuracy: The fraction of user turns for which the active intent has been correctly predicted.\n\nRequested Slot F1: The macro-averaged F1 score for requested slots over all eligible turns. Turns with no requested slots in ground truth and predictions are skipped.\n\nAverage Goal Accuracy: For each turn, we predict a single value for each slot present in the dialogue state. This is the average accuracy of predicting the value of a slot correctly.\n\nJoint Goal Accuracy: This is the average accuracy of predicting all slot assignments for a given service in a turn correctly. \n Question: How are the models evaluated?",
            "output": [
                "Active Intent Accuracy Requested Slot F1 Average Goal Accuracy Joint Goal Accuracy"
            ]
        },
        {
            "id": "task460-6c9fc21064f2414da6948eec4bc3c73a",
            "input": "As we only extracted references to other judicial decisions, we obtained 471,319 references from Supreme Court decisions, 167,237 references from Supreme Administrative Court decisions and 264,463 references from Constitutional Court Decisions. These are numbers of text spans identified as references prior the further processing described in Section SECREF3. \n Question: How big is the dataset?",
            "output": [
                "903019 references"
            ]
        },
        {
            "id": "task460-a7eac29927dc43d3919814b7f342cadf",
            "input": "Therefore, we used the crowdsourcing platform CrowdFlower (CF) for our data collection. \n Question: How was this data collected?",
            "output": [
                "CrowdFlower"
            ]
        },
        {
            "id": "task460-d477bfdd4e0e4a04a880438343b5bd68",
            "input": " For the embeddings, we relied on $AraVec$ BIBREF30 for Arabic, FastText BIBREF31 for French, and Word2vec Google News BIBREF32 for English .  \n Question: What monolingual word representations are used?",
            "output": [
                "AraVec for Arabic, FastText for French, and Word2vec Google News for English."
            ]
        },
        {
            "id": "task460-93ed6a479397403d8d0501309961d429",
            "input": "We built 16 models of word embeddings using the implementation of CBOW and Skip-gram methods in the FastText tool BIBREF9 . \n Question: What embedding algorithm is used to build the embeddings?",
            "output": [
                "CBOW and Skip-gram methods in the FastText tool BIBREF9"
            ]
        },
        {
            "id": "task460-8da345d174b7460c9bf82eccfd624589",
            "input": "The data used for this task is drawn from the Switchboard conversational English corpus BIBREF31 . \n Question: Which dataset do they use?",
            "output": [
                "Switchboard conversational English corpus"
            ]
        },
        {
            "id": "task460-cbc45b2c9328452cbdf85294bdffc2b5",
            "input": "We use the LSTM network as follows. The $x$ vector is fed through the LSTM network which outputs a vector $\\overrightarrow{h_i}$ for each time step $i$ from 0 to $n-1$. This is the forward LSTM. As we have access to the complete vector $x$, we can process a backward LSTM as well. This is done by computing a vector $\\overleftarrow{h_i}$ for each time step $i$ from $n-1$ to 0. Finally, we concatenate the backward LSTM with the forward LSTM:\n\nBoth $\\overrightarrow{h_i}$ and $\\overleftarrow{h_i}$ have a dimension of $l$, which is an optimized hyperparameter. The BiLSTM output $h$ thus has dimension $2l\\times n$. \n Question: Is the LSTM bidirectional?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-9019d126ce2946a38d5e6e5f42400805",
            "input": "For the Russian language, with its rich morphology, lemmatizing the training and testing data for ELMo representations yields small but consistent improvements in the WSD task.  \n Question: What other examples of morphologically-rich languages do the authors give?",
            "output": [
                "Russian"
            ]
        },
        {
            "id": "task460-33dd8b595f8e48948181ae0d5589fa46",
            "input": "Starting with this list, we can locate the profile page for a user, and subsequently extract additional information, which includes fields such as name, email, occupation, industry, and so forth. We also generate two maps that delineate the gender distribution in the dataset. Our dataset provides mappings between location, profile information, and language use, which we can leverage to generate maps that reflect demographic, linguistic, and psycholinguistic properties of the population represented in the dataset. \n Question: Which demographic dimensions of people do they obtain?",
            "output": [
                "occupation industry profile information language use gender "
            ]
        },
        {
            "id": "task460-e71d044f138b4224a155af203445b9f7",
            "input": "We evaluate the adversarially trained models, as shown in Table TABREF18.\n\nAfter adversarial training, the performance of all the target models raises significantly, while that on the original examples remain comparable. \n Question: How much in experiments is performance improved for models trained with generated adversarial examples?",
            "output": [
                "Answer with content missing: (Table 1) The performance of all the target models raises significantly, while that on the original\nexamples remain comparable (e.g. the overall accuracy of BERT on modified examples raises from 24.1% to 66.0% on Quora)"
            ]
        },
        {
            "id": "task460-12db6ef3d87942d2a83b7aa32f0cc57a",
            "input": "While it is obvious that our embeddings can be used as features for new predictive models, it is also very easy to incorporate our learned Dolores embeddings into existing predictive models on knowledge graphs. The only requirement is that the model accepts as input, an embedding layer (for entities and relations). If a model fulfills this requirement (which a large number of neural models on knowledge graphs do), we can just use Dolores embeddings as a drop-in replacement. We just initialize the corresponding embedding layer with Dolores embeddings. In our evaluation below, we show how to improve several state-of-the-art models on various tasks simply by incorporating Dolores as a drop-in replacement to the original embedding layer. \n Question: How are meaningful chains in the graph selected?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-eef2e0ce0bc543a599473c251ee51207",
            "input": "The second trend is that by using less than 50% of the data available the model tends to overfit the data, as indicated by the consistent increase in the validation loss after about 15 epochs (check dashed lines in right side of Figure FIGREF28 ). \n Question: What experimental results suggest that using less than 50% of the available training examples might result in overfitting?",
            "output": [
                "consistent increase in the validation loss after about 15 epochs"
            ]
        },
        {
            "id": "task460-2ba65ce11fc24a219a2eee093361edf5",
            "input": "Experimental Studies ::: Comparison with State-of-the-art Methods\nSince BERT has already achieved the state-of-the-art performance of question-answering, in this section we compare our proposed model with state-of-the-art question answering models (i.e. QANet BIBREF39) and BERT-Base BIBREF26. As BERT has two versions: BERT-Base and BERT-Large, due to the lack of computational resource, we can only compare with BERT-Base model instead of BERT-Large. \n Question: What baselines is the proposed model compared against?",
            "output": [
                "BERT-Base QANet"
            ]
        },
        {
            "id": "task460-ae205c8c0ce641e2962a7aae07e2a5fd",
            "input": "Understanding a user's intent and sentiment is of utmost importance for current intelligent chatbots to respond appropriately to human requests. However, current systems are not able to perform to their best capacity when presented with incomplete data, meaning sentences with missing or incorrect words. This scenario is likely to happen when one considers human error done in writing. In fact, it is rather naive to assume that users will always type fully grammatically correct sentences.  \n Question: How do the authors define or exemplify 'incorrect words'?",
            "output": [
                "typos in spellings or ungrammatical words"
            ]
        },
        {
            "id": "task460-55dd14e9ad554dd2af7e3df962ba8c8e",
            "input": "The evaluation of chatbots remains an open problem in the field. Recent work BIBREF25 has shown that the automatic evaluation metrics borrowed from machine translation such as BLEU score BIBREF26 tend to align poorly with human judgement. Therefore, in this paper, we mainly adopt human evaluation, along with perplexity, following the existing work. \n Question: Is some other metrics other then perplexity measured?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-26fea1dc128a4a0ca92d4d59bde9526d",
            "input": "We use the morphological tagging datasets provided by the Universal Dependencies (UD) treebanks (the concatenation of the $4^\\text{th}$ and $6^\\text{th}$ columns of the file format) BIBREF13 . \n Question: On which dataset is the experiment conducted?",
            "output": [
                "We use the morphological tagging datasets provided by the Universal Dependencies (UD) treebanks (the concatenation of the $4^\\text{th}$ and $6^\\text{th}$ columns of the file format) BIBREF13 . "
            ]
        },
        {
            "id": "task460-9c113d4e049e47e595a50f3b0207af6e",
            "input": "A causal attribution dataset is a collection of text pairs that reflect cause-effect relationships proposed by humans (for example, “virus causes sickness”). These written statements identify the nodes of the network (see also our graph fusion algorithm for dealing with semantically equivalent statements) while cause-effect relationships form the directed edges (“virus” $\\rightarrow $ “sickness”) of the causal attribution network. \n Question: What are causal attribution networks?",
            "output": [
                "networks where nodes represent causes and effects, and directed edges represent cause-effect relationships proposed by humans"
            ]
        },
        {
            "id": "task460-1cd6b5d08f254b19920b6f05c570e419",
            "input": "We train MaLOPa on the concantenation of training sections of all seven languages. \n Question: How many languages have this parser been tried on?",
            "output": [
                "seven"
            ]
        },
        {
            "id": "task460-78fd166293834b2181549677cbbc9440",
            "input": "We segment a hashtag into meaningful English phrases. In order to achieve this, we use a dictionary of English words. The sentiment of url: Since almost all the articles are written in well-formatted english, we analyze the sentiment of the first paragraph of the article using Standford Sentiment Analysis tool BIBREF4 .  \n Question: Do the authors report only on English language data?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-ec9df178146f45d7bca980f08a555be7",
            "input": "We found that 51.25% of the gang members collected have a tweet that links to a YouTube video. Following these links, a simple keyword search for the terms gangsta and hip-hop in the YouTube video description found that 76.58% of the shared links are related to hip-hop music, gangster rap, and the culture that surrounds this music genre. \n Question: What are the differences in the use of YouTube links between gang member and the rest of the Twitter population?",
            "output": [
                "76.58% of the shared links are related to hip-hop music, gangster rap, and the culture that surrounds this music genre"
            ]
        },
        {
            "id": "task460-a9ac5eb2f75144dc935aa8c2bc34a24c",
            "input": "The lexicon is built over all the data and therefore includes the vocabulary from both the training and testing sets. \n Question: Is the lexicon the same for all languages?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-a2b946a8857247ffa1a2863987d516c5",
            "input": "We validate our approach on the Gigaword corpus, which comprises of a training set of 3.8M article headlines (considered to be the full text) and titles (summaries), along with 200K validation pairs, and we report test performance on the same 2K set used in BIBREF7.  \n Question: What dataset they use for evaluation?",
            "output": [
                "The same 2K set from Gigaword used in BIBREF7"
            ]
        },
        {
            "id": "task460-4a99865d91e143908cc4c640e53c66af",
            "input": "Health Level Seven Fast Healthcare Interoperability Resources (HL7 FHIR)\nFHIR BIBREF5 is a new open standard for healthcare data developed by the same company that developed HL7v2. Resource Description Framework (RDF)\nRDF is the backbone of the semantic webBIBREF8. \n Question: What do FHIR and RDF stand for?",
            "output": [
                "Health Level Seven Fast Healthcare Interoperability Resources (HL7 FHIR) Resource Description Framework (RDF)"
            ]
        },
        {
            "id": "task460-2c421abc4bee4444b5b9da91d01392c7",
            "input": "As shown in Table TABREF12, the T-T model significantly outperforms the LSTM-based RNN-T baseline. \n Question: What was previous state of the art model?",
            "output": [
                "LSTM-based RNN-T"
            ]
        },
        {
            "id": "task460-1f77d34fcfa44a0b9e6d6e9c73633b56",
            "input": "Our second approach incorporates shallow syntactic information in downstream tasks via token-level chunk label embeddings. Task training (and test) data is automatically chunked, and chunk boundary information is passed into the task model via BIOUL encoding of the labels. We add randomly initialized chunk label embeddings to task-specific input encoders, which are then fine-tuned for task-specific objectives. \n Question: Which syntactic features are obtained automatically on downstream task data?",
            "output": [
                "token-level chunk label embeddings  chunk boundary information is passed into the task model via BIOUL encoding of the labels"
            ]
        },
        {
            "id": "task460-0e75afb8e7144f2cb59174fe885488f9",
            "input": "N-GrAM ranked first in all cases except for the language variety task. In this case, the baseline was the top-ranked system, and ours was second by a small margin. Our system significantly out-performed the baseline on the joint task, as the baseline scored significantly lower for the gender task than for the variety task.\n\n \n Question: On which task does do model do worst?",
            "output": [
                "Gender prediction task"
            ]
        },
        {
            "id": "task460-1001485317bb403b8477dfa1f494373b",
            "input": "For a purpose of learning from limited annotated linguistic resources, our preliminary discovery shows that it is possible to build a geometric space projection between embedding spaces to help cross-lingual NE recognition. \n Question: What is their model?",
            "output": [
                "cross-lingual NE recognition"
            ]
        },
        {
            "id": "task460-4ef9cc6c5b4c4c72bde05b3d3454a764",
            "input": "For the McGurk effect, we attempt an illusion for a language token (e.g. phoneme, word, sentence) $x$ by creating a video where an audio stream of $x$ is visually dubbed over by a person saying $x^{\\prime }\\ne x$ . The illusion $f(x^{\\prime },x)$ affects a listener if they perceive what is being said to be $y\\ne x$ if they watched the illusory video whereas they perceive $x$ if they had either listened to the audio stream without watching the video or had watched the original unaltered video, depending on specification.  A prototypical example is that the audio of the phoneme “baa,” accompanied by a video of someone mouthing “vaa”, can be perceived as “vaa” or “gaa” (Figure 1 ). \n Question: What is the McGurk effect?",
            "output": [
                "a perceptual illusion, where listening to a speech sound while watching a mouth pronounce a different sound changes how the audio is heard"
            ]
        },
        {
            "id": "task460-f25a883361f24178a2f69a0a566d62c1",
            "input": "We use four systems to evaluate the difficulty of this dataset.  The first two are an information retrieval system and a word-association method, following the designs of BIBREF26 Clark2016CombiningRS. These are naive baselines that do not parse the question, but nevertheless may find some signal in a large corpus of text that helps guess the correct answer. The third is a CCG-style rule-based semantic parser written specifically for friction questions (the QuaRel INLINEFORM0 subset), but prior to data being collected. The last is a state-of-the-art neural semantic parser. We briefly describe each in turn. \n Question: Which off-the-shelf tools do they use on QuaRel?",
            "output": [
                "information retrieval system word-association method  CCG-style rule-based semantic parser written specifically for friction questions state-of-the-art neural semantic parser"
            ]
        },
        {
            "id": "task460-18bed7f690114ba2ab89461b3ff86c9f",
            "input": "We conduct various experiments to illustrate the properties that are encouraged via different KL magnitudes. In particular, we revisit the interdependence between rate and distortion, and shed light on the impact of KL on the sharpness of the approximated posteriors. Then, through a set of qualitative and quantitative experiments for text generation, we demonstrate how certain generative behaviours could be imposed on VAEs via a range of maximum channel capacities. Finally, we run some experiments to find if any form of syntactic information is encoded in the latent space. \n Question: What different properties of the posterior distribution are explored in the paper?",
            "output": [
                "interdependence between rate and distortion impact of KL on the sharpness of the approximated posteriors demonstrate how certain generative behaviours could be imposed on VAEs via a range of maximum channel capacities some experiments to find if any form of syntactic information is encoded in the latent space"
            ]
        },
        {
            "id": "task460-aefbe77effd24eb0a442a7859f032dad",
            "input": "In this work, we make use of the widely-recognized state of the art entailment technique – BERT BIBREF18, and train it on three mainstream entailment datasets: MNLI BIBREF19, GLUE RTE BIBREF20, BIBREF21 and FEVER BIBREF22, respectively. We convert all datasets into binary case: “entailment” vs. “non-entailment”, by changing the label “neutral” (if exist in some datasets) into “non-entailment”.\n\nFor our label-fully-unseen setup, we directly apply this pretrained entailment model on the test sets of all $\\textsc {0shot-tc}$ aspects. For label-partially-unseen setup in which we intentionally provide annotated data, we first pretrain BERT on the MNLI/FEVER/RTE, then fine-tune on the provided training data. \n Question: Do they use pretrained models?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-7a10db24c9dd4bdaaae0146a6197c44f",
            "input": "In particular, we aggregate documents from the CommonCrawl dataset that has the most overlapping n-grams with the questions. We name this dataset STORIES since most of the constituent documents take the form of a story with long chain of coherent events. Figure 5 -left and middle show that STORIES always yield the highest accuracy for both types of input processing. \n Question: Which of their training domains improves performance the most?",
            "output": [
                "documents from the CommonCrawl dataset that has the most overlapping n-grams with the question"
            ]
        },
        {
            "id": "task460-629a98541ed7488f8c60a12f15fb8004",
            "input": "Our goal was to generate questions without templates and with minimal human involvement using machine learning transformers that have been demonstrated to train faster and better than RNNs. Such a system would benefit educators by saving time to generate quizzes and tests. \n Question: What is the motivation behind the work? Why question generation is an important task?",
            "output": [
                "Such a system would benefit educators by saving time to generate quizzes and tests."
            ]
        },
        {
            "id": "task460-eae096a48e764f25838e9abaa0621026",
            "input": "GhostVLAD is an extension of the NetVLAD approach, which we discussed in the previous section. GhostVLAD works exactly similar to NetVLAD except it adds Ghost clusters along with the NetVLAD clusters. So, now we will have a K+G number of clusters instead of K clusters. The Ghost clusters are added to map any noisy or irrelevant content into ghost clusters and are not included during the feature aggregation stage, as shown in Figure 1 (Right side). \n Question: What is the GhostVLAD approach?",
            "output": [
                "extension of the NetVLAD adds Ghost clusters along with the NetVLAD clusters"
            ]
        },
        {
            "id": "task460-a19e0738c5f5493ba6bfced994f85aa2",
            "input": "Modules: Modules are the basic building blocks of different models. In LeafNATS, we provide ready-to-use modules for constructing recurrent neural network (RNN)-based sequence-to-sequence (Seq2Seq) models for NATS, e.g., pointer-generator network BIBREF1 . These modules include embedder, RNN encoder, attention BIBREF24 , temporal attention BIBREF6 , attention on decoder BIBREF2 and others. We also use these basic modules to assemble a pointer-generator decoder module and the corresponding beam search algorithms. The embedder can also be used to realize the embedding-weights sharing mechanism BIBREF2 . \n Question: What models are included in the toolkit?",
            "output": [
                " recurrent neural network (RNN)-based sequence-to-sequence (Seq2Seq) models for NATS"
            ]
        },
        {
            "id": "task460-135ee58348d64e5ca0d276184350c10d",
            "input": "During the final training process, 500 validation data is used to generate the recurrent neural model, which is 3% of the training data. After finishing the training, the accuracy of the generated model using validation data from the source corpus was 74.40% \n Question: What dataset is used to measure accuracy?",
            "output": [
                "validation data"
            ]
        },
        {
            "id": "task460-62bf875c739642968f333b1275fe7057",
            "input": "For supertagging, we observe that the baseline cross entropy trained model improves its predictions with beam search decoding compared to greedy decoding by 2 accuracy points, which suggests that beam search is already helpful for this task, even without search-aware training. \n Question: By how much do they outperform basic greedy and cross-entropy beam decoding?",
            "output": [
                "2 accuracy points"
            ]
        },
        {
            "id": "task460-ded72ab608224d5bba660233c810751f",
            "input": "We first use predictors based on rules that have previously been proposed in the literature: word length, number of phonemes, number of syllables, alphabetical order, and frequency.  \n Question: What previously proposed rules for predicting binoial ordering are used?",
            "output": [
                "word length, number of phonemes, number of syllables, alphabetical order, and frequency"
            ]
        },
        {
            "id": "task460-1ac31e06da3049abb19a6aba0973545c",
            "input": "Logistic regression: To produce the representation of the input, we concatenate the Bag-Of-Words representation of the document with the Bag-Of-Words representation of the question. LSTM: We start with a concatenation of the sequence of indexes of the document with the sequence of indexes of the question. Them we feed an LSTM network with this vector and use the final state as the representation of the input. Finally, we apply a logistic regression over this representation to produce the final decision. End-to-end memory networks: This architecture is based on two different memory cells (input and output) that contain a representation of the document. Deep projective reader: This is a model of our own design, largely inspired by the efficient R-net reader BIBREF12 . \n Question: What baselines are presented?",
            "output": [
                "Logistic regression LSTM End-to-end memory networks Deep projective reader"
            ]
        },
        {
            "id": "task460-ac5ab5bff07442c0ad22c283e1a42dd7",
            "input": "We evaluate our approach on two well benchmarked sequence tagging tasks, the CoNLL 2003 NER task BIBREF13 and the CoNLL 2000 Chunking task BIBREF14 .  \n Question: what are the evaluation datasets?",
            "output": [
                "CoNLL 2003 CoNLL 2000"
            ]
        },
        {
            "id": "task460-e6f81d3ab70c41ad932947bbeb9df25a",
            "input": "We incorporate typing information by concatenating to the embedding vector of each input symbol one of three embedding vectors, S, E or R, where S is concatenated to structural elements (opening and closing brackets), E to entity symbols and R to relation symbol \n Question: How are typing hints suggested?",
            "output": [
                " concatenating to the embedding vector"
            ]
        },
        {
            "id": "task460-420ccb6c642a44d6b82babeb0fac07de",
            "input": "This step entails counting occurrences of all words in the training corpus and sorting them in order of decreasing occurrence. As mentioned, the vocabulary is taken to be the INLINEFORM0 most frequently occurring words, that occur at least some number INLINEFORM1 times. It is implemented in Spark as a straight-forward map-reduce job. \n Question: Do they perform any morphological tokenization?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-782e34f148714041a34f8bc8ebe17d0a",
            "input": "In order to provide directions for future work, we analyze the errors made by the classifier trained on the extended features on the four prediction tasks.\n\nErrors on Intention (I) prediction: The lack of background is a major problem when identifying trolling comments. Non-cursing aggressions and insults This is a challenging problem, since the majority of abusive and insulting comments rely on profanity and swearing.  Another source of error is the presence of controversial topic words such as “black”,“feminism”, “killing”, “racism”, “brown”, etc. that are commonly used by trolls. Errors on Disclosure (D) prediction: A major source of error that affects disclosure is the shallow meaning representation obtained from the BOW model even when augmented with the distributional features given by the glove vectors. Errors on Interpretation (R) prediction: it is a common practice from many users to directly ask the suspected troll if he/she is trolling or not.  Errors on Response Strategy (B) prediction: In some cases there is a blurry line between “Frustrate” and “Neutralize”.  Another challenging problem is the distinction between the classes “Troll” and “Engage”.  \n Question: What is an example of a difficult-to-classify case?",
            "output": [
                "The lack of background Non-cursing aggressions and insults the presence of controversial topic words   shallow meaning representation directly ask the suspected troll if he/she is trolling or not a blurry line between “Frustrate” and “Neutralize” distinction between the classes “Troll” and “Engage”"
            ]
        },
        {
            "id": "task460-dea1e7c14716406ca3be105bb011c9c6",
            "input": " Since we are interested in the zero-shot capabilities of our representation, we trained our sentiment analysis model only on the english IMDB Large Movie Review dataset and tested it on the chinese ChnSentiCorp dataset and german SB-10K BIBREF24 , BIBREF25 . A natural language inference task consists of two sentences; a premise and a hypothesis which are either contradictions, entailments or neutral. Learning a NLI task takes a certain nuanced understanding of language. Therefore it is of interest whether or not UG-WGAN captures the necessary linguistic features.  \n Question: Did they experiment with tasks other than word problems in math?",
            "output": [
                "They experimented with sentiment analysis and natural language inference task"
            ]
        },
        {
            "id": "task460-8a774f369cd5437589e73a3dafb2322a",
            "input": " Models include both neural nets (e.g. RNNs, CNNs) and standard machine learning tools (e.g. Naive Bayes with Laplace Smoothing, k-clustering, SVM with linear kernel).  \n Question: Which machine learning models are used?",
            "output": [
                "RNNs CNNs Naive Bayes with Laplace Smoothing k-clustering SVM with linear kernel"
            ]
        },
        {
            "id": "task460-63aa348857384876961e5db613e36298",
            "input": "To assess the predictive capability of this and other models, we require some method by which we can compare the models. For that purpose, we use receiver operating characteristic (ROC) curves as a visual representation of predictive effectiveness. ROC curves compare the true positive rate (TPR) and false positive rate (FPR) of a model's predictions at different threshold levels. The area under the curve (AUC) (between 0 and 1) is a numerical measure, where the higher the AUC is, the better the model performs. We cross-validate our model by first randomly splitting the corpus into a training set (95% of the corpus) and test set (5% of the corpus). We then fit the model to the training set, and use it to predict the response of the documents in the test set. We repeat this process 100 times. The threshold-averaged ROC curve BIBREF13 is found from these predictions, and shown in Figure 3 . Table 1 shows the AUC for each model considered. \n Question: How is performance measured?",
            "output": [
                "they use ROC curves and cross-validation"
            ]
        },
        {
            "id": "task460-10fee3925ff1461e8cf2d5c657a6faa1",
            "input": " In the future, we are planning to pay attention on a generalized language model for code-mixed texts which can also handle Hindi-code-mixed and other multi-lingual code-mixed datasets (i.e., trying to reduce the dependencies on language-specific code-mixed resources). Our system outperforms all the previous state of the art approaches used for aggression identification on English code-mixed TRAC data, while being trained only from Facebook comments the system outperforms other approaches on the additional Twitter test set. The fine-grained definition of the aggressiveness/aggression identification is provided by the organizers of TRAC-2018 BIBREF0, BIBREF2. They have classified the aggressiveness into three labels (Overtly aggressive(OAG), Covertly aggressive(CAG), Non-aggressive(NAG)). The detailed description for each of the three labels is described as follows:\n\nOvertly Aggressive(OAG) - This type of aggression shows direct verbal attack pointing to the particular individual or group. For example, \"Well said sonu..you have courage to stand against dadagiri of Muslims\".\n\nCovertly Aggressive(CAG) - This type of aggression the attack is not direct but hidden, subtle and more indirect while being stated politely most of the times. For example, \"Dear India, stop playing with the emotions of your people for votes.\"\n\nNon-Aggressive(NAG) - Generally these type of text lack any kind of aggression it is basically used to state facts, wishing on occasions and polite and supportive. \n Question: What is English mixed with in the TRAC dataset?",
            "output": [
                "Hindi"
            ]
        },
        {
            "id": "task460-0a90f59c81d5495699e68cf919999904",
            "input": "First, we introduce a two-stage labeling strategy for sentiment texts. In the first stage, annotators are invited to label a large number of short texts with relatively pure sentiment orientations. Each sample is labeled by only one annotator. In the second stage, a relatively small number of text samples with mixed sentiment orientations are annotated, and each sample is labeled by multiple annotators.  \n Question: What is the new labeling strategy?",
            "output": [
                "They use a two-stage labeling strategy where in the first stage single annotators label a large number of short texts with relatively pure sentiment orientations and in the second stage multiple annotators label few text samples with mixed sentiment orientations"
            ]
        },
        {
            "id": "task460-b53cd9ba4a1b4fbbbb8e76eac6768e36",
            "input": "In our dataset, we store the full tweet object including the id of the tweet, username, hashtags, and geolocation of the tweet. We created a list of the most common Arabic keywords associated with COVID-19. Using Twitter’s streaming API, we searched for any tweet containing the keyword(s) in the text of the tweet. Table TABREF1 shows the list of keywords used along with the starting date of tracking each keyword. Furthermore, Table TABREF2 shows the list of hashtags we have been tracking along with the number of tweets collected from each hashtag.  \n Question: What additional information is found in the dataset?",
            "output": [
                "the full tweet object including the id of the tweet, username, hashtags, and geolocation of the tweet"
            ]
        },
        {
            "id": "task460-130ed66b3a9f454786bed185dd59810c",
            "input": "We compare our proposed discrete CVAE (DCVAE) with the two-stage sampling approach to three categories of response generation models:\n\nBaselines: Seq2seq, the basic encoder-decoder model with soft attention mechanism BIBREF30 used in decoding and beam search used in testing; MMI-bidi BIBREF5, which uses the MMI to re-rank results from beam search.\n\nCVAE BIBREF14: We adjust the original work which is for multi-round conversation for our single-round setting. For a fair comparison, we utilize the same keywords used in our network pre-training as the knowledge-guided features in this model.\n\nOther enhanced encoder-decoder models: Hierarchical Gated Fusion Unit (HGFU) BIBREF12, which incorporates a cue word extracted using pointwise mutual information (PMI) into the decoder to generate meaningful responses; Mechanism-Aware Neural Machine (MANM) BIBREF13, which introduces latent embeddings to allow for multiple diverse response generation. \n Question: What other kinds of generation models are used in experiments?",
            "output": [
                " Seq2seq CVAE Hierarchical Gated Fusion Unit (HGFU) Mechanism-Aware Neural Machine (MANM)"
            ]
        },
        {
            "id": "task460-e47daeb59ad74f0f95662352d0d86d33",
            "input": "The dataset includes about 6,000 triples, comprised of videos, questions, and answer spans manually collected from screencast tutorial videos with spoken narratives for a photo-editing software.  \n Question: What kind of instructional videos are in the dataset?",
            "output": [
                "tutorial videos for a photo-editing software"
            ]
        },
        {
            "id": "task460-50ad4d5fc94b4e72a8e0f60fed0d06b8",
            "input": "For Turkish, as the first dataset, we utilised the movie reviews which are collected from a popular website. The number of reviews in this movie corpus is 20,244 and the average number of words in reviews is 39. Each of these reviews has a star-rating score which is indicative of sentiment. These polarity scores are between the values 0.5 and 5, at intervals of 0.5. We consider a review to be negative it the score is equal to or lower than 2.5. On the other hand, if it is equal to or higher than 4, it is assumed to be positive. We have randomly selected 7,020 negative and 7,020 positive reviews and processed only them.\n\n \n Question: What details are given about the movie domain dataset?",
            "output": [
                "there are 20,244 reviews divided into positive and negative with an average 39 words per review, each one having a star-rating score"
            ]
        },
        {
            "id": "task460-183a1078ee714a5c9c047afbfa25cc73",
            "input": "We thus use the dataset introduced by Krishnamurthy and Mitchell krishnamurthy-2015-semparse-open-vocabulary, which consists of the ClueWeb09 web corpus along with Google's FACC entity linking of that corpus to Freebase BIBREF9 . \n Question: What knowledge base do they use?",
            "output": [
                "Freebase"
            ]
        },
        {
            "id": "task460-65bd7df9a03b4f6dae6f2e2813fca7b3",
            "input": "We varied the number of experts between models, using ordinary MoE layers with 4, 32 and 256 experts and hierarchical MoE layers with 256, 1024 and 4096 experts. \n Question: How is the correct number of experts to use decided?",
            "output": [
                "varied the number of experts between models"
            ]
        },
        {
            "id": "task460-35460169e2224f9381cd7ff6c9cded55",
            "input": "If we take the best psr ensemble trained on CBT as a baseline, improving the model architecture as in BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 , BIBREF11 , continuing to use the original CBT training data, lead to improvements of INLINEFORM0 and INLINEFORM1 absolute on named entities and common nouns respectively. By contrast, inflating the training dataset provided a boost of INLINEFORM2 while using the same model. \n Question: How large are the improvements of the Attention-Sum Reader model when using the BookTest dataset?",
            "output": [
                "INLINEFORM2 "
            ]
        },
        {
            "id": "task460-047bf5f3518a466e812a82941128ee77",
            "input": "Our current feature set for training a machine learning classifier is relatively small, consisting of paragraph domain scores, bag-of-words, length of stay, and number of previous admissions, but we intend to factor in many additional features that extend beyond the scope of the present study. These include a deeper analysis of clinical narratives in EHRs: our next task will be to extend our EHR data pipeline by distinguishing between clinically positive and negative phenomena within each risk factor domain. This will involve a series of annotation tasks that will allow us to generate lexicon-based and corpus-based sentiment analysis tools. We can then use these clinical sentiment scores to generate a gradient of patient improvement or deterioration over time. \n Question: What additional features are proposed for future work?",
            "output": [
                "distinguishing between clinically positive and negative phenomena within each risk factor domain and accounting for structured data collected on the target cohort"
            ]
        },
        {
            "id": "task460-d93976f55f794a65840fdf464c7f3a25",
            "input": "We crowd-sourced the collection of the dataset on Amazon Mechanical Turk (MTurk). \n Question: Did the authors use crowdsourcing platforms?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-92218f119a2045959038a3be75907f0a",
            "input": "Crowdsourced annotators assigned similarity to word pairs during the word similarity task.  \n Question: did they use a crowdsourcing platform for annotations?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-ecdf0855875342a48e8f542341ffdde4",
            "input": "The informal setting/environment of social media often encourage multilingual speakers to switch back and forth between languages when speaking or writing. These all resulted in code-mixing and code-switching. Code-mixing refers to the use of linguistic units from different languages in a single utterance or sentence, whereas code-switching refers to the co-occurrence of speech extracts belonging to two different grammatical systemsBIBREF3. This language interchange makes the grammar more complex and thus it becomes tough to handle it by traditional algorithms. Thus the presence of high percentage of code-mixed content in social media text has increased the complexity of the aggression detection task. For example, the dataset provided by the organizers of TRAC-2018 BIBREF0, BIBREF2 is actually a code-mixed dataset. \n Question: What data/studies do the authors provide to support the assertion that the majority of aggressive conversations contain code-mixed languages?",
            "output": [
                "None"
            ]
        },
        {
            "id": "task460-36884b41085e4be19bd972693cba58c2",
            "input": "In our experiment, some widely used text clustering methods are compared with our approach. Besides K-means, Skip-thought Vectors, Recursive Neural Network and Paragraph Vector based clustering methods, four baseline clustering methods are directly based on the popular unsupervised dimensionality reduction methods as described in Section SECREF11 .  \n Question: Which popular clustering methods did they experiment with?",
            "output": [
                "K-means, Skip-thought Vectors, Recursive Neural Network and Paragraph Vector based clustering methods"
            ]
        },
        {
            "id": "task460-55d120cd127c4b26a0e11f9c55e70d9e",
            "input": "In this approach the similarity between two words is not strictly based on their co–occurrence frequencies, but rather on the frequencies of the other words which occur with both of them (i.e., second order co–occurrences). This approach has been shown to be successful in quantifying semantic relatedness BIBREF12 , BIBREF13 . \n Question: What is a second order co-ocurrence matrix?",
            "output": [
                "frequencies of the other words which occur with both of them (i.e., second order co–occurrences)"
            ]
        },
        {
            "id": "task460-39ac84ef866a4f228fd48376c459366c",
            "input": "Based on annotated corpora and token-based features, studies used machine learning approaches to build word segmentation systems with accuracy about 94%-97%. \n Question: How successful are the approaches used to solve word segmentation in Vietnamese?",
            "output": [
                "Their accuracy in word segmentation is about 94%-97%."
            ]
        },
        {
            "id": "task460-712e01bf76c34165834c9f1eab7a931c",
            "input": "Contexts are either ground-truth contexts from that dataset, or they are Wikipedia passages retrieved using TF-IDF BIBREF24 based on a HotpotQA question. A new non-overlapping set of contexts was again constructed from Wikipedia via HotpotQA using the same method as Round 1. In addition to contexts from Wikipedia for Round 3, we also included contexts from the following domains: News (extracted from Common Crawl), fiction (extracted from BIBREF27, and BIBREF28), formal spoken text (excerpted from court and presidential debate transcripts in the Manually Annotated Sub-Corpus (MASC) of the Open American National Corpus), and causal or procedural text, which describes sequences of events or actions, extracted from WikiHow. Finally, we also collected annotations using the longer contexts present in the GLUE RTE training data, which came from the RTE5 dataset BIBREF29. \n Question: What data sources do they use for creating their dataset?",
            "output": [
                "Wikipedia (of 250-600 characters) from the manually curated HotpotQA training set Manually Annotated Sub-Corpus (MASC) of the Open American National Corpus) RTE5"
            ]
        },
        {
            "id": "task460-313a8cac596e4e0a86a052a80bf79498",
            "input": "For the intitial data collection using the CRWIZ platform, 145 unique dialogues were collected (each dialogue consists of a conversation between two participants).  The average time per assignment was 10 minutes 47 seconds, very close to our initial estimate of 10 minutes, and the task was available for 5 days in AMT. Out of the 145 dialogues, 14 (9.66%) obtained the bonus of $0.2 for resolving the emergency. We predicted that only a small portion of the participants would be able to resolve the emergency in less than 6 minutes, thus it was framed as a bonus challenge rather than a requirement to get paid. The fastest time recorded to resolve the emergency was 4 minutes 13 seconds with a mean of 5 minutes 8 seconds. Table TABREF28 shows several interaction statistics for the data collected compared to the single lab-based WoZ study BIBREF4.\n\nData Analysis ::: Subjective Data\nTable TABREF33 gives the results from the post-task survey. We observe, that subjective and objective task success are similar in that the dialogues that resolved the emergency were rated consistently higher than the rest.\n\nMann-Whitney-U one-tailed tests show that the scores of the Emergency Resolved Dialogues for Q1 and Q2 were significantly higher than the scores of the Emergency Not Resolved Dialogues at the 95% confidence level (Q1: $U = 1654.5$, $p < 0.0001$; Q2: $U = 2195$, $p = 0.009$, both $p < 0.05$). This indicates that effective collaboration and information ease are key to task completion in this setting.\n\nRegarding the qualitative data, one of the objectives of the Wizard-of-Oz technique was to make the participant believe that they are interacting with an automated agent and the qualitative feedback seemed to reflect this: “The AI in the game was not helpful at all [...]” or “I was talking to Fred a bot assistant, I had no other partner in the game“.\n\nData Analysis ::: Single vs Multiple Wizards\nIn Table TABREF28, we compare various metrics from the dialogues collected with crowdsourcing with the dialogues previously collected in a lab environment for a similar task. Most figures are comparable, except the number of emergency assistant turns (and consequently the total number of turns). To further understand these differences, we have first grouped the dialogue acts in four different broader types: Updates, Actions, Interactions and Requests, and computed the relative frequency of each of these types in both data collections. In addition, Figures FIGREF29 and FIGREF30 show the distribution of the most frequent dialogue acts in the different settings. It is visible that in the lab setting where the interaction was face-to-face with a robot, the Wizard used more Interaction dialogue acts (Table TABREF32). These were often used in context where the Wizard needed to hold the turn while looking for the appropriate prompt or waiting for the robot to arrive at the specified goal in the environment. On the other hand, in the crowdsourced data collection utterances, the situation updates were a more common choice while the assistant was waiting for the robot to travel to the specified goal in the environment.\n\nPerhaps not surprisingly, the data shows a medium strong positive correlation between task success and the number of Action type dialogue acts the Wizard performs, triggering events in the world leading to success ($R=0.475$). There is also a positive correlation between task success and the number of Request dialogue acts requesting confirmation before actions ($R=0.421$), e.g., “Which robot do you want to send?”. As Table 3 shows, these are relatively rare but perhaps reflect a level of collaboration needed to further the task to completion. Table TABREF40 shows one of the dialogues collected where the Emergency Assistant continuously engaged with the Operator through these types of dialogue acts.\n\nThe task success rate was also very different between the two set-ups. In experiments reported in BIBREF4, 96% of the dialogues led to the extinction of the fire whereas in the crowdsourcing setting only 9.66% achieved the same goal. In the crowdsourced setting, the robots were slower moving at realistic speeds unlike the lab setting. A higher bonus and more time for the task might lead to a higher task success rate.\n\nData Analysis ::: Limitations\nIt is important to consider the number of available participants ready and willing to perform the task at any one time. This type of crowdsourcing requires two participants to connect within a few minutes of each other to be partnered together. As mentioned above, there were some issues with participants not collaborating and these dialogues had to be discarded as they were not of use. \n Question: Is CRWIZ already used for data collection, what are the results?",
            "output": [
                "Yes, CRWIZ has been used for data collection and its initial use resulted in 145 dialogues. The average time taken for the task was close to the estimate of 10 minutes, 14 dialogues (9.66%) resolved the emergency in the scenario, and these dialogues rated consistently higher in subjective and objective ratings than those which did not resolve the emergency. Qualitative results showed that participants believed that they were interacting with an automated assistant."
            ]
        },
        {
            "id": "task460-8e826b92bec7483a956f3a1437815b54",
            "input": "Table TABREF23 shows the unlabeled INLINEFORM0 scores for our models and various baselines. \n Question: what were the evaluation metrics?",
            "output": [
                "INLINEFORM0 scores"
            ]
        },
        {
            "id": "task460-9d966b5922d04cc693f30bf6b8e3246f",
            "input": "We demonstrated the utility of Katecheo by deploying the system for question answering in two topics, Medical Sciences and Christianity. \n Question: how many domains did they experiment with?",
            "output": [
                "2"
            ]
        },
        {
            "id": "task460-0749199a07214ae2848487fc237776b0",
            "input": "We evaluate our proposed model on a Twitter dataset obtained from the authors of BIBREF12. Our final dataset consists of 11,576 users (i.e, fact-checkers), 4,732 fact-checking URLs and 63,429 interactions. The dataset also contains each user's social network information. Note that each user's social relationship is restricted within available users in the dataset. \n Question: What dataset is used?",
            "output": [
                "Twitter dataset obtained from the authors of BIBREF12"
            ]
        },
        {
            "id": "task460-38c40de8ed8b41eca506148ac67d78b8",
            "input": "We used the Pasokh dataset BIBREF42 that contains 100 Persian news documents each of which is associated with 5 summaries. Each summary consists of several sentences of the original text, selected by a human expert. Some sentences are slightly modified and are not, therefore, an exact copy of any original sentences. Documents are categorized into six categories such as political, economic and so on. The length of documents ranges from 4 to 156 sentences. Overall, it has about 2,500 sentences. \n Question: What dataset is used for this task?",
            "output": [
                "the Pasokh dataset BIBREF42 "
            ]
        },
        {
            "id": "task460-685aceebd456493cb95099f1ab841221",
            "input": "Introduced by BIBREF16 , in this dataset, the input query is the concatenation of a Wikipedia article title with the title of one of its section. The relevant passages are the paragraphs within that section. The corpus consists of all of the English Wikipedia paragraphs, except the abstracts. The released dataset has five predefined folds, and we use the first four as a training set (approximately 3M queries), and the remaining as a validation set (approximately 700k queries). The test set is the same one used to evaluate the submissions to TREC-CAR 2017 (approx. 1,800 queries). \n Question: What is the TREC-CAR dataset?",
            "output": [
                "in this dataset, the input query is the concatenation of a Wikipedia article title with the title of one of its section. The relevant passages are the paragraphs within that section"
            ]
        },
        {
            "id": "task460-b5ebdd98d42042149ed59d99f1ca28a8",
            "input": "To evaluate the influence of our hypersphere feature for off-the-shelf NER systems, we perform the NE recognition on two standard NER benchmark datasets, CoNLL2003 and ONTONOTES 5.0. \n Question: Do they evaluate on NER data sets?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-f0fec59411254f80bba03830b8e90e34",
            "input": "As in the example above, we pre-process documents by removing all numbers and interjections. \n Question: what processing was done on the speeches before being parsed?",
            "output": [
                "Remove numbers and interjections"
            ]
        },
        {
            "id": "task460-ea7e84219d7745508013bbe9623e27e0",
            "input": "We use precision, recall and F-measure to evaluate the detected revisions. \n Question: What metrics are used to evaluation revision detection?",
            "output": [
                "precision recall F-measure"
            ]
        },
        {
            "id": "task460-a32e0d282558427282b4bb2d32f0d4a2",
            "input": " Overall, this resulted in a total of 34,432 user conversations. Together, these users gave Gunrock an average rating of 3.65 (median: 4.0), which was elicited at the end of the conversation (“On a scale from 1 to 5 stars, how do you feel about talking to this socialbot again?\"). \n Question: What is the sample size of people used to measure user satisfaction?",
            "output": [
                "34,432 user conversations"
            ]
        },
        {
            "id": "task460-bbe7d0d35c48442393df9836947e4dc6",
            "input": " Transfer learning based approaches\nMathur et al. in their paper for detecting offensive tweets proposed a Ternary Trans-CNN model where they train a model architecture comprising of 3 layers of Convolution 1D having filter sizes of 15, 12 and 10 and kernel size of 3 followed by 2 dense fully connected layer of size 64 and 3. The first dense FC layer has ReLU activation while the last Dense layer had Softmax activation. They were able to train this network on a parallel English dataset provided by Davidson et al. The authors were able to achieve Accuracy of 83.9%, Precision of 80.2%, Recall of 69.8%.\n\nThe approach looked promising given that the dataset was merely 3189 sentences divided into three categories and thus we replicated the experiment but failed to replicate the results. The results were poor than what the original authors achieved. But, most of the model hyper-parameter choices where inspired from this work.\n\nRelated Work ::: Hybrid models\nIn another localized setting of Vietnamese language, Nguyen et al. in 2017 proposed a Hybrid multi-channel CNN and LSTM model where they build feature maps for Vietnamese language using CNN to capture shorterm dependencies and LSTM to capture long term dependencies and concatenate both these feature sets to learn a unified set of features on the messages. These concatenated feature vectors are then sent to a few fully connected layers. They achieved an accuracy rate of 87.3% with this architecture. \n Question: What models do previous work use?",
            "output": [
                "Ternary Trans-CNN  Hybrid multi-channel CNN and LSTM"
            ]
        },
        {
            "id": "task460-dffc0182aef94d28b2015cc4438569af",
            "input": "For the sake of simplicity, we focus our analysis on Airbnb listings from Manhattan, NY, during the time period of January 1, 2016, to January 1, 2017. The data provided to us contained information for roughly 40,000 Manhattan listings that were posted on Airbnb during this defined time period. For each listing, we were given information of the amenities of the listing (number of bathrooms, number of bedrooms …), the listing’s zip code, the host’s description of the listing, the price of the listing, and the occupancy rate of the listing.  \n Question: What is the size of the Airbnb?",
            "output": [
                "roughly 40,000 Manhattan listings"
            ]
        },
        {
            "id": "task460-2e8f7e6add9f4a8180db80dd913e4ea5",
            "input": "We compare with the following baselines:\n\n(1) Naive: A non-domain-adaptive baseline with bag-of-words representations and SVM classifier trained on the source domain.\n\n(2) mSDA BIBREF7 : This is the state-of-the-art method based on discrete input features. Top 1000 bag-of-words features are kept as pivot features. We set the number of stacked layers to 3 and the corruption probability to 0.5.\n\n(3) NaiveNN: This is a non-domain-adaptive CNN trained on source domain, which is a variant of our model by setting INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 to zeros.\n\n(4) AuxNN BIBREF4 : This is a neural model that exploits auxiliary tasks, which has achieved state-of-the-art results on cross-domain sentiment classification. The sentence encoder used in this model is the same as ours.\n\n(5) ADAN BIBREF16 : This method exploits adversarial training to reduce representation difference between domains. The original paper uses a simple feedforward network as encoder. For fair comparison, we replace it with our CNN-based encoder. We train 5 iterations on the discriminator per iteration on the encoder and sentiment classifier as suggested in their paper.\n\n(6) MMD: MMD has been widely used for minimizing domain discrepancy on images. In those works BIBREF9 , BIBREF13 , variants of deep CNNs are used for encoding images and the MMDs of multiple layers are jointly minimized. In NLP, adding more layers of CNNs may not be very helpful and thus those models from image-related tasks can not be directly applied to our problem. To compare with MMD-based method, we train a model that jointly minimize the classification loss INLINEFORM0 on the source domain and MMD between INLINEFORM1 and INLINEFORM2 . For computing MMD, we use a Gaussian RBF which is a common choice for characteristic kernel. \n Question: What are the baseline methods?",
            "output": [
                "(1) Naive (2) mSDA BIBREF7 (3) NaiveNN (4) AuxNN BIBREF4 (5) ADAN BIBREF16 (6) MMD"
            ]
        },
        {
            "id": "task460-83f689c734aa4a86a69b8202a4a0fed0",
            "input": "We used two strategies in combining prediction results of two types of models. Specifically, the Max Score Ensemble model made the final decisions based on the maximum of two scores assigned by the two separate models; instead, the Average Score Ensemble model used the average score to make final decisions. \n Question: How do they combine the models?",
            "output": [
                "maximum of two scores assigned by the two separate models average score"
            ]
        },
        {
            "id": "task460-03d32dd0af5442fbb99413c45d00adcb",
            "input": "We also observe that the best combination seems to consist in training our model on the original out-of-context dataset and testing it on the in-context pairs. In this configuration we reach an F-score (0.72) only slightly lower than the one reported in BIBREF3 (0.74), and we record the highest Pearson correlation, 0.3 (which is still not strong, compared to BIBREF3 's best run, 0.75).  \n Question: What were the results of the first experiment?",
            "output": [
                "Best performance achieved is 0.72 F1 score"
            ]
        },
        {
            "id": "task460-0b42ff101a6f40498cb44b055a5c10de",
            "input": "BIBREF25 also proposed the idea of using average of word embeddings to represent the global context of a document. Different from their work, we choose to corrupt the original document by randomly removing significant portion of words, and represent the document using only the embeddings of the words remained. \n Question: Is their approach similar to making an averaged weighted sum of word vectors, where weights reflect word frequencies?",
            "output": [
                "Different from their work, we choose to corrupt the original document by randomly removing significant portion of words, and represent the document using only the embeddings of the words remained."
            ]
        },
        {
            "id": "task460-69ef3f4d01a240998be2cc1d35589b8d",
            "input": "Previous attempts to annotate QA-SRL initially involved trained annotators BIBREF4 but later resorted to crowdsourcing BIBREF5 to achieve scalability. \n Question: How was the corpus obtained?",
            "output": [
                " trained annotators BIBREF4 crowdsourcing BIBREF5 "
            ]
        },
        {
            "id": "task460-1d94018fb13448f6a15e16e4fb027c37",
            "input": "Baselines. We compare our models against a neural baseline models, hierarchical LSTM (hLSTM), with the attention ablated but with access to the complete context, and a strong, open-sourced feature-rich baseline BIBREF7 . We choose BIBREF7 over other prior works such as BIBREF0 since we do not have access to the dataset or the system used in their papers for replication. BIBREF7 is a logistic regression classifier with features inclusive of bag-of-words representation of the unigrams and thread length, normalised counts of agreements to previous posts, counts of non-lexical reference items such as URLs, and the Coursera forum type in which a thread appeared. We also report aggregated results from a hLSTM model with access only to the last post as context for comparison. Table TABREF17 compares the performance of these baselines against our proposed methods. \n Question: What was the previous state of the art for this task?",
            "output": [
                "hLSTM"
            ]
        },
        {
            "id": "task460-eae25c5829694873961d395df0b19e9c",
            "input": "We define a metric called the Semantic Text Exchange Score (STES) that evaluates the overall ability of a model to perform STE, and an adjustable parameter masking (replacement) rate threshold (MRT/RRT) that can be used to control the amount of semantic change. \n Question: Has STES been previously used in the literature to evaluate similar tasks?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-1fd3386ae2a147bead244c1e79642220",
            "input": "In this step we extract the AMR graphs of the summary sentences using story sentence AMRs. We divide this task in two parts. First is finding the important sentences from the story and then extracting the key information from those sentences using their AMR graphs. \n Question: How are sentences selected from the summary graph?",
            "output": [
                " finding the important sentences from the story extracting the key information from those sentences using their AMR graphs"
            ]
        },
        {
            "id": "task460-278074e99878452fab6355182b998cdd",
            "input": "While in this paper, we focus on the contexts and names of entities, there is a textual source of information about entities in KBs which we can also make use of: descriptions of entities. We extract Wikipedia descriptions of FIGMENT entities filtering out the entities ( $\\sim $ 40,000 out of $\\sim $ 200,000) without description. \n Question: How do you find the entity descriptions?",
            "output": [
                "Wikipedia"
            ]
        },
        {
            "id": "task460-43877d7203a44263bdee59d256095a55",
            "input": "Despite the focus on sharing datasets and source codes on popular software development platforms such as GitHub (github.com) or Zenodo (zenodo.org), it is still a challenge to use data or code from other groups. \n Question: Are datasets publicly available?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-e5ad0318441d4c65ab0e1b4732073f65",
            "input": "The dataset contains approximately 30M utterances of fictional characters. \n Question: How large is the dataset?",
            "output": [
                "30M utterances"
            ]
        },
        {
            "id": "task460-80f2f398dec34742ad5b30ea62172d54",
            "input": "We then perform the following analysis.\n\nSpeaker's Gender Effects: We search for first-person singular pronouns with subject case (ani, unmarked for gender, corresponding to the English I), and consider the gender of its governing verb (or adjectives in copular constructions such as `I am nice'). The possible genders are `masculine', `feminine' and `both', where the latter indicates a case where the none-diacriticized written form admits both a masculine and a feminine reading. We expect the gender to match the ones requested in the prefix.\n\nInterlocutors' Gender and Number Effects: We search for second-person pronouns and consider their gender and number. For pronouns in subject position, we also consider the gender and number of their governing verbs (or adjectives in copular constructions). For a singular audience, we expect the gender and number to match the requested ones. For a plural audience, we expect the masculine-plural forms. \n Question: What type of syntactic analysis is performed?",
            "output": [
                "Speaker's Gender Effects Interlocutors' Gender and Number Effects"
            ]
        },
        {
            "id": "task460-b130782697664eeda65a6a6cc7fb9056",
            "input": "the basic model yields good performance for recognizing explicit discourse relations as well, which is comparable with previous best result (92.05% macro F1-score and 93.09% accuracy as reported in BIBREF11 ). In summary, our full paragraph-level neural network model achieves the best macro-average F1-score of 48.82% in predicting implicit discourse relations, which outperforms previous neural tensor network models (e.g., BIBREF18 ) by more than 2 percents and outperforms the best previous system BIBREF19 by 1 percent. Then we also created ensemble models by applying majority voting to combine results of ten runs. From table 5 , each ensemble model obtains performance improvements compared with single model. The full model achieves performance boosting of (51.84 - 48.82 = 3.02) and (94.17 - 93.21 = 0.96) in macro F1-scores for predicting implicit and explicit discourse relations respectively.  \n Question: How much does this model improve state-of-the-art?",
            "output": [
                "the basic model yields good performance for recognizing explicit discourse relations as well, which is comparable with previous best result (92.05% macro F1-score and 93.09% accuracy as reported in BIBREF11 ). full paragraph-level neural network model achieves the best macro-average F1-score of 48.82% in predicting implicit discourse relations, which outperforms previous neural tensor network models (e.g., BIBREF18 ) by more than 2 percents and outperforms the best previous system BIBREF19 by 1 percent. Then we also created ensemble models by applying majority voting to combine results of ten runs. From table 5 , each ensemble model obtains performance improvements compared with single model. The full model achieves performance boosting of (51.84 - 48.82 = 3.02) and (94.17 - 93.21 = 0.96) in macro F1-scores for predicting implicit and explicit discourse relations respectively. "
            ]
        },
        {
            "id": "task460-3ae4bdb40412443488dabef3a779a24f",
            "input": "We sampled all papers published in the Computer Science subcategories of Artificial Intelligence (cs.AI), Machine Learning (cs.LG), Social and Information Networks (cs.SI), Computational Linguistics (cs.CL), Computers and Society (cs.CY), Information Retrieval (cs.IR), and Computer Vision (CS.CV), the Statistics subcategory of Machine Learning (stat.ML), and Social Physics (physics.soc-ph). We filtered for papers in which the title or abstract included at least one of the words “machine learning”, “classif*”, or “supervi*” (case insensitive). We then filtered to papers in which the title or abstract included at least “twitter” or “tweet” (case insensitive), which resulted in 494 papers. We used the same query on Elsevier's Scopus database of peer-reviewed articles, selecting 30 randomly sampled articles, which mostly selected from conference proceedings. One paper from the Scopus sample was corrupted, so only 29 papers were examined. \n Question: How were the machine learning papers from ArXiv sampled?",
            "output": [
                "sampled all papers published in the Computer Science subcategories of Artificial Intelligence (cs.AI), Machine Learning (cs.LG), Social and Information Networks (cs.SI), Computational Linguistics (cs.CL), Computers and Society (cs.CY), Information Retrieval (cs.IR), and Computer Vision (CS.CV), the Statistics subcategory of Machine Learning (stat.ML), and Social Physics (physics.soc-ph) filtered for papers in which the title or abstract included at least one of the words “machine learning”, “classif*”, or “supervi*” (case insensitive) filtered to papers in which the title or abstract included at least “twitter” or “tweet” (case insensitive)"
            ]
        },
        {
            "id": "task460-6041e3d0e34f4c589db33b9a1f52729f",
            "input": "We focus on the following aspects of NLP research: size, demographics, areas of research, impact, and correlation of citations with demographic attributes (age and gender). \n Question: What aspect of NLP research is examined?",
            "output": [
                "size, demographics, areas of research, impact, and correlation of citations with demographic attributes (age and gender)"
            ]
        },
        {
            "id": "task460-fe1946c705c0467587c6ff904bb0965f",
            "input": "The 10-fold cross validation with this setting gave a token-level accuracy of roughly 71%.  \n Question: Does the paper report translation accuracy for an automatic translation model for Tunisian to Arabish words?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-71f9653535d1400dbace7a646da2b9e9",
            "input": "The used classifiers are Support Vector Machine (SVM), Logistic regression (Log.Reg), Random Forest (RF) and gradient boosting (XGB). \n Question: What classical machine learning algorithms are used?",
            "output": [
                "Support Vector Machine (SVM) Logistic regression (Log.Reg) Random Forest (RF) gradient boosting (XGB)"
            ]
        },
        {
            "id": "task460-7cefb755e91c403cad5ba372a126b824",
            "input": "Our novelties include:\n\nUsing self-play learning for the neural response ranker (described in detail below).\n\nOptimizing neural models for specific metrics (e.g. diversity, coherence) in our ensemble setup.\n\nTraining a separate dialog model for each user, personalizing our socialbot and making it more consistent.\n\nUsing a response classification predictor and a response classifier to predict and control aspects of responses such as sentiment, topic, offensiveness, diversity etc.\n\nUsing a model predictor to predict the best responding model, before the response candidates are generated, reducing computational expenses.\n\nUsing our entropy-based filtering technique to filter all dialog datasets, obtaining higher quality training data BIBREF3.\n\nBuilding big, pre-trained, hierarchical BERT and GPT dialog models BIBREF6, BIBREF7, BIBREF8.\n\nConstantly monitoring the user input through our automatic metrics, ensuring that the user stays engaged. \n Question: What is novel in author's approach?",
            "output": [
                "They use self-play learning , optimize the model for specific metrics, train separate models per user, use model  and response classification predictors, and filter the dataset to obtain higher quality training data."
            ]
        },
        {
            "id": "task460-ce0c0b9c73014a29ad98c40dd5cf2c59",
            "input": "Accuracy of acquired phoneme sequences representing the names of places\nWe evaluated whether the names of places were properly learned for the considered teaching places. This experiment assumes a request for the best phoneme sequence INLINEFORM0 representing the self-position INLINEFORM1 for a robot. The robot moves close to each teaching place. The probability of a word INLINEFORM2 when the self-position INLINEFORM3 of the robot is given, INLINEFORM4 , can be obtained by using equation ( EQREF37 ). The word having the best probability was selected. We compared the PAR with the correct phoneme sequence and a selected name of the place. Because “kiqchiN” and “daidokoro” were taught for the same place, the word whose PAR was the higher score was adopted.\n\nFig. FIGREF63 shows the results of PAR for the word considered the name of a place. SpCoA (latticelm), the proposed method using the results of unsupervised word segmentation on the basis of the speech recognition results in the lattice format, showed the best PAR score. In the 1-best and BoS methods, a part syllable sequence of the name of a place was more minutely segmented as shown in Table TABREF55 . Therefore, the robot could not learn the name of the teaching place as a coherent phoneme sequence. In contrast, the robot could learn the names of teaching places more accurately by using the proposed method. \n Question: How do they evaluate how their model acquired words?",
            "output": [
                "PAR score"
            ]
        },
        {
            "id": "task460-26c5411620ce4b05a2639fd7d2e6d391",
            "input": "We introduce a list of 8 different competencies that a reading system should master in order to process reviews and text documents in general. These 8 tasks require different competencies and a different level of understanding of the document to be well answered. For instance, detecting if an aspect is mentioned in a review will require less understanding of the review than predicting explicitly the rating of this aspect. Table TABREF10 presents the 8 tasks we have introduced in this dataset with an example of a question that corresponds to each task. \n Question: What kind of questions are present in the dataset?",
            "output": [
                "These 8 tasks require different competencies and a different level of understanding of the document to be well answered"
            ]
        },
        {
            "id": "task460-2811dfb6c96f47348023a8f21c07e41b",
            "input": "We designed 3 evaluation sets: (1) Base Set (1,264 samples) held out from the simulated data. (2) Augmented Set (1,280 samples) built by adding two out-of-distribution symptoms, with corresponding dialogue contents and queries, to the Base Set (“bleeding” and “cold”, which never appeared in training data). (3) Real-World Set (944 samples) manually delineated from the the symptom checking portions (approximately 4 hours) of real-world dialogues, and annotated as evaluation samples. \n Question: How do they select instances to their hold-out test set?",
            "output": [
                "1264 instances from simulated data, 1280 instances by adding two out-of-distribution symptoms and 944 instances manually delineated from the symptom checking portions of real-word dialogues"
            ]
        },
        {
            "id": "task460-56dc3451baf94f848727fc61bb32d7b4",
            "input": "We conducted experiments on the Amazon reviews dataset BIBREF9, which is a benchmark dataset in the cross-domain sentiment analysis field. This dataset contains Amazon product reviews of four different product domains: Books (B), DVD (D), Electronics (E), and Kitchen (K) appliances. Each review is originally associated with a rating of 1-5 stars and is encoded in 5,000 dimensional feature vectors of bag-of-words unigrams and bigrams.\n\nExperiment ::: Dataset and Task Design ::: Binary-Class.\nFrom this dataset, we constructed 12 binary-class cross-domain sentiment analysis tasks: B$\\rightarrow $D, B$\\rightarrow $E, B$\\rightarrow $K, D$\\rightarrow $B, D$\\rightarrow $E, D$\\rightarrow $K, E$\\rightarrow $B, E$\\rightarrow $D, E$\\rightarrow $K, K$\\rightarrow $B, K$\\rightarrow $D, K$\\rightarrow $E. Following the setting of previous works, we treated a reviews as class `1' if it was ranked up to 3 stars, and as class `2' if it was ranked 4 or 5 stars. For each task, $\\mathcal {D}_S$ consisted of 1,000 examples of each class, and $\\mathcal {D}_T$ consists of 1500 examples of class `1' and 500 examples of class `2'. In addition, since it is reasonable to assume that $\\mathcal {D}_T$ can reveal the distribution of target domain data, we controlled the target domain testing dataset to have the same class ratio as $\\mathcal {D}_T$. Using the same label assigning mechanism, we also studied model performance over different degrees of $\\rm {P}(\\rm {Y})$ shift, which was evaluated by the max value of $\\rm {P}_S(\\rm {Y}=i)/\\rm {P}_T(\\rm {Y}=i), \\forall i=1, \\cdots , L$. Please refer to Appendix C for more detail about the task design for this study.\n\nExperiment ::: Dataset and Task Design ::: Multi-Class.\nWe additionally constructed 12 multi-class cross-domain sentiment classification tasks. Tasks were designed to distinguish reviews of 1 or 2 stars (class 1) from those of 4 stars (class 2) and those of 5 stars (class 3). For each task, $\\mathcal {D}_S$ contained 1000 examples of each class, and $\\mathcal {D}_T$ consisted of 500 examples of class 1, 1500 examples of class 2, and 1000 examples of class 3. Similarly, we also controlled the target domain testing dataset to have the same class ratio as $\\mathcal {D}_T$. \n Question: Which sentiment analysis tasks are addressed?",
            "output": [
                "12 binary-class classification and multi-class classification of reviews based on rating"
            ]
        },
        {
            "id": "task460-6c1a7123d2404314bdc830586b529ffd",
            "input": "For example, it can be used to extract features for usage with other machine learning tools, or to evaluate given features with the existing classifiers or regressors. Figure FIGREF19 demonstrates a simple feature extractor that retrieves the sentence length. \n Question: Do they show an example of usage for INFODENS?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-04f68a19b7be45b49d991cc3baf22631",
            "input": "All cases exhibit high scores—in the vast majority of the cases substantially higher than reported in previous work. In particular, in BIBREF1 we assess the ability of LSTMs to learn subject-verb agreement patterns in English, and evaluate on naturally occurring wikipedia sentences.  BIBREF2 also consider subject-verb agreement, but in a “colorless green ideas” setting in which content words in naturally occurring sentences are replaced with random words with the same part-of-speech and inflection, thus ensuring a focus on syntax rather than on selectional-preferences based cues.  BIBREF3 consider a wider range of syntactic phenomena (subject-verb agreement, reflexive anaphora, negative polarity items) using manually constructed stimuli, allowing for greater coverage and control than in the naturally occurring setting. \n Question: Were any of these tasks evaluated in any previous work?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-263bbc6fc647416d9e6f517702c3b8a2",
            "input": "We evaluate sentence-level semantics using averaged bag of vectors (BoV) representations on the Semantic Textual Similarity (STSB) task BIBREF21 and Word Content (WC) probing task (identify from a list of words which is contained in the sentence representation) from SentEval BIBREF22. Syntax: Similarly, we use the Google Syntactic analogies (GSyn) BIBREF9 to evaluate word-level syntactic information, and Depth (Dep) and Top Constituent (TopC) (of the input sentence's constituent parse tree) probing tasks from SentEval BIBREF22 for sentence-level syntax. \n Question: What semantic and syntactic tasks are used as probes?",
            "output": [
                "Word Content (WC) probing task Depth (Dep) and Top Constituent (TopC) (of the input sentence's constituent parse tree) probing tasks"
            ]
        },
        {
            "id": "task460-48ccd299428245baa5a9eb13522e5eb0",
            "input": "We use data from Mandarin Chinese and Cantonese. For each language, the data consists of a list of spoken words, recorded by the same speaker. The Mandarin dataset is from a female speaker and is provided by Shtooka, and the Cantonese dataset is from a male speaker and is downloaded from Forvo, an online crowd-sourced pronunciation dictionary. \n Question: What dataset is used for training?",
            "output": [
                "Mandarin dataset Cantonese dataset"
            ]
        },
        {
            "id": "task460-10089b189ace48a4a6626580735ac324",
            "input": "For the English-German language pair we use the full WMT 2019 parallel dataset. For the English-French language pair we use a restricted dataset containing the full TED corpus from MUST-C BIBREF10 and sampled sentences from WMT 2019 dataset.  \n Question: Which datasets were used in the experiment?",
            "output": [
                "WMT 2019 parallel dataset a restricted dataset containing the full TED corpus from MUST-C BIBREF10 sampled sentences from WMT 2019 dataset"
            ]
        },
        {
            "id": "task460-9786a8fb061a46ccbdf21664866f203f",
            "input": "In this context, we decided to study the three main types of architectures which have demonstrated promising results over traditional systems: 1) Connectionist Temporal Classification (CTC) BIBREF5, BIBREF6 which uses Markov assumptions (i.e. conditional independence between predictions at each time step) to efficiently solve sequential problems by dynamic programming, 2) Attention-based methods BIBREF7, BIBREF8 which rely on an attention mechanism to perform non-monotonic alignment between acoustic frames and recognized acoustic units and 3) RNN-tranducer BIBREF0, BIBREF9, BIBREF10 which extends CTC by additionally modeling the dependencies between outputs at different steps using a prediction network analogous to a language model. \n Question: What are the existing end-to-end ASR approaches for the French language?",
            "output": [
                "1) Connectionist Temporal Classification (CTC) 2) Attention-based methods 3) RNN-tranducer"
            ]
        },
        {
            "id": "task460-35e35183e3d44200ad7f7fdb16735ca0",
            "input": "We learn that a number of factors can influence the performance of adversarial attacks, including architecture of the classifier, sentence length and input domain. \n Question: What other factors affect the performance?",
            "output": [
                "architecture of the classifier sentence length  input domain"
            ]
        },
        {
            "id": "task460-d704494187844d99962baa5bfe723aa8",
            "input": "As a document classifier we employ a word-based CNN similar to Kim consisting of the following sequence of layers: $ \\texttt {Conv} \\xrightarrow{} \\texttt {ReLU} \\xrightarrow{} \\texttt {1-Max-Pool} \\xrightarrow{} \\texttt {FC} \\\\ $ Future work would include applying LRP to other neural network architectures (e.g. character-based or recurrent models) on further NLP tasks, as well as exploring how relevance information could be taken into account to improve the classifier's training procedure or prediction performance. \n Question: Do the experiments explore how various architectures and layers contribute towards certain decisions?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-1a0b02f2aa4040f086d5194d88d88109",
            "input": "Our architecture was designed to show that using a model that captures better information about the context and sequential nature of the text can outperform lexicon-based methods commonly used in the literature. \n Question: What meaningful information does the GRU model capture, which traditional ML models do not?",
            "output": [
                " the context and sequential nature of the text"
            ]
        },
        {
            "id": "task460-4bafdb93c75d4ab8aad90e319689220f",
            "input": "Ensemble, which selects a sentence using 100 agents trained on clustered dialogues as described in section SECREF4 – the agent in focus is chosen using a regressor as predictor of dialogue reward INLINEFORM0 using a similar neural net as the ChatDQN agents except for the final layer having one node and using Batch Normalisation BIBREF44 between hidden layers as in BIBREF36 ; \n Question: How many agents do they ensemble over?",
            "output": [
                "100 "
            ]
        },
        {
            "id": "task460-47a991fc32a040fb8ac7ce483009d1f7",
            "input": "We collect $37,263$ concept-sets as the inputs, each of which contains three to five common concepts. These concept-sets are sampled from several large corpora of image/video captions, such that the concepts inside them are more likely to co-occur in natural scenes. The expected concept-sets in our task are supposed to be likely co-occur in natural, daily-life scenes . The concepts in images/videos captions, which usually describe scenes in our daily life, thus possess the desired property. We therefore collect a large amount of caption sentences from a variety of datasets, including VATEX BIBREF4, LSMDC BIBREF12, ActivityNet BIBREF13, and SNLI BIBREF15, forming 1,040,330 sentences in total. \n Question: Where do the concept sets come from?",
            "output": [
                "These concept-sets are sampled from several large corpora of image/video captions"
            ]
        },
        {
            "id": "task460-9bb25458923d45feb1b2ac982dc192f2",
            "input": "The dataset needed to train the W2V was obtained using the information extracted from a dump of the Italian Wikipedia (dated 2019.04.01), from the main categories of Italian Google News (WORLD, NATION, BUSINESS, TECHNOLOGY, ENTERTAINMENT, SPORTS, SCIENCE, HEALTH) and from some anonymized chats between users and a customer care chatbot (Laila). \n Question: What dataset is used for training Word2Vec in Italian language?",
            "output": [
                "extracted from a dump of the Italian Wikipedia (dated 2019.04.01), from the main categories of Italian Google News (WORLD, NATION, BUSINESS, TECHNOLOGY, ENTERTAINMENT, SPORTS, SCIENCE, HEALTH) and from some anonymized chats between users and a customer care chatbot (Laila)"
            ]
        },
        {
            "id": "task460-7ef5a83bbe7645f8ac450ffe68283f51",
            "input": "We first introduce a method that detects bottlenecks in text-games using the overall reward gained and the knowledge graph state. This method freezes the policy used to reach the bottleneck and restarts the training from there on out, additionally conducting a backtracking search to ensure that a sub-optimal policy has not been frozen. The second contribution explore how to leverage knowledge graphs to improve existing exploration algorithms for dealing with combinatorial action-spaces such as Go-Explore BIBREF9.  \n Question: What are the two new strategies?",
            "output": [
                "a method that detects bottlenecks in text-games using the overall reward gained and the knowledge graph state to leverage knowledge graphs to improve existing exploration algorithms for dealing with combinatorial action-space"
            ]
        },
        {
            "id": "task460-20f5e3422eaa499daa1d454bf03d9b56",
            "input": "For a given transcribed utterance, it is firstly encoded with Byte Pair Encoding (BPE) BIBREF14, a compression algorithm splitting words to fundamental subword units (pairs of bytes or BPs) and reducing the embedded vocabulary size. Then we use a BiLSTM BIBREF15 encoder and the output state of the BiLSTM is regarded as a vector representation for this utterance. Finally, a fully connected Feed-forward Neural Network (FNN) followed by a softmax layer, labeled as a multilayer perceptron (MLP) module, is used to perform the domain/intent classification task based on the vector. We name it Oracle simply because we assume that hypotheses are noisy versions of transcription. \n Question: Which ASR system(s) is used in this work?",
            "output": [
                "Oracle "
            ]
        },
        {
            "id": "task460-8f28eead16d64ea3ae88d692c7c43d96",
            "input": "As we can see that, all variants of our CRU model could give substantial improvements over the traditional GRU model, where a maximum gain of 2.7%, 1.0%, and 1.9% can be observed in three datasets, respectively. \n Question: Do experiment results show consistent significant improvement of new approach over traditional CNN and RNN models?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-148170096ca24338ba21628091606c94",
            "input": ". In this work, we challenge current representation techniques and suggest to represent the state using natural language, similar to the way we, as humans, summarize and transfer information efficiently from one to the other BIBREF5. \n Question: How is state to learn and complete tasks represented via natural language?",
            "output": [
                " represent the state using natural language"
            ]
        },
        {
            "id": "task460-dbf6a53ecdc64ef0b7def0bf722f75a4",
            "input": "We evaluate our model on both English and Chinese segmentation \n Question: What language do they look at?",
            "output": [
                "English Chinese"
            ]
        },
        {
            "id": "task460-fd56ad0802064d4b81741272a67c0da7",
            "input": "To overcome the above-mentioned challenges, we propose a locally-linear meta-embedding learning method that (a) requires only the words in the vocabulary of each source embedding, without having to predict embeddings for missing words, (b) can meta-embed source embeddings with different dimensionalities, (c) is sensitive to the diversity of the neighbourhoods of the source embeddings.\n\nOur proposed method comprises of two steps: a neighbourhood reconstruction step (Section \"Nearest Neighbour Reconstruction\" ), and a projection step (Section \"Projection to Meta-Embedding Space\" ). In the reconstruction step, we represent the embedding of a word by the linearly weighted combination of the embeddings of its nearest neighbours in each source embedding space. \n Question: What is the introduced meta-embedding method introduced in this paper?",
            "output": [
                "proposed method comprises of two steps: a neighbourhood reconstruction step (Section \"Nearest Neighbour Reconstruction\" ), and a projection step (Section \"Projection to Meta-Embedding Space\" ). In the reconstruction step, we represent the embedding of a word by the linearly weighted combination of the embeddings of its nearest neighbours in each source embedding space. "
            ]
        },
        {
            "id": "task460-5f1cbd1b1d274e5eac1a775e039fc272",
            "input": "Tests and Gold-Standard Data for Intrinsic Evaluation\nUsing the gold standard data (described below), we performed three types of tests:\n\nClass Membership Tests: embeddings corresponding two member of the same semantic class (e.g. “Months of the Year\", “Portuguese Cities\", “Smileys\") should be close, since they are supposed to be found in mostly the same contexts.\n\nClass Distinction Test: this is the reciprocal of the previous Class Membership test. Embeddings of elements of different classes should be different, since words of different classes ere expected to be found in significantly different contexts.\n\nWord Equivalence Test: embeddings corresponding to synonyms, antonyms, abbreviations (e.g. “porque\" abbreviated by “pq\") and partial references (e.g. “slb and benfica\") should be almost equal, since both alternatives are supposed to be used be interchangeable in all contexts (either maintaining or inverting the meaning).\n\nTherefore, in our tests, two words are considered:\n\ndistinct if the cosine of the corresponding embeddings is lower than 0.70 (or 0.80).\n\nto belong to the same class if the cosine of their embeddings is higher than 0.70 (or 0.80).\n\nequivalent if the cosine of the embeddings is higher that 0.85 (or 0.95). \n Question: What intrinsic evaluation metrics are used?",
            "output": [
                "Class Membership Tests Class Distinction Test Word Equivalence Test"
            ]
        },
        {
            "id": "task460-b931fa9c6a144478ad57bc7d5bc7051d",
            "input": "In the first scenario we equip the decoder with an additional morphology table including target-side affixes. \n Question: What type of morphological information is contained in the \"morphology table\"?",
            "output": [
                "target-side affixes"
            ]
        },
        {
            "id": "task460-c974cd11b2f0435e8863460237c91eb0",
            "input": "To investigate whether our BERT-based model can transfer knowledge beyond language, we consider image features as simple visual tokens that can be presented to the model analogously to textual token embeddings. In order to make the $o_j$ vectors (of dimension $2048+4=2052$) comparable to BERT embeddings (of dimension 768), we use a simple linear cross-modal projection layer $W$ of dimensions $2052\\hspace{-1.00006pt}\\times \\hspace{-1.00006pt}768$. The $N$ object regions detected in an image, are thus represented as $X_{img} = (W.o_1,...,W.o_N)$. Once mapped into the BERT embedding space with $W$, the image is seen by the rest of the model as a sequence of units with no explicit indication if it is a text or an image embedding. \n Question: How are multimodal representations combined?",
            "output": [
                "The image feature vectors are mapped into BERT embedding dimensions and treated like a text sequence afterwards."
            ]
        },
        {
            "id": "task460-b91a29bb596c464998eee466b7f277f2",
            "input": "The SG model achieved a high average similarity score of 0.650 followed by CBoW with a 0.632 average similarity score. The GloVe also achieved a considerable average score of 0.591 respectively. However, the average similarity score of SdfastText is 0.388 and the word pair Microsoft-Bill Gates is not available in the vocabulary of SdfastText. Moreover, the average semantic relatedness similarity score between countries and their capitals is shown in Table TABREF78 with English translation, where SG also yields the best average score of 0.663 followed by CBoW with 0.611 similarity score. The GloVe also yields better semantic relatedness of 0.576 and the SdfastText yield an average score of 0.391. \n Question: How does proposed word embeddings compare to Sindhi fastText word representations?",
            "output": [
                "Proposed SG model vs SINDHI FASTTEXT:\nAverage cosine similarity score: 0.650 vs 0.388\nAverage semantic relatedness similarity score between countries and their capitals: 0.663 vs 0.391"
            ]
        },
        {
            "id": "task460-7ec1ff2bfccb45db81b30f5b9efa916b",
            "input": "Moreover, to be able to make use of multiple answer styles within a single system, our model introduces an artificial token corresponding to the target style at the beginning of the answer sentence ( $y_1$ ), like BIBREF14 . At test time, the user can specify the first token to control the answer styles. \n Question: Does their model also take the expected answer style as input?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-90a46244b16940aeb276ad382f10915d",
            "input": " Our model improves the F1 score by almost 2%, which corresponds to a 12.3% error rate reduction. \n Question: How better is proposed model compared to baselines?",
            "output": [
                " improves the F1 score by almost 2%, which corresponds to a 12.3% error rate reduction"
            ]
        },
        {
            "id": "task460-f112885ec1d44495bc67367418273b75",
            "input": "MTMSN BIBREF4 is the first, and only model so far, that specifically tried to tackle the multi-span questions of DROP. Their approach consisted of two parts. The first was to train a dedicated categorical variable to predict the number of spans to extract. The second was to generalize the single-span head method of extracting a span, by utilizing the non-maximum suppression (NMS) algorithm BIBREF7 to find the most probable set of non-overlapping spans. The number of spans to extract was determined by the aforementioned categorical variable \n Question: What approach did previous models use for multi-span questions?",
            "output": [
                "Only MTMSM specifically tried to tackle the multi-span questions. Their approach consisted of two parts: first train a dedicated categorical variable to predict the number of spans to extract and the second was to generalize the single-span head method of extracting a span"
            ]
        },
        {
            "id": "task460-644bee8ee276478c8105402092d66c69",
            "input": "We then ask a separate set of Turkers to rate the stories for overall quality and the three improvement areas. All ratings are on a five-point scale. We collect two ratings per story, and throw out ratings that disagree by more than 2 points. A total of 11% of ratings were thrown out, leaving four metrics across 241 stories for analysis. \n Question: How do they evaluate generated stories?",
            "output": [
                "separate set of Turkers to rate the stories for overall quality and the three improvement areas"
            ]
        },
        {
            "id": "task460-0d871814827f4259a46bde87469ee991",
            "input": "The first model, referred as `LSTM (PTB)' in the following sections, was trained on the sentences from Penn Treebank BIBREF12. The second model, referred as `LSTM (FTB)', was trained on the sentences from French Treebank BIBREF13. We incorporate two pretrained English language models: one trained on the Billion Word benchmark (referred as `LSTM (1B)') from BIBREF14, and the other trained on English Wikipedia (referred as `LSTM (enWiki)') from BIBREF3. For French, we trained a large LSTM language model (referred as `LSTM (frWaC)') on a random subset (about 4 million sentences, 138 million word tokens) of the frWaC dataset BIBREF15. \n Question: What is the size of the datasets employed?",
            "output": [
                "(about 4 million sentences, 138 million word tokens) one trained on the Billion Word benchmark"
            ]
        },
        {
            "id": "task460-3b7c3f79bae24e17844e17371ead20d3",
            "input": "The dataset MSParS is published by NLPCC 2019 evaluation task. The whole dataset consists of 81,826 samples annotated by native English speakers. 80% of them are used as training set. 10% of them are used as validation set while the rest is used as test set. 3000 hard samples are selected from the test set. Metric for this dataset is the exactly matching accuracy on both full test set and hard test subset. Each sample is composed of the question, the logical form, the parameters(entity/value/type) and question type as the Table TABREF3 demonstrates. \n Question: Does the training dataset provide logical form supervision?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-d7afc708077f42999d63d2c6dd114dad",
            "input": "In Figure FIGREF28, we show some examples of the annotation results in CORD-19-NER. We can see that our distantly- or weakly supervised methods achieve high quality recognizing the new entity types, requiring only several seed examples as the input. For example, we recognized “SARS-CoV-2\" as the “CORONAVIRUS\" type, “bat\" and “pangolins\" as the “WILDLIFE\" type and “Van der Waals forces\" as the “PHYSICAL_SCIENCE\" type. This NER annotation results help downstream text mining tasks in discovering the origin and the physical nature of the virus. Our NER methods are domain-independent that can be applied to corpus in different domains. In addition, we show another example of NER annotation on New York Times with our system in Figure FIGREF29.\n\nIn Figure FIGREF30, we show the comparison of our annotation results with existing NER/BioNER systems. In Figure FIGREF30, we can see that only our method can identify “SARS-CoV-2\" as a coronavirus. In Figure FIGREF30, we can see that our method can identify many more entities such as “pylogenetic\" as a evolution term and “bat\" as a wildlife. In Figure FIGREF30, we can also see that our method can identify many more entities such as “racism\" as a social behavior. In summary, our distantly- and weakly-supervised NER methods are reliable for high-quality entity recognition without requiring human effort for training data annotation. \n Question: Did they experiment with the dataset?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-82d1d66570d944e7aa8a97bca19dabee",
            "input": "We use two state-of-art neural coreference resolution models described by BIBREF2 and BIBREF1 . \n Question: What is the state-of-the-art neural coreference resolution model?",
            "output": [
                "BIBREF2  BIBREF1 "
            ]
        },
        {
            "id": "task460-8a4a56d25b7f4367bcf793efa4b6b948",
            "input": "First, the GRU only allows sentences to have context from sentences before them, but not after them. This prevents information propagation from future sentences. Second, the supporting sentences may be too far away from each other on a word level to allow for these distant sentences to interact through the word level GRU. \n Question: Why is supporting fact supervision necessary for DMN?",
            "output": [
                "First, the GRU only allows sentences to have context from sentences before them, but not after them. This prevents information propagation from future sentences. Second, the supporting sentences may be too far away from each other on a word level to allow for these distant sentences to interact through the word level GRU."
            ]
        },
        {
            "id": "task460-5d9f862e3ef64127920e08002e7b6fbe",
            "input": "Our study uses WikiHop BIBREF0, as it is an entity-based multi-hop QA dataset and has been actively used. \n Question: What dataset was used in the experiment?",
            "output": [
                "WikiHop"
            ]
        },
        {
            "id": "task460-b92ff90eac5c48bab3be44eefe31925b",
            "input": "We have implemented the following interfaces for Macaw:\n\n[leftmargin=*]\n\nFile IO: This interface is designed for experimental purposes, such as evaluating the performance of a conversational search technique on a dataset with multiple queries. This is not an interactive interface.\n\nStandard IO: This interactive command line interface is designed for development purposes to interact with the system, see the logs, and debug or improve the system.\n\nTelegram: This interactive interface is designed for interaction with real users (see FIGREF4). Telegram is a popular instant messaging service whose client-side code is open-source. We have implemented a Telegram bot that can be used with different devices (personal computers, tablets, and mobile phones) and different operating systems (Android, iOS, Linux, Mac OS, and Windows). This interface allows multi-modal interactions (text, speech, click, image). It can be also used for speech-only interactions. For speech recognition and generation, Macaw relies on online APIs, e.g., the services provided by Google Cloud and Microsoft Azure. In addition, there exist multiple popular groups and channels in Telegram, which allows further integration of social networks with conversational systems. For example, see the Naseri and Zamani's study on news popularity in Telegram BIBREF12. \n Question: What interface does Macaw currently have?",
            "output": [
                "File IO Standard IO Telegram"
            ]
        },
        {
            "id": "task460-d13f9fd3cc6e4f308e073501a8bcf817",
            "input": "We adopt the annotation machinery of BIBREF5 implemented using Amazon's Mechanical Turk, and annotate each predicate by 2 trained workers independently, while a third consolidates their annotations into a final set of roles and arguments. In this consolidation task, the worker validates questions, merges, splits or modifies answers for the same role according to guidelines, and removes redundant roles by picking the more naturally phrased questions.  \n Question: What is different in the improved annotation protocol?",
            "output": [
                "a trained worker consolidates existing annotations "
            ]
        },
        {
            "id": "task460-2fdf31fbb8284cd8aac7742587d3f9e8",
            "input": "While the plan generation stage is guaranteed to be faithful to the input, the translation process from plans to text is based on a neural seq2seq model and may suffer from known issues with such models: hallucinating facts that do not exist in the input, repeating facts, or dropping facts. While the clear mapping between plans and text helps to reduce these issues greatly, the system in BIBREF0 still has 2% errors of these kinds. Recent work in neural text generation and summarization attempt to address these issues by trying to map the textual outputs back to structured predicates, and comparing these predicates to the input data. \n Question: What is the effectiveness plan generation?",
            "output": [
                "clear mapping between plans and text helps to reduce these issues greatly, the system in BIBREF0 still has 2% errors work in neural text generation and summarization attempt to address these issues"
            ]
        },
        {
            "id": "task460-236b24d05ebf4415a057d501b6415bc4",
            "input": "The main questions this paper attempts to answer are: Does compressing BERT impede it's ability to transfer to new tasks? And does fine-tuning make BERT more or less compressible?\n\nTo explore these questions, we compressed English BERT using magnitude weight pruning BIBREF8 and observed the results on transfer learning to the General Language Understanding Evaluation (GLUE) benchmark BIBREF9, a diverse set of natural language understanding tasks including sentiment analysis, NLI, and textual similarity evaluation. We chose magnitude weight pruning, which compresses models by removing weights close to 0, because it is one of the most fine-grained and effective compression methods and because there are many interesting ways to view pruning, which we explore in the next section. Our findings are as follows: Low levels of pruning (30-40%) do not increase pre-training loss or affect transfer to downstream tasks at all. Medium levels of pruning increase the pre-training loss and prevent useful pre-training information from being transferred to downstream tasks. \n Question: How they observe that fine-tuning BERT on a specific task does not improve its prunability?",
            "output": [
                "we compressed English BERT using magnitude weight pruning BIBREF8 and observed the results on transfer learning to the General Language Understanding Evaluation (GLUE) benchmark BIBREF9, a diverse set of natural language understanding tasks including sentiment analysis, NLI, and textual similarity evaluation. "
            ]
        },
        {
            "id": "task460-c7136142e5854a08bd2da24cb435bc65",
            "input": "Specifically, we group the models based on the objective function it optimizes. We believe this work can aid the understanding of the existing literature.  We believe that the performance of these models is highly dependent on the objective function it optimizes – predicting adjacent word (within-tweet relationships), adjacent tweet (inter-tweet relationships), the tweet itself (autoencoder), modeling from structured resources like paraphrase databases and weak supervision.  \n Question: How do they encourage understanding of literature as part of their objective function?",
            "output": [
                "They group the existing works in terms of the objective function they optimize - within-tweet relationships, inter-tweet relationships, autoencoder, and weak supervision."
            ]
        },
        {
            "id": "task460-fe760b769e094c2ab9c0aa1d90f883f9",
            "input": "The number of fake news that have a duplicate in the training dataset are 1018 whereas, the number of articles with genuine content that have a duplicate article in the training set is 322. \n Question: what is the size of the dataset?",
            "output": [
                "The number of fake news that have a duplicate in the training dataset are 1018 whereas, the number of articles with genuine content that have a duplicate article in the training set is 322."
            ]
        },
        {
            "id": "task460-98f7fa483b164fe684ef6a315ca051db",
            "input": "We evaluate our method on two widely-studied datasets provided by Waseem and Hovey BIBREF5 and Davidson et al. BIBREF9. \n Question: Which publicly available datasets are used?",
            "output": [
                "Waseem-dataset Davidson-dataset,"
            ]
        },
        {
            "id": "task460-fbcf5f035abf48fda0928b02834ff14a",
            "input": "Methodology ::: US dataset\nWe collected tweets associated to a dozen US mainstream news websites, i.e. most trusted sources described in BIBREF18, with the Streaming API, and we referred to Hoaxy API BIBREF16 for what concerns tweets containing links to 100+ US disinformation outlets.  Methodology ::: Italian dataset\nFor what concerns the Italian scenario we first collected tweets with the Streaming API in a 3-week period (April 19th, 2019-May 5th, 2019), filtering those containing URLs pointing to Italian official newspapers websites as described in BIBREF22; these correspond to the list provided by the association for the verification of newspaper circulation in Italy (Accertamenti Diffusione Stampa). We instead referred to the dataset provided by BIBREF23 to obtain a set of tweets, collected continuously since January 2019 using the same Twitter endpoint, which contain URLs to 60+ Italian disinformation websites. \n Question: What are the two large-scale datasets used?",
            "output": [
                "US dataset Italian dataset"
            ]
        },
        {
            "id": "task460-996bbffa930c4acd971eac1df31ce9b7",
            "input": "This architecture builds upon sogaard2016deep, but is adapted in two ways: first, we add an unsupervised sequence labeling task (Language Modeling), second, we add a low-dimensional embedding layer between tasks in the hierarchy to learn dense representations of label tags. \n Question: What is the unsupervised task in the final layer?",
            "output": [
                "Language Modeling"
            ]
        },
        {
            "id": "task460-ae6fca90998343b3aa9bbc343f5e6480",
            "input": "The primary feed for the analysis collected INLINEFORM0 million tweets containing the keywords `breast' AND `cancer'.  \n Question: How were breast cancer related posts compiled from the Twitter streaming API?",
            "output": [
                "By using  keywords `breast' AND `cancer' in tweet collecting process. \n"
            ]
        },
        {
            "id": "task460-ca457d16b03a4ec3b29147917824c432",
            "input": "Experimental Setup \n Question: which chinese datasets were used?",
            "output": [
                "Answer with content missing: (Data section) Chinese with version 5.1 of the Chinese Penn Treebank (CTB)"
            ]
        },
        {
            "id": "task460-a0c5c4dd158c4e718de22547ebb19ee0",
            "input": "The model has around 836M parameters, of which only 66K are byte embeddings. \n Question: How many parameters does the model have?",
            "output": [
                "model has around 836M parameters"
            ]
        },
        {
            "id": "task460-8cbadc61a6644cdba254b5ba4370b82f",
            "input": "Our baseline is a GRU network for each of the three tasks. \n Question: Are the models compared to some baseline models?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-9314cd7330164ab7b32b44626603e7e7",
            "input": "Specifically, we firstly extract multiple relations with an off-the-shelf Open Information Extraction (OpenIE) toolbox BIBREF7, then we select the relation that is most relevant to the answer with carefully designed heuristic rules. \n Question: How they extract \"structured answer-relevant relation\"?",
            "output": [
                "Using the OpenIE toolbox and applying heuristic rules to select the most relevant relation."
            ]
        },
        {
            "id": "task460-a867f51677a14779820e4860903682b9",
            "input": "They can answer each question with either `yes', `rather yes', `rather no', or `no'. They can supplement each answer with a comment of at most 500 characters. \n Question: What annotations are present in dataset?",
            "output": [
                "answer each question with either `yes', `rather yes', `rather no', or `no'. can supplement each answer with a comment of at most 500 characters"
            ]
        },
        {
            "id": "task460-930ae1a43cc84e31b507ccb423ead55e",
            "input": "To account for sequences of words and characters that might carry useful information, we extracted word unigrams, bigrams, and trigrams as features. \n Question: Which features do they use to model Twitter messages?",
            "output": [
                "word unigrams, bigrams, and trigrams"
            ]
        },
        {
            "id": "task460-bd3c0068378640f298bcc1d7507a8974",
            "input": "To help solve the data problem we present Taskmaster-1, a dataset consisting of 13,215 dialogs, including 5,507 spoken and 7,708 written dialogs created with two distinct procedures. Each conversation falls into one of six domains: ordering pizza, creating auto repair appointments, setting up ride service, ordering movie tickets, ordering coffee drinks and making restaurant reservations.  \n Question: Which six domains are covered in the dataset?",
            "output": [
                "ordering pizza, creating auto repair appointments, setting up ride service, ordering movie tickets, ordering coffee drinks and making restaurant reservations"
            ]
        },
        {
            "id": "task460-1876e1ea7d6a43209918ab05d5bb14b2",
            "input": "Without ELMo (the same setting as 4-th row in Table 3 ), our data settings is the same as qin-EtAl:2017:Long whose performance was state-of-the-art and will be compared directly. We see that even without the pre-trained ELMo encoder, our performance is better, which is mostly attributed to our better sentence pair representations. \n Question: Why does their model do better than prior models?",
            "output": [
                "better sentence pair representations"
            ]
        },
        {
            "id": "task460-73792f6e65c748a9b39cdd027aed4cbe",
            "input": "For the competition, we provided 1,000 labeled dialogues from each dataset for training, and 240 unlabeled dialogues from each dataset for evaluation.  \n Question: What is the size of the second dataset?",
            "output": [
                "1 000 labeled dialogues for training and 240 unlabeled dialogues for evaluation"
            ]
        },
        {
            "id": "task460-f7d2de86e88b4fc2b7be5412f8526850",
            "input": " Result for BLEU score for our model and Google's Neural Machine Translation is compared in table TABREF19 \n Question: What is their baseline?",
            "output": [
                "Google's Neural Machine Translation"
            ]
        },
        {
            "id": "task460-399f0e0f7379486f85732ea431c2d3c6",
            "input": "BIBREF0 (BIBREF0) developed Moral Choice Machine computes the cosine similarity in a sentence embedding space of an arbitrary action embedded in question/answer pairs. \n Question: What is the Moral Choice Machine?",
            "output": [
                "Moral Choice Machine computes the cosine similarity in a sentence embedding space of an arbitrary action embedded in question/answer pairs"
            ]
        },
        {
            "id": "task460-ee7d8c5faee743fca251a1696e817a5d",
            "input": " In recent years, researchers have proposed various filter based feature selection methods to raise the performance of document text classification BIBREF19.  Furthermore, in order to rank the terms based on their discriminative power among the classes, we use filter based feature selection method named as Normalized Difference Measure (NDM)BIBREF5. Considering the features contour plot, Rehman et al. BIBREF5 suggested that all those features which exist in top left, and bottom right corners of the contour are extremely significant as compared to those features which exist around diagonals. State-of-the-art filter based feature selection algorithms such as ACC2 treat all those features in the same fashion which exist around the diagonals BIBREF5. \n Question: Is the filter based feature selection (FSE) a form of regularization?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-7ae18254338c4572b512529a97ae4b42",
            "input": "In the multiple-choice setting, which is the variety of question-answering (QA) that we focus on in this paper, there is also pragmatic reasoning involved in selecting optimal answer choices (e.g., while greenhouse effect might in some other context be a reasonable answer to the second question in Figure FIGREF1, global warming is a preferable candidate). \n Question: Do they focus on Reading Comprehension or multiple choice question answering?",
            "output": [
                "MULTIPLE CHOICE QUESTION ANSWERING"
            ]
        },
        {
            "id": "task460-ac85feed63cd426bb63dafb2eea80181",
            "input": "We propose a simple and practical human evaluation for evaluating text summarization, where the summary is evaluated against the source content instead of the reference. It handles both of the problems of paraphrasing and lack of high-quality reference.  To avoid the deficiencies, we propose a simple human evaluation method to assess the semantic consistency. Each summary candidate is evaluated against the text rather than the reference. If the candidate is irrelevant or incorrect to the text, or the candidate is not understandable, the candidate is labeled bad. Otherwise, the candidate is labeled good. Then, we can get an accuracy of the good summaries. The proposed evaluation is very simple and straight-forward. It focuses on the relevance between the summary and the text. The semantic consistency should be the major consideration when putting the text summarization methods into practice, but the current automatic methods cannot judge properly. For detailed guidelines in human evaluation, please refer to Appendix SECREF6 .  \n Question: What human evaluation method is proposed?",
            "output": [
                "comparing the summary with the text instead of the reference and labeling the candidate bad if it is incorrect or irrelevant"
            ]
        },
        {
            "id": "task460-85106ccaafc34f67a9b6a2307bb40794",
            "input": "To address this drawback in ROUGE, we propose a new evaluation metric: Critical Information Completeness (CIC). Formally, CIC is a recall of semantic slot information between a candidate summary and a reference summary. CIC is defined as follows:\n\nwhere $V$ stands for a set of delexicalized values in the reference summary, $Count_{match}(v)$ is the number of values co-occurring in the candidate summary and reference summary, and $m$ is the number of values in set $V$. In our experiments, CIC is computed as the arithmetic mean over all the dialog domains to retain the overall performance.\n\nCIC is a suitable complementary metric to ROUGE because it accounts for the most important information within each dialog domain. CIC can be applied to any summarization task with predefined essential entities. \n Question: How does new evaluation metric considers critical informative entities?",
            "output": [
                "Answer with content missing: (formula for CIC) it accounts for the most important information within each dialog domain. CIC can be applied to any summarization task with predefined essential entities"
            ]
        },
        {
            "id": "task460-ad0e0623adac49ddba88a39bbcdbcf0e",
            "input": "Then we define the similarity between INLINEFORM7 and INLINEFORM8 by, DISPLAYFORM0 We note that the similarity metric defined above for sentences is equally applicable for n-gram retrieval. \n Question: Which similarity measure do they use in their n-gram retrieval approach?",
            "output": [
                "we define the similarity between INLINEFORM7 and INLINEFORM8 by, DISPLAYFORM0"
            ]
        },
        {
            "id": "task460-9bf0fe1bd59b429297455754ca347b07",
            "input": "Besides, we report the sentiment accuracy (Senti ACC) which measures whether the output sentence has the same sentiment polarity as the input sentence based on our standardized sentiment classifiers. The BLEU score BIBREF25 between the input sentences and the output sentences is calculated to evaluate the content preservation performance. In order to evaluate the overall performance of different models, we also report the geometric mean (G2) and harmonic mean (H2) of the sentiment accuracy and the BLEU score. As for the irony accuracy, we only report it in human evaluation results because it is more accurate for the human to evaluate the quality of irony as it is very complicated. \n Question: Who judged the irony accuracy, sentiment preservation and content preservation?",
            "output": [
                "Irony accuracy is judged only by human ; senriment preservation and content preservation are judged  both by human and using automatic metrics (ACC and BLEU)."
            ]
        },
        {
            "id": "task460-fc20b1b220ca4877bb1fc72a4806f771",
            "input": "We pit our model against the following baselines: 1) SVM with unigram, bigram, and trigram features, which is a standard yet rather strong classifier for text features; 2) SVM with average word embedding, where a document is represented as a continuous representation by averaging the embeddings of the composite words; 3) SVM with average transformed word embeddings (the INLINEFORM0 in equation EQREF6 ), where a document is represented as a continuous representation by averaging the transformed embeddings of the composite words; 4) two mature deep learning models on text classification, CNN BIBREF3 and Recurrent Convolutional Neural Networks (RCNN) BIBREF0 , where the hyperparameters are based on their work; 5) the above SVM and deep learning models with comment information;  \n Question: What are the baselines?",
            "output": [
                "SVM with unigram, bigram, and trigram features SVM with average word embedding SVM with average transformed word embeddings CNN ecurrent Convolutional Neural Networks SVM and deep learning models with comment information"
            ]
        },
        {
            "id": "task460-58b8d7c5136d4001a831c48364c11c4c",
            "input": "In our setup, our starting point is a base model, trained on NLI data. Rather than employing automated adversarial methods, here the model's “adversary” is a human annotator. Given a context (also often called a “premise” in NLI), and a desired target label, we ask the human writer to provide a hypothesis that fools the model into misclassifying the label. One can think of the writer as a “white hat” hacker, trying to identify vulnerabilities in the system. For each human-generated example that is misclassified, we also ask the writer to provide a reason why they believe it was misclassified. \n Question: Do they use active learning to create their dataset?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-d0bc2b5c2d1a4f418b2c0729a390ea47",
            "input": "For the first coarse-tuning stage with NLI tasks, we use MultiNLI BIBREF15 and SNLI BIBREF16 as the out-of-domain source datasets.  \n Question: What out of domain datasets authors used for coarse-tuning stage?",
            "output": [
                "MultiNLI BIBREF15 and SNLI BIBREF16 "
            ]
        },
        {
            "id": "task460-e6af7b6c971f4be1aea44556d54efedf",
            "input": "Therefore, in order to create fair systems it is necessary to take into account the representation problems in society that are going to be encapsulated in the data. \n Question: What is the goal of investigating NLP gender bias specifically in the news broadcast domain and Anchor role?",
            "output": [
                "create fair systems"
            ]
        },
        {
            "id": "task460-129f535a44a146cdaa6ff710874727d4",
            "input": "The end-to-end system (prefix E2E) uses the DNN topology depicted in Figure FIGREF8 . We present results with 3 distinct size configurations (infixes 700K, 318K, and 40K) each representing the number of approximate parameters, and 2 types of training recipes (suffixes 1stage and 2stage) corresponding to end-to-end and encoder+decoder respectively, as described in UID7 . \n Question: How many parameters does the presented model have?",
            "output": [
                "(infixes 700K, 318K, and 40K) each representing the number of approximate parameters"
            ]
        },
        {
            "id": "task460-455abc17d96641aaa089f196a7d76dcb",
            "input": "Table TABREF24 shows that while the acceleration of trading decoder layers for encoding layers in training is small, in decoding is significant. Specifically, the Transformer with 10 encoder layers and 2 decoder layers is $2.32$ times as fast as the 6-layer Transformer while achieving a slightly higher BLEU. \n Question: How much is decoding speed increased by increasing encoder and decreasing decoder depth?",
            "output": [
                "the Transformer with 10 encoder layers and 2 decoder layers is $2.32$ times as fast as the 6-layer Transformer"
            ]
        },
        {
            "id": "task460-0ca7e87591834f0f8b220d5a58c26353",
            "input": "We conduct our experiments using the evaluation methodology of SemEval 2010 Task 14: Word Sense Induction & Disambiguation BIBREF5 .  \n Question: What evaluation is conducted?",
            "output": [
                "Word Sense Induction & Disambiguation"
            ]
        },
        {
            "id": "task460-7e06d4bf1a594eba81a90d8f49e39340",
            "input": "The Business Language Testing Service (BULATS) test of Cambridge Assessment English BIBREF27 is a multi-level computer-based English test. It consists of read speech and free-speaking components, with the candidate responding to prompts. The BULATS spoken test has five sections, all with materials appropriate to business scenarios.  In this work, non-native speech from the BULATS test is used as both training and test data for the speaker verification systems. To investigate how the systems generalise, data for testing is also taken from the Cambridge Assessment English Linguaskill online test. Like BULATS, this is also a multi-level test and has a similar format composed of the same five sections as described before but assesses general English ability. \n Question: What standard large speaker verification corpora is used for evaluation?",
            "output": [
                "non-native speech from the BULATS test "
            ]
        },
        {
            "id": "task460-c5b6d7ca85ab4af8aa1f5d17452f1810",
            "input": " Among the GLUE tasks, these state-of-the-art systems make their greatest gains on the acceptability task with the Corpus of Linguistic Acceptability BIBREF0 . CoLA contains example sentences from linguistics publications labeled by experts for grammatical acceptability, and written to show subtle grammatical features. Because minimal syntactic differences can separate acceptable sentences from unacceptable ones (What did Bo write a book about? / *What was a book about written by Bo?), and acceptability classifiers are more reliable when trained on GPT and BERT than on recurrent models, it stands to reason that GPT and BERT have better implicit knowledge of syntactic features relevant to acceptability. \n Question: Where does the data in CoLA come from?",
            "output": [
                " CoLA contains example sentences from linguistics publications labeled by experts"
            ]
        },
        {
            "id": "task460-1bdd4d27917f40e08e6047072e10bfbf",
            "input": "The same 36 questions were answered using four QALD tools: WDAqua BIBREF0 , QAKiS BIBREF7 , gAnswer BIBREF6 and Platypus BIBREF8 . \n Question: Which four QA systems do they use?",
            "output": [
                "WDAqua BIBREF0 , QAKiS BIBREF7 , gAnswer BIBREF6 and Platypus BIBREF8"
            ]
        },
        {
            "id": "task460-4038231828ce407dbeb7de33b9cf85e8",
            "input": "In future work we also intend to add these types of studies to the ERP predictions.\n\nDiscussion \n Question: What datasets are used?",
            "output": [
                "Answer with content missing: (Whole Method and Results sections) The primary dataset we use is the ERP data collected and computed by Frank et al. (2015), and we also use behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013) which were collected on the same set of 205 sentences.\nSelect:\n- ERP data collected and computed by Frank et al. (2015)\n- behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013)"
            ]
        },
        {
            "id": "task460-5d8a328ef407496ca699e2144f1e6359",
            "input": "Our unsupervised ranking model outperforms the supervised IMS system by 1.02% on the CoNLL F1 score, and achieves competitive performance with the latent tree model. Moreover, our approach considerably narrows the gap to other supervised systems listed in Table 3 . \n Question: Is the model presented in the paper state of the art?",
            "output": [
                "No, supervised models perform better for this task."
            ]
        },
        {
            "id": "task460-ab3b95a940e84ae5a816124cfaddb7b8",
            "input": "Motivated by this, we introduce resolution mode variables $\\Pi = \\lbrace \\pi _1, \\ldots , \\pi _n\\rbrace $ , where for each mention $j$ the variable $\\pi _j \\in \\lbrace str, prec, attr\\rbrace $ indicates in which mode the mention should be resolved. In our model, we define three resolution modes — string-matching (str), precise-construct (prec), and attribute-matching (attr) — and $\\Pi $ is deterministic when $D$ is given (i.e. $P(\\Pi |D)$ is a point distribution). We determine $\\pi _j$ for each mention $m_j$ in the following way:\n\n$\\pi _j = str$ , if there exists a mention $m_i, i < j$ such that the two mentions satisfy the String Match sieve, the Relaxed String Match sieve, or the Strict Head Match A sieve in the Stanford multi-sieve system BIBREF1 .\n\n$\\pi _j = prec$ , if there exists a mention $m_i, i < j$ such that the two mentions satisfy the Speaker Identification sieve, or the Precise Constructs sieve.\n\n$\\pi _j = attr$ , if there is no mention $m_i, i < j$ satisfies the above two conditions. \n Question: Are resolution mode variables hand crafted?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-8a121a06ff5c4fc595ebabe06492977e",
            "input": "We first measure the utility of various components in Logician to select the optimal model, and then compare this model to the state-of-the-art methods in four types of information extraction tasks: verb/preposition-based relation, nominal attribute, descriptive phrase and hyponymy relation. \n Question: What open relation extraction tasks did they experiment on?",
            "output": [
                "verb/preposition-based relation, nominal attribute, descriptive phrase and hyponymy relation."
            ]
        },
        {
            "id": "task460-26b14462e7814fc5bcdd3714aad7ab03",
            "input": "Frege promoted what we could call sentence holism: “Only in the context of a sentence does a word have a meaning.” BIBREF10 \n Question: What does Frege's holistic and functional approach to meaning states?",
            "output": [
                "Only in the context of a sentence does a word have a meaning."
            ]
        },
        {
            "id": "task460-046ced6ac0814a67a51dde97e4fccdfa",
            "input": "We compare Blse (Sections UID23 – UID30 ) to VecMap, Muse, and Barista (Section \"Previous Work\" ) as baselines, which have similar data requirements and to machine translation (MT) and monolingual (Mono) upper bounds which request more resources. \n Question: what baseline do they compare to?",
            "output": [
                "VecMap Muse Barista"
            ]
        },
        {
            "id": "task460-3d2fd5f9e19f42ff8a02c32e8d14489c",
            "input": "We pair 11'248 standard German written words with their phonetical representations in six different Swiss dialects: Zürich, St. Gallen, Basel, Bern, Visp, and Stans (Figure FIGREF1). \n Question: How many words are coded in the dictionary?",
            "output": [
                "11'248"
            ]
        },
        {
            "id": "task460-aa95c7fe47824224944ca75976b8fc5b",
            "input": "Results are shown in Table TABREF11 . Results show that incorporating self-attention mechanism in the encoder is beneficial for most tasks. However, original models were better in some tasks (CR, MPQA, MRPC), suggesting that self-attention mechanism could sometimes introduce noise in sentence features. Overall, utilizing self-attentive sentence representation further improves performances in 5 out of 8 tasks. \n Question: How much better performing is the proposed method over the baselines?",
            "output": [
                "original models were better in some tasks (CR, MPQA, MRPC) utilizing self-attentive sentence representation further improves performances in 5 out of 8 tasks"
            ]
        },
        {
            "id": "task460-8b0f583ede39468f9d6d85e0584594dd",
            "input": "Firstly, LastStateRNN is the classic RNN model, where the last state passes through an MLP and then the LR Layer estimates the corresponding probability. In contrast, in the AvgRNN model we consider the average vector of all states that come out of the cells. The AttentionRNN model is the one that it has been presented in BIBREF9. \n Question: What was the baseline?",
            "output": [
                "LastStateRNN AvgRNN AttentionRNN"
            ]
        },
        {
            "id": "task460-c6b983c2e9264f73af6ead025f4ef56f",
            "input": "We evaluated our attention transformations on three language pairs. We use the IWSLT 2014 corpus for De-En, the KFTT corpus for Ja-En BIBREF19 , and the WMT 2016 dataset for Ro-En. \n Question: What are the language pairs explored in this paper?",
            "output": [
                "De-En Ja-En Ro-En"
            ]
        },
        {
            "id": "task460-7db47a83ae764c1d9edcea29f58dcbb5",
            "input": "Given the full text of a scientific publication, we want to rank its citations according to the author's judgments. We collected recent publications from the open-access PLoS journals and asked the authors to rank by closeness five citations we selected from their paper. \n Question: what crowdsourcing platform is used?",
            "output": [
                "asked the authors to rank by closeness five citations we selected from their paper"
            ]
        },
        {
            "id": "task460-b557c22639dc416cba64f085e5b032ef",
            "input": "We obtained 1260 unique utterances having commands to AMIE from our in-cabin data-set. We expanded this data-set via Amazon Mechanical Turk and ended up with 3347 utterances having intents. The annotations for intents and slots are obtained on the transcribed utterances by majority voting of 3 annotators. \n Question: What is the size of their collected dataset?",
            "output": [
                "3347 unique utterances "
            ]
        },
        {
            "id": "task460-a94b2c3b4d9844249416a4205a182b20",
            "input": "Algorithm \"Online Learning\" demonstrates how our CNN model can be trained in a purely online setting. We first initialize the model parameters $\\theta _0$ (line 1), which can be a trained model from other disaster events or it can be initialized randomly to start from scratch.\n\nAs a new batch of labeled tweets $B_t= \\lbrace \\mathbf {s}_1 \\ldots \\mathbf {s}_n \\rbrace $ arrives, we first compute the log-loss (cross entropy) in Equation 11 for $B_t$ with respect to the current parameters $\\theta _t$ (line 2a). Then, we use backpropagation to compute the gradients $f^{\\prime }(\\theta _{t})$ of the loss with respect to the current parameters (line 2b). Finally, we update the parameters with the learning rate $\\eta _t$ and the mean of the gradients (line 2c). We take the mean of the gradients to deal with minibatches of different sizes. Notice that we take only the current minibatch into account to get an updated model.  \n Question: What exactly is new about this stochastic gradient descent algorithm?",
            "output": [
                "CNN model can be trained in a purely online setting. We first initialize the model parameters $\\theta _0$ (line 1), which can be a trained model from other disaster events or it can be initialized randomly to start from scratch.\n\nAs a new batch of labeled tweets $B_t= \\lbrace \\mathbf {s}_1 \\ldots \\mathbf {s}_n \\rbrace $ arrives, we first compute the log-loss (cross entropy) in Equation 11 for $B_t$ with respect to the current parameters $\\theta _t$ (line 2a). Then, we use backpropagation to compute the gradients $f^{\\prime }(\\theta _{t})$ of the loss with respect to the current parameters (line 2b). Finally, we update the parameters with the learning rate $\\eta _t$ and the mean of the gradients (line 2c). We take the mean of the gradients to deal with minibatches of different sizes. Notice that we take only the current minibatch into account to get an updated model. "
            ]
        },
        {
            "id": "task460-cd6b69973672405b80e34c8f877d2183",
            "input": "For the English version, we performed both a thorough manual analysis and automatic evaluation across three commonly used TS datasets from two different domains in order to assess the performance of our framework with regard to the sentence splitting subtask. The results show that our proposed sentence splitting approach outperforms the state of the art in structural TS, returning fine-grained simplified sentences that achieve a high level of grammaticality and preserve the meaning of the input. The full evaluation methodology and detailed results are reported in niklaus-etal-2019-transforming. In addition, a comparative analysis with the annotations contained in the RST Discourse Treebank BIBREF6 demonstrates that we are able to capture the contextual hierarchy between the split sentences with a precision of almost 90% and reach an average precision of approximately 70% for the classification of the rhetorical relations that hold between them. The evaluation of the German version is in progress. \n Question: Is the model evaluated?",
            "output": [
                "the English version is evaluated. The German version evaluation is in progress "
            ]
        },
        {
            "id": "task460-d6110ab0aeb24bbe9a3840c034a4f0c2",
            "input": "The answer styles considered in the experiments corresponded to the two tasks. The NLG task requires a well-formed answer that is an abstractive summary of the question and ten passages, averaging 16.6 words. The Q&A task also requires an abstractive answer but prefers a more concise answer than the NLG task, averaging 13.1 words, where many of the answers do not contain the context of the question. \n Question: What do they mean by answer styles?",
            "output": [
                "well-formed sentences vs concise answers"
            ]
        },
        {
            "id": "task460-fc91ca07c89544b9bc8a87b70822641a",
            "input": "These are four conjunctively written Nguni languages (zul, xho, nbl, ssw), Afrikaans (afr) and English (eng), three disjunctively written Sotho languages (nso, sot, tsn), as well as tshiVenda (ven) and Xitsonga (tso). The Nguni languages are similar to each other and harder to distinguish. The same is true of the Sotho languages. Similar languages are to each other are:\n- Nguni languages: zul, xho, nbl, ssw\n- Sotho languages: nso, sot, tsn \n Question: Which languages are similar to each other?",
            "output": [
                "Nguni languages (zul, xho, nbl, ssw) Sotho languages (nso, sot, tsn)"
            ]
        },
        {
            "id": "task460-d897445dca2442c5a365d2fb546063de",
            "input": "We evaluate our method on three tagging tasks: POS tagging (Pos), morphological tagging (Morph) and supertagging (Stag). We select these tasks as examples for tagging applications because they differ strongly in tag set sizes. The test results for the three tasks are shown in Table TABREF17 in three groups. The first group of seven columns are the results for Pos, where both LSTM and CNN have three variations of input features: word only ( INLINEFORM0 ), character only ( INLINEFORM1 ) and both ( INLINEFORM2 ). For Morph and Stag, we only use the INLINEFORM3 setting for both LSTM and CNN. \n Question: Do they jointly tackle multiple tagging problems?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-7b8728e50c864f9baead912c34d4f44d",
            "input": "The second data batch consists of event-related tweets for five natural disasters occurring in the U.S. in 2018. These are: the East Coast Bomb Cyclone (Jan. 2 - 6); the Mendocino, California wildfires (Jul. 27 - Sept. 18); Hurricane Florence (Aug. 31 - Sept. 19); Hurricane Michael (Oct. 7 - 16); and the California Camp Fires (Nov. 8 - 25).  \n Question: Which five natural disasters were examined?",
            "output": [
                "the East Coast Bomb Cyclone  the Mendocino, California wildfires Hurricane Florence Hurricane Michael the California Camp Fires"
            ]
        },
        {
            "id": "task460-00d249cfed0d4261aac785cde92a4241",
            "input": "The Block Zoo is an open framework, and more modules can be added in the future. Embedding Layer: Word/character embedding and extra handcrafted feature embedding such as pos-tagging are supported. Neural Network Layers: Block zoo provides common layers like RNN, CNN, QRNN BIBREF2 , Transformer BIBREF3 , Highway network, Encoder Decoder architecture, etc. Furthermore, attention mechanisms are widely used in neural networks. Thus we also support multiple attention layers, such as Linear/Bi-linear Attention, Full Attention BIBREF4 , Bidirectional attention flow BIBREF5 , etc. Meanwhile, regularization layers such as Dropout, Layer Norm, Batch Norm, etc are also supported for improving generalization ability. Loss Function: Besides of the loss functions built in PyTorch, we offer more options such as Focal Loss BIBREF6 .\n\nMetrics: For classification task, AUC, Accuracy, Precision/Recall, F1 metrics are supported. For sequence labeling task, F1/Accuracy are supported. For knowledge distillation task, MSE/RMSE are supported. For MRC task, ExactMatch/F1 are supported. \n Question: What neural network modules are included in NeuronBlocks?",
            "output": [
                "Embedding Layer Neural Network Layers Loss Function Metrics"
            ]
        },
        {
            "id": "task460-460d02146fab4396a7252ee307d5de15",
            "input": "Our dataset is annotated based on Chinese pathology reports provided by the Department of Gastrointestinal Surgery, Ruijin Hospital. It contains 17,833 sentences, 826,987 characters and 2,714 question-answer pairs.  \n Question: How big is QA-CTS task dataset?",
            "output": [
                "17,833 sentences, 826,987 characters and 2,714 question-answer pairs"
            ]
        },
        {
            "id": "task460-6f38425be1ff43cc93dd04f8433cc508",
            "input": "Here we describe the components of our probabilistic model of question generation.  The details of optimization are as follows. First, a large set of 150,000 questions is sampled in order to approximate the gradient at each step via importance sampling. Second, to run the procedure for a given model and training set, we ran 100,000 iterations of gradient ascent at a learning rate of 0.1. \n Question: Is it a neural model? How is it trained?",
            "output": [
                "No, it is a probabilistic model trained by finding feature weights through gradient ascent"
            ]
        },
        {
            "id": "task460-19df51d4ea064395a1749abbdd4bd34e",
            "input": "We collect HLA data from TV Tropes BIBREF3, a knowledge-based website dedicated to pop culture, containing information on a plethora of characters from a variety of sources. Similar to Wikipedia, its content is provided and edited collaboratively by a massive user-base. These attributes are determined by human viewers and their impressions of the characters, and are correlated with human-like characteristics. We believe that TV Tropes is better for our purpose of fictional character modeling than data sources used in works such as BIBREF25 shuster2019engaging because TV Tropes' content providers are rewarded for correctly providing content through community acknowledgement. \n Question: How does dataset model character's profiles?",
            "output": [
                "attributes are determined by human viewers and their impressions of the characters, and are correlated with human-like characteristics"
            ]
        },
        {
            "id": "task460-6ee9dc5f1a7f4dc39d54b13bde198927",
            "input": "We use three datasets for our experiments: IWSLT14 German-English, Turkish-English, and WMT14 English-German. \n Question: what datasets were used?",
            "output": [
                "IWSLT14 German-English, IWSLT14 Turkish-English, WMT14 English-German"
            ]
        },
        {
            "id": "task460-c3253a7cff524548aa75ff56e2df5dcc",
            "input": "We train both of the systems on the WMT15 German-to-English training data, see Table TABREF18 for some statistics. For this purpose, we use manual alignments provided by RWTH German-English dataset as the hard alignments. \n Question: What datasets are used?",
            "output": [
                "WMT15 German-to-English RWTH German-English dataset"
            ]
        },
        {
            "id": "task460-5a0146b3569446719c02dc7a4e43215f",
            "input": "The dataset needed to train the W2V was obtained using the information extracted from a dump of the Italian Wikipedia (dated 2019.04.01), from the main categories of Italian Google News (WORLD, NATION, BUSINESS, TECHNOLOGY, ENTERTAINMENT, SPORTS, SCIENCE, HEALTH) and from some anonymized chats between users and a customer care chatbot (Laila). The dataset (composed of 2.6 GB of raw text) includes $421\\,829\\,960$ words divided into $17\\,305\\,401$ sentences. All the words were then transformed to lowercase (to avoid a double presence) finally producing a vocabulary of $618\\,224$ words. \n Question: What is the dataset used as input to the Word2Vec algorithm?",
            "output": [
                "Italian Wikipedia and Google News extraction producing final vocabulary of 618224 words"
            ]
        },
        {
            "id": "task460-0b81d82d25214db587e5e953d461ecf4",
            "input": "We re-implement the model proposed in BIBREF3, and use it as a baseline for our problem. The rationale behind choosing this particular model as a baseline is it's proven good predictive performance on multilingual text classification. \n Question: What is their baseline model?",
            "output": [
                "the model proposed in BIBREF3"
            ]
        },
        {
            "id": "task460-c78bb55a381c47acaffee09881e0ad38",
            "input": "For the labeled data, we use audio from the IEMOCAP dataset, which comes with labels for activation and valence, both measured on a 5-point Likert scale from three distinct annotators. \n Question: Which multitask annotated corpus is used?",
            "output": [
                "IEMOCAP"
            ]
        },
        {
            "id": "task460-4c567a37343a4e47b6c4d77a6b910c84",
            "input": "The aim of this section is to validate the applicability of our theoretical results—which state that self-attention can perform convolution—and to examine whether self-attention layers in practice do actually learn to operate like convolutional layers, when being trained on standard image classification tasks. In particular, we study the relationship between self-attention and convolution with quadratic and learned relative positional encodings. We find that for both cases, the attention probabilities learned tend to respect the conditions of Lemma UNKREF15, corroborating our hypothesis. Nevertheless, to validate that our model learns a meaningful classifier we compare it to the standard ResNet18 BIBREF14 on the CIFAR-10 dataset BIBREF15. \n Question: What numerical experiments they perform?",
            "output": [
                "attention probabilities learned tend to respect the conditions of Lemma UNKREF15, corroborating our hypothesis validate that our model learns a meaningful classifier we compare it to the standard ResNet18"
            ]
        },
        {
            "id": "task460-40518455b70449ba8e8ad5df38e62ac5",
            "input": "To implement the first tier of our framework and detect moral relevance, we complement our morally relevant seed words with a corresponding set of seed words approximating moral irrelevance based on the notion of valence, i.e., the degree of pleasantness or unpleasantness of a stimulus. We refer to the emotional valence ratings collected by BIBREF28 for approximately 14,000 English words, and choose the words with most neutral valence rating that do not occur in the MFD as our set of morally irrelevant seed words, for an equal total number of morally relevant and morally irrelevant words. \n Question: How do they quantify moral relevance?",
            "output": [
                "By complementing morally relevant seed words with a set of morally irrelevant seed words based on the notion of valence"
            ]
        },
        {
            "id": "task460-5dda46bf581845c886cdc94e35cdbc85",
            "input": "We computed bag-of-words-based benchmarks using the following methods:\n\nClassification with TF-IDF + Linear SVM (TF-IDF + SVM)\n\nClassification with Depeche++ Emotion lexicons BIBREF12 + Linear SVM (Depeche + SVM)\n\nClassification with NRC Emotion lexicons BIBREF13, BIBREF14 + Linear SVM (NRC + SVM)\n\nCombination of TF-IDF and NRC Emotion lexicons (TF-NRC + SVM)\n\nBenchmarks ::: Doc2Vec + SVM\nWe also used simple classification models with learned embeddings. We trained a Doc2Vec model BIBREF15 using the dataset and used the embedding document vectors as features for a linear SVM classifier.\n\nBenchmarks ::: Hierarchical RNN\nFor this benchmark, we considered a Hierarchical RNN, following BIBREF16. We used two BiLSTMs BIBREF17 with 256 units each to model sentences and documents. The tokens of a sentence were processed independently of other sentence tokens. For each direction in the token-level BiLSTM, the last outputs were concatenated and fed into the sentence-level BiLSTM as inputs.\n\nThe outputs of the BiLSTM were connected to 2 dense layers with 256 ReLU units and a Softmax layer. We initialized tokens with publicly available embeddings trained with GloVe BIBREF18. Sentence boundaries were provided by SpaCy. Dropout was applied to the dense hidden layers during training.\n\nBenchmarks ::: Bi-directional RNN and Self-Attention (BiRNN + Self-Attention)\nOne challenge with RNN-based solutions for text classification is finding the best way to combine word-level representations into higher-level representations.\n\nSelf-attention BIBREF19, BIBREF20, BIBREF21 has been adapted to text classification, providing improved interpretability and performance. We used BIBREF20 as the basis of this benchmark.\n\nThe benchmark used a layered Bi-directional RNN (60 units) with GRU cells and a dense layer. Both self-attention layers were 60 units in size and cross-entropy was used as the cost function.\n\nNote that we have omitted the orthogonal regularizer term, since this dataset is relatively small compared to the traditional datasets used for training such a model. We did not observe any significant performance gain while using the regularizer term in our experiments.\n\nBenchmarks ::: ELMo embedding and Bi-directional RNN (ELMo + BiRNN)\nDeep Contextualized Word Representations (ELMo) BIBREF22 have shown recent success in a number of NLP tasks. The unsupervised nature of the language model allows it to utilize a large amount of available unlabelled data in order to learn better representations of words.\n\nWe used the pre-trained ELMo model (v2) available on Tensorhub for this benchmark. We fed the word embeddings of ELMo as input into a one layer Bi-directional RNN (16 units) with GRU cells (with dropout) and a dense layer. Cross-entropy was used as the cost function.\n\nBenchmarks ::: Fine-tuned BERT\nBidirectional Encoder Representations from Transformers (BERT) BIBREF11 has achieved state-of-the-art results on several NLP tasks, including sentence classification.\n\nWe used the fine-tuning procedure outlined in the original work to adapt the pre-trained uncased BERT$_\\textrm {{\\scriptsize LARGE}}$ to a multi-class passage classification task. This technique achieved the best result among our benchmarks, with an average micro-F1 score of 60.4%. \n Question: What are the baseline benchmarks?",
            "output": [
                "TF-IDF + SVM Depeche + SVM NRC + SVM TF-NRC + SVM Doc2Vec + SVM  Hierarchical RNN BiRNN + Self-Attention ELMo + BiRNN  Fine-tuned BERT"
            ]
        },
        {
            "id": "task460-8e4d3c70d3044a75969d3ec54deb6be9",
            "input": " The first, referred to here as seq2seq, is the standard RNN-based neural machine translation system with attention BIBREF0 . This baseline does not use the parsed data. The multi-source systems improve strongly over both baselines, with improvements of up to 1.5 BLEU over the seq2seq baseline and up to 1.1 BLEU over the parse2seq baseline. \n Question: Whas is the performance drop of their model when there is no parsed input?",
            "output": [
                " improvements of up to 1.5 BLEU over the seq2seq baseline"
            ]
        },
        {
            "id": "task460-d8c8cedf019740c9a82ac7a3ff56e691",
            "input": "We propose a new unsupervised model, Sent2Vec, for learning universal sentence embeddings Formally, we learn a source (or context) embedding INLINEFORM0 and target embedding INLINEFORM1 for each word INLINEFORM2 in the vocabulary, with embedding dimension INLINEFORM3 and INLINEFORM4 as in ( EQREF6 ). The sentence embedding is defined as the average of the source word embeddings of its constituent words, as in ( EQREF8 ). We augment this model furthermore by also learning source embeddings for not only unigrams but also n-grams present in each sentence, and averaging the n-gram embeddings along with the words, i.e., the sentence embedding INLINEFORM5 for INLINEFORM6 is modeled as DISPLAYFORM0 \n Question: How do the n-gram features incorporate compositionality?",
            "output": [
                "by also learning source embeddings for not only unigrams but also n-grams present in each sentence, and averaging the n-gram embeddings along with the words"
            ]
        },
        {
            "id": "task460-69f2a3dd10034b4a8bca40d882dff91f",
            "input": "In this paper we have formalized the problem of automatic fill-on-the-blanks quiz generation using two well-defined learning schemes: sequence classification and sequence labeling. \n Question: Which two schemes are used?",
            "output": [
                "sequence classification sequence labeling"
            ]
        },
        {
            "id": "task460-2ca25744b7524b87b338f753eaa15d1c",
            "input": "We used two different corpora for the experiments: small_parallel_enja and Asian Scientific Paper Excerpt Corpus (ASPEC) BIBREF5. \n Question: Which dataset do they use?",
            "output": [
                "small_parallel_enja Asian Scientific Paper Excerpt Corpus (ASPEC) BIBREF5"
            ]
        },
        {
            "id": "task460-8677b35d97644166990ae421dbb3a39b",
            "input": "Thus, for this work, we build upon Hybrid Code Networks (HCN) BIBREF4 since HCNs achieve state-of-the-art performance in a data-efficient way for task-oriented dialogs, and propose AE-HCNs which extend HCNs with an autoencoder (Figure FIGREF8 ).  AE-HCN(-CNN) outperforms HCN on Test-OOD by a large margin about 17(20) points on average while keeping the minimum performance trade-off compared to Test.  \n Question: By how much does their method outperform state-of-the-art OOD detection?",
            "output": [
                "AE-HCN outperforms by 17%, AE-HCN-CNN outperforms by 20% on average"
            ]
        },
        {
            "id": "task460-b1579ad71cb6446c9c7e3352656d0b6e",
            "input": "In sub-task C the goal is to classify the target of the offensive language. Only posts labeled as targeted insults (TIN) in sub-task B are considered in this task BIBREF17 . Samples are annotated with one of the following:\n\nIndividual (IND): Posts targeting a named or unnamed person that is part of the conversation. In English this could be a post such as @USER Is a FRAUD Female @USER group paid for and organized by @USER. In Danish this could be a post such as USER du er sku da syg i hoved. These examples further demonstrate that this category captures the characteristics of cyberbullying, as it is defined in section \"Background\" .\n\nGroup (GRP): Posts targeting a group of people based on ethnicity, gender or sexual orientation, political affiliation, religious belief, or other characteristics. In English this could be a post such as #Antifa are mentally unstable cowards, pretending to be relevant. In Danish this could be e.g. Åh nej! Svensk lorteret!\n\nOther (OTH): The target of the offensive language does not fit the criteria of either of the previous two categories. BIBREF17 . In English this could be a post such as And these entertainment agencies just gonna have to be an ass about it.. In Danish this could be a post such as Netto er jo et tempel over lort. \n Question: How many categories of offensive language were there?",
            "output": [
                "3"
            ]
        },
        {
            "id": "task460-c898906131cb41408174af706434157f",
            "input": "To test the difficulty of our dataset, we checked the majority class label and the accuracies of five state-of-the-art NLI models adopting different approaches: BiMPM (Bilateral Multi-Perspective Matching Model; BIBREF31 , BIBREF31 ), ESIM (Enhanced Sequential Inference Model; BIBREF32 , BIBREF32 ), Decomposable Attention Model BIBREF33 , KIM (Knowledge-based Inference Model; BIBREF34 , BIBREF34 ), and BERT (Bidirectional Encoder Representations from Transformers model; BIBREF35 , BIBREF35 ). Regarding BERT, we checked the performance of a model pretrained on Wikipedia and BookCorpus for language modeling and trained with SNLI and MultiNLI. \n Question: What NLI models do they analyze?",
            "output": [
                "BiMPM ESIM Decomposable Attention Model KIM BERT"
            ]
        },
        {
            "id": "task460-978f5b9255ba46db991a310518102da4",
            "input": "In contrast to existing bottlenecks, this work targets three different types of social networks (Formspring: a Q&A forum, Twitter: microblogging, and Wikipedia: collaborative knowledge repository) for three topics of cyberbullying (personal attack, racism, and sexism) without doing any explicit feature engineering by developing deep learning based models along with transfer learning. \n Question: What cyberbulling topics did they address?",
            "output": [
                "personal attack, racism, and sexism"
            ]
        },
        {
            "id": "task460-5867f360693f48c986fe9704ab54aa9f",
            "input": "In the first task, participants were instructed to read the sentences naturally, without any specific task other than comprehension. Participants were told to read the sentences normally without any special instructions. \n Question: What is a normal reading paradigm?",
            "output": [
                "read the sentences normally without any special instructions"
            ]
        },
        {
            "id": "task460-c0e48d0dc9524163a12e0980ca7cd103",
            "input": "Comparing Eq.DISPLAY_FORM14 with Eq.DISPLAY_FORM22, we can see that Eq.DISPLAY_FORM14 is actually a soft form of $F1$, using a continuous $p$ rather than the binary $\\mathbb {I}( p_{i1}>0.5)$. This gap isn't a big issue for balanced datasets, but is extremely detrimental if a big proportion of training examples are easy-negative ones: easy-negative examples can easily dominate training since their probabilities can be pushed to 0 fairly easily. Meanwhile, the model can hardly distinguish between hard-negative examples and positive ones, which has a huge negative effect on the final F1 performance.\n\nTo address this issue, we propose to multiply the soft probability $p$ with a decaying factor $(1-p)$, changing Eq.DISPLAY_FORM22 to the following form:\n\nOne can think $(1-p_{i1})$ as a weight associated with each example, which changes as training proceeds. The intuition of changing $p_{i1}$ to $(1-p_{i1}) p_{i1}$ is to push down the weight of easy examples. For easy examples whose probability are approaching 0 or 1, $(1-p_{i1}) p_{i1}$ makes the model attach significantly less focus to them. Figure FIGREF23 gives gives an explanation from the perspective in derivative: the derivative of $\\frac{(1-p)p}{1+(1-p)p}$ with respect to $p$ approaches 0 immediately after $p$ approaches 0, which means the model attends less to examples once they are correctly classified.\n\nA close look at Eq.DISPLAY_FORM14 reveals that it actually mimics the idea of focal loss (FL for short) BIBREF16 for object detection in vision. Focal loss was proposed for one-stage object detector to handle foreground-background tradeoff encountered during training. It down-weights the loss assigned to well-classified examples by adding a $(1-p)^{\\beta }$ factor, leading the final loss to be $(1-p)^{\\beta }\\log p$. \n Question: How are weights dynamically adjusted?",
            "output": [
                "One can think $(1-p_{i1})$ as a weight associated with each example, which changes as training proceeds. The intuition of changing $p_{i1}$ to $(1-p_{i1}) p_{i1}$ is to push down the weight of easy examples. For easy examples whose probability are approaching 0 or 1, $(1-p_{i1}) p_{i1}$ makes the model attach significantly less focus to them. Figure FIGREF23 gives gives an explanation from the perspective in derivative: the derivative of $\\frac{(1-p)p}{1+(1-p)p}$ with respect to $p$ approaches 0 immediately after $p$ approaches 0, which means the model attends less to examples once they are correctly classified."
            ]
        },
        {
            "id": "task460-7fe309f2671d4836a7af5f349396fe01",
            "input": "We use the unshuffled version of the French OSCAR corpus, which amounts to 138GB of uncompressed text and 32.7B SentencePiece tokens. \n Question: What data is used for training CamemBERT?",
            "output": [
                "unshuffled version of the French OSCAR corpus"
            ]
        },
        {
            "id": "task460-71d705c64fb64ac588f1765e43aa2e3c",
            "input": "For the benchmarks, we selected five systems. We picked first the langid.py library which is frequently used to compare systems in the literature. Since our work is in neural-network LID, we selected two neural network systems from the literature, specifically the encoder-decoder EquiLID system of BIBREF6 and the GRU neural network LanideNN system of BIBREF10. Finally, we included CLD2 and CLD3, two implementations of the Naïve Bayes LID software used by Google in their Chrome web browser BIBREF4, BIBREF0, BIBREF8 and sometimes used as a comparison system in the LID literature BIBREF7, BIBREF6, BIBREF8, BIBREF2, BIBREF10. \n Question: Which existing language ID systems are tested?",
            "output": [
                "langid.py library encoder-decoder EquiLID system GRU neural network LanideNN system CLD2 CLD3"
            ]
        },
        {
            "id": "task460-0c6c829dc20041aaba2c24a224a9c4ff",
            "input": "We use three creative English datasets with distinct linguistic characteristics: (1) A corpus of 740 classical and contemporary English poems, (2) a corpus of 14950 metaphor sentences retrieved from a metaphor database website and (3) a corpus of 1500 song lyrics ranging across genres.  We first pre-train our generator on the Gutenberg dataset BIBREF24 for 20 epochs and then fine-tune BIBREF19 them to our target datasets with a language modeling objective. \n Question: Which datasets are used?",
            "output": [
                "A corpus of 740 classical and contemporary English poems  a corpus of 14950 metaphor sentences retrieved from a metaphor database website  a corpus of 1500 song lyrics ranging across genres Gutenberg dataset "
            ]
        },
        {
            "id": "task460-9c48e0446b9b4cc59e12edcec4122cbd",
            "input": "A representation that is more in line with observed user behavior is a concept map BIBREF5 , a labeled graph showing concepts as nodes and relationships between them as edges (Figure FIGREF2 ). \n Question: How do the authors define a concept map?",
            "output": [
                "concept map BIBREF5 , a labeled graph showing concepts as nodes and relationships between them as edges"
            ]
        },
        {
            "id": "task460-8668a2d9f32f4e318bcdd1d8650afe5e",
            "input": "Experiment 1 directly tested the hypothesis that speakers increase their specificity in contexts with asymmetry in visual access. We found that speakers are not only context-sensitive in choosing referring expressions that distinguish target from distractors in a shared context, but are occlusion-sensitive, adaptively compensating for uncertainty. These results strongly suggest that the speaker's informativity influences listener accuracy. In support of this hypothesis, we found a strong negative correlation between informativity and error rates across items and conditions: listeners make fewer errors when utterances are a better fit for the target relative to the distractor ( $\\rho = -0.81$ , bootstrapped 95% CI $= [-0.9, -0.7]$ ; Fig. 6 B). This result suggests that listener behavior is driven by an expectation of speaker informativity: listeners interpret utterances proportionally to how well they fit objects in context. Our Rational Speech Act (RSA) formalization of cooperative reasoning in this context predicts that speakers (directors) naturally increase the informativity of their referring expressions to hedge against the increased risk of misunderstanding; Exp. 1 presents direct evidence in support of this hypothesis. Exp. 2 is consistent with this hypothesis; when directors used underinformative scripted instructions (taken from prior work), listeners made significantly more errors than when speakers were allowed to provide referring expressions at their natural level of informativity, and speaker informativeness strongly modulated listener error rates. \n Question: Did participants behave unexpectedly?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-8302dc292b344b21a904f5410f84c9a3",
            "input": "We provide a hand-curated collection of complete inflection tables for 198 lemmata. The morphological tags follow the guidelines of the UniMorph schema BIBREF6, BIBREF7, in order to allow for the potential of cross-lingual transfer learning, and they are tagged with respect to:\n\nPerson: first (1), second (2), and third (3)\n\nNumber: singular (SG) ad plural (PL)\n\nInclusivity (only applicable to first person plural verbs: inclusive (INCL) and exclusive (EXCL)\n\nAspect/mood: completive (CPL), progressive (PROG), potential (POT), and habitual (HAB). \n Question: How was annotation done?",
            "output": [
                " hand-curated collection of complete inflection tables for 198 lemmata"
            ]
        },
        {
            "id": "task460-32ad3d5b4164458ab25ffac279a40cb9",
            "input": "The study of the mathematical structure of grammar has indicated that the fundamental things making up sentences are not the words, but some atomic grammatical types, such as the noun-type and the sentence-type BIBREF23 , BIBREF24 , BIBREF25 . The transitive verb-type is not an atomic grammatical type, but a composite made up of two noun-types and one sentence-type. Hence, particularly interesting here is that atomic doesn't really mean smallest... \n Question: Do they argue that all words can be derived from other (elementary) words?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-4f0d06952c8a43cebad03df846c48543",
            "input": "Tweets related to Forex, specifically to EUR and USD, were acquired through the Twitter search API with the following query: “EURUSD”, “USDEUR”, “EUR”, or “USD”. In the period of three years (January 2014 to December 2016) almost 15 million tweets were collected. A subset of them (44,000 tweets) was manually labeled by knowledgeable students of finance.  \n Question: How many tweets were manually labelled? ",
            "output": [
                "44,000 tweets"
            ]
        },
        {
            "id": "task460-b8fed9c290df4fcba3c40a9bc4ccf523",
            "input": " Plackett-Luce Model for SMT Reranking\nAfter being de-duplicated, the N-best list has an average size of around 300, and with 7491 features. This experiment displays, in large-scale features, the Plackett-Luce model correlates with BLEU score very well, and alleviates overfitting in some degree. \n Question: What experiments with large-scale features are performed?",
            "output": [
                "Plackett-Luce Model for SMT Reranking"
            ]
        },
        {
            "id": "task460-05c0e26c79834eb3b5b951f4f3ee8e3b",
            "input": "We propose extended middle context, a new context representation for CNNs for relation classification. The contexts are split into three disjoint regions based on the two relation arguments: the left context, the middle context and the right context.  Hence, we propose to use two contexts: (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. The two contexts are processed by two independent convolutional and max-pooling layers. After pooling, the results are concatenated to form the sentence representation. \n Question: How do they obtain the new context represetation?",
            "output": [
                "They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation."
            ]
        },
        {
            "id": "task460-7eb99df975f14276a538108dd38cabf7",
            "input": "There are 8757 news records in our preprocessed data set.  \n Question: How large is the dataset?",
            "output": [
                "8757 news records"
            ]
        },
        {
            "id": "task460-f9a0fa6a2e6842c7aff5496d3226b681",
            "input": "We experiment with five benchmark attacking methods for texts: FGM, FGVM, DeepFool BIBREF5, HotFlip BIBREF3) and TYC BIBREF4. \n Question: What are the benchmark attacking methods?",
            "output": [
                "FGM, FGVM, DeepFool BIBREF5, HotFlip BIBREF3) and TYC BIBREF4"
            ]
        },
        {
            "id": "task460-c10b922fe3b049b3aad4d7ef71c20fbb",
            "input": "We extract data from the WMT'14 English-French (En-Fr) and English-German (En-De) datasets. To create a larger discrepancy between the tasks, so that there is a clear dataset size imbalance, the En-De data is artificially restricted to only 1 million parallel sentences, while the full En-Fr dataset, comprising almost 40 million parallel sentences, is used entirely.  \n Question: What datasets are used for experiments?",
            "output": [
                "the WMT'14 English-French (En-Fr) and English-German (En-De) datasets."
            ]
        },
        {
            "id": "task460-14cf68dab0884577b32ee50fdf1e97cb",
            "input": "Kneser–Ney smoothing In particular, we compare Kneser–Ney smoothing, widely accepted as the state of the art prior to NLMs, to the best NLMs today. \n Question: what classic language models are mentioned in the paper?",
            "output": [
                "Kneser–Ney smoothing"
            ]
        },
        {
            "id": "task460-e346c0b4f98147eb9f9735bd0d6aec86",
            "input": "The BioNLP 2009 Shared Task BIBREF195 was based on the GENIA corpus BIBREF196 which contains PubMed abstracts of articles on transcription factors in human blood cells. \n Question: Which datasets are used in this work?",
            "output": [
                "GENIA corpus"
            ]
        },
        {
            "id": "task460-80a9544b7c114cc8aad0868e18c23d74",
            "input": "In other cases, the model points out plausible signals which were passed over by an annotator, and may be considered errors in the gold standard. For example, the model easily notices that question marks indicate the solutionhood relation, even where these were skipped by annotators in favor of marking WH words instead:\n\n. [RGB]230, 230, 230Which [RGB]230, 230, 230previous [RGB]230, 230, 230Virginia [RGB]230, 230, 230Governor(s) [RGB]230, 230, 230do [RGB]230, 230, 230you [RGB]230, 230, 230most [RGB]230, 230, 230admire [RGB]230, 230, 230and [RGB]230, 230, 230why [RGB]12, 12, 12? $\\xrightarrow[\\text{pred:solutionhood}]{\\text{gold:solutionhood}}$ [RGB]230, 230, 230Thomas [RGB]230, 230, 230Jefferson [RGB]183, 183, 183. However, it also picks up on a recurring tendency in how-to guides in which the second person pronoun referring to the reader is often the benefactee of some action, which contributes to the purpose reading and helps to disambiguate so, despite not being considered a signal by annotators. \n Question: Where does proposed metric differ from juman judgement?",
            "output": [
                "model points out plausible signals which were passed over by an annotator it also picks up on a recurring tendency in how-to guides in which the second person pronoun referring to the reader is often the benefactee of some action"
            ]
        },
        {
            "id": "task460-953cfc36d42a4ca89e2bdf0dc122aee3",
            "input": "Datasets: We experimented on four standard datasets: WN18 and FB15k are extracted by BIBREF5 from Wordnet BIBREF32 Freebase BIBREF33 . \n Question: What datasets are used to evaluate the model?",
            "output": [
                "WN18 and FB15k"
            ]
        },
        {
            "id": "task460-7f1ea22ab39145c1bc4d1abf995513ee",
            "input": "In this section we detail the discussions we use to test our metric and how we determine the ground truth (i.e. if the discussion is controversial or not). We use thirty different discussions that took place between March 2015 and June 2019, half of them with controversy and half without it. We considered discussions in four different languages: English, Portuguese, Spanish and French, occurring in five regions over the world: South and North America, Western Europe, Central and Southern Asia. We also studied these discussions taking first 140 characters and then 280 from each tweet to analyze the difference in performance and computing time wrt the length of the posts.\n\n \n Question: How many languages do they experiment with?",
            "output": [
                "four different languages: English, Portuguese, Spanish and French"
            ]
        },
        {
            "id": "task460-547012511f6147a697e6dbc40b034bf2",
            "input": "Named Entity Recognition (NER) in the Biomedical domain usually includes recognition of entities such as proteins, genes, diseases, treatments, drugs, etc.  \n Question: What is NER?",
            "output": [
                "Named Entity Recognition"
            ]
        },
        {
            "id": "task460-a4afa66546e64db394b79c0d19166515",
            "input": "Our WordDecoding (WDec) model achieves F1 scores that are $3.9\\%$ and $4.1\\%$ higher than HRL on the NYT29 and NYT24 datasets respectively. Similarly, our PtrNetDecoding (PNDec) model achieves F1 scores that are $3.0\\%$ and $1.3\\%$ higher than HRL on the NYT29 and NYT24 datasets respectively. \n Question: How higher are F1 scores compared to previous work?",
            "output": [
                "WordDecoding (WDec) model achieves F1 scores that are $3.9\\%$ and $4.1\\%$ higher than HRL on the NYT29 and NYT24 datasets respectively PtrNetDecoding (PNDec) model achieves F1 scores that are $3.0\\%$ and $1.3\\%$ higher than HRL on the NYT29 and NYT24 datasets respectively"
            ]
        },
        {
            "id": "task460-62ab4a60129b41ec90d2bbc5efec1c29",
            "input": "Results show that the CJFA encoder obtains significantly better phone classification accuracy than the VAE baseline and also than the CJFS encoder. These results are replicated for speaker recognition tasks. \n Question: Which approach out of two proposed in the paper performed better in experiments?",
            "output": [
                "CJFA encoder "
            ]
        },
        {
            "id": "task460-01f6b33a599d41429e070879b8cbfc92",
            "input": " Here a small portion of the large parallel corpus for English-German is used as a simulation for the scenario where we do not have much parallel data: Translating texts in English to German.  \n Question: Which languages do they test on for the under-resourced scenario?",
            "output": [
                "English German"
            ]
        },
        {
            "id": "task460-1fe7bf9370fa4f1bb7dadc3648672b1e",
            "input": "INLINEFORM0 in the subsampled BIBREF2 training corpus and incrementing cell INLINEFORM1 for every context word INLINEFORM2 appearing within this window (forming a INLINEFORM3 pair). LexVec adjusts the PPMI matrix using context distribution smoothing BIBREF3 . We compare 1) the use of n-gram subwords, like fastText, and 2) unsupervised morphemes identified using Morfessor BIBREF11 to learn whether more linguistically motivated subwords offer any advantage over simple n-grams. \n Question: What types of subwords do they incorporate in their model?",
            "output": [
                "n-gram subwords unsupervised morphemes identified using Morfessor BIBREF11 to learn whether more linguistically motivated subwords "
            ]
        },
        {
            "id": "task460-3f36d29a827d44e2b9f63669b8a2adc5",
            "input": "Using the fine-grained gating mechanism conditioned on the lexical features, we can accurately control the information flows between word-level and char-level. Intuitively, the formulation is as follows: INLINEFORM0\n\nwhere INLINEFORM0 is the element-wise multiplication operator. when the gate has high value, more information flows from the word-level representation; otherwise, char-level will take the dominating place. It is practical in real scenarios. For example, for unfamiliar noun entities, the gates tend to bias towards char-level representation in order to care richer morphological structure. \n Question: How does the gatint mechanism combine word and character information?",
            "output": [
                "when the gate has high value, more information flows from the word-level representation; otherwise, char-level will take the dominating place  for unfamiliar noun entities, the gates tend to bias towards char-level representation in order to care richer morphological structure"
            ]
        },
        {
            "id": "task460-003cef0279864135a9d700f18bc4fc08",
            "input": " Table 1 shows the results of all models on WikiLarge dataset. We can see that our method (NMT+synthetic) can obtain higher BLEU, lower FKGL and high SARI compared with other models, except Dress on FKGL and SBMT-SARI on SARI. It verified that including synthetic data during training is very effective, and yields an improvement over our baseline NMF by 2.11 BLEU, 1.7 FKGL and 1.07 SARI.  Results on WikiSmall dataset are shown in Table 2. We see substantial improvements (6.37 BLEU) than NMT from adding simplified training data with synthetic ordinary sentences.  \n Question: by how much did their model improve?",
            "output": [
                "For the WikiLarge dataset, the improvement over baseline NMT is 2.11 BLEU, 1.7 FKGL and 1.07 SARI.\nFor the WikiSmall dataset, the improvement over baseline NMT is  8.37 BLEU."
            ]
        },
        {
            "id": "task460-631f3935e6ae4928b0a829cce0f82f68",
            "input": "The dataset consists of 198,112 news articles. \n Question: How many articles did they have?",
            "output": [
                "198,112"
            ]
        },
        {
            "id": "task460-81924fc0415048daacda24ed580ce931",
            "input": "To verify our assumption that target encoding and orthogonal regularization help to boost the diversity of generated sequences, we use two metrics, one quantitative and one qualitative, to measure diversity of generation. First, we simply calculate the average unique predictions produced by both INLINEFORM0 and INLINEFORM1 in experiments shown in Section SECREF36 .  In this example there are 29 ground truth phrases. Neither of the models is able to generate all of the keyphrases, but it is obvious that the predictions from INLINEFORM0 all start with “test”, while predictions from INLINEFORM1 are diverse. \n Question: How is keyphrase diversity measured?",
            "output": [
                "average unique predictions illustrate the difference of predictions between our proposed models, we show an example chosen from the KP20k validation set"
            ]
        },
        {
            "id": "task460-a821445ba69041ce82fede1776fc9d21",
            "input": "While the mixing strategy compensates for most of the gap between the Fr-De* and the Fr*-De (3.01 $\\rightarrow $ 0.17) in the De $\\rightarrow $ Fr case, the resulting PSEUDOmix still shows lower BLEU than the target-originated Fr-De* corpus. We thus enhance the quality of the synthetic examples of the source-originated Fr*-De data by further training its mother translation model (En $\\rightarrow $ Fr). As illustrated in Figure 2 , with the target-originated Fr-De* corpus being fixed, the quality of the models trained with the source-originated Fr*-De data and PSEUDOmix increases in proportion to the quality of the mother model for the Fr*-De corpus. Eventually, PSEUDOmix shows the highest BLEU, outperforming both Fr*-De and Fr-De* data.  As presented in Table 6 , we observe that fine-tuning using ground truth parallel data brings substantial improvements in the translation qualities of all NMT models. Among all fine-tuned models, PSEUDOmix shows the best performance in all experiments. This is particularly encouraging for the case of De $\\rightarrow $ Fr, where PSEUDOmix reported lower BLEU than the Fr-De* data before it was fine-tuned. Even in the case where PSEUDOmix shows comparable results with other synthetic corpora in the Pseudo Only scenario, it shows higher improvements in the translation quality when fine-tuned with the real parallel data.  \n Question: How many improvements on the French-German translation benchmark?",
            "output": [
                "one"
            ]
        },
        {
            "id": "task460-6dbf2e0dff8c4979b8f525b61c82f251",
            "input": "Database Construction: we crawled travel information in Beijing from the Web, including Hotel, Attraction, and Restaurant domains (hereafter we name the three domains as HAR domains). Then, we used the metro information of entities in HAR domains to build the metro database. Goal Generation: a multi-domain goal generator was designed based on the database. The relation across domains is captured in two ways. One is to constrain two targets that locate near each other. The other is to use a taxi or metro to commute between two targets in HAR domains mentioned in the context.  Dialogue Collection: before the formal data collection starts, we required the workers to make a small number of dialogues and gave them feedback about the dialogue quality. Then, well-trained workers were paired to converse according to the given goals.  Dialogue Annotation: we used some rules to automatically annotate dialogue acts according to user states, system states, and dialogue histories.  \n Question: How was the dataset collected?",
            "output": [
                "Database Construction: we crawled travel information in Beijing from the Web, including Hotel, Attraction, and Restaurant domains (hereafter we name the three domains as HAR domains). Then, we used the metro information of entities in HAR domains to build the metro database.  Goal Generation: a multi-domain goal generator was designed based on the database. The relation across domains is captured in two ways. One is to constrain two targets that locate near each other. The other is to use a taxi or metro to commute between two targets in HAR domains mentioned in the context. Dialogue Collection: before the formal data collection starts, we required the workers to make a small number of dialogues and gave them feedback about the dialogue quality. Then, well-trained workers were paired to converse according to the given goals. The workers were also asked to annotate both user states and system states. Dialogue Annotation: we used some rules to automatically annotate dialogue acts according to user states, system states, and dialogue histories. "
            ]
        },
        {
            "id": "task460-781659a1bacd448085cc1e49ddd13981",
            "input": "There are various possible extensions for this work. For example, using all frames assigned to a phone, rather than using only the middle frame. \n Question: Do they propose any further additions that could be made to improve generalisation to unseen speakers?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-33779acde55c4cb8be4813569e4591e7",
            "input": " To support this claim, we measure our system's performance for datasets across various domains. The evaluations are done by managed judges who understands the knowledge base and then judge user queries relevance to the QA pairs (binary labels). Each query-QA pair is judged by two judges. We filter out data for which judges do not agree on the label. Chit-chat in itself can be considered as a domain. Thus, we evaluate performance on given KB both with and without chit-chat data (last two rows in Table TABREF19), as well as performance on just chit-chat data (2nd row in Table TABREF19). \n Question: What experiments do the authors present to validate their system?",
            "output": [
                " we measure our system's performance for datasets across various domains evaluations are done by managed judges who understands the knowledge base and then judge user queries relevance to the QA pairs"
            ]
        },
        {
            "id": "task460-60aae3b6969b4ccfb63b5b8c4366d336",
            "input": "Deep convolutional neural networks (CNNs) with 2D convolutions and small kernels BIBREF1, have achieved state-of-the-art results for several speech recognition tasks BIBREF2, BIBREF3, BIBREF4, BIBREF5, BIBREF6. r Models: Our baseline CNN model BIBREF21 consists of 15 convolutional and one fully-connected layer. \n Question: Is model compared against state of the art models on these datasets?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-0b8ea3f8a72a4ba09c068e0e5ae8ea82",
            "input": "As the first step, we build three baseline LID systems, one based on the i-vector model, and the other two based on LSTM-RNN, using the speech data of two languages from Babel: Assamese and Georgian (AG). The two RNN LID baselines are: a standard RNN LID system (AG-RNN-LID) that discriminates between the two languages in its output, and a multi-task system (AG-RNN-MLT) that was trained to discriminate between the two languages as well as the phones. \n Question: Which is the baseline model?",
            "output": [
                "The three baseline models are the i-vector model, a standard RNN LID system and a multi-task RNN LID system. "
            ]
        },
        {
            "id": "task460-9409582844744d488c4ce93a6e9ed482",
            "input": "Eventually, we have a balanced testing dataset, where each term-sense pair has around 15 samples for testing (on average, each pair has 14.56 samples and the median sample number is 15), and a comparison with training dataset is shown in Figure FIGREF11. Due to the difficulty for collecting the testing dataset, we decided to only collect for a random selection of 30 terms.  \n Question: How big is dataset for testing?",
            "output": [
                "30 terms, each term-sanse pair has around 15 samples for testing"
            ]
        },
        {
            "id": "task460-f7e967b9bd304625a171d475ecb28224",
            "input": "It may be difficult to spot patterns by just looking at a collection of images. Another method is to tag all descriptions with part-of-speech information, so that it becomes possible to see e.g. which adjectives are most commonly used for particular nouns. One method readers may find particularly useful is to leverage the structure of Flickr30K Entities BIBREF8 . Following this, I applied Louvain clustering BIBREF9 to the coreference graph, resulting in clusters of expressions that refer to similar entities. \n Question: Which methods are considered to find examples of biases and unwarranted inferences??",
            "output": [
                "spot patterns by just looking at a collection of images tag all descriptions with part-of-speech information I applied Louvain clustering"
            ]
        },
        {
            "id": "task460-15e82fcacc9d4109b68d72705d7c3265",
            "input": "We then proceed to connect these mentions i) if they co-occur within the same document (we will refer to this as DOC-BASED edges), ii) if the pair of named entity mentions is identical (MATCH edges—these may connect nodes across and within documents), or iii) if they are in the same coreference chain, as predicted by the external coreference system (COREF edges).  \n Question: How did they get relations between mentions?",
            "output": [
                "Assign a value to the relation based on whether mentions occur in the same document, if mentions are identical, or if mentions are in the same coreference chain."
            ]
        },
        {
            "id": "task460-5a8e3fd5a6054bc2a2668a012eac78f8",
            "input": "Cloze Track User Query Track \n Question: What two types the Chinese reading comprehension dataset consists of?",
            "output": [
                "cloze-style reading comprehension and user query reading comprehension questions"
            ]
        },
        {
            "id": "task460-172a8d8684a646fd9a3b9039755c3691",
            "input": "We use an unsupervised word segmentation method latticelm that can directly segment words from the lattices of the speech recognition results of the uttered sentences BIBREF22 . \n Question: Which method do they use for word segmentation?",
            "output": [
                "unsupervised word segmentation method latticelm"
            ]
        },
        {
            "id": "task460-e1162a0cd643430dba874c6e7aabea55",
            "input": "We compare our approach to two other, the first one uses almost the same tweet data as we use for training, and the second one is the CrowdFlower dataset annotated for emotions. In the first one Wang et al. BIBREF21 downloaded over 5M tweets which included one of 131 emotional hashtags based on Parrott's three-level categorization of emotions in seven categories: joy, sadness, anger, love, fear, thankfulness, surprise.  \n Question: What data is used in experiments?",
            "output": [
                "Wang et al. CrowdFlower dataset "
            ]
        },
        {
            "id": "task460-3a6d55c2f20441bbb54cc8f158007c75",
            "input": "Our model BiLSTM+CNN(grapheme-level) and BiLSTM+CNN(G)+POS outperforms all other model experimented in OurNepali and ILPRL dataset respectively. \n Question: What is the best model?",
            "output": [
                "BiLSTM+CNN(grapheme-level) and BiLSTM+CNN(G)+POS "
            ]
        },
        {
            "id": "task460-94f285460d7d41699489e8914be160fc",
            "input": "We also run additional CTC experiments with 36 layers Transformer (total parameters $\\pm $120 millions). The baseline with 36 layers has the same performance with 24 layers, but by adding the proposed methods, the 36 layer performance improved to give the best results.  \n Question: How many layers do they use in their best performing network?",
            "output": [
                "36"
            ]
        },
        {
            "id": "task460-879491515d4c422f85b866356d0497eb",
            "input": "E2ECM BIBREF11: In dialogue policy maker, it adopts a classic classification for skeletal sentence template. In our implement, we construct multiple binary classifications for each act to search the sentence template according to the work proposed by BIBREF11.\n\nCDM BIBREF10: This approach designs a group of classifications (two multi-class classifications and some binary classifications) to model the dialogue policy. \n Question: What are state-of-the-art baselines?",
            "output": [
                "E2ECM CDM"
            ]
        },
        {
            "id": "task460-ebd310a08a204db781678690a3d70311",
            "input": "The word intrusion test is expensive to apply since it requires manual evaluations by human observers separately for each embedding dimension. Furthermore, the word intrusion test does not quantify the interpretability levels of the embedding dimensions, instead it yields a binary decision as to whether a dimension is interpretable or not. However, using continuous values is more adequate than making binary evaluations since interpretability levels may vary gradually across dimensions. \n Question: What advantages does their proposed method of quantifying interpretability have over the human-in-the-loop evaluation they compare to?",
            "output": [
                "it is less expensive and quantifies interpretability using continuous values rather than binary evaluations"
            ]
        },
        {
            "id": "task460-ec248d0797094e54a63ff53252837021",
            "input": "In this paper we proposed a methodology to identify words that could lead to confusion at any given node of a speech recognition based system. We used edit distance as the metric to identifying the possible confusion between the active words.  There is a significant saving in terms of being able to identify recognition bottlenecks in a menu based speech solution through this analysis because it does not require actual people testing the system.  Actual use of this analysis was carried out for a speech solution developed for Indian Railway Inquiry System to identify bottlenecks in the system before its actual launch. \n Question: what bottlenecks were identified?",
            "output": [
                "Confusion in recognizing the words that are active at a given node by a speech recognition solution developed for Indian Railway Inquiry System."
            ]
        },
        {
            "id": "task460-6a2f8c3e729f4fd48801ae3281d57ddb",
            "input": "MonaLog utilizes two auxiliary sets. First, a knowledge base ${K}$ that stores the world knowledge needed for inference, e.g., semanticist $\\le $ linguist and swim $\\le $ move, which captures the facts that $[\\![\\mbox{\\em semanticist}]\\!]$ denotes a subset of $[\\![\\mbox{\\em linguist}]\\!]$, and that $[\\![\\mbox{\\em swim}]\\!]$ denotes a subset of $[\\![\\mbox{\\em move}]\\!]$, respectively. Such world knowledge can be created manually for the problem at hand, or derived easily from existing resources such as WordNet BIBREF22. Note that we do not blindly add all relations from WordNet to our knowledge base, since this would hinge heavily on word sense disambiguation (we need to know whether the “bank” is a financial institution or a river bank to extract its relations correctly). In the current implementation, we avoid this by adding x $\\le $ y or x $\\perp $ y relations only if both x and y are words in the premise-hypothesis pair. Additionally, some relations that involve quantifiers and prepositions need to be hard-coded, since WordNet does not include them: every $=$ all $=$ each $\\le $ most $\\le $ many $\\le $ a few $=$ several $\\le $ some $=$ a; the $\\le $ some $=$ a; on $\\perp $ off; up $\\perp $ down; etc. \n Question: How do they select monotonicity facts?",
            "output": [
                "They derive it from Wordnet"
            ]
        },
        {
            "id": "task460-3968de6a5e4c4f73a94fee0f1be3e1ba",
            "input": " Finally, when investigating the relatedness between European vs. non European languages (cf. (En/Fr)$\\rightarrow $Ar), we obtain similar results than those obtained in the monolingual experiment (macro F-score 62.4 vs. 68.0) and best results are achieved by Ar $\\rightarrow $(En/Fr). This shows that there are pragmatic devices in common between both sides and, in a similar way, similar text-based patterns in the narrative way of the ironic tweets. \n Question: Do the authors identify any cultural differences in irony use?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-d8872850934d4640b0f13f28e5d30dce",
            "input": "Proposed Fusion Techniques ::: Step-Wise Decoder Fusion\nOur first proposed technique is the step-wise decoder fusion of visual features during every prediction step i.e. we concatenate the visual encoding as context at each step of the decoding process. Proposed Fusion Techniques ::: Multimodal Attention Modulation\nSimilar to general attention BIBREF8, wherein a variable-length alignment vector $a_{th}(s)$, whose size equals the number of time steps on the source side, is derived by comparing the current target hidden state $h_{t}$ with each source hidden state $\\overline{h_{s}}$; we consider a variant wherein the visual encoding $v_{t}$ is used to calculate an attention distribution $a_{tv}(s)$ over the source encodings as well. Proposed Fusion Techniques ::: Visual-Semantic (VS) Regularizer\nIn terms of leveraging the visual modality for supervision, BIBREF1 use multi-task learning to learn grounded representations through image representation prediction. \n Question: What are 3 novel fusion techniques that are proposed?",
            "output": [
                "Step-Wise Decoder Fusion Multimodal Attention Modulation Visual-Semantic (VS) Regularizer"
            ]
        },
        {
            "id": "task460-f1e5732d966f4b438173cca0217c8059",
            "input": "Our annotation scheme introduces opportunities for the educational community to conduct further research on the relationship between features of student talk, student learning, and discussion quality. Once automated classifiers are developed, such relations between talk and learning can be examined at scale. Also, automatic labeling via a standard coding scheme can support the generalization of findings across studies, and potentially lead to automated tools for teachers and students.  The development of an annotation scheme explicitly designed for this problem is the first step towards collecting and annotating corpora that can be used by the NLP community to advance the field in this particular area. \n Question: what opportunities are highlighted?",
            "output": [
                "Our annotation scheme introduces opportunities for the educational community to conduct further research  Once automated classifiers are developed, such relations between talk and learning can be examined at scale  automatic labeling via a standard coding scheme can support the generalization of findings across studies, and potentially lead to automated tools for teachers and students collecting and annotating corpora that can be used by the NLP community to advance the field in this particular area"
            ]
        },
        {
            "id": "task460-ab49d125773944699dc4f3f7364d288f",
            "input": "Above all, we introduce two common baselines. The first one just selects the leading sentences to form a summary. It is often used as an official baseline of DUC, and we name it “LEAD”. The other system is called “QUERY_SIM”, which directly ranks sentences according to its TF-IDF cosine similarity to the query. Above all, we introduce two common baselines. The first one just selects the leading sentences to form a summary. It is often used as an official baseline of DUC, and we name it “LEAD”. The other system is called “QUERY_SIM”, which directly ranks sentences according to its TF-IDF cosine similarity to the query. In addition, we implement two popular extractive query-focused summarization methods, called MultiMR BIBREF2 and SVR BIBREF20 . Since our model is totally data-driven, we introduce a recent summarization system DocEmb BIBREF9 that also just use deep neural network features to rank sentences. To verify the effectiveness of the joint model, we design a baseline called ISOLATION, which performs saliency ranking and relevance ranking in isolation. \n Question: What models do they compare to?",
            "output": [
                "LEAD QUERY_SIM MultiMR SVR DocEmb ISOLATION"
            ]
        },
        {
            "id": "task460-fd7c322190af4102b0bd7cfe9d821b67",
            "input": "Work on VAE in BIBREF17 to learn acoustic embeddings conducted experiments using the TIMIT data set. The baseline performance for VAE based phone classification experiments in BIBREF17 report an accuracy of 72.2%. The re-implementation forming the basis for our work gave an accuracy of 72.0%, a result that was considered to provide a credible basis for further work. \n Question: What classification baselines are used for comparison?",
            "output": [
                "VAE"
            ]
        },
        {
            "id": "task460-5c8bf868eced46738150a74aca0b3ca6",
            "input": "We compare our model with several baselines including:\n\nAttn seq2seq BIBREF22: A model with simple attention over the input context at each time step during decoding.\n\nPtr-UNK BIBREF23: Ptr-UNK is the model which augments a sequence-to-sequence architecture with attention-based copy mechanism over the encoder context.\n\nKV Net BIBREF6: The model adopted and argumented decoder which decodes over the concatenation of vocabulary and KB entities, which allows the model to generate entities.\n\nMem2Seq BIBREF7: Mem2Seq is the model that takes dialogue history and KB entities as input and uses a pointer gate to control either generating a vocabulary word or selecting an input as the output.\n\nDSR BIBREF9: DSR leveraged dialogue state representation to retrieve the KB implicitly and applied copying mechanism to retrieve entities from knowledge base while decoding. \n Question: What were the baseline systems?",
            "output": [
                "Attn seq2seq Ptr-UNK KV Net Mem2Seq DSR"
            ]
        },
        {
            "id": "task460-3aae54bee6f34a7b891b1ace670f3b34",
            "input": "We first test our model on the single domain dataset, WoZ2.0 BIBREF19 . It consists of 1,200 dialogues from the restaurant reservation domain with three pre-defined slots: food, price range, and area. Since the name slot rarely occurs in the dataset, it is not included in our experiments, following previous literature BIBREF3 , BIBREF20 . Our model is also tested on the multi-domain dataset, MultiWoZ BIBREF9 . It has a more complex ontology with 7 domains and 25 predefined slots. Since the combined slot-value pairs representation of the belief states has to be applied for the model with $O(n)$ ITC, the total number of slots is 35.  \n Question: Which datasets are used to evaluate performance?",
            "output": [
                "the single domain dataset, WoZ2.0  the multi-domain dataset, MultiWoZ"
            ]
        },
        {
            "id": "task460-4721761c01dc46859bbf726d7c712673",
            "input": "WMT14 En-Fr and En-De datasets The WMT 2014 English-French translation dataset, consisting of $36M$ sentence pairs, is adopted as a big dataset to test our model. For medium dataset, we borrow the setup of BIBREF0 and adopt the WMT 2014 English-German translation dataset which consists of $4.5M$ sentence pairs, the BPE vocabulary size is set to $32K$. IWSLT De-En and En-Vi datasets Besides, we perform experiments on two small IWSLT datasets to test the small version of MUSE with other comparable models. The IWSLT 2014 German-English translation dataset consists of $160k$ sentence pairs. We also adopt a joint source and target BPE factorization with the vocabulary size of $32K$. The IWSLT 2015 English-Vietnamese translation dataset consists of $133K$ training sentence pairs. \n Question: What datasets are used?",
            "output": [
                "WMT14 En-Fr and En-De datasets IWSLT De-En and En-Vi datasets"
            ]
        },
        {
            "id": "task460-c20129b169824b9b9d35e4704df33cef",
            "input": "We manually inspect 148 samples from the seen part of the test set, containing 440 relations, counting expressed, omitted, wrong and over-generated (hallucinated) facts. \n Question: How is faithfulness of the resulting text evaluated?",
            "output": [
                "manually inspect"
            ]
        },
        {
            "id": "task460-30066e61dde7461db5edddd6ce2fdc2c",
            "input": "Table TABREF18 shows the Spearman correlation values of GM$\\_$KL model evaluated on the benchmark word similarity datasets: SL BIBREF20, WS, WS-R, WS-S BIBREF21, MEN BIBREF22, MC BIBREF23, RG BIBREF24, YP BIBREF25, MTurk-287 and MTurk-771 BIBREF26, BIBREF27, and RW BIBREF28.  Table TABREF19 shows the evaluation results of GM$\\_$KL model on the entailment datasets such as entailment pairs dataset BIBREF29 created from WordNet with both positive and negative labels, a crowdsourced dataset BIBREF30 of 79 semantic relations labelled as entailed or not and annotated distributionally similar nouns dataset BIBREF31 \n Question: What are the qualitative experiments performed on benchmark datasets?",
            "output": [
                "Spearman correlation values of GM_KL model evaluated on the benchmark word similarity datasets.\nEvaluation results of GM_KL model on the entailment datasets such as entailment pairs dataset created from WordNet, crowdsourced dataset of 79 semantic relations labelled as entailed or not and annotated distributionally similar nouns dataset."
            ]
        },
        {
            "id": "task460-34d4ba7e269340f78cc3050e672e558d",
            "input": "In this work, we use pretraining on the unlabeled data of each task and show that it can increase the performance of classification systems. \n Question: Where is MVCNN pertained?",
            "output": [
                "on the unlabeled data of each task"
            ]
        },
        {
            "id": "task460-dcb4b27cc9c3433b9750d707fb38fc8a",
            "input": "We first describe our models individually and then the ensembling technique that we employ. In the following, MN denotes Memory Networks to encode conversational history, RCNN signify R-CNN for object level representations of an image, Wt represents additional linear layer in the decoder, and LF a late fusion mechanism as defined in BIBREF0.\n\nModels ::: LF-RCNN\nLate fusion encoder BIBREF0 with concatenated history. We use two-layered LSTMs with 512 hidden units for embedding questions and history. The object-level features are weighed using only question embeddings. The word embeddings from Glove vectors are frozen and are not fine-tuned. Figure FIGREF6 gives an overview of the architecture.\n\nModels ::: MN-RCNN\nMemory network encoder BIBREF0 with bi-directional GRUs and word embeddings fine-tuned. Object-level features are weighed by question and caption embedding. The rest of the scheme is same as above. (Figure FIGREF6)\n\nModels ::: MN-RCNN-Wt\nSame as above but with an additional linear layer applied to the dot product of candidate answer and encoder output, and gated using tanh function. Compare Figure FIGREF6 with Figure FIGREF6 \n Question: Which three discriminative models did they use?",
            "output": [
                "LF-RCNN MN-RCNN MN-RCNN-Wt"
            ]
        },
        {
            "id": "task460-bbd85d9e29f7480b99eee8834e5c1ccf",
            "input": "The size of the training datasets varies considerably from 469 posts to 17 million; a difference of four orders of magnitude. \n Question: How big are this dataset and catalogue?",
            "output": [
                " from 469 posts to 17 million"
            ]
        },
        {
            "id": "task460-b5d41e8c934245e7b40b8cec9cef1d86",
            "input": "The 16 classes are inspired by the OntoNotes5 corpus BIBREF7 as well as the ACE (Automatic Content Extraction) English Annotation Guidelines for Entities Version 6.6 2008.06.13 BIBREF8. \n Question: How did they determine the distinct classes?",
            "output": [
                "inspired by the OntoNotes5 corpus BIBREF7 as well as the ACE (Automatic Content Extraction) English Annotation Guidelines for Entities Version 6.6 2008.06.13 BIBREF8"
            ]
        },
        {
            "id": "task460-fb0aaf20e3c34cc1ba5ee6e3c9fba5a3",
            "input": "We carried out a reliability study for the proposed scheme using two pairs of expert annotators, P1 and P2.  \n Question: do they use a crowdsourcing platform?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-a82219cfb76b45eeab3d579775a3f4f1",
            "input": "We carried out a reliability study for the proposed scheme using two pairs of expert annotators, P1 and P2.  Inter-rater reliability was assessed using Cohen's kappa: unweighted for argumentation and knowledge domain, but quadratic-weighted for specificity given its ordered labels. \n Question: what experiments are conducted?",
            "output": [
                "a reliability study for the proposed scheme "
            ]
        },
        {
            "id": "task460-5de37a29d8324c9ea52cb3452a078b0b",
            "input": "We then train the sensationalism scorer by classifying sensational and non-sensational headlines using a one-layer CNN with a binary cross entropy loss $L_{\\text{sen}}$. Firstly, 1-D convolution is used to extract word features from the input embeddings of a headline. This is followed by a ReLU activation layer and a max-pooling layer along the time dimension. All features from different channels are concatenated together and projected to the sensationalism score by adding another fully connected layer with sigmoid activation. Binary cross entropy is used to compute the loss $L_{\\text{sen}}$. \n Question: How is sensationalism scorer trained?",
            "output": [
                "by classifying sensational and non-sensational headlines using a one-layer CNN with a binary cross entropy loss $L_{\\text{sen}}$"
            ]
        },
        {
            "id": "task460-d4b74fca07a54a10bd73fefe06c75d27",
            "input": "For the cross-domain polarity classification experiments, we use the second version of Multi-Domain Sentiment Dataset BIBREF0 . The data set contains Amazon product reviews of four different domains: Books (B), DVDs (D), Electronics (E) and Kitchen appliances (K). Reviews contain star ratings (from 1 to 5) which are converted into binary labels as follows: reviews rated with more than 3 stars are labeled as positive, and those with less than 3 stars as negative. In each domain, there are 1000 positive and 1000 negative reviews. \n Question: How long is the dataset?",
            "output": [
                "8000"
            ]
        },
        {
            "id": "task460-6b677feef56344d5b072d520709e1048",
            "input": "An attention-based sequence-to-sequence model with the emoji vector as additional input as discribed in MojiTalk BIBREF16. CVAE.\nAn RNN-based conditional variational autoencoder for dialogue response generation BIBREF16, which uses a multivariate Gaussian latent variable to model the response and concatenate it with the last hidden state of the encoder as the initial state of the decoder. KL annealing, early stopping strategy and bag-of-word auxiliary loss are applied during the training. We use the implementation released by BIBREF16 \n Question: What baselines other than standard transformers are used in experiments?",
            "output": [
                "attention-based sequence-to-sequence model  CVAE"
            ]
        },
        {
            "id": "task460-67874875613148e682350901bda6ba8d",
            "input": "We collected a total of 657 ratings by 14 volunteers, 5 Italian and 9 non-Italian listeners, spread over the 24 clips and three testing conditions. \n Question: How many people are employed for the subjective evaluation?",
            "output": [
                "14 volunteers"
            ]
        },
        {
            "id": "task460-cc801a8167f04341acfb3c02165ff1de",
            "input": "time: decade (classes between 1900s and 2010s) and year representative of the time when the genre became meainstream \n Question: Which decades did they look at?",
            "output": [
                "between 1900s and 2010s"
            ]
        },
        {
            "id": "task460-fba6cb283cd84f5b81320a8bd6600c83",
            "input": "We use the method of BIBREF5 to train neural sequence-to-sequence Spanish-English ST models. Obtaining gold topic labels for our data would require substantial manual annotation, so we instead use the human translations from the 1K (train20h) training set utterances to train the NMF topic model with scikit-learn BIBREF14 \n Question: What is the architecture of the model?",
            "output": [
                "BIBREF5 to train neural sequence-to-sequence NMF topic model with scikit-learn BIBREF14"
            ]
        },
        {
            "id": "task460-b6e207fae58f41c391c58b357cc6313c",
            "input": "In fact, realizing the necessity of large text corpus for Sindhi, we started this research by collecting raw corpus from multiple web resource using web-scrappy framwork for extraction of news columns of daily Kawish and Awami Awaz Sindhi newspapers, Wikipedia dumps, short stories and sports news from Wichaar social blog, news from Focus Word press blog, historical writings, novels, stories, books from Sindh Salamat literary websites, novels, history and religious books from Sindhi Adabi Board and tweets regarding news and sports are collected from twitter. \n Question: How is the data collected, which web resources were used?",
            "output": [
                "daily Kawish and Awami Awaz Sindhi newspapers Wikipedia dumps short stories and sports news from Wichaar social blog news from Focus Word press blog historical writings, novels, stories, books from Sindh Salamat literary website novels, history and religious books from Sindhi Adabi Board  tweets regarding news and sports are collected from twitter"
            ]
        },
        {
            "id": "task460-5834850f0b074dc09c5a3c709c0cb347",
            "input": "We evaluate with all commonly-used metrics in question generation BIBREF13: BLEU-1 (B1), BLEU-2 (B2), BLEU-3 (B3), BLEU-4 (B4) BIBREF17, METEOR (MET) BIBREF18 and ROUGE-L (R-L) BIBREF19. We use the evaluation script released by Chen2015MicrosoftCC. \n Question: What metrics do they use?",
            "output": [
                "BLEU-1 (B1), BLEU-2 (B2), BLEU-3 (B3), BLEU-4 (B4) BIBREF17, METEOR (MET) BIBREF18 and ROUGE-L (R-L) BIBREF19"
            ]
        },
        {
            "id": "task460-54003babfb21424eadde96305971241a",
            "input": "We therefore propose attention models to infer the latent context, i.e., the series of posts that trigger an intervention. \n Question: What type of latent context is used to predict instructor intervention?",
            "output": [
                "the series of posts that trigger an intervention"
            ]
        },
        {
            "id": "task460-f2f8ebfebf7e4d08ae3aebf55fe1e2b7",
            "input": " After having sentence representation, we use some classification models to classify input sentences. Those models will be described in detail in the section SECREF13. The first model is TextCNN (figure FIGREF2) proposed in BIBREF11. It only contains CNN blocks following by some Dense layers. The output of multiple CNN blocks with different kernel sizes is connected to each other.\n\nThe second model is VDCNN (figure FIGREF5) inspired by the research in BIBREF12. Like the TextCNN model, it contains multiple CNN blocks. The addition in this model is its residual connection.\n\nThe third model is a simple LSTM bidirectional model (figure FIGREF15). It contains multiple LSTM bidirectional blocks stacked to each other.\n\nThe fourth model is LSTMCNN (figure FIGREF24). Before going through CNN blocks, series of word embedding will be transformed by LSTM bidirectional block. The final model is the system named SARNN (figure FIGREF25). It adds an attention block between LTSM blocks. In this system, we use the Stacking method. In this method, the output of each model is not only class id but also the probability of each class in the set of three classes. This probability will become a feature for the ensemble model. The stacking ensemble model here is a simple full-connection model with input is all of probability that output from sub-model. The output is the probability of each class. \n Question: What classifier do they use?",
            "output": [
                "Stacking method LSTMCNN SARNN simple LSTM bidirectional model TextCNN"
            ]
        },
        {
            "id": "task460-36d9eaaaed8142bda2738a6a312e91a2",
            "input": "We improve over the previous state-of-the-art BIBREF35 for VQA dataset by around 6% in BLEU score and 10% in METEOR score. In the VQG-COCO dataset, we improve over BIBREF5 by 3.7% and BIBREF36 by 3.5% in terms of METEOR scores. \n Question: What were the previous state of the art benchmarks?",
            "output": [
                "BIBREF35 for VQA dataset BIBREF5 BIBREF36"
            ]
        },
        {
            "id": "task460-650cd314cbde47f49320d93acfd3db04",
            "input": "We use the following three evaluation metrics:\n\nPhoneme Error Rate (PER) is the Levenshtein distance between the predicted phoneme sequences and the gold standard phoneme sequences, divided by the length of the gold standard phoneme sequences.\n\nWord Error Rate (WER) is the percentage of words in which the predicted phoneme sequence does not exactly match the gold standard phoneme sequence.\n\nWord Error Rate 100 (WER 100) is the percentage of words in the test set for which the correct guess is not in the first 100 guesses of the system. \n Question: what evaluation metrics were used?",
            "output": [
                "Phoneme Error Rate (PER) Word Error Rate (WER) Word Error Rate 100 (WER 100)"
            ]
        },
        {
            "id": "task460-6ac9ef98a0ff4ea28d706fb0885fe83a",
            "input": " Logician is trained under the attention-based sequence-to-sequence paradigm, with three mechanisms: restricted copy mechanism to ensure literally honestness, coverage mechanism to alleviate the under extraction and over extraction problem, and gated dependency attention mechanism to incorporate dependency information.  \n Question: How is Logician different from traditional seq2seq models?",
            "output": [
                "restricted copy mechanism to ensure literally honestness, coverage mechanism to alleviate the under extraction and over extraction problem, and gated dependency attention mechanism to incorporate dependency information"
            ]
        },
        {
            "id": "task460-46fbab15ea96422b88d714c6cd2f547d",
            "input": " In order to explore the problem space, we thus specifically target five binary classification problems addressed in BIBREF17 , BIBREF18 , i.e presence/absence of consonants, phonemic nasal, bilabial, high-front vowels and high-back vowels. \n Question: What are the five different binary classification tasks?",
            "output": [
                " presence/absence of consonants, phonemic nasal, bilabial, high-front vowels and high-back vowels."
            ]
        },
        {
            "id": "task460-dae9f7dac9be40e89286c371f471608a",
            "input": "For each qualifying diagnosis tweet we retrieve the timeline of the corresponding Twitter user using the Twitter user_timeline API endpoint . Subsequently, we remove all non-English tweets (Twitter API machine-detected“lang” field), all retweets, and tweets that contain “diagnos*” or “depress*”, but not a valid diagnosis statement. The resulting Depressed cohort contains 1,207 individuals and 1,759,644 tweets ranging from from May 2008 to September 2018. \n Question: Do they report results only on English datasets?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-a4417f7319df4f078016dae1beb72796",
            "input": "The study of the mathematical structure of grammar has indicated that the fundamental things making up sentences are not the words, but some atomic grammatical types, such as the noun-type and the sentence-type BIBREF23 , BIBREF24 , BIBREF25 . The transitive verb-type is not an atomic grammatical type, but a composite made up of two noun-types and one sentence-type. Hence, particularly interesting here is that atomic doesn't really mean smallest...\n\nOn the other hand, just like in particle physics where we have particles and anti-particles, the atomic types include types as well as anti-types. But unlike in particle physics, there are two kinds of anti-types, namely left ones and right ones. This makes language even more non-commutative than quantum theory! \n Question: Do they break down word meanings into elementary particles as in the standard model of quantum theory?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-663729a16d864a968e6d077da55817ca",
            "input": "In this study, we investigate the efficacy of bias reduction during training by introducing a new loss function which encourages the language model to equalize the probabilities of predicting gendered word pairs like he and she.  \n Question: what kinds of male and female words are looked at?",
            "output": [
                "gendered word pairs like he and she"
            ]
        },
        {
            "id": "task460-530c09ea1f5f41b9b1122844158fab4b",
            "input": "Table TABREF20 shows that, on WoZ and DSTC2 datasets, SIM model has the same number of parameters, which is only 23% and 19% of that in GLAD model. \n Question: How do they measure model size?",
            "output": [
                "By the number of parameters."
            ]
        },
        {
            "id": "task460-b9c9f5aa9460421e872b33d76ce89091",
            "input": "We typically start by identifying the questions we wish to explore. Can text analysis provide a new perspective on a “big question” that has been attracting interest for years? Or can we raise new questions that have only recently emerged, for example about social media? For social scientists working in computational analysis, the questions are often grounded in theory, asking: How can we explain what we observe? Computational analysis of text motivated by these questions is insight driven: we aim to describe a phenomenon or explain how it came about. For example, what can we learn about how and why hate speech is used or how this changes over time? Is hate speech one thing, or does it comprise multiple forms of expression? Is there a clear boundary between hate speech and other types of speech, and what features make it more or less ambiguous? Sometimes we also hope to connect to multiple disciplines. For example, while focusing on the humanistic concerns of an archive, we could also ask social questions such as “is this archive more about collaborative processes, culture-building or norm creation?” or “how well does this archive reflect the society in which it is embedded?\" BIBREF3 used quantitative methods to tell a story about Darwin's intellectual development—an essential biographical question for a key figure in the history of science. \n Question: What kind of issues (that are not on the forefront of computational text analysis) do they tackle?",
            "output": [
                "identifying the questions we wish to explore Can text analysis provide a new perspective on a “big question” that has been attracting interest for years? How can we explain what we observe? hope to connect to multiple disciplines"
            ]
        },
        {
            "id": "task460-46c200307dad4c0a997b2b4bb0469bea",
            "input": " The flexibility of the DNN model allowed us to include many more surface level features such as affixes, leading and trailing characters in words and stems, and the presence of words in large gazetteers of named entities. As we show later, these additional features significantly lowered CEER. \n Question: what surface-level features are used?",
            "output": [
                "affixes, leading and trailing characters in words and stems, and the presence of words in large gazetteers of named entities"
            ]
        },
        {
            "id": "task460-7ea75df990274d5cb813f20613326a01",
            "input": "Moreover, the add and add-gated variants of Bertram perform surprisingly well for more frequent words, improving the score for WNLaMPro-medium by 50% compared to BERT$_\\text{base}$ and 31% compared to Attentive Mimicking. \n Question: How much is representaton improved for rare/medum frequency words compared to standalone BERT and previous work?",
            "output": [
                "improving the score for WNLaMPro-medium by 50% compared to BERT$_\\text{base}$ and 31% compared to Attentive Mimicking"
            ]
        },
        {
            "id": "task460-8405d9956ace4564bc106b94afbb63b5",
            "input": "Datasets\nWe conduct experiments on two Chinese question answering datasets from NLPCC-2016 evaluation task BIBREF13 . DBQA is a document based question answering dataset.  KBRE is a knowledge based relation extraction dataset. \n Question: Which dataset(s) do they evaluate on?",
            "output": [
                "DBQA KBRE"
            ]
        },
        {
            "id": "task460-e93544d75c6748b28afbd23543c01332",
            "input": "We use two basic features:\n\nParts of Speech (POS) tags: We use the POS tagger of NLTK to tag the tweet texts BIBREF0 . We use counts of noun, adjective, adverb, verb words in a tweet as POS features.\n\nPrior polarity of the words: We use a polarity dictionary BIBREF3 to get the prior polarity of words. The dictionary contains positive, negative and neutral words along with their polarity strength (weak or strong). The polarity of a word is dependent on its POS tag. For example, the word `excuse' is negative when used as `noun' or `adjective', but it carries a positive sense when used as a `verb'. We use the tags produced by NLTK postagger while selecting the prior polarity of a word from the dictionary. We also employ stemming (Porter Stemmer implementation from NLTK) while performing the dictionary lookup to increase number of matches. We use the counts of weak positive words, weak negative words, strong positive words and strong negative words in a tweet as features.\n\nWe have also explored some advanced features that helps improve detecting sentiment of tweets.\n\nEmoticons: We use the emoticon dictionary from BIBREF2 , and count the positive and negtive emocicons for each tweet.\n\nThe sentiment of url: Since almost all the articles are written in well-formatted english, we analyze the sentiment of the first paragraph of the article using Standford Sentiment Analysis tool BIBREF4 . It predicts sentiment for each sentence within the article. We calculate the fraction of sentences that are negative, positive, and neutral and use these three values as features.\n\nHashtag: We count the number of hashtags in each tweet.\n\nCapitalization: We assume that capitalization in the tweets has some relationship with the degree of sentiment. We count the number of words with capitalization in the tweets.\n\nRetweet: This is a boolean feature indicating whether the tweet is a retweet or not.\n\nUser Mention: A boolean feature indicating whether the tweet contains a user mention.\n\nNegation: Words like `no', `not', `won't' are called negation words since they negate the meaning of the word that is following it. As for example `good' becomes `not good'. We detect all the negation words in the tweets. If a negation word is followed by a polarity word, then we negate the polarity of that word. For example, if `good' is preceeded by a `not', we change the polarity from `weak positive' to `weak negative'.\n\nText Feature: We use tf-idf based text features to predict the sentiment of a tweet. We perform tf-idf based scoring of words in a tweet and the hashtags present in the tweets. We use the tf-idf vectors to train a classifier and predict the sentiment. This is then used as a stacked prediction feature in the final classifier. \n Question: What linguistic features are used?",
            "output": [
                "Parts of Speech (POS) tags Prior polarity of the words Capitalization Negation Text Feature"
            ]
        },
        {
            "id": "task460-e100052a42b24cf3aaf52b910c4a4517",
            "input": "We apply our adaptively sparse Transformers on four machine translation tasks.  IWSLT 2017 German $\\rightarrow $ English BIBREF27: 200K sentence pairs.\n\nKFTT Japanese $\\rightarrow $ English BIBREF28: 300K sentence pairs.\n\nWMT 2016 Romanian $\\rightarrow $ English BIBREF29: 600K sentence pairs.\n\nWMT 2014 English $\\rightarrow $ German BIBREF30: 4.5M sentence pairs. \n Question: What tasks are used for evaluation?",
            "output": [
                "four machine translation tasks: German -> English, Japanese -> English, Romanian -> English, English -> German"
            ]
        },
        {
            "id": "task460-ded9ec0b304b45409b77f87bd592f4f9",
            "input": "Secondly, we examine the effect of Washington Post on the views of the users. This is done by looking at the sentiments of the candidates (to predict winners) of a debate before and after the winners are announced by the experts in Washington Post. This way, we can see if Washington Post has had any effect on the sentiments of the users. One can see the winners suggested by the Washington Post in Table TABREF35.  \n Question: How do you establish the ground truth of who won a debate?",
            "output": [
                "experts in Washington Post"
            ]
        },
        {
            "id": "task460-393c70825e6a48ccb1f9170691a644fb",
            "input": "The main reason is that the datasets only contain a small portion of multi-aspect sentences with different polarities. The distraction of attention will not impact the sentiment prediction much in single-aspect sentences or multi-aspect sentences with the same polarities. \n Question: Is the model evaluated against the baseline also on single-aspect sentences?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-bede1589979c49adad864edab207096f",
            "input": "Using sequence-to-sequence networks, the approach here is jointly training annotated utterance-level intents and slots/intent keywords by adding / tokens to the beginning/end of each utterance, with utterance-level intent-type as labels of such tokens. Our approach is an extension of BIBREF2 , in which only an term is added with intent-type tags associated to this sentence final token, both for LSTM and Bi-LSTM cases. However, we experimented with adding both and terms as Bi-LSTMs will be used for seq2seq learning, and we observed that slightly better results can be achieved by doing so. The idea behind is that, since this is a seq2seq learning problem, at the last time step (i.e., prediction at ) the reverse pass in Bi-LSTM would be incomplete (refer to Fig. FIGREF24 (a) to observe the last Bi-LSTM cell). Therefore, adding token and leveraging the backward LSTM output at first time step (i.e., prediction at ) would potentially help for joint seq2seq learning. An overall network architecture can be found in Fig. FIGREF30 for our joint models. We will report the experimental results on two variations (with and without intent keywords) as follows:\n\nJoint-1: Seq2seq Bi-LSTM for utterance-level intent detection (jointly trained with slots)\n\nJoint-2: Seq2seq Bi-LSTM for utterance-level intent detection (jointly trained with slots & intent keywords) \n Question: What is shared in the joint model?",
            "output": [
                "jointly trained with slots"
            ]
        },
        {
            "id": "task460-d5797a78ec894e169ba80f572e70536a",
            "input": "The Wikipedia revision dumps that were previously introduced by Leskovec et al. leskovec2010governance contain eight GB (compressed size) revision edits with meta data. \n Question: How large is the Wikipedia revision dump dataset?",
            "output": [
                "eight GB"
            ]
        },
        {
            "id": "task460-3e43cf593e834d978f2ea328e8fe96a7",
            "input": "Then, we compute embeddings of concepts (by GloVe) for interview descriptions and for examination descriptions separately. The simplest way to generate text embeddings based on term embeddings is to use some kind of aggregation of term embeddings such as an average. This approach was tested for example by BIBREF21 and BIBREF13 . BIBREF22 computed a weighted mean of term embeddings by the construction of a loss function and training weights by the gradient descent method.\n\nFinal embeddings for visits are obtained by concatenation of average embeddings calculated separately for the interview and for the medical examination, see Figure FIGREF4 . \n Question: Which word embeddings do they use to represent medical visits?",
            "output": [
                "GloVe concatenation of average embeddings calculated separately for the interview and for the medical examination"
            ]
        },
        {
            "id": "task460-05b23ee7016b4c89a24f15f9dc168d60",
            "input": "Among all three pre-training tasks, SR works slightly better than the other two tasks (i.e., NSG and MDG). \n Question: Which of the three pretraining tasks is the most helpful?",
            "output": [
                "SR"
            ]
        },
        {
            "id": "task460-8b18ed2c23324a59bf2dd4b57b531318",
            "input": "We employ two sources of e-book annotation data: (i) editor tags, and (ii) Amazon search terms. For editor tags, we collect data of 48,705 e-books from 13 publishers, namely Kunstmann, Delius-Klasnig, VUR, HJR, Diogenes, Campus, Kiwi, Beltz, Chbeck, Rowohlt, Droemer, Fischer and Neopubli.  For the Amazon search terms, we collect search query logs of 21,243 e-books for 12 months (i.e., November 2017 to October 2018).  \n Question: what dataset was used?",
            "output": [
                "48,705 e-books from 13 publishers search query logs of 21,243 e-books for 12 months"
            ]
        },
        {
            "id": "task460-16bdba0961c04203841e11b086fd00ac",
            "input": "To validate the effectiveness of AEM for extracting events from social media (e.g. Twitter) and news media sites (e.g. Google news), three datasets (FSD BIBREF12 , Twitter, and Google datasets) are employed. \n Question: What datasets are used?",
            "output": [
                "FSD BIBREF12 , Twitter, and Google datasets"
            ]
        },
        {
            "id": "task460-ec9c28f5ab7a405baa56503da634e8cd",
            "input": "To collect a diverse training dataset, we have randomly sampled 1000 posts each from the subreddits politics, business, science, and AskReddit, and 1000 additional posts from the Reddit frontpage. \n Question: what are the topics pulled from Reddit?",
            "output": [
                "politics, business, science, and AskReddit, and 1000 additional posts from the Reddit frontpage. "
            ]
        },
        {
            "id": "task460-07c246b29a0e44b2a6562095d255d592",
            "input": "For our study, we used the number of retweets to single-out those that went viral within our sample. Tweets within that subset (viral tweets hereafter) are varied and relate to different topics. We consider that a tweet contains fake news if its text falls within any of the following categories described by Rubin et al. BIBREF7 (see next section for the details of such categories): serious fabrication, large-scale hoaxes, jokes taken at face value, slanted reporting of real facts and stories where the truth is contentious. The dataset BIBREF8 , manually labelled by an expert, has been publicly released and is available to researchers and interested parties. \n Question: How did they determine fake news tweets?",
            "output": [
                "an expert annotator determined if the tweet fell under a specific category"
            ]
        },
        {
            "id": "task460-516b9b4e96714fdb8673597765ffe631",
            "input": "The CoNLL-2012 shared task BIBREF21 corpus is used as the evaluation dataset, which is selected from the Ontonotes 5.0. \n Question: What dataset do they evaluate their model on?",
            "output": [
                "CoNLL-2012 shared task BIBREF21 corpus"
            ]
        },
        {
            "id": "task460-f38d4ddd2d0b4a9ba3bf6521aa846180",
            "input": "Dataset: We perform experiments on two widely-used tasks for the English-to-Japanese language pair: KFTT BIBREF12 and BTEC BIBREF13 . KFTT is a collection of Wikipedia article about city of Kyoto and BTEC is a travel conversation corpus. BTEC is an easier translation task than KFTT, because KFTT covers a broader domain, has a larger vocabulary of rare words, and has relatively long sentences.  \n Question: What datasets were used?",
            "output": [
                "KFTT BIBREF12 and BTEC BIBREF13"
            ]
        },
        {
            "id": "task460-5ad6652d02084f66bdaaa7b4d97500ca",
            "input": "We compare our personalized models against two baselines. The first is a name-based Nearest-Neighbor model (NN). We initially adapted the Neural Checklist Model of BIBREF0 as a baseline; however, we ultimately use a simple Encoder-Decoder baseline with ingredient attention (Enc-Dec), which provides comparable performance and lower complexity. \n Question: What are the baseline models?",
            "output": [
                "name-based Nearest-Neighbor model (NN) Encoder-Decoder baseline with ingredient attention (Enc-Dec)"
            ]
        },
        {
            "id": "task460-1c20e96c16d547dd8429d46c47f419d7",
            "input": "The corpus consists of 53 documents, which contain an average number of 156.1 sentences per document, each with 19.55 tokens on average. The corpus comprises 8,275 sentences and 167,739 words in total. \n Question: How large is the corpus?",
            "output": [
                "8,275 sentences and 167,739 words in total"
            ]
        },
        {
            "id": "task460-02a35ce55371432aa5a63d217660b5d1",
            "input": "Arab-Tweet. For modeling age and gender, we use Arap-Tweet BIBREF19 , which we will refer to as Arab-Tweet. UBC Twitter Gender Dataset. We also develop an in-house Twitter dataset for gender. We manually labeled 1,989 users from each of the 21 Arab countries. The dialect identification model in AraNet is based on our winning system in the MADAR shared task 2 BIBREF20 as described in BIBREF12. The corpus is divided into train, dev and test, and the organizers masked test set labels. We make use of two datasets, the LAMA-DINA dataset from BIBREF22, a Twitter dataset with a combination of gold labels from BIBREF23 and distant supervision labels. The rest of the dataset is only labeled using distant supervision (LAMA-DIST) ($182,605$ tweets) . For more information about the dataset, readers are referred to BIBREF22.  We use the dataset for irony identification on Arabic tweets released by IDAT@FIRE2019 shared-task BIBREF24. We collect 15 datasets related to sentiment analysis of Arabic, including MSA and dialects BIBREF25, BIBREF26, BIBREF27, BIBREF1, BIBREF28, BIBREF29, BIBREF30, BIBREF31, BIBREF32, BIBREF33, BIBREF34. Table TABREF28 shows all the corpora we use.  \n Question: What datasets are used in training?",
            "output": [
                "Arap-Tweet BIBREF19  an in-house Twitter dataset for gender the MADAR shared task 2 BIBREF20 the LAMA-DINA dataset from BIBREF22 LAMA-DIST Arabic tweets released by IDAT@FIRE2019 shared-task BIBREF24 BIBREF25, BIBREF26, BIBREF27, BIBREF1, BIBREF28, BIBREF29, BIBREF30, BIBREF31, BIBREF32, BIBREF33, BIBREF34"
            ]
        },
        {
            "id": "task460-70a7f13fc79a40e58bb892858944f327",
            "input": "A gated neural network is employed over character embeddings to generate distributed representations of candidate words, which are sent to a LSTM model. \n Question: Which neural network architectures are employed?",
            "output": [
                "gated neural network "
            ]
        },
        {
            "id": "task460-56a1dd022b904382b3f6ae6e80ffe9df",
            "input": "This working note presents RNN and LSTM based embedding system for social media health text classification. Though the data sets of task 1 and task 2 are limited, this paper proposes RNN and LSTM based embedding method. \n Question: What type of RNN is used?",
            "output": [
                "RNN LSTM"
            ]
        },
        {
            "id": "task460-0e8e3f9183e945669ea65d18ffc8e27b",
            "input": "45 clinically interpretable features per admission were extracted as inputs to the readmission risk classifier. These features can be grouped into three categories (See Table TABREF5 for complete list of features):\n\nSociodemographics: gender, age, marital status, etc.\n\nPast medical history: number of previous admissions, history of suicidality, average length of stay (up until that admission), etc.\n\nInformation from the current admission: length of stay (LOS), suicidal risk, number and length of notes, time of discharge, evaluation scores, etc.\n\nThe Current Admission feature group has the most number of features, with 29 features included in this group alone. These features can be further stratified into two groups: `structured' clinical features and `unstructured' clinical features.\n\nFeature Extraction ::: Structured Features\nStructure features are features that were identified on the EHR using regular expression matching and include rating scores that have been reported in the psychiatric literature as correlated with increased readmission risk, such as Global Assessment of Functioning, Insight and Compliance:\n\nGlobal Assessment of Functioning (GAF): The psychosocial functioning of the patient ranging from 100 (extremely high functioning) to 1 (severely impaired) BIBREF13.\n\nInsight: The degree to which the patient recognizes and accepts his/her illness (either Good, Fair or Poor).\n\nCompliance: The ability of the patient to comply with medication and to follow medical advice (either Yes, Partial, or None).\n\nThese features are widely-used in clinical practice and evaluate the general state and prognosis of the patient during the patient's evaluation.\n\nFeature Extraction ::: Unstructured Features\nUnstructured features aim to capture the state of the patient in relation to seven risk factor domains (Appearance, Thought Process, Thought Content, Interpersonal, Substance Use, Occupation, and Mood) from the free-text narratives on the EHR. These seven domains have been identified as associated with readmission risk in prior work BIBREF14.\n\nThese unstructured features include: 1) the relative number of sentences in the admission notes that involve each risk factor domain (out of total number of sentences within the admission) and 2) clinical sentiment scores for each of these risk factor domains, i.e. sentiment scores that evaluate the patient’s psychosocial functioning level (positive, negative, or neutral) with respect to each of these risk factor domain. \n Question: What features are used?",
            "output": [
                "Sociodemographics: gender, age, marital status, etc. Past medical history: number of previous admissions, history of suicidality, average length of stay (up until that admission), etc. Information from the current admission: length of stay (LOS), suicidal risk, number and length of notes, time of discharge, evaluation scores, etc."
            ]
        },
        {
            "id": "task460-06a48c79d40c4eb3b0533fadfe257220",
            "input": "Metrics. We use tolerance accuracy BIBREF16, which measures how far away the predicted span is from the gold standard span, as a metric. The rationale behind the metric is that, in practice, it suffices to recommend a rough span which contains the answer – a difference of a few seconds would not matter much to the user. Metrics. We used accuracy and MRR (Mean Reciprocal Ranking) as metrics.  Metrics. To evaluate our pipeline approach we use overall accuracy after filtering and accuracy given that the segment is in the top 10 videos.  \n Question: What evaluation metrics were used in the experiment?",
            "output": [
                "For sentence-level prediction they used tolerance accuracy, for segment retrieval accuracy and MRR and for the pipeline approach they used overall accuracy"
            ]
        },
        {
            "id": "task460-8a9d3e4a31ef4553bb9a8b8b12d6f31e",
            "input": "The News Category Dataset BIBREF11 is a collection of headlines published by HuffPost BIBREF12 between 2012 and 2018, and was obtained online from Kaggle BIBREF13. The full dataset contains 200k news headlines with category labels, publication dates, and short text descriptions. \n Question: What news dataset was used?",
            "output": [
                "collection of headlines published by HuffPost BIBREF12 between 2012 and 2018"
            ]
        },
        {
            "id": "task460-647b8e7d60af413ca92bb76d985093d4",
            "input": "Analysis of challenges from UTD\nOur system relies on the pseudotext produced by ZRTools (the only freely available UTD system we are aware of), which presents several challenges for MT.  Assigning wrong words to a cluster\nSince UTD is unsupervised, the discovered clusters are noisy.  Splitting words across different clusters\nAlthough most UTD matches are across speakers, recall of cross-speaker matches is lower than for same-speaker matches. As a result, the same word from different speakers often appears in multiple clusters, preventing the model from learning good translations. UTD is sparse, giving low coverage We found that the patterns discovered by ZRTools match only 28% of the audio. This low coverage reduces training data size, affects alignment quality, and adversely affects translation, which is only possible when pseudoterms are present. \n Question: what challenges are identified?",
            "output": [
                "Assigning wrong words to a cluster Splitting words across different clusters sparse, giving low coverage"
            ]
        },
        {
            "id": "task460-7d7b25952e5843d2b5452cdbf51292eb",
            "input": "Since we were also interested in whether argumentation differs across registers, we included four different registers — namely (1) user comments to newswire articles or to blog posts, (2) posts in discussion forums (forum posts), (3) blog posts, and (4) newswire articles.  \n Question: How is the data in the new corpus come sourced?",
            "output": [
                "user comments to newswire articles or to blog posts forum posts blog posts newswire articles"
            ]
        },
        {
            "id": "task460-6df7511aca874bb5ba38e0ad136292ba",
            "input": "Finally, we transform text to be classified into scalars representing their distance from the constructed hate vector and use these as input to a Random Forest classifier. \n Question: What classifier did they use?",
            "output": [
                "Random Forest"
            ]
        },
        {
            "id": "task460-92ca22c65331433cbe77b7b10bb4b5a6",
            "input": "Each dataset consisted of five Arabic dialects: Egyptian (EGY), Levantine (LEV), Gulf (GLF), North African (NOR), and Modern Standard Arabic (MSA). \n Question: Which are the four Arabic dialects?",
            "output": [
                "Egyptian (EGY) Levantine (LEV) Gulf (GLF) North African (NOR)"
            ]
        },
        {
            "id": "task460-995bbab9d9634d94824a51564e449c13",
            "input": "For instance, the sentence “The girls sang a song and they danced” is translated into French as “Les filles ont chanté une chanson et ils ont dansé” by Google Translate (GT), Bing Translate, and Yandex. \n Question: Do the authors conduct experiments on the tasks mentioned?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-a54476939cd44ca88bdc0c31e8717ce1",
            "input": "We decided to evaluate our model using weighted F1-score, i.e., the per-class F1 score is calculated and averaged by weighting each label by its support.  \n Question: what evaluation metrics were used?",
            "output": [
                "weighted F1-score"
            ]
        },
        {
            "id": "task460-73551932657148469343d6ed6ab35a18",
            "input": "In this paper we present our work, which is a part of the SociaL Media And Harassment Competition of the ECML PKDD 2019 Conference.   We are using the dataset of the competition, which includes text from tweets having the aforementioned categories. \n Question: What were the datasets used in this paper?",
            "output": [
                "The dataset from the SociaL Media And Harassment Competition of the ECML PKDD 2019 Conference. "
            ]
        },
        {
            "id": "task460-1e5db6f5af3d46be9b49ed8c2cd67893",
            "input": "Past research took a reductionist approach, separately considering these two problems of “what” and “how” via content selection and question construction.  In contrast, neural models motivate an end-to-end architectures. Deep learned frameworks contrast with the reductionist approach, admitting approaches that jointly optimize for both the “what” and “how” in an unified framework.  \n Question: What learning paradigms do they cover in this survey?",
            "output": [
                "Considering \"What\" and \"How\" separately versus jointly optimizing for both."
            ]
        },
        {
            "id": "task460-93b63cb1c21a4fb0b794ee177f816a29",
            "input": "We proposed joint learning NLP models that use convolutional neural network (CNN) BIBREF8 and bi-directional long short-term memory (BiLSTM) BIBREF9, BIBREF10 as basic units. Our models can automatically extract the key elements from the sexual harassment stories and at the same time categorize the stories in different dimensions. The proposed models outperformed the single task models, and achieved higher than previously reported accuracy in classifications of harassment forms BIBREF6. \n Question: What model did they use?",
            "output": [
                "joint learning NLP models that use convolutional neural network (CNN) BIBREF8 and bi-directional long short-term memory (BiLSTM)"
            ]
        },
        {
            "id": "task460-655e6b10664e4dbeb67df05e4eb0c96f",
            "input": "All data were downloaded from Twitter in two separate batches using the “twint\" scraping tool BIBREF5 to sample historical tweets for several different search terms; queries always included either “climate change\" or “global warming\", and further included disaster-specific search terms (e.g., “bomb cyclone,\" “blizzard,\" “snowstorm,\" etc.).  \n Question: Do they report results only on English data?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-875d6f9633394255ab359720f86f6421",
            "input": "Evaluation metric. Since our problem is imbalanced, we use the F1 score as our evaluation metric. For the tagging approach, we average the labels of words with the same stemmed version to obtain a single prediction for the stemmed word. To establish a baseline, we consider a random method that predicts the positive label with 0.15 probability (the base rate of positive instances). \n Question: What metrics are used in evaluation of this task?",
            "output": [
                "F1 score"
            ]
        },
        {
            "id": "task460-8a6b4ca157cf47bdb8a2e649cd0627af",
            "input": "Task processing: converting data exploration tasks to algebraic operations on the embedding space by following task-specific conversion templates. Some important tasks and their conversion templates are discussed in Section SECREF5.\n\nQuery processing: executing semantic query on the embedding space and return results. Note that the algebraic operations on embedding vectors are linear and can be performed in parallel. Therefore, the semantic query is efficient. \n Question: What data exploration is supported by the analysis of these semantic structures?",
            "output": [
                "Task processing: converting data exploration tasks to algebraic operations on the embedding space Query processing: executing semantic query on the embedding space and return results"
            ]
        },
        {
            "id": "task460-9a49cb26f059487d965934fcee7accbc",
            "input": "As of June 2019, AA had $\\sim $50K entries, however, this includes some number of entries that are not truly research publications (for example, forewords, prefaces, table of contents, programs, schedules, indexes, calls for papers/participation, lists of reviewers, lists of tutorial abstracts, invited talks, appendices, session information, obituaries, book reviews, newsletters, lists of proceedings, lifetime achievement awards, erratum, and notes). We discard them for the analyses here. (Note: CL journal includes position papers like squibs, letter to editor, opinion, etc. We do not discard them.) We are then left with 44,896 articles. \n Question: How many papers are used in experiment?",
            "output": [
                "44,896 articles"
            ]
        },
        {
            "id": "task460-15a797fca5774c1eafe27484f71e354d",
            "input": "Our dataset covers 218 videos from NALCS and 103 from LMS for a total of 321 videos from week 1 to week 9 in 2017 spring series from each tournament.  \n Question: How big was the dataset presented?",
            "output": [
                "321 videos"
            ]
        },
        {
            "id": "task460-3004e73a44de429589d18c0e4cb773e8",
            "input": " The unlabeled set of development data was used in the training of both the UBM and the i-vector extractor. \n Question: Do they single out a validation set from the fixed SRE training set?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-02340d3269b64f50893ba222df7a0567",
            "input": "Pretraining. We collect three years of online news articles from June 2016 to June 2019. We filter out articles overlapping with the evaluation data on media domain and time range. We then conduct several data cleaning strategies. \n Question: What did they pretrain the model on?",
            "output": [
                "hree years of online news articles from June 2016 to June 2019"
            ]
        },
        {
            "id": "task460-ba1be22066d04f9f90c4be5a097dca24",
            "input": "Suppose we are given a specific language model such as GPT-2 BIBREF6, GROVER BIBREF8, or CTRL BIBREF7, and it is characterized in terms of estimates of either cross-entropy $H(P,Q)$ or perplexity $\\mathrm {PPL}(P,Q)$.\n\nWe can see directly that the Neyman-Pearson error of detection in the case of i.i.d. tokens is:\n\nand similar results hold for ergodic observations.\n\nSince we think of $H(P)$ as a constant, we observe that the error exponent for the decision problem is precisely an affine shift of the cross-entropy. Outputs from models that are better in the sense of cross-entropy or perplexity are harder to distinguish from authentic text.\n\nThus we see that intuitive measures of generative text quality match a formal operational measure of indistinguishability that comes from the hypothesis testing limit. \n Question: Which language models generate text that can be easier to classify as genuine or generated?",
            "output": [
                "Outputs from models that are better in the sense of cross-entropy or perplexity are harder to distinguish from authentic text."
            ]
        },
        {
            "id": "task460-1f361a4072564572b66f41100c13f561",
            "input": "Our semi-supervised approach is quite straightforward: first a model is trained on the training set and then this model is used to predict the labels of the silver data. This silver data is then simply added to our training set, after which the model is retrained. However, an extra step is applied to ensure that the silver data is of reasonable quality. \n Question: What semi-supervised learning is applied?",
            "output": [
                "first a model is trained on the training set and then this model is used to predict the labels of the silver data This silver data is then simply added to our training set, after which the model is retrained"
            ]
        },
        {
            "id": "task460-97a99e59df084b5eaac12f61d5f9d974",
            "input": "RC-QED$^{\\rm E}$ can be naturally solved by path ranking-based KGC (PRKGC), where the query triplet and the sampled paths correspond to a question and derivation steps, respectively. \n Question: What is the baseline?",
            "output": [
                " path ranking-based KGC (PRKGC)"
            ]
        },
        {
            "id": "task460-f8b742f384ff48adba3beb3389b9d4e9",
            "input": "Recently, Wen et al. wensclstm15 proposed an RNN-based approach, which outperformed previous methods on several metrics. However, the generated sentences often did not include all desired attributes. \n Question: How is some information lost in the RNN-based generation models?",
            "output": [
                "the generated sentences often did not include all desired attributes."
            ]
        },
        {
            "id": "task460-0175fd1633f948ceb0abc9e9eeca4d7e",
            "input": "The aim of this study is to provide empirical evidence indicating that leveraging open data sources of german texts, automated political bias prediction is possible with above chance accuracy. \n Question: Which countries and languages do the political speeches and manifestos come from?",
            "output": [
                "german "
            ]
        },
        {
            "id": "task460-d1b1497f22ba48ec9c68c4b957a2ae7e",
            "input": " To establish a baseline, we consider a random method that predicts the positive label with 0.15 probability (the base rate of positive instances). To examine the utility of our features in a neural framework, we further adapt our word-level task as a tagging task, and use LSTM as a baseline. Specifically, we concatenate an OP and PC with a special token as the separator so that an LSTM model can potentially distinguish the OP from PC, and then tag each word based on the label of its stemmed version. We use GloVe embeddings to initialize the word embeddings BIBREF40. We concatenate our proposed features of the corresponding stemmed word to the word embedding; the resulting difference in performance between a vanilla LSTM demonstrates the utility of our proposed features. \n Question: What is the baseline?",
            "output": [
                "random method  LSTM "
            ]
        },
        {
            "id": "task460-08e23acbc4974c1183939a7326488a64",
            "input": "Each one of these tokenizers, we combine with some types of representation methods, including word to vector methods such as continuous bag of words BIBREF5, pre-trained embedding as fasttext (trained on Wiki Vietnamese language) BIBREF6 and sonvx (trained on Vietnamese newspaper) BIBREF7 The dataset in this HSD task is really imbalance. Clean class dominates with 91.5%, offensive class takes 5% and the rest belongs to hate class with 3.5%. \n Question: What dataset do they use?",
            "output": [
                "They used Wiki Vietnamese language and Vietnamese newspapers to pretrain embeddings and dataset provided in HSD task to train model (details not mentioned in paper)."
            ]
        },
        {
            "id": "task460-37605643552d43d4a7ea2195ba345a99",
            "input": "ACL Anthology Reference Corpus contains the canonical 10,921 computational linguistics papers, from which I have generated 622,144 sentences after filtering out sentences with lower quality. \n Question: What dataset is used?",
            "output": [
                "ACL Anthology Reference Corpus"
            ]
        },
        {
            "id": "task460-43855c062220406cadf766a51c836fac",
            "input": "We can identify three main cases for which the model produces an error:\n\nthe correct class can be directly inferred from the text content easily, even without background knowledge\n\nthe correct class can be inferred from the text content, given that event-specific knowledge is provided\n\nthe correct class can be inferred from the text content if the text is interpreted correctly \n Question: What type of errors do the classifiers use?",
            "output": [
                "correct class can be directly inferred from the text content easily, even without background knowledge correct class can be inferred from the text content, given that event-specific knowledge is provided orrect class can be inferred from the text content if the text is interpreted correctly"
            ]
        },
        {
            "id": "task460-42237e8b8e6d4b31b9adf804744e1585",
            "input": "One would expect that training with more data would improve the quality of the embeddings, but we found out with the results obtained with the C3 dataset, that only high-quality data helps. \n Question: What turn out to be more important high volume or high quality data?",
            "output": [
                "only high-quality data helps"
            ]
        },
        {
            "id": "task460-4822a9ce6abf4d728d1fb2be4817e103",
            "input": "Approach ::: Method: Training and Testing ::: Experiment 1: Representation\nSome of the problems encountered by prior approaches seem to be attributable to the use of infix notation. In this experiment, we compare translation BLEU-2 scores to spot the differences in representation interpretability. Approach ::: Method: Training and Testing ::: Experiment 2: State-of-the-art\nThis experiment compares our networks to recent previous work. We count a given test score by a simple “correct versus incorrect\" method. The answer to an expression directly ties to all of the translation terms being correct, which is why we do not consider partial precision. We compare average accuracies over 3 test trials on different randomly sampled test sets from each MWP dataset. \n Question: How is this problem evaluated?",
            "output": [
                "BLEU-2 average accuracies over 3 test trials on different randomly sampled test sets"
            ]
        },
        {
            "id": "task460-46c8bc1e59b5485a9435ad9a18491f47",
            "input": "Controls are calculated heuristically. All words found in the control word lists are then removed from the reference sentence. The remaining words, which represent the content, are used as input into the model, along with their POS tags and lemmas.\n\nIn this way we encourage models to construct a sentence using content and style independently. \n Question: How they know what are content words?",
            "output": [
                " words found in the control word lists are then removed The remaining words, which represent the content"
            ]
        },
        {
            "id": "task460-a062ab4dd31e4953840be4b7ba64458f",
            "input": "The Gallup survey asked for opinions on legality of “homosexual relations\" until 2008, but then changed the wording to “gay and lesbian relations\". This was likely because many people who identify as gay and lesbian find the word homosexual to be outdated and derogatory. \n Question: Do they analyze specific derogatory words?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-842ac5bbadcd436dab785f2dbced3cdd",
            "input": "Our training data consists of 2.09M sentence pairs extracted from LDC corpus. To test different approaches on Chinese-to-English translation task, we use NIST 2003(MT03) dataset as the validation set, and NIST 2004(MT04), NIST 2005(MT05), NIST 2006(MT06) datasets as our test sets. For English-to-Chinese translation task, we also use NIST 2003(MT03) dataset as the validation set, and NIST 2008(MT08) will be used as test set. \n Question: What dataset did they use?",
            "output": [
                "LDC corpus NIST 2003(MT03) NIST 2004(MT04), NIST 2005(MT05), NIST 2006(MT06) NIST 2008(MT08)"
            ]
        },
        {
            "id": "task460-eb668a3ceb964b0ca6cff1c57167f2e7",
            "input": "Ternary and fine-grained sentiment classification were part of the SemEval-2016 “Sentiment Analysis in Twitter” task BIBREF16 . We use the high-quality datasets the challenge organizers released. \n Question: What dataset did they use?",
            "output": [
                " high-quality datasets  from SemEval-2016 “Sentiment Analysis in Twitter” task"
            ]
        },
        {
            "id": "task460-b7fa24ab7e10433189b2969136e86a32",
            "input": "Spoken-SQuAD is chosen as the target domain data for training and testing. Spoken-SQuAD BIBREF5 is an automatically generated corpus in which the document is in spoken form and the question is in text form. The reference transcriptions are from SQuAD BIBREF1 . There are 37,111 and 5,351 question answer pairs in the training and testing sets respectively, and the word error rate (WER) of both sets is around 22.7%.\n\nThe original SQuAD, Text-SQuAD, is chosen as the source domain data, where only question answering pairs appearing in Spoken-SQuAD are utilized. In our task setting, during training we train the proposed QA model on both Text-SQuAD and Spoken-SQuAD training sets. While in the testing stage, we evaluate the performance on Spoken-SQuAD testing set. \n Question: Which datasets did they use for evaluation?",
            "output": [
                "Spoken-SQuAD testing set"
            ]
        },
        {
            "id": "task460-d719eb2353f049bea59c12a03d55e137",
            "input": "We make copies of the monolingual model for each language and add additional crosslingual latent variables (CLVs) to couple the monolingual models, capturing crosslingual semantic role patterns. Concretely, when training on parallel sentences, whenever the head words of the arguments are aligned, we add a CLV as a parent of the two corresponding role variables.  \n Question: Which additional latent variables are used in the model?",
            "output": [
                "CLV as a parent of the two corresponding role variables"
            ]
        },
        {
            "id": "task460-780e63e0770e42a9a0a51c6dc1777955",
            "input": "We have evaluated the output of Nefnir against a reference corpus of 21,093 tokens and their correct lemmas.\n\nSamples for the reference corpus were extracted from two larger corpora, in order to obtain a diverse vocabulary: \n Question: Which dataset do they use?",
            "output": [
                "a reference corpus of 21,093 tokens and their correct lemmas"
            ]
        },
        {
            "id": "task460-f850b7c5bc1648b8a74b5867c5e731fd",
            "input": "We evaluate our method in translating English into three morphologically-rich languages each with a distinct morphological typology: Arabic, Czech and Turkish, and show that our model is able to obtain better translation accuracy and generalization capacity than conventional approaches to open-vocabulary NMT. \n Question: What are the three languages studied in the paper?",
            "output": [
                "Arabic, Czech and Turkish"
            ]
        },
        {
            "id": "task460-be58949c1f994fc58a25afce6a06ad6c",
            "input": "We use the publicly available dataset KVRET BIBREF5 in our experiments. This dataset includes dialogues in 3 domains: calendar, weather, navigation (POI) which is suitable for our mix-domain dialogue experiments.  \n Question: What multi-domain dataset is used?",
            "output": [
                "KVRET"
            ]
        },
        {
            "id": "task460-adf4402b0a694a009bf880ca25442f8f",
            "input": "Hence, the continuous relaxation to top-k-argmax operation can be simply implemented by iteratively using the max operation which is continuous and allows for gradient flow during backpropagation. \n Question: Which loss metrics do they try in their new training procedure evaluated on the output of beam search?",
            "output": [
                " continuous relaxation to top-k-argmax"
            ]
        },
        {
            "id": "task460-9055386165fd4d6a86ad7a326fa5120c",
            "input": "The average content score across the paintings is 3.7 which demonstrates that the prose generated is relevant to the painting. The average creativity score is 3.9 which demonstrates that the model captures more than basic objects in the painting successfully using poetic clues in the scene. The average style score is 3.9 which demonstrates that the prose generated is perceived to be in the style of Shakespeare. \n Question: How does final model rate on Likert scale?",
            "output": [
                "average content score across the paintings is 3.7 average creativity score is 3.9 average style score is 3.9 "
            ]
        },
        {
            "id": "task460-8706d95e7c544a2289d8a3279bb62d7b",
            "input": "In order to derive the discourse features, an entity grid is constructed by feeding the document through an NLP pipeline to identify salient entities. Two flavors of discourse features are created by populating the entity grid with either (i) grammatical relations (GR) or (ii) RST discourse relations (RST).  Denoting a feature as such with INLINEFORM4 , we construct the pooling vector INLINEFORM5 for the char-bigrams, and concatenate INLINEFORM6 to INLINEFORM7 before feeding the resulting vector to the softmax layer. \n Question: How are discourse features incorporated into the model?",
            "output": [
                "They derive entity grid with grammatical relations and RST discourse relations and concatenate them with pooling vector for the char-bigrams before feeding to the resulting vector to the softmax layer."
            ]
        },
        {
            "id": "task460-b2ad8a05cc6b441bbc6a31c175dd4a3f",
            "input": "String kernels represent a way of using information at the character level by measuring the similarity of strings through character n-grams. \n Question: What is a string kernel?",
            "output": [
                "String kernel is a technique that uses character n-grams to measure the similarity of strings"
            ]
        },
        {
            "id": "task460-6364f56ba8d642828784396f9023f644",
            "input": "Parallel Scan Inference\nThe commutative properties of semiring algorithms allow flexibility in the order in which we compute $A(\\ell )$. Typical implementations of dynamic programming algorithms are serial in the length of the sequence. Vectorized Parsing\nComputational complexity is even more of an issue for parsing algorithms, which cannot be as easily parallelized. Semiring Matrix Operations\nThe two previous optimizations reduce most of the cost to semiring matrix multiplication. In the specific case of the $(\\sum , \\times )$ semiring these can be computed very efficiently using matrix multiplication, which is highly-tuned on GPU hardware. Unfortunately for other semirings, such as log and max, these operations are either slow or very memory inefficient. For instance, for matrices $T$ and $U$ of sized $N \\times M$ and $M \\times O$, we can broadcast with $\\otimes $ to a tensor of size $N \\times M \\times O$ and then reduce dim $M$ by $\\bigoplus $ at a huge memory cost. \n Question: What baselines are used in experiments?",
            "output": [
                "Typical implementations of dynamic programming algorithms are serial in the length of the sequence Computational complexity is even more of an issue for parsing algorithms, which cannot be as easily parallelized Unfortunately for other semirings, such as log and max, these operations are either slow or very memory inefficient"
            ]
        },
        {
            "id": "task460-126735008f5a47b4b5272ebf8b36c4c5",
            "input": "Another widely used method for classifying misleading content is to use distant annotations, for example to classify a tweet based on the domain of a URL that is shared by the tweet, or a hashtag that is contained in the tweet BIBREF6, BIBREF7, BIBREF8. In order to scale annotations that go beyond metadata to larger datasets, Natural Language Processing (NLP) models can be used to automatically label text content. \n Question: What proxies for data annotation were used in previous datasets?",
            "output": [
                "widely used method for classifying misleading content is to use distant annotations, for example to classify a tweet based on the domain of a URL that is shared by the tweet, or a hashtag that is contained in the tweet Natural Language Processing (NLP) models can be used to automatically label text content"
            ]
        },
        {
            "id": "task460-c6661a52ac1b4dfab9c991d67f99ebaf",
            "input": "Three incremental levels of document preprocessing are experimented with: raw text, text cleaning through document logical structure detection, and removal of keyphrase sparse sections of the document. In doing so, we present the first consistent comparison of different keyphrase extraction models and study their robustness over noisy text. \n Question: what levels of document preprocessing are looked at?",
            "output": [
                "raw text text cleaning through document logical structure detection removal of keyphrase sparse sections of the document"
            ]
        },
        {
            "id": "task460-92c5e9c3901142eea0a43bd7a046a29b",
            "input": "For all tasks, we use a TensorFlow implementation. AraNet predicts age, dialect, gender, emotion, irony, and sentiment from social media posts. It delivers state-of-the-art and competitive performance on these tasks and has the advantage of using a unified, simple framework based on the recently-developed BERT model.  \n Question: Did they experiment on all the tasks?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-a717a78d339c477d85ff5b1f4c5c5304",
            "input": "We extract 5 Surface and Lexical features, namely sequence length in number of tokens, average word length, type-token ratio, and lexical to tokens ratio (ratio of adjectives, verbs, nouns, and adverbs to tokens). We extracted two features that use a learned representation: Firstly, we get a sentence embedding feature that is built by averaging the word embeddings of an input sentence. Secondly, we extract a fastText representation using the fastText library with the same parameters as reported in Joulin et al. joulin2016bag. \n Question: Do they differentiate insights where they are dealing with learned or engineered representations?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-5c1260e14161461989296b8fedc3aea0",
            "input": "The systems were trained on all parallel data available for the WMT 2016. The news commentary corpus, the European parliament proceedings and the common crawl corpus sum up to 3.7M sentences and around 90M words. \n Question: Which dataset do they use?",
            "output": [
                "parallel data available for the WMT 2016"
            ]
        },
        {
            "id": "task460-555f1f05611f41b2a8620ce087bcc296",
            "input": "The attention function BIBREF11 is used to compute the similarity score between passages and questions as: INLINEFORM2 \n Question: Do they use attention?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-f2089911affb408990c06f6c6bbd3f5c",
            "input": "Adding word alignments in parallel sentences results in small, non significant improvements, even if there is some labeled data available in the source language. This difficulty in showing the usefulness of parallel corpora for SRI may be due to the current assumptions about role alignments, which mean that only a small percentage of roles are aligned. Further analyses reveals that annotating small amounts of data can easily outperform the performance gains obtained by adding large unlabeled dataset as well as adding parallel corpora. \n Question: Overall, does having parallel data improve semantic role induction across multiple languages?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-3c30a1c96db04a8a8fefecce7ba642d7",
            "input": " We performed the annotation with freely available tools for the Portuguese language. \n Question: Are the annotations automatic or manually created?",
            "output": [
                "Automatic"
            ]
        },
        {
            "id": "task460-93893f51c9774a6bb401450cb1c2a9a0",
            "input": "The MCD splits achieve a significantly higher compound divergence at a similar atom divergence when compared to the other experiments.  \n Question: What are results of comparison between novel method to other approaches for creating compositional generalization benchmarks?",
            "output": [
                "The MCD splits achieve a significantly higher compound divergence at a similar atom divergence when compared to the other experiments"
            ]
        },
        {
            "id": "task460-77173d8079c540cabcac3244066fb433",
            "input": "We train our models on Sentiment140 and Amazon product reviews. Both of these datasets concentrates on sentiment represented by a short text.  For training the softmax model, we divide the text sentiment to two kinds of emotion, positive and negative. And for training the tanh model, we convert the positive and negative emotion to [-1.0, 1.0] continuous sentiment score, while 1.0 means positive and vice versa.  \n Question: Was the introduced LSTM+CNN model trained on annotated data in a supervised fashion?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-fcf4f37be3544a23b9dca2384cfcfaab",
            "input": "We extracted Japanese word pairs from the Evaluation Dataset of Japanese Lexical Simplification kodaira.  \n Question: where does the data come from?",
            "output": [
                "Evaluation Dataset of Japanese Lexical Simplification kodaira"
            ]
        },
        {
            "id": "task460-ee45d9736b2c4f9cb75fbc4a44df7ae9",
            "input": "The main contribution of our work is thus a new parallel data-to-text NLG corpus that (1) is more conversational, rather than information seeking or question answering, and thus more suitable for an open-domain dialogue system, (2) represents a new, unexplored domain which, however, has excellent potential for application in conversational agents, and (3) has high-quality, manually cleaned human-produced utterances. \n Question: How the authors made sure that corpus is clean despite being crowdsourced?",
            "output": [
                "manually cleaned human-produced utterances"
            ]
        },
        {
            "id": "task460-08c7a15917924beb82eb5ff2340f9ef6",
            "input": "They used 20-million-words data randomly sampled from the raw text released by the CoNLL 2017 Shared Task - Automatically Annotated Raw Texts and Word Embeddings BIBREF8, which is a combination of Wikipedia dump and common crawl.  For example, we compared the Latvian model by ELMoForManyLangs with a model we trained on a complete (wikidump + common crawl) Latvian corpus, which has about 280 million tokens. \n Question: How larger are the training sets of these versions of ELMo compared to the previous ones?",
            "output": [
                "By 14 times."
            ]
        },
        {
            "id": "task460-07ccd3edb9074729b2004259986e61c9",
            "input": "We compare our approaches with related approaches of pivoting, multilingual NMT (MNMT) BIBREF19, and cross-lingual transfer without pretraining BIBREF16. Although it is challenging for one model to translate all zero-shot directions between multiple distant language pairs of MultiUN, MLM+BRLM-SA still achieves better performances on Es $\\rightarrow $ Ar and Es $\\rightarrow $ Ru than strong pivoting$_{\\rm m}$, which uses MNMT to translate source to pivot then to target in two separate steps with each step receiving supervised signal of parallel corpora.  \n Question: what are the pivot-based baselines?",
            "output": [
                "pivoting pivoting$_{\\rm m}$"
            ]
        },
        {
            "id": "task460-a9841108715b4ff48b4b0a0dd78bd260",
            "input": "The participants are asked to score each summary on three indicators: relevance, conciseness and readability on a 1 to 5 scale, and rank the summary pair (tie allowed). We observe that SPNet reaches the highest score in both ROUGE and CIC \n Question: What automatic and human evaluation metrics are used to compare SPNet to its counterparts?",
            "output": [
                "ROUGE and CIC relevance, conciseness and readability on a 1 to 5 scale, and rank the summary pair"
            ]
        },
        {
            "id": "task460-6517bcbfa8754cbc90c6124c0fe1783b",
            "input": "In this part, we discuss three significant limitations of BLEU and ROUGE. These metrics can assign: High scores to semantically opposite translations/summaries, Low scores to semantically related translations/summaries and High scores to unintelligible translations/summaries.\n\nChallenges with BLEU and ROUGE ::: High score, opposite meanings\nSuppose that we have a reference summary s1. By adding a few negation terms to s1, one can create a summary s2 which is semantically opposite to s1 but yet has a high BLEU/ROUGE score.\n\nChallenges with BLEU and ROUGE ::: Low score, similar meanings\nIn addition not to be sensitive to negation, BLEU and ROUGE score can give low scores to sentences with equivalent meaning. If s2 is a paraphrase of s1, the meaning will be the same ;however, the overlap between words in s1 and s2 will not necessarily be significant.\n\nChallenges with BLEU and ROUGE ::: High score, unintelligible sentences\nA third weakness of BLEU and ROUGE is that in their simplest implementations, they are insensitive to word permutation and can give very high scores to unintelligible sentences. Let s1 be \"On a morning, I saw a man running in the street.\" and s2 be “On morning a, I saw the running a man street”. s2 is not an intelligible sentence. The unigram version of ROUGE and BLEU will give these 2 sentences a score of 1. \n Question: What are the three limitations?",
            "output": [
                "High scores to semantically opposite translations/summaries, Low scores to semantically related translations/summaries and High scores to unintelligible translations/summaries."
            ]
        },
        {
            "id": "task460-2883455a09244a79a91ecad93db9ebc9",
            "input": "Macaw also supports Wizard of Oz studies or intermediary-based information seeking studies. The architecture of Macaw for such setup is presented in FIGREF16. As shown in the figure, the seeker interacts with a real conversational interface that supports multi-modal and mixed-initiative interactions in multiple devices. The intermediary (or the wizard) receives the seeker's message and performs different information seeking actions with Macaw. All seeker-intermediary and intermediary-system interactions will be logged for further analysis. \n Question: What is a wizard of oz setup?",
            "output": [
                "seeker interacts with a real conversational interface intermediary (or the wizard) receives the seeker's message and performs different information seeking actions"
            ]
        },
        {
            "id": "task460-61a61fbd023d405ba7eace1470efd554",
            "input": "An emerging body of work in natural language processing and computational social science has investigated how NLP systems can detect moral sentiment in online text. For example, moral rhetoric in social media and political discourse BIBREF19, BIBREF20, BIBREF21, the relation between moralization in social media and violent protests BIBREF22, and bias toward refugees in talk radio shows BIBREF23 have been some of the topics explored in this line of inquiry. In contrast to this line of research, the development of a formal framework for moral sentiment change is still under-explored, with no existing systematic and formal treatment of this topic BIBREF16. \n Question: Does the paper discuss previous models which have been applied to the same task?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-78824e093eb04e6880a02edc78c0e97a",
            "input": "Following work by doukhan2018open, we wanted to explore the corpora looking at the number of speakers of each gender category as well as their speech duration, considering both variables as good features to account for gender representation.  \n Question: What representations are presented by this paper?",
            "output": [
                "the number of speakers of each gender category their speech duration"
            ]
        },
        {
            "id": "task460-fb53df88790e4a3a90d0448e21eece52",
            "input": "To evaluate model robustness, we devise a test set consisting of ‘adversarial’ examples, i.e, perturbed examples that can potentially change the base model's prediction. These could stem from paraphrasing a sentence, e.g., lexical and syntactical changes. We use two approaches described in literature: back-translation and noisy sequence autoencoder. Note that these examples resemble black-box attacks but are not intentionally designed to fool the system and hence, we use the term 'adversarial' broadly. We use these techniques to produce many paraphrases and find a subset of utterances that though very similar to the original test set, result in wrong predictions. We will measure the model robustness against such changes. \n Question: How authors create adversarial test set to measure model robustness?",
            "output": [
                "we devise a test set consisting of ‘adversarial’ examples, i.e, perturbed examples that can potentially change the base model's prediction.  We use two approaches described in literature: back-translation and noisy sequence autoencoder."
            ]
        },
        {
            "id": "task460-e3c168480d714d5692541c578b43bef1",
            "input": "Methods ::: Baselines ::: BiGRU s with attention:\nThis is very similar to Sum-QE but now $\\mathcal {E}$ is a stack of BiGRU s with self-attention BIBREF21, instead of a BERT instance. The final summary representation ($h$) is the sum of the resulting context-aware token embeddings ($h = \\sum _i a_i h_i$) weighted by their self-attention scores ($a_i$). We again have three flavors: one single-task (BiGRU-ATT-S-1) and two multi-task (BiGRU-ATT-M-1 and BiGRU-ATT-M-5).\n\nMethods ::: Baselines ::: ROUGE:\nThis baseline is the ROUGE version that performs best on each dataset, among the versions considered by BIBREF13. Although ROUGE focuses on surface similarities between peer and reference summaries, we would expect properties like grammaticality, referential clarity and coherence to be captured to some extent by ROUGE versions based on long $n$-grams or longest common subsequences.\n\nMethods ::: Baselines ::: Language model (LM):\nFor a peer summary, a reasonable estimate of $\\mathcal {Q}1$ (Grammaticality) is the perplexity returned by a pre-trained language model. We experiment with the pre-trained GPT-2 model BIBREF22, and with the probability estimates that BERT can produce for each token when the token is treated as masked (BERT-FR-LM). Given that the grammaticality of a summary can be corrupted by just a few bad tokens, we compute the perplexity by considering only the $k$ worst (lowest LM probability) tokens of the peer summary, where $k$ is a tuned hyper-parameter.\n\nMethods ::: Baselines ::: Next sentence prediction:\nBERT training relies on two tasks: predicting masked tokens and next sentence prediction. The latter seems to be aligned with the definitions of $\\mathcal {Q}3$ (Referential Clarity), $\\mathcal {Q}4$ (Focus) and $\\mathcal {Q}5$ (Structure & Coherence). Intuitively, when a sentence follows another with high probability, it should involve clear referential expressions and preserve the focus and local coherence of the text. We, therefore, use a pre-trained BERT model (BERT-FR-NS) to calculate the sentence-level perplexity of each summary:\n\nwhere $p(s_i|s_{i-1})$ is the probability that BERT assigns to the sequence of sentences $\\left< s_{i-1}, s \\right>$, and $n$ is the number of sentences in the peer summary. \n Question: What simpler models do they look at?",
            "output": [
                "BiGRU s with attention ROUGE Language model (LM) Next sentence prediction"
            ]
        },
        {
            "id": "task460-fd4446c2936648e3ab36c6bdc708208b",
            "input": "For experiments that use parallel data to initialize foreign specific parameters, we use the same datasets in the work of BIBREF6. Specifically, we use United Nations Parallel Corpus BIBREF18 for en-ru, en-ar, en-zh, and en-fr. We collect en-hi parallel data from IIT Bombay corpus BIBREF19 and en-vi data from OpenSubtitles 2018. \n Question: What datasets are used for evaluation?",
            "output": [
                "United Nations Parallel Corpus IIT Bombay corpus OpenSubtitles 2018"
            ]
        },
        {
            "id": "task460-47aed29ab3024cc693e617dd86706224",
            "input": "In the speaker-closed condition, two episodes were set aside from each speaker as development and test sets. Thereafter, the total sizes of the development and test sets turns out to be 1585 IPUs spanning 2 hours 23 minutes and 1841 IPUs spanning 2 hours and 48 minutes respectively. The ASR model is trained with the rest data. In the speaker-open condition, all the data except for the test speaker's were used for training As it would be difficult to train the model if all of the data of speaker KM or UT were removed, experiments using their speaker-open conditions were not conducted. \n Question: What is the difference between speaker-open and speaker-closed setting?",
            "output": [
                "In the speaker-closed condition, two episodes were set aside from each speaker as development and test sets. In the speaker-open condition, all the data except for the test speaker's were used for training"
            ]
        },
        {
            "id": "task460-cb730d73ce174d8d9f93c7861cc87565",
            "input": "Our model differs by learning the subword vectors and resulting representation jointly as weighted factorization of a word-context co-occurrence matrix is performed. \n Question: Which matrix factorization methods do they use?",
            "output": [
                "weighted factorization of a word-context co-occurrence matrix "
            ]
        },
        {
            "id": "task460-98069a2731c64c769beff784eb1d50c6",
            "input": "Figure FIGREF9 illustrates the overall architecture of the proposed Attentional Encoder Network (AEN), which mainly consists of an embedding layer, an attentional encoder layer, a target-specific attention layer, and an output layer. Embedding layer has two types: GloVe embedding and BERT embedding. Accordingly, the models are named AEN-GloVe and AEN-BERT. \n Question: How is their model different from BERT?",
            "output": [
                "overall architecture of the proposed Attentional Encoder Network (AEN), which mainly consists of an embedding layer, an attentional encoder layer, a target-specific attention layer, and an output layer."
            ]
        },
        {
            "id": "task460-37380e16cd524d93a04e75b4647c8a3a",
            "input": "We draw on a recently released corpus of state speeches delivered during the annual UN General Debate that provides the first dataset of textual output from states that is recorded at regular time-series intervals and includes a sample of all countries that deliver speeches BIBREF11 .  \n Question: Which dataset do they use?",
            "output": [
                "corpus of state speeches delivered during the annual UN General Debate"
            ]
        },
        {
            "id": "task460-b8cc11e274514fb883395255abcf6124",
            "input": "All documents are segmented into paragraphs and processed at the paragraph level (both training and inference); this is acceptable because we observe that most paragraphs are less than 200 characters. The input sequences are segmented by the BERT tokenizer, with the special [CLS] token inserted at the beginning and the special [SEP] token added at the end. \n Question: Was the structure of regulatory filings exploited when training the model? ",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-bbee57ff63894c8da700dfad6fe24895",
            "input": "Particularly, it is not clear whether BERT shed light on solving tasks such as the Pronoun Disambiguation Problem (PDP) and Winograd Schema Challenge (WSC). On a Pronoun Disambiguation dataset, PDP-60, our method achieves 68.3% accuracy, which is better than the state-of-art accuracy of 66.7%. On a WSC dataset, WSC-273, our method achieves 60.3%.  \n Question: Which datasets do they evaluate on?",
            "output": [
                "PDP-60 WSC-273"
            ]
        },
        {
            "id": "task460-bf3233ccb0d844be8d21a9ed1f3e0cf6",
            "input": "This step was followed by a pre-processing stage where the texts were normalized by replacing zero-width-non-joiner (ZWNJ) BIBREF2 and manually verifying the orthography based on the reference orthography of the Kurdistan Region of Iraq. \n Question: How is the corpus normalized?",
            "output": [
                "by replacing zero-width-non-joiner (ZWNJ) BIBREF2 and manually verifying the orthography"
            ]
        },
        {
            "id": "task460-0054305244544e2b83ba9be7b6a74ae4",
            "input": "We use the text corpora (S) from BIBREF6 aristo2016:combining to build our tuple KB.  Specifically, for each multiple-choice question $(q,A) \\in Q_\\mathit {tr}$ and each choice $a \\in A$ , we use all non-stopword tokens in $q$ and $a$ as an ElasticSearch query against S. We take the top 200 hits, run Open IE v4, and aggregate the resulting tuples over all $a \\in A$ and over all questions in $Q_\\mathit {tr}$ to create the tuple KB (T). The Sentence corpus (S) consists of domain-targeted $~$ 80K sentences and 280 GB of plain text extracted from web pages used by BIBREF6 aristo2016:combining.  \n Question: What corpus was the source of the OpenIE extractions?",
            "output": [
                "domain-targeted $~$ 80K sentences and 280 GB of plain text extracted from web pages used by BIBREF6 aristo2016:combining"
            ]
        },
        {
            "id": "task460-3d450b77515c4cdfb2f9fea97970a447",
            "input": "Each training sample is generated by three steps: align, mask and select, which we call as AMS method. Each sample in the dataset consists of a question and several candidate answers, which has the same form as the CommonsenseQA dataset. \n Question: How do they select answer candidates for their QA task?",
            "output": [
                "AMS method."
            ]
        },
        {
            "id": "task460-b4d76f6ea9cf4a4799b8905a8d04d843",
            "input": "The policies trained with the NUS achieved an average success rate (SR) of 94.0% and of 96.6% when tested on the ABUS and the NUS, respectively. \n Question: by how much did nus outperform abus?",
            "output": [
                "Average success rate is higher by 2.6 percent points."
            ]
        },
        {
            "id": "task460-1d823b1e658e499ab3cea5bb88a0555a",
            "input": "Deep learning has unquestionably advanced the state of the art in many natural language processing tasks, from syntactic dependency parsing BIBREF0 to named-entity recognition BIBREF1 to machine translation BIBREF2 . The same certainly applies to language modeling, where recent advances in neural language models (NLMs) have led to dramatically better approaches as measured using standard metrics such as perplexity BIBREF3 , BIBREF4 . \n Question: What is a commonly used evaluation metric for language models?",
            "output": [
                "perplexity"
            ]
        },
        {
            "id": "task460-53649229b360457982594727fd294253",
            "input": "CN-Celeb specially focuses on Chinese celebrities, and contains more than $130,000$ utterances from $1,000$ persons.\n\nCN-Celeb covers more genres of speech. We intentionally collected data from 11 genres, including entertainment, interview, singing, play, movie, vlog, live broadcast, speech, drama, recitation and advertisement. The speech of a particular speaker may be in more than 5 genres. As a comparison, most of the utterances in VoxCeleb were extracted from interview videos. The diversity in genres makes our database more representative for the true scenarios in unconstrained conditions, but also more challenging. \n Question: What kind of settings do the utterances come from?",
            "output": [
                "entertainment, interview, singing, play, movie, vlog, live broadcast, speech, drama, recitation and advertisement"
            ]
        },
        {
            "id": "task460-cb796bfc6a9a465da4a098486520bac7",
            "input": "It begins as ordinary tournament selection evolutionary architecture search with early stopping, with each child model training for a relatively small $s_0$ number of steps before being evaluated for fitness. However, after a predetermined number of child models, $m$ , have been evaluated, a hurdle, $h_0$ , is created by calculating the the mean fitness of the current population. For the next $m$ child models produced, models that achieve a fitness greater than $h_0$ after $s_0$ train steps are granted an additional $s_1$ steps of training and then are evaluated again to determine their final fitness. Once another $m$ models have been considered this way, another hurdle, $h_1$ , is constructed by calculating the mean fitness of all members of the current population that were trained for the maximum number of steps. For the next $m$ child models, training and evaluation continues in the same fashion, except models with fitness greater than $m$0 after $m$1 steps of training are granted an additional $m$2 number of train steps, before being evaluated for their final fitness. This process is repeated until a satisfactory number of maximum training steps is reached. \n Question: How does Progressive Dynamic Hurdles work?",
            "output": [
                "It begins as ordinary tournament selection evolutionary architecture search with early stopping, with each child model training for a relatively small $s_0$ number of steps before being evaluated for fitness. However, after a predetermined number of child models, $m$ , have been evaluated, a hurdle, $h_0$ , is created by calculating the the mean fitness of the current population. For the next $m$ child models produced, models that achieve a fitness greater than $h_0$ after $s_0$ train steps are granted an additional $s_1$ steps of training and then are evaluated again to determine their final fitness. Once another $m$ models have been considered this way, another hurdle, $h_1$ , is constructed by calculating the mean fitness of all members of the current population that were trained for the maximum number of steps. For the next $m$ child models, training and evaluation continues in the same fashion, except models with fitness greater than $m$0 after $m$1 steps of training are granted an additional $m$2 number of train steps, before being evaluated for their final fitness. This process is repeated until a satisfactory number of maximum training steps is reached."
            ]
        },
        {
            "id": "task460-3ec013b4de3c42c686f4a5bf996a1b38",
            "input": "We use two datasets in this work: the training is done on the Fisher Corpus English Part 1 (LDC2004S13) BIBREF15 and testing on the Suicide Risk Assessment corpus BIBREF16 , along with Fisher. \n Question: Which dataset do they use to learn embeddings?",
            "output": [
                "Fisher Corpus English Part 1"
            ]
        },
        {
            "id": "task460-d71c5e93799145ad9f05abcb0c50d71e",
            "input": "Our VG training set consists of 85% of the data: 16k images and 740k corresponding region descriptions. \n Question: How big is data provided by this research?",
            "output": [
                "16k images and 740k corresponding region descriptions"
            ]
        },
        {
            "id": "task460-c2d52ff7398546fe8d993d56ed7f2c12",
            "input": "The top performing approaches Caravel, COAV and NNCD deserve closer attention. \n Question: Which is the best performing method?",
            "output": [
                "Caravel, COAV and NNCD"
            ]
        },
        {
            "id": "task460-963bbaf0a3a94a0caa8cd235cdceb3e5",
            "input": "Despite this, all models in the literature rely on word-level representations, which keeps the models from being able to easily capture some of the lexical and morpho-syntactic cues known to denote irony, such as all caps, quotation marks and emoticons, and in Twitter, also emojis and hashtags. \n Question: Which morphosyntactic features are thought to indicate irony or sarcasm?",
            "output": [
                "all caps quotation marks emoticons emojis hashtags"
            ]
        },
        {
            "id": "task460-4f210b5226b34def8173febcf3c8f914",
            "input": "When predicting anger, joy, or valence, the number of systems consistently giving higher scores to sentences with female noun phrases (21–25) is markedly higher than the number of systems giving higher scores to sentences with male noun phrases (8–13). (Recall that higher valence means more positive sentiment.) In contrast, on the fear task, most submissions tended to assign higher scores to sentences with male noun phrases (23) as compared to the number of systems giving higher scores to sentences with female noun phrases (12). When predicting sadness, the number of submissions that mostly assigned higher scores to sentences with female noun phrases (18) is close to the number of submissions that mostly assigned higher scores to sentences with male noun phrases (16). These results are in line with some common stereotypes, such as females are more emotional, and situations involving male agents are more fearful BIBREF27  The majority of the systems assigned higher scores to sentences with African American names on the tasks of anger, fear, and sadness intensity prediction. On the joy and valence tasks, most submissions tended to assign higher scores to sentences with European American names. These tendencies reflect some common stereotypes that associate African Americans with more negative emotions BIBREF28 . \n Question: Which race and gender are given higher sentiment intensity predictions?",
            "output": [
                "Females are given higher sentiment intensity when predicting anger, joy or valence, but males are given higher sentiment intensity when predicting  fear.\nAfrican American names are given higher score on the tasks of anger, fear, and sadness intensity prediction,  but European American names are given higher scores on joy and valence task."
            ]
        },
        {
            "id": "task460-3cb65c0d6e954516bf21a2fcdb1f1c61",
            "input": "Span detector. We adopt a multi-turn answer module for the span detector BIBREF1 . Formally, at time step INLINEFORM0 in the range of INLINEFORM1 , the state is defined by INLINEFORM2 . The initial state INLINEFORM3 is the summary of the INLINEFORM4 : INLINEFORM5 , where INLINEFORM6 . Here, INLINEFORM7 is computed from the previous state INLINEFORM8 and memory INLINEFORM9 : INLINEFORM10 and INLINEFORM11 . Finally, a bilinear function is used to find the begin and end point of answer spans at each reasoning step INLINEFORM12 : DISPLAYFORM0 DISPLAYFORM1\n\nThe final prediction is the average of each time step: INLINEFORM0 . We randomly apply dropout on the step level in each time step during training, as done in BIBREF1 . \n Question: What is the architecture of the span detector?",
            "output": [
                "adopt a multi-turn answer module for the span detector BIBREF1"
            ]
        },
        {
            "id": "task460-46aeb5b56da34af1b05c20d76f1cdd8f",
            "input": "The CreateDebate dataset was collected from an English online debate forum discussing four topics: abortion (ABO), gay rights (GAY), Obama (OBA), and marijuana (MAR).  \n Question: What topics are included in the debate data?",
            "output": [
                "abortion gay rights Obama marijuana"
            ]
        },
        {
            "id": "task460-cd8ec85216474d4db5802bb83eb8fc87",
            "input": "We build on the state-of-the-art publicly available question answering system by docqa. The system extends BiDAF BIBREF4 with self-attention and performs well on document-level QA.  \n Question: What is the underlying question answering algorithm?",
            "output": [
                "The system extends BiDAF BIBREF4 with self-attention"
            ]
        },
        {
            "id": "task460-89a51a88acd04074acc9062143dc9367",
            "input": "A simple choice of non-sparse gating function BIBREF17 is to multiply the input by a trainable weight matrix INLINEFORM0 and then apply the INLINEFORM1 function. DISPLAYFORM0\n\nWe add two components to the Softmax gating network: sparsity and noise. Before taking the softmax function, we add tunable Gaussian noise, then keep only the top k values, setting the rest to INLINEFORM0 (which causes the corresponding gate values to equal 0). The sparsity serves to save computation, as described above. While this form of sparsity creates some theoretically scary discontinuities in the output of gating function, we have not yet observed this to be a problem in practice. The noise term helps with load balancing, as will be discussed in Appendix SECREF51 . The amount of noise per component is controlled by a second trainable weight matrix INLINEFORM1 . DISPLAYFORM0 DISPLAYFORM1 A simple choice of non-sparse gating function BIBREF17 is to multiply the input by a trainable weight matrix INLINEFORM0 and then apply the INLINEFORM1 function. DISPLAYFORM0\n\nWe add two components to the Softmax gating network: sparsity and noise. Before taking the softmax function, we add tunable Gaussian noise, then keep only the top k values, setting the rest to INLINEFORM0 (which causes the corresponding gate values to equal 0). The sparsity serves to save computation, as described above. While this form of sparsity creates some theoretically scary discontinuities in the output of gating function, we have not yet observed this to be a problem in practice. The noise term helps with load balancing, as will be discussed in Appendix SECREF51 . The amount of noise per component is controlled by a second trainable weight matrix INLINEFORM1 . DISPLAYFORM0 DISPLAYFORM1 \n Question: What equations are used for the trainable gating network?",
            "output": [
                "DISPLAYFORM0 DISPLAYFORM0 DISPLAYFORM1"
            ]
        },
        {
            "id": "task460-38a076fc10534cb4879bc43051051974",
            "input": "We conduct experiments on the SQuAD dataset BIBREF3. \n Question: On what datasets are experiments performed?",
            "output": [
                "SQuAD"
            ]
        },
        {
            "id": "task460-6d0d17de39ea4de5a17b0875efc8e3ce",
            "input": "In addition, we have another independent human evaluation task about the style strength – we present the generated headlines from TitleStylist and baselines to the human judges and let them choose the one that most conforms to the target style such as humor. Then we define the style strength score as the proportion of choices. \n Question: How is presence of three target styles detected?",
            "output": [
                "human evaluation task about the style strength"
            ]
        },
        {
            "id": "task460-28850ac4fcee45aba6aed5ee5bec924c",
            "input": "These unstructured features include: 1) the relative number of sentences in the admission notes that involve each risk factor domain (out of total number of sentences within the admission) and 2) clinical sentiment scores for each of these risk factor domains, i.e. sentiment scores that evaluate the patient’s psychosocial functioning level (positive, negative, or neutral) with respect to each of these risk factor domain.\n\nThese sentiment scores were automatically obtained through the topic extraction and sentiment analysis pipeline introduced in our prior work BIBREF15 and pretrained on in-house psychiatric EHR text. 45 clinically interpretable features per admission were extracted as inputs to the readmission risk classifier. \n Question: How do they incorporate sentiment analysis?",
            "output": [
                "features per admission were extracted as inputs to the readmission risk classifier"
            ]
        },
        {
            "id": "task460-26b0e03378a64aa8b2a7077c91c94ea9",
            "input": "However, the coherence of the topics produced by LDA is poorer than expected.\n\nTo address this lack of coherence, we applied non-negative matrix factorization (NMF). \n Question: How are prominent topics idenified in Dabiq and Rumiyah?",
            "output": [
                "LDA non-negative matrix factorization (NMF)"
            ]
        },
        {
            "id": "task460-0fd38400393642978a039af0a73d6f64",
            "input": "Maximum Entropy theory is applied to solve Vietnamese word segmentation BIBREF15 , BIBREF22 , BIBREF23 . There are several studies about the Vietnamese word segmentation task over the last decade. Dinh et al. started this task with Weighted Finite State Transducer (WFST) approach and Neural Network approach BIBREF9 . In addition, machine learning approaches are studied and widely applied to natural language processing and word segmentation as well. In fact, several studies used support vector machines (SVM) and conditional random fields (CRF) for the word segmentation task BIBREF7 , BIBREF8 . \n Question: Which approaches have been applied to solve word segmentation in Vietnamese?",
            "output": [
                "Maximum Entropy Weighted Finite State Transducer (WFST)  support vector machines (SVM) conditional random fields (CRF)"
            ]
        },
        {
            "id": "task460-66e9f2b56b7942198d559ac57ac3b101",
            "input": "Excluding the embedding weights, our model requires 100k parameters , while BIBREF8 requires 250k parameters. \n Question: how many parameters did their model use?",
            "output": [
                "Excluding the embedding weights, our model requires 100k parameters"
            ]
        },
        {
            "id": "task460-22ecc1a0c4e74d1ebe2db59d34cc06f3",
            "input": "We extract user's age by applying regular expression patterns to profile descriptions (such as \"17 years old, self-harm, anxiety, depression\") BIBREF41 . We compile \"age prefixes\" and \"age suffixes\", and use three age-extraction rules: 1. I am X years old 2. Born in X 3. X years old, where X is a \"date\" or age (e.g., 1994). We selected a subset of 1464 users INLINEFORM0 from INLINEFORM1 who disclose their gender in their profile description. \n Question: Where does the information on individual-level demographics come from?",
            "output": [
                "From Twitter profile descriptions of the users."
            ]
        },
        {
            "id": "task460-a70fa6474b154a13852b7432d2e5dec5",
            "input": "We use the MedWeb (“Medical Natural Language Processing for Web Document”) dataset BIBREF4 that was provided as part of a subtask at the NTCIR-13 Conference BIBREF5. The data is summarised in Table TABREF1. There are a total of 2,560 pseudo-tweets in three different languages: Japanese (ja), English (en) and Chinese (zh). \n Question: How big is dataset used for fine-tuning model for detection of red flag medical symptoms in individual statements?",
            "output": [
                "a total of 2,560 pseudo-tweets in three different languages: Japanese (ja), English (en) and Chinese (zh)"
            ]
        },
        {
            "id": "task460-ac93a14a44d3408cafdebe1c78a91544",
            "input": "The corpus consists of a collection of 2,346 clinical notes (admission notes, progress notes, and discharge summaries), which amounts to 2,372,323 tokens in total (an average of 1,011 tokens per note). All the notes were written in English and extracted from the EHRs of 183 psychosis patients from McLean Psychiatric Hospital in Belmont, MA, all of whom had in their history at least one instance of 30-day readmission. \n Question: What is the dataset used?",
            "output": [
                "EHRs of 183 psychosis patients from McLean Psychiatric Hospital in Belmont, MA"
            ]
        },
        {
            "id": "task460-30a494e44c1a4d859b51587cd8a50669",
            "input": "Consequently, our architecture is based on Embeddings from Language Model or ELMo BIBREF10 Concretely, we use a pre-trained ELMo model, obtained using the 1 Billion Word Benchmark which contains about 800M tokens of news crawl data from WMT 2011 BIBREF24 . Subsequently, the contextualized embeddings are passed on to a BiLSTM with 2,048 hidden units. We aggregate the LSTM hidden states using max-pooling, which in our preliminary experiments offered us better results, and feed the resulting vector to a 2-layer feed-forward network, where each layer has 512 units. The output of this is then fed to the final layer of the model, which performs the binary classification. \n Question: What type of model are the ELMo representations used in?",
            "output": [
                "A bi-LSTM with max-pooling on top of it"
            ]
        },
        {
            "id": "task460-925d8e4a94764e9cb1493be5f32727dd",
            "input": "In the VQABQ model, there are two main modules, the basic question generation module (Module 1) and co-attention visual question answering module (Module 2).  \n Question: What two main modules their approach consists of?",
            "output": [
                "the basic question generation module (Module 1) and co-attention visual question answering module (Module 2)"
            ]
        },
        {
            "id": "task460-3b2082407844477b8c9ce06c4a6195c9",
            "input": "Most lexical resources for sentiment analysis are in English. To still be able to benefit from these sources, the lexicons in the AffectiveTweets package were translated to Spanish, using the machine translation platform Apertium BIBREF5  \n Question: How was the training data translated?",
            "output": [
                "using the machine translation platform Apertium "
            ]
        },
        {
            "id": "task460-fe38798c6add453fbc50fa3d9cb0dc85",
            "input": "Our dataset is annotated based on Chinese pathology reports provided by the Department of Gastrointestinal Surgery, Ruijin Hospital. It contains 17,833 sentences, 826,987 characters and 2,714 question-answer pairs. \n Question: How many questions are in the dataset?",
            "output": [
                "2,714 "
            ]
        },
        {
            "id": "task460-b82ae53e873147c599a34ceec657103b",
            "input": "In order to benchmark toxic comment detection, The Wikipedia Toxic Comments dataset (which we study in this work) was collected and extracted from Wikipedia Talk pages and featured in a Kaggle competition BIBREF12, BIBREF15.  \n Question: What datasets are used?",
            "output": [
                "The Wikipedia Toxic Comments dataset"
            ]
        },
        {
            "id": "task460-bb7d086d3ced46429e97b6075f45ce79",
            "input": "Our dataset contains tweets about `ObamaCare' in USA collected during march 2010.  \n Question: What dataset of tweets is used?",
            "output": [
                "tweets about `ObamaCare' in USA collected during march 2010"
            ]
        },
        {
            "id": "task460-544a387c40ba43de8aa5f586d157227a",
            "input": "For our classification experiments, we use the MH17 Twitter dataset introduced by BIBREF4, a dataset collected in order to study the flow of (dis)information about the MH17 plane crash on Twitter. It contains tweets collected based on keyword search that were posted between July 17, 2014 (the day of the plane crash) and December 9, 2016.\n\nBIBREF4 provide annotations for a subset of the English tweets contained in the dataset. \n Question: What languages are included in the dataset?",
            "output": [
                "English"
            ]
        },
        {
            "id": "task460-d71705246c0c4a96a4e38dea10a0c624",
            "input": "Let all word vectors be normalized and $W$ be the word matrix. Inspired by BIBREF21 , where vector space models are used for evaluating topic coherence, we suggest to estimate the interpretability of $k$ th component as $ \\operatorname{interp}_k W = \\sum _{i,j=1}^N W_{i,k} W_{j,k} \\left(W_i \\cdot W_j \\right). $  \n Question: How do they evaluate interpretability in this paper?",
            "output": [
                "we suggest to estimate the interpretability of $k$ th component as $ \\operatorname{interp}_k W = \\sum _{i,j=1}^N W_{i,k} W_{j,k} \\left(W_i \\cdot W_j \\right). $"
            ]
        },
        {
            "id": "task460-3be0d628d72748f0a856df48e4f3e990",
            "input": "The best performing systems being for character unit the RNN-transducer with additional attention module, achieving 7.8% in terms of CER and 17.6% on WER. For subword units, classic RNN-transducer, RNN-transducer with attention and joint CTC-attention show comparable performance on subword error rate and WER, with the first one being slightly better on WER ($17.4\\%$) and the last one having a lower error rate on subword ($14.5\\%$). \n Question: Which model have the smallest Character Error Rate and which have the smallest Word Error Rate?",
            "output": [
                "character unit the RNN-transducer with additional attention module For subword units, classic RNN-transducer, RNN-transducer with attention and joint CTC-attention show comparable performance"
            ]
        },
        {
            "id": "task460-d1b72e2ea5a14f16af9312377c67bf89",
            "input": "This algorithm finds the optimal number by minimising a cost function based on the eigenvector structure of the word similarity matrix. We refer the reader to the relevant literature for further details.\n\nResults and Discussion \n Question: What are the six target languages?",
            "output": [
                "Answer with content missing: (3 Experimental Setup) We experiment with six target languages: French (FR), Brazilian Portuguese (PT), Italian (IT), Polish (PL), Croatian (HR), and Finnish (FI)."
            ]
        },
        {
            "id": "task460-02c4b4ef209b4791ad3e3807157d281e",
            "input": "Our system was ranked second in the competition only 0.3 BLEU points behind the winning team UPC-TALP. The relative low BLEU and high TER scores obtained by all teams are due to out-of-domain data provided in the competition which made the task equally challenging to all participants. The fact that out-of-domain data was provided by the organizers resulted in a challenging but interesting scenario for all participants. \n Question: Does the use of out-of-domain data improve the performance of the method?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-21822094d31f4ffe9289bbb79552e869",
            "input": "While the performances of a purely content-based model naturally stays stable, the performance of the other systems decrease notably – they perform worse than the content-based model. \n Question: How do they demonstrate the robustness of their results?",
            "output": [
                "performances of a purely content-based model naturally stays stable"
            ]
        },
        {
            "id": "task460-18e81cccd28a42bb848b16ad86498c93",
            "input": "We proposed to use visual renderings of documents to capture implicit document quality indicators, such as font choices, images, and visual layout, which are not captured in textual content. We applied neural network models to capture visual features given visual renderings of documents. Experimental results show that we achieve a 2.9% higher accuracy than state-of-the-art approaches based on textual features over Wikipedia, and performance competitive with or surpassing state-of-the-art approaches over arXiv. We further proposed a joint model, combining textual and visual representations, to predict the quality of a document. Experimental results show that our joint model outperforms the visual-only model in all cases, and the text-only model on Wikipedia and two subsets of arXiv. These results underline the feasibility of assessing document quality via visual features, and the complementarity of visual and textual document representations for quality assessment. \n Question: Which is more useful, visual or textual features?",
            "output": [
                "It depends on the dataset. Experimental results over two datasets reveal that textual and visual features are complementary. "
            ]
        },
        {
            "id": "task460-fc2125cfc8274534a294567d85006885",
            "input": "Next, we analyzed all tweets labeled as offensive to better understand how Arabic speakers use offensive language. Here is a breakdown of usage:\n\nDirect name calling: The most frequent attack is to call a person an animal name, and the most used animals were كلب> (“klb” – “dog”), حمار> (“HmAr” – “donkey”), and بهيم> (“bhym” – “beast”). The second most common was insulting mental abilities using words such as غبي> (“gby” – “stupid”) and عبيط> (“EbyT” –“idiot”). Some culture-specific differences should be considered. Not all animal names are used as insults. For example, animals such as أسد> (“Asd” – “lion”), صقر> (“Sqr” – “falcon”), and غزال> (“gzAl” – “gazelle”) are typically used for praise. For other insults, people use: some bird names such as دجاجة> (“djAjp” – “chicken”), بومة> (“bwmp” – “owl”), and غراب> (“grAb” – “crow”); insects such as ذبابة> (“*bAbp” – “fly”), صرصور> (“SrSwr” – “cockroach”), and حشرة> (“H$rp” – “insect”); microorganisms such as جرثومة> (“jrvwmp” – “microbe”) and طحالب> (“THAlb” – “algae”); inanimate objects such as جزمة> (“jzmp” – “shoes”) and سطل> (“sTl” – “bucket”) among other usages.\n\nSimile and metaphor: Users use simile and metaphor were they would compare a person to: an animal as in زي الثور> (“zy Alvwr” – “like a bull”), سمعني نهيقك> (“smEny nhyqk” – “let me hear your braying”), and هز ديلك> (“hz dylk” – “wag your tail”); a person with mental or physical disability such as منغولي> (“mngwly” – “Mongolian (down-syndrome)”), معوق> (“mEwq” – “disabled”), and قزم> (“qzm” – “dwarf”); and to the opposite gender such as جيش نوال> (“jy$ nwAl” – “Nawal's army (Nawal is female name)”) and نادي زيزي> (“nAdy zyzy” – “Zizi's club (Zizi is a female pet name)”).\n\nIndirect speech: This type of offensive language includes: sarcasm such as أذكى إخواتك> (“A*kY AxwAtk” – “smartest one of your siblings”) and فيلسوف الحمير> (“fylswf AlHmyr” – “the donkeys' philosopher”); questions such as ايه كل الغباء ده> (“Ayh kl AlgbA dh” – “what is all this stupidity”); and indirect speech such as النقاش مع البهايم غير مثمر> (“AlnqA$ mE AlbhAym gyr mvmr” – “no use talking to cattle”).\n\nWishing Evil: This entails wishing death or major harm to befall someone such as ربنا ياخدك> (“rbnA yAxdk” – “May God take (kill) you”), الله يلعنك> (“Allh ylEnk” – “may Allah/God curse you”), and روح في داهية> (“rwH fy dAhyp” – equivalent to “go to hell”).\n\nName alteration: One common way to insult others is to change a letter or two in their names to produce new offensive words that rhyme with the original names. Some examples of such include changing الجزيرة> (“Aljzyrp” – “Aljazeera (channel)”) to الخنزيرة> (“Alxnzyrp” – “the pig”) and خلفان> (“xlfAn” – “Khalfan (person name)”) to خرفان> (“xrfAn” – “crazed”).\n\nSocietal stratification: Some insults are associated with: certain jobs such as بواب> (“bwAb” – “doorman”) or خادم> (“xAdm” – “servant”); and specific societal components such بدوي> (“bdwy” – “bedouin”) and فلاح> (“flAH” – “farmer”).\n\nImmoral behavior: These insults are associated with negative moral traits or behaviors such as حقير> (“Hqyr” – “vile”), خاين> (“xAyn” – “traitor”), and منافق> (“mnAfq” – “hypocrite”).\n\nSexually related: They include expressions such as خول> (“xwl” – “gay”), وسخة> (“wsxp” – “prostitute”), and عرص> (“ErS” – “pimp”). \n Question: What are the distinctive characteristics of how Arabic speakers use offensive language?",
            "output": [
                "Frequent use of direct animal name calling, using simile and metaphors, through indirect speech like sarcasm, wishing evil to others, name alteration, societal stratification, immoral behavior and sexually related uses."
            ]
        },
        {
            "id": "task460-ecfd843c1fa6426f911bbbb2174e5fc6",
            "input": "This study focuses on Switchboard-300, a standard 300-hour English conversational speech recognition task. As a contrast to our best results on Switchboard-300, we also train a seq2seq model on the 2000-hour Switchboard+Fisher data.  \n Question: How much bigger is Switchboard-2000 than Switchboard-300 database?",
            "output": [
                "Switchboard-2000 contains 1700 more hours of speech data."
            ]
        },
        {
            "id": "task460-6c332eabe2c943ed8127d4539dd684d7",
            "input": "The corpus consists of 53 documents, which contain an average number of 156.1 sentences per document, each with 19.55 tokens on average. \n Question: How many documents are in the new corpus?",
            "output": [
                "53 documents"
            ]
        },
        {
            "id": "task460-87f9a46c2f474fb6a52295c509f8662b",
            "input": "To adapt the FactorCell BIBREF4 for our purposes, we replace user embeddings with a low-dimensional image representation. Thus, we are able to modify each query completion to be personalized to a specific image representation. \n Question: How they complete a user query prefix conditioned upon an image?",
            "output": [
                "we replace user embeddings with a low-dimensional image representation"
            ]
        },
        {
            "id": "task460-f58151bcbac14621baa410eb09a12a4e",
            "input": "ACL-ARC is a dataset of citation intents released by BIBREF7 . The dataset is based on a sample of papers from the ACL Anthology Reference Corpus BIBREF15 and includes 1,941 citation instances from 186 papers and is annotated by domain experts in the NLP field. \n Question: What is the size of ACL-ARC datasets?",
            "output": [
                "includes 1,941 citation instances from 186 papers"
            ]
        },
        {
            "id": "task460-e5367e2f4be84f5e8a6e9f023621c63c",
            "input": "The dataset is composed of 90 annotated documents with 5 citations each ranked 1 to 5, where 1 is least relevant and 5 is most relevant for a total of 450 annotated citations. \n Question: what is the size of this built corpus?",
            "output": [
                "90 annotated documents with 5 citations each ranked 1 to 5, where 1 is least relevant and 5 is most relevant for a total of 450 annotated citations"
            ]
        },
        {
            "id": "task460-01551379b0f64e65af5f658091590679",
            "input": "Since we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for \"Starry Night\" with a low average content score. This happens when the style transfer dataset does not have similar words in the training set of sentences. A solution would be to expand the style transfer dataset, for a better representation of the poem data. \n Question: What limitations do the authors demnostrate of their model?",
            "output": [
                "Since we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer"
            ]
        },
        {
            "id": "task460-cee7092ee86641dbb7c3097c5f9d4bc0",
            "input": "For the MT task, we use the WMT 2014 En $\\leftrightarrow $ Fr parallel corpus. The dataset contains 36 million En $\\rightarrow $ Fr sentence pairs. We swapped the source and target sentences to obtain parallel data for the Fr $\\rightarrow $ En translation task. We use these two datasets (72 million sentence pairs) to train a single multilingual NMT model to learn both these translation directions simultaneously.  \n Question: What data were they used to train the multilingual encoder?",
            "output": [
                "WMT 2014 En-Fr parallel corpus"
            ]
        },
        {
            "id": "task460-71bdc0da7d3b4873847f17411cc3efa4",
            "input": "Our presented QA-CTS task unifies the output format of the traditional CTS task and make the training data shareable, thus enriching the training data. All question-answer pairs are annotated and reviewed by four clinicians with three types of questions, namely tumor size, proximal resection margin and distal resection margin.  Experimental results on real-world dataset demonstrate that our proposed model competes favorably with strong baseline models in all three specific tasks. \n Question: What are the specific tasks being unified?",
            "output": [
                " three types of questions, namely tumor size, proximal resection margin and distal resection margin"
            ]
        },
        {
            "id": "task460-be03b3fe46824b37a761a122f90ca8e4",
            "input": "We experimented with four different classifiers, namely, support vector machine BIBREF18 , random forest, extra tree and naive bayes classifier BIBREF19 . Chi square feature selection algorithm is applied to reduces the size of our feature vector. For training our system classifier, we used Scikit-learn BIBREF19 . \n Question: What type of system does the baseline classification use?",
            "output": [
                "support vector machine BIBREF18 , random forest, extra tree and naive bayes classifier BIBREF19"
            ]
        },
        {
            "id": "task460-2c9de59caa8a4e87a8785cc5229962b0",
            "input": "In this paper, we use three data sets from the literature to train and evaluate our own classifier. Data collected by BIBREF3 , which we term the Sexist/Racist (SR) data set, was collected using an initial Twitter search followed by analysis and filtering by the authors and their team who identified 17 common phrases, hashtags, and users that were indicative of abusive speech. BIBREF4 collected the HATE dataset by searching for tweets using a lexicon provided by Hatebase.org. The final data set we used, which we call HAR, was collected by BIBREF9 ; we removed all retweets reducing the dataset to 20,000 tweets. Tweets were labeled as “Harrassing” or “Non-Harrassing”; hate speech was not explicitly labeled, but treated as an unlabeled subset of the broader “Harrassing” category BIBREF9 . Many of the false negatives we see are specific references to characters in the TV show “My Kitchen Rules”, rather than something about women in general.  While this may be a limitation of considering only the content of the tweet, it could also be a mislabel.\n\nDebra are now my most hated team on #mkr after least night's ep. Snakes in the grass those two.\n\nAlong these lines, we also see correct predictions of innocuous speech, but find data mislabeled as hate speech:\n\n@LoveAndLonging ...how is that example \"sexism\"?\n\n@amberhasalamb ...in what way? \n Question: Do they report results only on English data?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-0156c86a67b4461bb6d7985b988fc8aa",
            "input": "We compare our approaches with related approaches of pivoting, multilingual NMT (MNMT) BIBREF19, and cross-lingual transfer without pretraining BIBREF16.  The results show that our approaches consistently outperform other approaches across languages and datasets, especially surpass pivoting, which is a strong baseline in the zero-shot scenario that multilingual NMT systems often fail to beat BIBREF19, BIBREF20, BIBREF23. \n Question: which multilingual approaches do they compare with?",
            "output": [
                "BIBREF19 BIBREF20"
            ]
        },
        {
            "id": "task460-911aa019582c4629b6c93bb17341797c",
            "input": "We built a phrase-based Chinese-to-English SMT system by using Moses BIBREF18 .  In the end, the final parallel text consists of around 8.8M sentence pairs, 228M Chinese tokens, and 254M English tokens (a token can be a word or punctuation symbol).  The total number of words in these two corpora is 1.81M for Chinese and 2.03M for English. \n Question: Did they only experiment with one language pair?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-ef3677d699954e69846efc4e778d9acf",
            "input": "Given a news Twitter account, we read its tweets from the account's timeline. Then we sort the tweets by the posting date in ascending way and we split them into $N$ chunks. Each chunk consists of a sorted sequence of tweets labeled by the label of its corresponding account. Consequently, we investigate ways to detect suspicious accounts by considering their tweets in groups (chunks). Our hypothesis is that suspicious accounts have a unique pattern in posting tweet sequences. Since their intention is to mislead, the way they transition from one set of tweets to the next has a hidden signature, biased by their intentions. Therefore, reading these tweets in chunks has the potential to improve the detection of the fake news accounts. \n Question: How are chunks defined?",
            "output": [
                "Chunks is group of tweets from single account that  is consecutive in time - idea is that this group can show secret intention of malicious accounts."
            ]
        },
        {
            "id": "task460-1bae246cde6e4facb3377225d1f30e50",
            "input": "To answer RQ2, which is concerned with in-domain ATSC performance, we see in tab:results that for the in-domain training case, our models BERT-ADA Lapt and BERT-ADA Rest achieve performance close to state-of-the-art on the laptops dataset and new state-of-the-art on the restaurants dataset with accuracies of $79.19\\%$ and $87.14\\%$, respectively. In general, the ATSC task generalizes well cross-domain, with about 2-$3\\%$ drop in accuracy compared to in-domain training. \n Question: What are the performance results?",
            "output": [
                "results that for the in-domain training case, our models BERT-ADA Lapt and BERT-ADA Rest achieve performance close to state-of-the-art on the laptops dataset new state-of-the-art on the restaurants dataset with accuracies of $79.19\\%$ and $87.14\\%$, respectively."
            ]
        },
        {
            "id": "task460-a5832d8a219247a899183a6e8e986008",
            "input": ". It is evident from Table TABREF17 that GM$\\_$KL achieves better correlation than existing approaches for various metrics on SCWS dataset. \n Question: How does this approach compare to other WSD approaches employing word embeddings?",
            "output": [
                "GM$\\_$KL achieves better correlation than existing approaches for various metrics on SCWS dataset."
            ]
        },
        {
            "id": "task460-4903f41f41a74fc080cb99f57d7b8732",
            "input": "We tried Glove BIBREF2 and Twitter word2vec BIBREF3 code for training embeddings for the processed tweets. \n Question: What embeddings do they use?",
            "output": [
                "Glove Twitter word2vec"
            ]
        },
        {
            "id": "task460-4120e0c455ce47459c0e1f6a2a204df9",
            "input": "To answer RQ3, which is concerned with domain adaptation, we can see in the grayed out cells in tab:results, which correspond to the cross-domain adaption case where the BERT language model is trained on the target domain, that domain adaptation works well with $2.2\\%$ absolute accuracy improvement on the laptops test set and even $3.6\\%$ accuracy improvement on the restaurants test set compared to BERT-base. \n Question: By how much does their model outperform the baseline in the cross-domain evaluation?",
            "output": [
                "$2.2\\%$ absolute accuracy improvement on the laptops test set $3.6\\%$ accuracy improvement on the restaurants test set"
            ]
        },
        {
            "id": "task460-f6fbfc3f58124301b7958f19fd813df0",
            "input": "We perform a direct comparison of expert and crowd contributors, for 1000 term groups based on the number of total annotations(200 term groups with 2 total annotations, 200 term groups with 3 total annotations, and so on up to term groups with 6 total annotations).  Evaluators were given a summary of the annotations received for the term group in the form of:The term group \"inequality inequity\" received annotations as 50.0% sadness, 33.33% disgust, 16.67% anger. Then, they were asked to evaluate on a scale from 1 to 5, how valid these annotations were considered. \n Question: How do they compare lexicons?",
            "output": [
                "Human evaluators were asked to evaluate on a scale from 1 to 5 the validity of the lexicon annotations made by the experts and crowd contributors."
            ]
        },
        {
            "id": "task460-fd465db961ae46c0915dc6740358e385",
            "input": "Another insight gained from these charts is that a random summarizer resulted in scores more than 50% in all measures, and without using document-aware features, the model achieves a small improvement over a random summarizer. \n Question: Is new approach tested against state of the art?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-86bbcbbc84e141de89b0b2e3434ddc11",
            "input": "We evaluate our method on IWSLT16 German-English (DE-EN) translation in both directions, WMT15 English-German (EN-DE) translation in both directions, and NIST Chinese-to-English (ZH$\\rightarrow $EN) translation. \n Question: Which languages do they experiment on?",
            "output": [
                "German English Chinese"
            ]
        },
        {
            "id": "task460-025d4769bcd441b388250744d1b0f9c7",
            "input": "pretrained word embeddings (WE): For the PDT experiments, we generate the word embeddings with word2vec on a concatenation of large raw Czech corpora available from the LINDAT/CLARIN repository. For UD Czech, we use FastText word embeddings BIBREF27 of dimension 300, which we pretrain on Czech Wikipedia using segmentation and tokenization trained from the UD data.\n\nBERT BIBREF1: Pretrained contextual word embeddings of dimension 768 from the Base model. We average the last four layers of the BERT model to produce the embeddings. Because BERT utilizes word pieces, we decompose UD words into appropriate subwords and then average the generated embeddings over subwords belonging to the same word.\n\nFlair BIBREF2: Pretrained contextual word embeddings of dimension 4096. \n Question: What data is used to build the embeddings?",
            "output": [
                "large raw Czech corpora available from the LINDAT/CLARIN repository Czech Wikipedia"
            ]
        },
        {
            "id": "task460-07180827e89746c5aaa7663c9fbd0b57",
            "input": "We conduct experiments on our self-collected CAIS to evaluate the generalizability in different language. We apply two baseline models for comparison, one is the popular BiLSTMs + CRF architecture BIBREF36 for sequence labeling task, and the other one is the more powerful sententce-state LSTM BIBREF21. The results listed in Table TABREF50 demonstrate the generalizability and effectiveness of our CM-Net when handling various domains and different languages. \n Question: What were the baselines models?",
            "output": [
                "BiLSTMs + CRF architecture BIBREF36 sententce-state LSTM BIBREF21"
            ]
        },
        {
            "id": "task460-d688f7a47f8d418cb1eaec51dab7882a",
            "input": "PERPLEXITY \n Question: Does the paper consider the use of perplexity in order to identify text anomalies?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-11b6f41d64be4b35b1442b354abb3a92",
            "input": "Our submissions ranked second (EI-Reg), second (EI-Oc), fourth (V-Reg) and fifth (V-Oc), demonstrating that the proposed method is accurate in automatically determining the intensity of emotions and sentiment of Spanish tweets. \n Question: What subtasks did they participate in?",
            "output": [
                "Answer with content missing: (Subscript 1: \"We did not participate in subtask 5 (E-c)\") Authors participated in EI-Reg, EI-Oc, V-Reg and V-Oc subtasks."
            ]
        },
        {
            "id": "task460-cf16d97fae034ee1b5a03dbcd678b65a",
            "input": "These sentiment scores were automatically obtained through the topic extraction and sentiment analysis pipeline introduced in our prior work BIBREF15 and pretrained on in-house psychiatric EHR text. \n Question: How do they extract topics?",
            "output": [
                " automatically obtained through the topic extraction and sentiment analysis pipeline introduced in our prior work BIBREF15"
            ]
        },
        {
            "id": "task460-88daeb1d858c458d9b0b4fe41e1ff05c",
            "input": "Inspired by the StackLite tag recommendation task on Kaggle, we build a new benchmark based on the public StackExchange data. We use questions with titles as source, and user-assigned tags as target keyphrases.\n\nSince oftentimes the questions on StackExchange contain less information than in scientific publications, there are fewer keyphrases per data point in StackEx. Furthermore, StackExchange uses a tag recommendation system that suggests topic-relevant tags to users while submitting questions; therefore, we are more likely to see general terminology such as Linux and Java. This characteristic challenges models with respect to their ability to distill major topics of a question rather than selecting specific snippets from the text. \n Question: How was the StackExchange dataset collected?",
            "output": [
                "they obtained computer science related topics by looking at titles and user-assigned tags"
            ]
        },
        {
            "id": "task460-6894bdca43c34c9ab68e0093d044b01c",
            "input": "For the English version, we performed both a thorough manual analysis and automatic evaluation across three commonly used TS datasets from two different domains  For the English version, we performed both a thorough manual analysis and automatic evaluation across three commonly used TS datasets from two different domains  The evaluation of the German version is in progress. \n Question: What are the corpora used for the task?",
            "output": [
                "For the English version, we performed both a thorough manual analysis and automatic evaluation across three commonly used TS datasets from two different domains The evaluation of the German version is in progress."
            ]
        },
        {
            "id": "task460-8d918fb2b8304582b85f25d83f438a0c",
            "input": "Each game's video ranges from 30 to 50 minutes in length which contains image and chat data linked to the specific timestamp of the game. \n Question: What is the average length of the recordings?",
            "output": [
                "40 minutes"
            ]
        },
        {
            "id": "task460-b774bd6459214eabaeeffca52e8e534f",
            "input": "Majority: the text picks the label of the largest size.\n\n ESA: A dataless classifier proposed in BIBREF0. It maps the words (in text and label names) into the title space of Wikipedia articles, then compares the text with label names. This method does not rely on train.\n\nWe implemented ESA based on 08/01/2019 Wikipedia dump. There are about 6.1M words and 5.9M articles. Word2Vec BIBREF23: Both the representations of the text and the labels are the addition of word embeddings element-wisely. Then cosine similarity determines the labels. This method does not rely on train either. Binary-BERT: We fine-tune BERT on train, which will yield a binary classifier for entailment or not; then we test it on test – picking the label with the maximal probability in single-label scenarios while choosing all the labels with “entailment” decision in multi-label cases. \n Question: What are their baseline models?",
            "output": [
                "Majority ESA Word2Vec  Binary-BERT"
            ]
        },
        {
            "id": "task460-955e4439319d43d484ff005f2026dc84",
            "input": "There are two aspects that we defer to future work. First, the systems designed here assumed that the input are valid claim sentences. To make use of such systems, one needs to develop mechanisms to recognize valid argumentative structures. In addition, we ignore trustworthiness and credibility issues, important research issues that are addressed in other works. \n Question: What challenges are highlighted?",
            "output": [
                "one needs to develop mechanisms to recognize valid argumentative structures we ignore trustworthiness and credibility issues"
            ]
        },
        {
            "id": "task460-0b489fb4e8c84dbfa1e06a8d5d0a308a",
            "input": "The split methods (beyond random split) are the following:\n\nOutput length: Variation of the setup described by BIBREF2 where the train set consists of examples with output (sparql query or action sequence) length $\\le \\hspace{-2.5pt} N$, while the test set consists of examples with output length $> \\hspace{-2.5pt} N$. For CFQ, we use $N = 7$ constraints. For scan, we use $N = 22$ actions.\n\nInput length: Variation of the above setup, in which the train set consists of examples with input (question or command) length $\\le N$, while test set consists of examples with input length $> N$. For CFQ, we use $N=19$ grammar leaves. For SCAN, we use $N=8$ tokens.\n\nOutput pattern: Variation of setup described by BIBREF8, in which the split is based on randomly assigning clusters of examples sharing the same output (query or action sequence) pattern. Query patterns are determined by anonymizing entities and properties; action sequence patterns collapse primitive actions and directions.\n\nInput pattern: Variation of the previous setup in which the split is based on randomly assigning clusters of examples sharing the same input (question or command) pattern. Question patterns are determined by anonymizing entity and property names ; command patterns collapse verbs and the interchangeable pairs left/right, around/opposite, twice/thrice. \n Question: What are other approaches into creating compositional generalization benchmarks?",
            "output": [
                "random  Output length Input length Output pattern Input pattern"
            ]
        },
        {
            "id": "task460-ded0db20b3584f2dbdd9ac77b572e35c",
            "input": "We use the publicly available dataset KVRET BIBREF5 in our experiments. There are 2,425 dialogues for training, 302 for validation and 302 for testing, as shown in the upper half of Table TABREF12. \n Question: What is the size of the dataset?",
            "output": [
                "3029"
            ]
        },
        {
            "id": "task460-2e8dfd9d7b6d4473a395e2de68071b78",
            "input": "To design an interpretable word embedding model for small corpora, we identify novel connections between word embeddings and topic models, and adapt advances from topic modeling. Following the distributional hypothesis BIBREF23 , the skip-gram's word embeddings parameterize discrete probability distributions over words INLINEFORM0 which tend to co-occur, and tend to be semantically coherent – a property leveraged by the Gaussian LDA model of BIBREF21  We thus reinterpret the skip-gram as a parameterization of a certain supervised naive Bayes topic model (Table TABREF2 , top-right). As in LDA, this model can be improved by replacing the naive Bayes assumption with a mixed membership assumption. By applying the mixed membership representation to this topic model version of the skip-gram, we obtain the model in the bottom-right of Table TABREF2 . After once again parameterizing this model with word embeddings, we obtain our final model, the mixed membership skip-gram (MMSG)  \n Question: Which techniques for word embeddings and topic models are used?",
            "output": [
                " skip-gram LDA"
            ]
        },
        {
            "id": "task460-e69ad56edcac4fdfbb8d29e912a39b88",
            "input": "Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP. \n Question: What are method improvements of F1 for paraphrase identification?",
            "output": [
                "Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP"
            ]
        },
        {
            "id": "task460-a4797758b2604a25bed2733afddf1e27",
            "input": "Since none of the datasets from previous works have been published, we decide to build a new one. We randomly collect INLINEFORM0 cases from China Judgments Online, among which INLINEFORM1 cases are for training, INLINEFORM2 each for validation and testing. \n Question: what datasets are used in the experiment?",
            "output": [
                "build a new one collect INLINEFORM0 cases from China Judgments Online"
            ]
        },
        {
            "id": "task460-bdd1bfee7c9b42de9f4c0d5c17c676a4",
            "input": "Evaluation is done using the development set, consisting of 22 documents and 1112 PIE candidates, and the test set, which consists of 23 documents and 1127 PIE candidates. For each method the best set of parameters and/or options is determined using the development set, after which the best variant by F1-score of each method is evaluated on the test set.\n\nSince these documents in the corpus are exhaustively annotated for PIEs (see Section SECREF40), we can calculate true and false positives, and false negatives, and thus precision, recall and F1-score. \n Question: Are PIEs extracted automatically subjected to human evaluation?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-65e81a7169724a28ad04775ee7b2206d",
            "input": " The method is evaluated on the BERTbase model, which has 12 layers, 12 self-attention heads with a hidden size of 768. \n Question: What BERT model do they test?",
            "output": [
                "BERTbase"
            ]
        },
        {
            "id": "task460-f57eab02cb01429588e82f052e38f1a2",
            "input": "As introduced above, the `One Million Post' corpus provides annotation labels for more than 11,000 user comments. Although there is no directly comparable category capturing `offensive language' as defined in the shared task, there are two closely related categories. From the resource, we extract all those comments in which a majority of the annotators agree that they contain either `inappropriate' or `discriminating' content, or none of the aforementioned. We treat the first two cases as examples of `offense' and the latter case as examples of `other'. \n Question: What are the near-offensive language categories?",
            "output": [
                "inappropriate discriminating"
            ]
        },
        {
            "id": "task460-240e967889064b0993c37b55dc5eebc9",
            "input": "Overall, we observe a positive correlation between retweeting and co-voting, which is significantly different from zero. The strongest positive correlations are in the areas Area of freedom, security and justice, External relations of the Union, and Internal markets. Weaker, but still positive, correlations are observed in the areas Economic, social and territorial cohesion, European citizenship, and State and evolution of the Union. The only exception, with a significantly negative coefficient, is the area Economic and monetary system. This implies that in the area Economic and monetary system we observe a significant deviation from the usual co-voting patterns. \n Question: What is the relationship between the co-voting and retweeting patterns?",
            "output": [
                "we observe a positive correlation between retweeting and co-voting strongest positive correlations are in the areas Area of freedom, security and justice, External relations of the Union, and Internal markets Weaker, but still positive, correlations are observed in the areas Economic, social and territorial cohesion, European citizenship, and State and evolution of the Union significantly negative coefficient, is the area Economic and monetary system"
            ]
        },
        {
            "id": "task460-6802414112054109ac108132d0372b14",
            "input": "Experiments ::: Main Results and Analysis ::: Aspect-Category Sentiment Analysis Task Compared with GCAE, our AGDT improves the performance by 2.4% and 1.6% in the “DS” part of the two dataset, respectively. These results demonstrate that our AGDT can sufficiently exploit the given aspect to generate the aspect-guided sentence representation, and thus conduct accurate sentiment prediction. The “HDS”, which is designed to measure whether a model can detect different sentiment polarities in a sentence, consists of replicated sentences with different sentiments towards multiple aspects. Our AGDT surpasses GCAE by a very large margin (+11.4% and +4.9% respectively) on both datasets. Experiments ::: Main Results and Analysis ::: Aspect-Term Sentiment Analysis Task In the “HDS” part, the AGDT model obtains +3.6% higher accuracy than GCAE on the restaurant domain and +4.2% higher accuracy on the laptop domain, which shows that our AGDT has stronger ability for the multi-sentiment problem against GCAE. These results further demonstrate that our model works well across tasks and datasets. \n Question: How big is the improvement over the state-of-the-art results?",
            "output": [
                "AGDT improves the performance by 2.4% and 1.6% in the “DS” part of the two dataset Our AGDT surpasses GCAE by a very large margin (+11.4% and +4.9% respectively) on both datasets In the “HDS” part, the AGDT model obtains +3.6% higher accuracy than GCAE on the restaurant domain and +4.2% higher accuracy on the laptop domain"
            ]
        },
        {
            "id": "task460-dc885254e63f42f6a967d0608c015011",
            "input": "After Korektor, there are 4262 unique sentences (including 150 seed sentences) that form the COSTRA 1.0 dataset. \n Question: How many sentence transformations on average are available per unique sentence in dataset?",
            "output": [
                "27.41 transformation on average of single seed sentence is available in dataset."
            ]
        },
        {
            "id": "task460-c74e9b209cfb4f388227bb81040d00a6",
            "input": "Experimental Setup \n Question: What domains are detected in this paper?",
            "output": [
                "Answer with content missing: (Experimental setup not properly rendered) In our experiments we used seven target domains: “Business and Commerce” (BUS), “Government and Politics” (GOV), “Physical and Mental Health” (HEA), “Law and Order” (LAW),\n“Lifestyle” (LIF), “Military” (MIL), and “General Purpose” (GEN). Exceptionally, GEN does\nnot have a natural root category."
            ]
        },
        {
            "id": "task460-6ae74d6e725a4fb4bb55db31c6f21348",
            "input": " Experimentally, on three benchmark datasets for machine translation – WMT2014, WMT2016 and IWSLT-2014, FlowSeq achieves comparable performance with state-of-the-art non-autoregressive models, and almost constant decoding time w.r.t. the sequence length compared to a typical left-to-right Transformer model, which is super-linear. \n Question: What are three neural machine translation (NMT) benchmark datasets used for evaluation?",
            "output": [
                "WMT2014, WMT2016 and IWSLT-2014"
            ]
        },
        {
            "id": "task460-f77d61415c2141eea65d2fea33e887de",
            "input": "Baseline Results ::: Speech Synthesis\nIn our previous work on building speech systems on found data in 700 languages, BIBREF7, we addressed alignment issues (when audio is not segmented into turn/sentence sized chunks) and correctness issues (when the audio does not match the transcription). We used the same techniques here, as described above.\n\nFor the best quality speech synthesis we need a few hours of phonetically-balanced, single-speaker, read speech. Our first step was to use the start and end points for each turn in the dialogues, and select those of the most frequent speaker, nmlch. This gave us around 18250 segments. We further automatically removed excessive silence from the start, middle and end of these turns (based on occurrence of F0). This gave us 13 hours and 48 minutes of speech.\n\nWe phonetically aligned this data and built a speech clustergen statistical speech synthesizer BIBREF9 from all of this data. For speech recognition (ASR) we used Kaldi BIBREF11. We built neural end-to-end machine translation systems between Mapudungun and Spanish in both directions, using state-of-the-art Transformer architecture BIBREF14 with the toolkit of BIBREF15. \n Question: What are the models used for the baseline of the three NLP tasks?",
            "output": [
                "state-of-the-art Transformer architecture Kaldi speech clustergen statistical speech synthesizer"
            ]
        },
        {
            "id": "task460-7ed8922346474a3da8ef7eb2bbb2ad3d",
            "input": "Bi-LSTM BIBREF11 is a baseline for neural models. Bi-LSTM$_{+ att. + LEX + POS}$ BIBREF10 is a multi-task learning framework for WSD, POS tagging, and LEX with self-attention mechanism, which converts WSD to a sequence learning task. GAS$_{ext}$ BIBREF12 is a variant of GAS which is a gloss-augmented variant of the memory network by extending gloss knowledge. CAN$^s$ and HCAN BIBREF13 are sentence-level and hierarchical co-attention neural network models which leverage gloss knowledge. \n Question: How does the neural network architecture accomodate an unknown amount of senses per word?",
            "output": [
                "converts WSD to a sequence learning task  leverage gloss knowledge by extending gloss knowledge"
            ]
        },
        {
            "id": "task460-bb7e7406839b4079aa338f408a707c69",
            "input": "Three metrics in text simplification are chosen in this paper. BLEU BIBREF5 is one traditional machine translation metric to assess the degree to which translated simplifications differed from reference simplifications. FKGL measures the readability of the output BIBREF23 . A small FKGL represents simpler output. SARI is a recent text-simplification metric by comparing the output against the source and reference simplifications BIBREF20 . \n Question: what evaluation metrics did they use?",
            "output": [
                "BLEU  FKGL  SARI "
            ]
        },
        {
            "id": "task460-306936cb026442a3946683d0e5fbfd2c",
            "input": "In terms of speed, our system was able to lemmatize 7.4 million words on a personal laptop in almost 2 minutes compared to 2.5 hours for MADAMIRA, i.e. 75 times faster.  \n Question: How was speed measured?",
            "output": [
                "how long it takes the system to lemmatize a set number of words"
            ]
        },
        {
            "id": "task460-4a5f045fe6a94b8fbc3d683c8c520b37",
            "input": "n this section, we first describe the phenomenon of mention of political parties names in the profile attributes of users. This is followed by the analysis of profiles that make specific mentions of political handles in their profile attributes. Both of these constitute an organic way of showing support to a party and does not involve any direct campaigning by the parties We opine that the whole campaign of adding Chowkidar to the profile attributes show an inorganic behavior, with political leaders acting as the catalyst. We believe, the effect of changing the profile attribute in accordance with Prime Minister's campaign is an example of inorganic behavior contagion BIBREF6, BIBREF9.   However, we argue that the addition of election campaign related keywords to the profile is a form of inorganic behavior.  \n Question: What are the organic and inorganic ways to show political affiliation through profile changes?",
            "output": [
                "Organic: mention of political parties names in the profile attributes, specific mentions of political handles in the profile attributes.\nInorganic:  adding Chowkidar to the profile attributes, the effect of changing the profile attribute in accordance with Prime Minister's campaign, the addition of election campaign related keywords to the profile."
            ]
        },
        {
            "id": "task460-5e4d2ad76b594dbabc5aa177378b3ee0",
            "input": "Two datasets are exploited in this article. Both datasets consist of plain text containing clinical narrative written in Spanish, and their respective manual annotations of sensitive information in BRAT BIBREF13 standoff format. NUBes BIBREF4 is a corpus of around 7,000 real medical reports written in Spanish and annotated with negation and uncertainty information. In order to avoid confusion between the two corpus versions, we henceforth refer to the version relevant in this paper as NUBes-PHI (from `NUBes with Personal Health Information'). The organisers of the MEDDOCAN shared task BIBREF3 curated a synthetic corpus of clinical cases enriched with sensitive information by health documentalists \n Question: What are the clinical datasets used in the paper?",
            "output": [
                "MEDDOCAN NUBes-PHI"
            ]
        },
        {
            "id": "task460-69549fa486954aefa8499d388d218abd",
            "input": "We draw our data from news publications, wine reviews, and Reddit, which in addition to large volume, also let us characterize binomials in new ways, and analyze differences in binomial orderings across communities and over time \n Question: What types of various community texts have been investigated for exploring global structure of binomials?",
            "output": [
                "news publications, wine reviews, and Reddit"
            ]
        },
        {
            "id": "task460-475b4bfe69f149f78758c7842d1447b5",
            "input": "In view of the differences between English and Chinese, we made adaptations for the PDTB-3 scheme such as removing AltLexC and adding Progression into our sense hierarchy. \n Question: How are resources adapted to properties of Chinese text?",
            "output": [
                "removing AltLexC and adding Progression into our sense hierarchy"
            ]
        },
        {
            "id": "task460-d536113e003349388e1f3a49c881a23b",
            "input": "To compare our results, we use the provided baseline, which is a non-parameter optimized linear-kernel SVM that uses TF-IDF bag-of-word vectors as inputs.  \n Question: What was the baseline model?",
            "output": [
                "a non-parameter optimized linear-kernel SVM that uses TF-IDF bag-of-word vectors as inputs"
            ]
        },
        {
            "id": "task460-ae122dc1678344a9aa8edeb6034ee400",
            "input": "Fourteen such feature extractors have been implemented which can be clubbed into 3 major categories:\n\n[noitemsep]\n\nLexicon Features\n\nWord Vectors\n\nSyntax Features \n Question: how many total combined features were there?",
            "output": [
                "Fourteen "
            ]
        },
        {
            "id": "task460-15e3723cca8548f0859c74b6d5f43c69",
            "input": "Further, FSDM has a new module called response slot binary classifier that adds extra supervision to generate the slots that will be present in the response more precisely before generating the final textual agent response (see Section \"Methodology\" for details). \n Question: How do slot binary classifiers improve performance?",
            "output": [
                "by adding extra supervision to generate the slots that will be present in the response"
            ]
        },
        {
            "id": "task460-6254758944e34549bd74a9a9a36eb91d",
            "input": "The words are represented as fixed length vectors INLINEFORM0 resulting from the concatenation of GloVe pre-trained embeddings and DepecheMood BIBREF19 lexicon representation. Since we cannot directly concatenate token-based embeddings (provided in GloVe) with the lemma#PoS-based representation available in DepecheMood, we proceeded to re-build the latter in token-based form, applying the exact same methodology albeit with two differences: we started from a larger dataset (51.9K news articles instead of 25.3K) and used a frequency cut-off, i.e. keeping only those tokens that appear at least 5 times in the corpus. \n Question: How do they incorporate lexicon into the neural network?",
            "output": [
                "concatenation of GloVe pre-trained embeddings and DepecheMood BIBREF19 lexicon representation cannot directly concatenate  re-build the latter in token-based form"
            ]
        },
        {
            "id": "task460-638fefbe3c9a4a7bb06cc02e48703983",
            "input": "We see that gated architectures almost always outperform recurrent, attention and linear models BoW, TFIDF, PV. This is largely because while training and testing on same domains, these models especially recurrent and attention based may perform better. However, for Domain Adaptation, as they lack gated structure which is trained in parallel to learn importance, their performance on target domain is poor as compared to gated architectures. As gated architectures are based on convolutions, they exploit parallelization to give significant boost in time complexity as compared to other models. The effectiveness of gated architectures rely on the idea of training a gate with sole purpose of identifying a weightage. In the task of sentiment analysis this weightage corresponds to what weights will lead to a decrement in final loss or in other words, most accurate prediction of sentiment. In doing so, the gate architecture learns which words or n-grams contribute to the sentiment the most, these words or n-grams often co-relate with domain independent words. On the other hand the gate gives less weightage to n-grams which are largely either specific to domain or function word chunks which contribute negligible to the overall sentiment. This is what makes gated architectures effective at Domain Adaptation. \n Question: Does the fact that GCNs can perform well on this tell us that the task is simpler than previously thought?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-53d7dcb1ce9c4304b1d17d3ade79003b",
            "input": "The grammar induction method works on the premise of curriculum learning BIBREF7 , where the parser first learns to parse simple sentences, then proceeds to learn more complex ones. The induction method is iterative, semi-automatic and based on frequent patterns. A context-free grammar (CFG) is induced from the text, which is represented by several layers of semantic annotations. \n Question: How did they induce the CFG?",
            "output": [
                "the parser first learns to parse simple sentences, then proceeds to learn more complex ones. The induction method is iterative, semi-automatic and based on frequent patterns"
            ]
        },
        {
            "id": "task460-1832b3d59ac74258ac92e540c6b48f79",
            "input": "Hot spots were originally annotated with 8 levels and degrees, ranging from `not hot' to `luke warm' to `hot +'. Every utterance was labeled with one of these discrete labels by a single annotator. Hightened involvement is rare, being marked on only 1% of utterances. \n Question: What annotations are available in ICSI meeting corpus?",
            "output": [
                "8 levels and degrees, ranging from `not hot' to `luke warm' to `hot +'. Every utterance was labeled with one of these discrete labels by a single annotator"
            ]
        },
        {
            "id": "task460-acf69bfe8b0d4e3384a3403713c6b7e0",
            "input": "We evaluate our approach by training an autocomplete system on 500K randomly sampled sentences from Yelp reviews BIBREF6 (see Appendix for details). We quantify the efficiency of a communication scheme $(q_{\\alpha },p_{\\beta })$ by the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords. The accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence. \n Question: How are models evaluated in this human-machine communication game?",
            "output": [
                "by training an autocomplete system on 500K randomly sampled sentences from Yelp reviews"
            ]
        },
        {
            "id": "task460-eae72d7d89ef4b54b692d712f8887905",
            "input": " We used Convolutional Neural Network (CNN) network whose structure is similar to the one proposed by BIBREF29.  \n Question: What neural architectures are used?",
            "output": [
                "Convolutional Neural Network (CNN)"
            ]
        },
        {
            "id": "task460-c6084751ae454b1094d5f3e26f8556de",
            "input": "The following different sense inventories have been used during the evaluation: Watlink, a word sense network constructed automatically. It uses the synsets induced in an unsupervised way by the Watset[CWnolog, MCL] method BIBREF2 and the semantic relations from such dictionaries as Wiktionary referred as Joint INLINEFORM0 Exp INLINEFORM1 SWN in Ustalov:17:dialogue. \n Question: Which corpus of synsets are used?",
            "output": [
                "Wiktionary"
            ]
        },
        {
            "id": "task460-29a78a410aa94c8f9ffd7135067b780e",
            "input": "While BERT-QC achieves large gains over existing methods on the ARC dataset, here we demonstrate that BERT-QC also matches state-of-the-art performance on TREC BIBREF6 , while surpassing state-of-the-art performance on the GARD corpus of consumer health questions BIBREF3 and MLBioMedLAT corpus of biomedical questions BIBREF4 . \n Question: Which datasets are used for evaluation?",
            "output": [
                "ARC  TREC GARD  MLBioMedLAT "
            ]
        },
        {
            "id": "task460-e5713151929249acb196031ff7894f24",
            "input": "To compare fairly, we train and evaluate these systems on the VLSP corpora. In particular, we conduct experiments on Viet Treebank corpus for POS tagging and chunking tasks, and on VLSP shared task 2016 corpus for NER task. All of these corpora are converted to CoNLL format. The corpus of POS tagging task consists of two columns namely word, and POS tag. For chunking task, there are three columns namely word, POS tag, and chunk in the corpus. The corpus of NER task consists of four columns. The order of these columns are word, POS tag, chunk, and named entity. While NER corpus has been separated into training and testing parts, the POS tagging and chunking data sets are not previously divided. For this reason, we use INLINEFORM0 of these data sets as a training set, and the remaining as a testing set. Because our system adopts early stopping method, we use INLINEFORM1 of these data sets from the training set as a development set when training NNVLP system. Table TABREF24 and Table TABREF25 shows the statistics of each corpus. \n Question: What datasets do they use for the tasks?",
            "output": [
                " Viet Treebank corpus for POS tagging and chunking tasks, and on VLSP shared task 2016 corpus for NER task"
            ]
        },
        {
            "id": "task460-febc80d7b1e949188b8607706b57a0ac",
            "input": "Stacked LSTMs Cell-aware Stacked LSTMs\nNow we extend the stacked LSTM formulation defined above to address the problem noted in the previous subsection. Sentence Encoders\nThe sentence encoder network we use in our experiments takes INLINEFORM0 words (assumed to be one-hot vectors) as input. Top-layer Classifiers\nFor the natural language inference experiments, we use the following heuristic function proposed by BIBREF36 in feature extraction: DISPLAYFORM0\n\nwhere INLINEFORM0 means vector concatenation, and INLINEFORM1 and INLINEFORM2 are applied element-wise. \n Question: Which models did they experiment with?",
            "output": [
                "Stacked LSTMs Cell-aware Stacked LSTMs Sentence Encoders Top-layer Classifiers"
            ]
        },
        {
            "id": "task460-2219de96c772474bb0cf5e6d2d51fc4b",
            "input": "While many focus on giving a vector representation of a word, an increasing number now exist that will give a vector representation of a entire sentence or text. Following on from this work, we seek to devise a system that can run online, performing text clustering on the embeddings of texts one at a time \n Question: How is an incoming claim used to retrieve similar factchecked claims?",
            "output": [
                "text clustering on the embeddings of texts"
            ]
        },
        {
            "id": "task460-9a6803f0a3de4ed79403377b15a39f06",
            "input": "Subsequently, an LSTM decoder generates the characters in the output word form using encoder states and an attention mechanism. \n Question: Does the model have attention?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-d8a28ee49153451b82fb8646b3779c4c",
            "input": "While neural machine translation (NMT) has achieved impressive performance in high-resource data conditions, becoming dominant in the field BIBREF0 , BIBREF1 , BIBREF2 , recent research has argued that these models are highly data-inefficient, and underperform phrase-based statistical machine translation (PBSMT) or unsupervised methods in low-data conditions BIBREF3 , BIBREF4 .  \n Question: what pitfalls are mentioned in the paper?",
            "output": [
                "highly data-inefficient underperform phrase-based statistical machine translation"
            ]
        },
        {
            "id": "task460-00186192426f4e79a4c514ebbcd6cbbd",
            "input": "We focus here on the Europarl domain, for which we have ample data in several languages, and use as in-domain training data the Europarl corpus BIBREF5 for two translation directions: English INLINEFORM0 German and English INLINEFORM1 French. When measuring out-of-domain performance, we will use the WMT newstest 2014. or French we use samples from News-Commentary-11 and Wikipedia from WMT 2014 shared translation task, as well as the Multi-UN BIBREF9 and EU-Bookshop BIBREF10 corpora. For German, we use samples from News-Commentary-11, Rapid, Common-Crawl (WMT 2017) and Multi-UN (see table TABREF5 ).  \n Question: what dataset is used?",
            "output": [
                "Europarl corpus  WMT newstest 2014 News-Commentary-11 Wikipedia from WMT 2014 Multi-UN EU-Bookshop Rapid Common-Crawl (WMT 2017)"
            ]
        },
        {
            "id": "task460-d3b933655a0847a6b8fbc695bea68b0f",
            "input": "Experiments ::: Baselines\nFor comparison, we select several public models as baselines including semantic parsing models: BiDAF BIBREF3, an MRC model which utilizes a bi-directional attention flow network to encode the question and passage;\n\nQANet BIBREF12, which utilizes convolutions and self-attentions as the building blocks of encoders to represent the question and passage;\n\nBERT BIBREF23, a pre-trained bidirectional Transformer-based language model which achieves state-of-the-art performance on lots of public MRC datasets recently;\n\nand numerical MRC models: NAQANet BIBREF6, a numerical version of QANet model.\n\nNAQANet+, an enhanced version of NAQANet implemented by ourselves, which further considers real number (e.g. “2.5”), richer arithmetic expression, data augmentation, etc. Syn Dep BIBREF6, the neural semantic parsing model (KDG) BIBREF22 with Stanford dependencies based sentence representations;\n\nOpenIE BIBREF6, KDG with open information extraction based sentence representations;\n\nSRL BIBREF6, KDG with semantic role labeling based sentence representations;\n\nand traditional MRC models: \n Question: what are the existing models they compared with?",
            "output": [
                "Syn Dep OpenIE SRL BiDAF QANet BERT NAQANet NAQANet+"
            ]
        },
        {
            "id": "task460-f4f28c92daff422b9139dde0bfe24334",
            "input": "Three baseline classification methods: Support Vector Machines (SVM), Adaboost, and Random Forests are adopted to evaluate our extracted features. On each dataset, the employed classifiers are trained with individual feature first, and then with the combination of the two features. \n Question: LDA is an unsupervised method; is this paper introducing an unsupervised approach to spam detection?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-4da0260828114a28850c4972b62ecaeb",
            "input": "Yet, we noticed a common misbehavior for all our multimodal models: if the attention loose track of the objects in the picture and \"gets lost\", the model still takes it into account and somehow overrides the information brought by the text-based annotations. The translation is then totally mislead.  \n Question: What misbehavior is identified?",
            "output": [
                "if the attention loose track of the objects in the picture and \"gets lost\", the model still takes it into account and somehow overrides the information brought by the text-based annotations"
            ]
        },
        {
            "id": "task460-40cbc249d4fa4ad59eda414a57db8a6e",
            "input": "This chapter describes the underlying formalism of the theorem provers, as it is visible during an interactive proof trace, and present the general strategy followed by the theorem provers.  The rest of this chapter is structured as follows. Section \"Type-logical grammars\" presents a general introduction to type-logical grammars and illustrates its basic concepts using the Lambek calculus, ending the section with some problems at the syntax-semantics interface for the Lambek calculus. Type-logical grammars are a family of grammar formalisms built on a foundation of logic and type theory. Type-logical grammars originated when BIBREF4 introduced his Syntactic Calculus (called the Lambek calculus, L, by later authors). Though Lambek built on the work of BIBREF5 , BIBREF6 and others, Lambek's main innovation was to cast the calculus as a logic, giving a sequent calculus and showing decidability by means of cut elimination. \n Question: What formalism does Grail use?",
            "output": [
                "a family of grammar formalisms built on a foundation of logic and type theory. Type-logical grammars originated when BIBREF4 introduced his Syntactic Calculus (called the Lambek calculus, L, by later authors)."
            ]
        },
        {
            "id": "task460-0d3aa667efeb455786997ec10a8ecead",
            "input": "$D_a$ contained all of the tweets collected on the attack day of the five attacks mentioned in section 4.2. And $D_b$ contained all of the tweets collected before the five attacks. There are 1180 tweets in $D_a$ and 7979 tweets in $D_b$. The tweets on the attack days ($D_a$) are manually annotated and only 50 percent of those tweets are actually about a DDoS attack. \n Question: Do twitter users tend to tweet about the DOS attack when it occurs? How much data supports this assumption?",
            "output": [
                "The dataset contains about 590 tweets about DDos attacks."
            ]
        },
        {
            "id": "task460-1373d067328140daac2517254c3dd9b5",
            "input": "The MP framework is based on the core idea of recursive neighborhood aggregation. That is, at every iteration, the representation of each vertex is updated based on messages received from its neighbors. All spectral GNNs can be described in terms of the MP framework.\n\nGNNs have been applied with great success to bioinformatics and social network data, for node classification, link prediction, and graph classification. However, a few studies only have focused on the application of the MP framework to representation learning on text. This paper proposes one such application. The concept of message passing over graphs has been around for many years BIBREF0, BIBREF1, as well as that of graph neural networks \n Question: What is the message passing framework?",
            "output": [
                "It is a framework used to describe algorithms for neural networks represented as graphs. Main idea is that that representation of each vertex is updated based on messages from its neighbors."
            ]
        },
        {
            "id": "task460-6e08238ef36742d8be6fc2b8175a7fc2",
            "input": "The approaches we use and what we mean by `success' are thus guided by our research questions.\n\nDomain experts and fellow researchers can provide feedback on questions and help with dynamically revising them. Sometimes we also hope to connect to multiple disciplines. Questions about potential “dual use” may also arise. \n Question: What approaches do they use towards text analysis?",
            "output": [
                "Domain experts and fellow researchers can provide feedback on questions and help with dynamically revising them. connect to multiple disciplines dual use"
            ]
        },
        {
            "id": "task460-e212b7bee951422ebfbb1e760d914c60",
            "input": "The main set of experiments uses a Linear Support Vector Machine (Joachims, 1998) to classify dialects using CxG features. This paper has used data-driven language mapping to select national dialects of English to be included in a global dialect identification model.  \n Question: What do the models that they compare predict?",
            "output": [
                "national dialects of English"
            ]
        },
        {
            "id": "task460-2600e3febe754cc88d31d4b3fa79a2d7",
            "input": "Overall, MMM has achieved a new SOTA, i.e., test accuracy of 88.9%, which exceeds the previous best by 16.9%. \n Question: How big are improvements of MMM over state of the art?",
            "output": [
                "test accuracy of 88.9%, which exceeds the previous best by 16.9%"
            ]
        },
        {
            "id": "task460-c4c106370ddf4dc2b24acdb581a3ae52",
            "input": "We evaluate the effectiveness of our LAN model on two typical knowledge graph completion tasks, i.e., link prediction and triplet classification. \n Question: Which knowledge graph completion tasks do they experiment with?",
            "output": [
                "link prediction  triplet classification"
            ]
        },
        {
            "id": "task460-a146c38cb01e4e69885726e8287be8fe",
            "input": "In this work, we develop a technique to rapidly transfer an existing pre-trained model from English to other languages in an energy efficient way BIBREF8.  \n Question: How much training data from the non-English language is used by the system?",
            "output": [
                "No data. Pretrained model is used."
            ]
        },
        {
            "id": "task460-3c333c4d17cc4953959561767661b518",
            "input": "To demonstrate our method in a black-box setting, we focus our experiments on Google's machine translation system (GMT), accessed through its Cloud API. \n Question: Which neural machine translation system is used?",
            "output": [
                "Google's machine translation system (GMT)"
            ]
        },
        {
            "id": "task460-8670ca13550a4dc1901d8754d71cb843",
            "input": "Text Representation.\nThe model, implemented as a deep neural network, learns to respond by training on hundreds of millions context-reply $(c,r)$ pairs. First, similar to Henderson:2017arxiv, raw text from both $c$ and $r$ is converted to unigrams and bigrams. Photo Representation.\nPhotos are represented using convolutional neural net (CNN) models pretrained on ImageNet BIBREF17. We use a MobileNet model with a depth multiplier of 1.4, and an input dimension of $224 \\times 224$ pixels as in BIBREF18. \n Question: How does PolyResponse architecture look like?",
            "output": [
                "Henderson:2017 MobileNet model"
            ]
        },
        {
            "id": "task460-10408992397c4648accd8e3af5fe4bd5",
            "input": "We modeled the relationship between word count and the two metrics of user engagement (overall rating, mean number of turns) in separate linear regressions.\n\nResults showed that users who, on average, produced utterances with more words gave significantly higher ratings ($\\beta $=0.01, SE=0.002, t=4.79, p$<$0.001)(see Figure 2) and engaged with Gunrock for significantly greater number of turns ($\\beta $=1.85, SE=0.05, t=35.58, p$<$0.001) (see Figure 2). These results can be interpreted as evidence for Gunrock's ability to handle complex sentences, where users are not constrained to simple responses to be understood and feel engaged in the conversation – and evidence that individuals are more satisfied with the conversation when they take a more active role, rather than the system dominating the dialog. On the other hand, another interpretation is that users who are more talkative may enjoy talking to the bot in general, and thus give higher ratings in tandem with higher average word counts. \n Question: How do they correlate user backstory queries to user satisfaction?",
            "output": [
                "modeled the relationship between word count and the two metrics of user engagement (overall rating, mean number of turns) in separate linear regressions"
            ]
        },
        {
            "id": "task460-ee2cf61cc82047d085b2fc5492abe059",
            "input": "The Zurich Cognitive Language Processing Corpus (ZuCo) 2.0, including raw and preprocessed eye-tracking and electroencephalography (EEG) data of 18 subjects, as well as the recording and preprocessing scripts, is publicly available. It contains physiological data of each subject reading 739 English sentences from Wikipedia (see example in Figure FIGREF1). During the recording session, the participants read 739 sentences that were selected from the Wikipedia corpus provided by culotta2006integrating.  \n Question: What kind of sentences were read?",
            "output": [
                "sentences that were selected from the Wikipedia corpus provided by culotta2006integrating"
            ]
        },
        {
            "id": "task460-31a93d38f4f446ff8ebac31348a24c92",
            "input": "We built a dataset for the task of hate speech detection in memes with 5,020 images that were weakly labeled into hate or non-hate memes, depending on their source. \n Question: How is each instance of the dataset annotated?",
            "output": [
                "weakly labeled into hate or non-hate memes, depending on their source"
            ]
        },
        {
            "id": "task460-a0ffc20e7432479781712b9c01c65e0f",
            "input": "BLEU: We use the Bilingual Evaluation Understudy (BLEU) BIBREF34 metric which is commonly used in machine translation tasks. The BLEU metric can be used to evaluate dialogue generation models as in BIBREF5, BIBREF35. The BLEU metric is a word-overlap metric which computes the co-occurrence of N-grams in the reference and the generated response and also applies the brevity penalty which tries to penalize far too short responses which are usually not desired in task-oriented chatbots. We compute the BLEU score using all generated responses of our systems.\n\nPer-turn Accuracy: Per-turn accuracy measures the similarity of the system generated response versus the target response. Eric and Manning eric2017copy used this metric to evaluate their systems in which they considered their response to be correct if all tokens in the system generated response matched the corresponding token in the target response. This metric is a little bit harsh, and the results may be low since all the tokens in the generated response have to be exactly in the same position as in the target response.\n\nPer-Dialogue Accuracy: We calculate per-dialogue accuracy as used in BIBREF8, BIBREF5. For this metric, we consider all the system generated responses and compare them to the target responses. A dialogue is considered to be true if all the turns in the system generated responses match the corresponding turns in the target responses. Note that this is a very strict metric in which all the utterances in the dialogue should be the same as the target and in the right order.\n\nF1-Entity Score: Datasets used in task-oriented chores have a set of entities which represent user preferences. For example, in the restaurant domain chatbots common entities are meal, restaurant name, date, time and the number of people (these are usually the required entities which are crucial for making reservations, but there could be optional entities such as location or rating). Each target response has a set of entities which the system asks or informs the user about. Our models have to be able to discern these specific entities and inject them into the generated response. To evaluate our models we could use named-entity recognition evaluation metrics BIBREF36. The F1 score is the most commonly used metric used for the evaluation of named-entity recognition models which is the harmonic average of precision and recall of the model. We calculate this metric by micro-averaging over all the system generated responses. \n Question: Is human evaluation performed?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-fc51673b023f4246b082f465549a7950",
            "input": "Among all available parameters to tune the D2V algorithm released by Gensim, six of them were selected for optimisation BIBREF14. The window_size parameter affects the size of the sliding window used to parse texts. The alpha parameter represents the learning rate of the network. The sample setting allows the model to reduce the importance given to high-frequency words. The dm parameter defines the training used architecture (PV-DM or PV-DBOW).  \n Question: What Doc2Vec architectures other than PV-DBOW have been tried?",
            "output": [
                "PV-DM"
            ]
        },
        {
            "id": "task460-91665ff193de4617b91a6dc99c0f7f14",
            "input": "To make the annotated data publicly available, we selected 70 news articles from Arabic WikiNews site. These articles cover recent news from year 2013 to year 2015 in multiple genres (politics, economics, health, science and technology, sports, arts, and culture.) Articles contain 18,300 words, and they are evenly distributed among these 7 genres with 10 articles per each. \n Question: Where did they collect their dataset from?",
            "output": [
                "from Arabic WikiNews site"
            ]
        },
        {
            "id": "task460-6335633e15964bc6b4eac62b8edcc447",
            "input": "The input for both tasks consists of news articles in free-text format, collected from 36 propagandist and 12 non-propagandist news outlets and then annotated by professional annotators.  \n Question: What dataset was used?",
            "output": [
                " news articles in free-text format"
            ]
        },
        {
            "id": "task460-b18b9056b15d437e825ce67b4587e949",
            "input": "In a recent work on sentiment analysis in Turkish BIBREF10, they learn embeddings using Turkish social media. They use the word2vec algorithm, create several unsupervised hand-crafted features, generate document vectors and feed them as input into the support vector machines (SVM) approach. We outperform this baseline approach using more effective word embeddings and supervised hand-crafted features. \n Question: What baseline method is used?",
            "output": [
                "using word2vec to create features that are used as input to the SVM"
            ]
        },
        {
            "id": "task460-a99090ece1e64bff99bb825d550932aa",
            "input": "The high nearest neighbours accuracy indicates that syntax information was successfully captured by the embeddings. \n Question: Do they do quantitative quality analysis of learned embeddings?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-44d0e934e91849d88c286b071013d02c",
            "input": "(Information Retrieval). This baseline has been successfully used for related tasks like Question Answering BIBREF39 . We create two versions of this baseline: one with the pool of perspectives INLINEFORM0 and one with the pool of evidences INLINEFORM1 . \n Question: Which machine baselines are used?",
            "output": [
                "Information Retrieval"
            ]
        },
        {
            "id": "task460-9df96dd6ea444f4fa2877f2be78496ac",
            "input": "One of the several formats into which FHIR can be serialized is RDF. However, because RDF was designed as an abstract information model and FHIR was designed for operational use in a healthcare setting, there is the potential for a slight mismatch between the models. This comes up in two ways: One, RDF makes statements of fact, whereas FHIR makes records of events. Two, RDF is intended to have the property of monotonicity, meaning that previous facts cannot be invalidated by new facts. \n Question: How are FHIR and RDF combined?",
            "output": [
                "RDF was designed as an abstract information model and FHIR was designed for operational use in a healthcare setting RDF makes statements of fact, whereas FHIR makes records of events RDF is intended to have the property of monotonicity, meaning that previous facts cannot be invalidated by new facts"
            ]
        },
        {
            "id": "task460-21875071800346b78cd6667aebcbb702",
            "input": "We use the following cheap ways to generate pseudo-source texts:\n\ncopy: in this setting, the source side is a mere copy of the target-side data. copy-marked: another way to integrate copies without having to deal with OOVs is to augment the source vocabulary with a copy of the target vocabulary.  copy-dummies: instead of using actual copies, we replace each word with “dummy” tokens.  \n Question: what data simulation techniques were introduced?",
            "output": [
                "copy copy-marked copy-dummies"
            ]
        },
        {
            "id": "task460-074cc7654af342a2821c0202900fa169",
            "input": "Our experiments on a goal-oriented dialog corpus, the personalized bAbI dialog dataset, show that leveraging personal information can significantly improve the performance of dialog systems.  \n Question: What datasets did they use?",
            "output": [
                "the personalized bAbI dialog dataset"
            ]
        },
        {
            "id": "task460-deddb36898194a6fbcc6e3db58cc9c5c",
            "input": "These workers completed 1001 tasks: 496 tasks in the control and 505 in the AUI.  \n Question: How many responses did they obtain?",
            "output": [
                "1001"
            ]
        },
        {
            "id": "task460-181c39401ca64c40b58e6e4db613666b",
            "input": "To compare with our model, we select a number of strong summarization models as baseline systems. $\\textsc {Lead-X}$ uses the top $X$ sentences as a summary BIBREF19. $\\textsc {DRM}$ BIBREF10 leverages deep reinforcement learning for summarization. $\\textsc {TConvS2S}$ BIBREF2 is based on convolutional neural networks. $\\textsc {BottomUp}$ BIBREF11 uses a bottom-up approach to generate summarization. ABS BIBREF26 uses neural attention for summary generation. DRGD BIBREF27 is based on a deep recurrent generative decoder. To compare with our pretrain-only model, we include several unsupervised abstractive baselines: SEQ$^3$ BIBREF28 employs the reconstruction loss and topic loss for summarization. BottleSum BIBREF23 leverages unsupervised extractive and self-supervised abstractive methods. GPT-2 BIBREF7 is a large-scaled pretrained language model which can be directly used to generate summaries. \n Question: What were the baselines?",
            "output": [
                "$\\textsc {Lead-X}$ $\\textsc {PTGen}$ $\\textsc {DRM}$ $\\textsc {TConvS2S}$  $\\textsc {BottomUp}$ ABS DRGD SEQ$^3$ BottleSum GPT-2"
            ]
        },
        {
            "id": "task460-4d42eda93d7740848bf541d3d6233ce7",
            "input": "We focus here on the Europarl domain, for which we have ample data in several languages, and use as in-domain training data the Europarl corpus BIBREF5 for two translation directions: English INLINEFORM0 German and English INLINEFORM1 French.  \n Question: what language is the data in?",
            "output": [
                "English  German French"
            ]
        },
        {
            "id": "task460-f58aeb35929e496897d0919934a2ab5d",
            "input": "We chose SVMhmm BIBREF111 implementation of Structural Support Vector Machines for sequence labeling. SVM implementation of   \n Question: Which machine learning methods are used in experiments?",
            "output": [
                "Structural Support Vector Machine"
            ]
        },
        {
            "id": "task460-2a9fc41351a5423590cbc40439b5ff5e",
            "input": "This results in three additional vectors corresponding to INLINEFORM3 , INLINEFORM4 and INLINEFORM5 difference vectors.\n\nResults \n Question: How are the EAU text spans annotated?",
            "output": [
                "Answer with content missing: (Data and pre-processing section) The data is suited for our experiments because the annotators were explicitly asked to provide annotations on a clausal level."
            ]
        },
        {
            "id": "task460-54a82edd00f0413fb7d3386508182486",
            "input": "Our first baseline is ROUGE-L BIBREF1 , since it is the most commonly used metric for compression tasks. We compare to the best n-gram-overlap metrics from toutanova2016dataset; We further compare to the negative LM cross-entropy, i.e., the log-probability which is only normalized by sentence length. Our next baseline is perplexity, which corresponds to the exponentiated cross-entropy: Due to its popularity, we also performed initial experiments with BLEU BIBREF17 .  \n Question: Is ROUGE their only baseline?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-68273022df8e4303aafc66192935f750",
            "input": "We compare our method to the following baselines: (1) Single-task CNN: training a CNN model for each task individually; (2) Single-task FastText: training one FastText model BIBREF23 with fixed embeddings for each individual task; (3) Fine-tuned the holistic MTL-CNN: a standard transfer-learning approach, which trains one MTL-CNN model on all the training tasks offline, then fine-tunes the classifier layer (i.e. $\\mathrm {M}^{(cls)}$ Figure 1 (a)) on each target task; (4) Matching Network: a metric-learning based few-shot learning model trained on all training tasks; (5) Prototypical Network: a variation of matching network with different prediction function as Eq. 9 ; (6) Convex combining all single-task models: training one CNN classifier on each meta-training task individually and taking the encoder, then for each target task training a linear combination of all the above single-task encoders with Eq. ( 24 ).  \n Question: Do they compare with the MAML algorithm?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-76e27272ce764d218e20516bfad5ed58",
            "input": "In the experiments, we compare model performance between GluonCV/NLP and other open source implementations with Caffe, Caffe2, Theano, and TensorFlow, including ResNet BIBREF8 and MobileNet BIBREF9 for image classification (ImageNet), Faster R-CNN BIBREF10 for object detection (COCO), Mask R-CNN BIBREF11 for instance segmentation, Simple Pose BIBREF12 for pose estimation (COCO), textCNN BIBREF13 for sentiment analysis (TREC), and BERT BIBREF14 for question answering (SQuAD 1.1), sentiment analysis (SST-2), natural langauge inference (MNLI-m), and paraphrasing (MRPC). \n Question: Do they experiment with the toolkits?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-80b67cf99cba451099eac9fb3a92ba59",
            "input": "We used two datasets for the task - AMR Bank BIBREF10 and CNN-Dailymail ( BIBREF11 BIBREF12 ).  \n Question: What dataset is used in this paper?",
            "output": [
                "AMR Bank CNN-Dailymail"
            ]
        },
        {
            "id": "task460-b72eee2ecb184dcb8c086dad2d0db65f",
            "input": "For both POS tagging and dependency parsing, we run experiments on the Wall Street Journal (WSJ) portion of the Penn Treebank. \n Question: Do they evaluate only on English datasets?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-f276bc7d7e744b219c5e3e4f52c564ae",
            "input": "The first approach is based on hierarchical modeling BIBREF13 , which assumes that the group-specific embedding representations are tied through a global embedding. \n Question: What hierarchical modelling approach is used?",
            "output": [
                "the group-specific embedding representations are tied through a global embedding"
            ]
        },
        {
            "id": "task460-15394363f1fc40a1a91ad6305c2133f6",
            "input": "To deal with negative values, we propose clipped $\\mathit {PMI}$,\n\nwhich is equivalent to $\\mathit {PPMI}$ when $z = 0$. In practice we find this works poorly if done symmetrically, so we introduce a variant called $\\mathit {NNEGPMI}$ which only normalizes $\\mathit {\\texttt {-}PMI}$: \n Question: What novel PMI variants are introduced?",
            "output": [
                "clipped PMI; NNEGPMI"
            ]
        },
        {
            "id": "task460-013a0d358e2d466f804c95d93f4a7bcd",
            "input": "For the evaluation of performance of the proposed method on the NLI task, SNLI BIBREF22 and MultiNLI BIBREF23 datasets are used. We use Quora Question Pairs dataset BIBREF24 in evaluating the performance of our method on the PI task. In evaluating sentiment classification performance, the Stanford Sentiment Treebank (SST) BIBREF25 is used. \n Question: Which datasets were used?",
            "output": [
                "SNLI BIBREF22 and MultiNLI BIBREF23 Quora Question Pairs dataset BIBREF24  Stanford Sentiment Treebank (SST) BIBREF25"
            ]
        },
        {
            "id": "task460-508b9649eb36461c8bec3a14caaad8fd",
            "input": "hierarchical In this paper, we work with the dataset of the 2019 GermEval shared task on hierarchical text classification BIBREF0 and use the predefined set of labels to evaluate our approach to this classification task. \n Question: What dataset do they use?",
            "output": [
                "2019 GermEval shared task on hierarchical text classification"
            ]
        },
        {
            "id": "task460-dfb7c7855b3b45038091f36e012e00c5",
            "input": "The test-set accuracies obtained by different learning methods, including the current state-of-the-art results, are presented in Table TABREF11 . \n Question: what models did they compare to?",
            "output": [
                "High-order CNN, Tree-LSTM, DRNN, DCNN, CNN-MC, NBoW and SVM "
            ]
        },
        {
            "id": "task460-0b3a8c0319fb430886349646281a27e8",
            "input": "For dataset collection, we utilize the headlines collected in qin2018automatic, lin2019learning from Tencent News, one of the most popular Chinese news websites, as the positive samples. We follow the same data split as the original paper. As some of the links are not available any more, we get 170,754 training samples and 4,511 validation samples. For the negative training samples collection, we randomly select generated headlines from a pointer generator BIBREF0 model trained on LCSTS dataset BIBREF5 and create a balanced training corpus which includes 351,508 training samples and 9,022 validation samples. To evaluate our trained classifier, we construct a test set by randomly sampling 100 headlines from the test split of LCSTS dataset and the labels are obtained by 11 human annotators. We use LCSTS BIBREF5 as our dataset to train the summarization model. The dataset is collected from the Chinese microblogging website Sina Weibo. It contains over 2 million Chinese short texts with corresponding headlines given by the author of each text. The dataset is split into 2,400,591 samples for training, 10,666 samples for validation and 725 samples for testing. We tokenize each sentence with Jieba and a vocabulary size of 50000 is saved. \n Question: Did they used dataset from another domain for evaluation?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-5038db7074724c76953a0184ce10e4f9",
            "input": "Following BIBREF3, we hypothesize that a sentence representation in mBERT is composed of a language-specific component, which identifies the language of the sentence, and a language-neutral component, which captures the meaning of the sentence in a language-independent way. We assume that the language-specific component is similar across all sentences in the language.\n\nWe thus try to remove the language-specific information from the representations by centering the representations of sentences in each language so that their average lies at the origin of the vector space. We then analyze the semantic properties of both the original and the centered representations using a range of probing tasks. \n Question: How they show that mBERT representations can be split into a language-specific component and a language-neutral component?",
            "output": [
                "We thus try to remove the language-specific information from the representations by centering the representations of sentences in each language so that their average lies at the origin of the vector space."
            ]
        },
        {
            "id": "task460-16d281ea534a49f3a05231922f594074",
            "input": " We draw our data from news publications, wine reviews, and Reddit, which in addition to large volume, also let us characterize binomials in new ways, and analyze differences in binomial orderings across communities and over time.  \n Question: What online text resources are used to test binomial lists?",
            "output": [
                "news publications, wine reviews, and Reddit"
            ]
        },
        {
            "id": "task460-fbd393221b0944a988b2936a93825d1e",
            "input": "Wav2vec BIBREF22 proposed a multi-layer convolutional neural network optimized via a noise contrastive binary classification and was applied to WSJ ASR tasks. Our experiments consisted of three different setups: 1) a fully-supervised system using all labeled data; 2) an SSL system using wav2vec features; 3) an SSL system using our proposed DeCoAR features. All models used were based on deep BLSTMs with the CTC loss criterion. \n Question: What are baseline models on WSJ eval92 and LibriSpeech test-clean?",
            "output": [
                "Wav2vec BIBREF22 a fully-supervised system using all labeled data"
            ]
        },
        {
            "id": "task460-af6df604566445168ccebdda83c07d83",
            "input": "Within ICU notes (e.g., text example in top-left box in Figure 2), we first identify all abbreviations using regular expressions and then try to find all possible expansions of these abbreviations from domain-specific knowledge base as candidates. \n Question: Do they use any knowledge base to expand abbreviations?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-817445d30513452a885527130e70cc53",
            "input": "We train Word2Vec with vector size $d_\\mathrm {W2V} = d_\\mathrm {LM} = 768$ on PubMed+PMC (see Appendix for details). Then, we follow the procedure described in Section SECREF3 to update the wordpiece embedding layer and tokenizer of general-domain BERT. \n Question: What in-domain text did they use?",
            "output": [
                "PubMed+PMC"
            ]
        },
        {
            "id": "task460-03b16a02102f41be921785e35f2b7554",
            "input": "In our final system, after pre-training the forward and backward LMs separately, we remove the top layer softmax and concatenate the forward and backward LM embeddings to form bidirectional LM embeddings, i.e., INLINEFORM0 .  \n Question: how are the bidirectional lms obtained?",
            "output": [
                "They pre-train forward and backward LMs separately, remove top layer softmax, and concatenate to obtain the bidirectional LMs."
            ]
        },
        {
            "id": "task460-b38eb9d5e85b46ebbeaedf5db9c10272",
            "input": " In the pyramid scoring, the content units in the gold human written summaries are organized in a pyramid. In this pyramid, the content units are organized in tiers and higher tiers of the pyramid indicate higher importance.  \n Question: What manual Pyramid scores are used?",
            "output": [
                " higher tiers of the pyramid"
            ]
        },
        {
            "id": "task460-34777860a1c04126b24fb2f2151486cf",
            "input": "In our methodology to design new evaluation metrics for comparing reference summaries/translations to hypothesis ones, we established first-principles criteria on what a good evaluator should do. The first one is that it should be highly correlated with human judgement of similarity. The second one is that it should be able to distinguish sentences which are in logical contradiction, logically unrelated or in logical agreement. The third one is that a robust evaluator should also be able to identify unintelligible sentences. The last criteria is that a good evaluation metric should not give high scores to semantically distant sentences and low scores to semantically related sentences. \n Question: What is the criteria for a good metric?",
            "output": [
                "The first one is that it should be highly correlated with human judgement of similarity. The second one is that it should be able to distinguish sentences which are in logical contradiction, logically unrelated or in logical agreement. The third one is that a robust evaluator should also be able to identify unintelligible sentences. The last criteria is that a good evaluation metric should not give high scores to semantically distant sentences and low scores to semantically related sentences."
            ]
        },
        {
            "id": "task460-878d9d427c2f4aee954a154b7e2d005a",
            "input": "As pointed out in the section “sec:coalitionpolicy”, there is a strong separation in two blocks between supporters and opponents of European integration, which is even more clearly observed in Fig FIGREF42 B. In the area of State and Evolution of the Union we again observe a strong division in two blocks (see Fig FIGREF42 E). This is different to the Economic and monetary system, however, where we observe a far-left and far-right cooperation, where the division is along the traditional left-right axis.\n\nThe patterns of coalitions forming on Twitter closely resemble those in the European Parliament. \n Question: Does the analysis find that coalitions are formed in the same way for different policy areas?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-7e8024c2adef4e208567263710c00b5b",
            "input": "We construct the embeddings for each document using BERT models finetuned on MLDoc. We mean-pool each document embedding to create a single vector per document. We then calculate the cosine similarity between the embeddings for the English document and its translation. In Table TABREF21 , we observe that the median cosine similarity increases dramatically with adversarial training, which suggests that the embeddings became more language-independent. \n Question: How do they quantify alignment between the embeddings of a document and its translation?",
            "output": [
                "median cosine similarity"
            ]
        },
        {
            "id": "task460-93a280ca26a54df482d78ddd89a1a4d8",
            "input": "The real-time tweets scores were calculated in the same way as the historical data and summed up for a minute and sent to the machine learning model with the Bitcoin price in the previous minute and the rolling average price. It predicted the next minute's Bitcoin price from the given data. After the actual price arrived, the RMS value was calculated and the machine learning model updated itself to predict with better understanding the next value. \n Question: What experimental evaluation is used?",
            "output": [
                "root mean square error between the actual and the predicted price of Bitcoin for every minute"
            ]
        },
        {
            "id": "task460-7090548f83fd4c1998c47d58037cc228",
            "input": "A group of 50 native people who were well-versed in both English and Tamil languages acted as annotators for the evaluation. A collection of samples of about 100 sentences were taken from the test set results for comparison. This set included a randomized selection of the translation results to ensure the objectivity of evaluation. Fluency and adequacy results for the RNNMorph results are tabulated. Adequacy rating was calculated on a 5-point scale of how much of the meaning is conveyed by the translation (All, Most, Much, Little, None). The fluency rating was calculated based on grammatical correctness on a 5-point scale of (Flawless, Good, Non-native, Disfluent, Incomprehensive). For the comparison process, the RNNMorph and the RNNSearch + Word2Vec models’ sentence level translations were individually ranked between each other, permitting the two translations to have ties in the ranking. \n Question: How were the human judgements assembled?",
            "output": [
                "50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale."
            ]
        },
        {
            "id": "task460-f92f17ad468549d2890f48d4a42a6fda",
            "input": "To ground moral sentiment in text, we leverage the Moral Foundations Dictionary BIBREF27. The MFD is a psycholinguistic resource that associates each MFT category with a set of seed words, which are words that provide evidence for the corresponding moral category in text. To implement the first tier of our framework and detect moral relevance, we complement our morally relevant seed words with a corresponding set of seed words approximating moral irrelevance based on the notion of valence, i.e., the degree of pleasantness or unpleasantness of a stimulus. We refer to the emotional valence ratings collected by BIBREF28 for approximately 14,000 English words, and choose the words with most neutral valence rating that do not occur in the MFD as our set of morally irrelevant seed words, for an equal total number of morally relevant and morally irrelevant words. We divide historical time into decade-long bins, and use two sets of embeddings provided by BIBREF30, each trained on a different historical corpus of English:\n\nGoogle N-grams BIBREF31: a corpus of $8.5 \\times 10^{11}$ tokens collected from the English literature (Google Books, all-genres) spanning the period 1800–1999.\n\nCOHA BIBREF32: a smaller corpus of $4.1 \\times 10^8$ tokens from works selected so as to be genre-balanced and representative of American English in the period 1810–2009. \n Question: Which datasets are used in the paper?",
            "output": [
                "Google N-grams\nCOHA\nMoral Foundations Dictionary (MFD)\n"
            ]
        },
        {
            "id": "task460-e78af095b8ac4b4a982e642870be15e5",
            "input": "To generate it, we explore two implicit user-feedback labelling strategies: five-minute reuse and one-day return.  \n Question: What feedback labels are used?",
            "output": [
                "five-minute reuse and one-day return"
            ]
        },
        {
            "id": "task460-6925e6afd58b45bea1ac1d77bf9f4c49",
            "input": "The parameters of the entire MSD (auxiliary-task) decoder are shared across languages.\n\nSince a grouping of the languages based on language family would have left several languages in single-member groups (e.g. Russian is the sole representative of the Slavic family), we experiment with random groupings of two to three languages. Multilingual training is performed by randomly alternating between languages for every new minibatch. \n Question: How do they perform multilingual training?",
            "output": [
                "Multilingual training is performed by randomly alternating between languages for every new minibatch"
            ]
        },
        {
            "id": "task460-326f509f416b4899bde56c69452170c9",
            "input": "We used Amazon Mechanical Turk (MTurk) to collect new labels and explanations for SNLI-VE. 2,060 workers participated in the annotation effort, with an average of 1.98 assignments per worker and a standard deviation of 5.54. \n Question: How many annotators are used to write natural language explanations to SNLI-VE-2.0?",
            "output": [
                "2,060 workers"
            ]
        },
        {
            "id": "task460-e9a78e8451514252b1d5eb39b2530b02",
            "input": "Violations The most challenging features are all related to Violations. Low performance on Infl/Agr Violations, which marks morphological violations (He washed yourself, This is happy), is especially striking because a relatively high proportion (29%) of these sentences are Simple. These models are likely to be deficient in encoding morphological features is that they are word level models, and do not have direct access sub-word information like inflectional endings, which indicates that these features are difficult to learn effectively purely from lexical distributions. \n Question: Do the authors have a hypothesis as to why morphological agreement is hardly learned by any model?",
            "output": [
                "These models are likely to be deficient in encoding morphological features is that they are word level models, and do not have direct access sub-word information like inflectional endings, which indicates that these features are difficult to learn effectively purely from lexical distributions."
            ]
        },
        {
            "id": "task460-a69e705a58394ca38bdd91b426f3919f",
            "input": "It has been shown that one can significantly increase the semantic information carried by a NER system when we successfully linking entities from a deep learning method to the related entities from a knowledge base BIBREF26 , BIBREF27 . Redirection: For the Wikidata linking element, we recognize that the lookup will be constrained by the most common lookup name for each entity.  \n Question: How do they combine a deep learning model with a knowledge base?",
            "output": [
                "Entities from a deep learning model are linked to the related entities from a knowledge base by a lookup."
            ]
        },
        {
            "id": "task460-60dd40caffc54d1fa7de77ed31ca64a1",
            "input": "Since the present study focuses on Spanish tweets, all tweets from the English datasets were translated into Spanish.  \n Question: What other languages did they translate the data from?",
            "output": [
                "English "
            ]
        },
        {
            "id": "task460-ed51d5e653a841fbbfe1072ad1fcf3e2",
            "input": "Our second submission obtains 82.4 $F_1$ on the official test set, and ranks $4th$ on this shared task. \n Question: Where did this model place in the final evaluation of the shared task?",
            "output": [
                "$4th$"
            ]
        },
        {
            "id": "task460-bf366b6c165b4387b130fcc7fc407bb4",
            "input": "Conditional Random Fields\nConditional Random Fields (CRF) BIBREF10 are a standard approach when dealing with sequential data in the context of sequence labeling. BiLSTM-CRF\nPrior to the emergence of deep neural language models, BiLSTM-CRF models BIBREF11 had achieved state-of-the-art results for the task of sequence labeling. Multi-Task Learning\nMulti-Task Learning (MTL) BIBREF15 has become popular with the progress in deep learning. BioBERT\nDeep neural language models have recently evolved to a successful method for representing text. In particular, Bidirectional Encoder Representations from Transformers (BERT) outperformed previous state-of-the-art methods by a large margin on various NLP tasks BIBREF17. \n Question: What baseline systems are proposed?",
            "output": [
                "Conditional Random Fields BiLSTM-CRF Multi-Task Learning BioBERT"
            ]
        },
        {
            "id": "task460-0f63b0f02a0c4be2ae473cb1a2113be3",
            "input": "Vanilla ST baseline: The vanilla ST BIBREF9 has only a speech encoder and a decoder. It is trained from scratch on the ST-TED corpus.\n\nPre-training baselines: We conduct three pre-training baseline experiments: 1) encoder pre-training, in which the ST encoder is initialized from an ASR model; 2) decoder pre-training, in which the ST decoder is initialized from an MT model; and 3) encoder-decoder pre-training, where both the encoder and decoder are pre-trained. The ASR model has the same architecture with vanilla ST model, trained on the mixture of ST-TED and TED-LIUM2 corpus. The MT model has a text encoder and decoder with the same architecture of which in TCEN. It is first trained on WMT data (out-of-domain) and then fine-tuned on in-domain data.\n\nMulti-task baselines: We also conduct three multi-task baseline experiments including one-to-many setting, many-to-one setting, and many-to-many setting. In the first two settings, we train the model with $\\alpha _{st}=0.75$ while $\\alpha _{asr}=0.25$ or $\\alpha _{mt}=0.25$. For many-to-many setting, we use $\\alpha _{st}=0.6, \\alpha _{asr}=0.2$ and $\\alpha _{mt}=0.2$.. For MT task, we use only in-domain data.\n\nMany-to-many+pre-training: We train a many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models. Triangle+pre-train: BIBREF18 DBLP:conf/naacl/AnastasopoulosC18 proposed a triangle multi-task strategy for speech translation. Their model solves the subnet waste issue by concatenating an ST decoder to an ASR encoder-decoder model. Notably, their ST decoder can consume representations from the speech encoder as well as the ASR decoder. For a fair comparison, the speech encoder and the ASR decoder are initialized from the pre-trained ASR model. The Triangle model is fine-tuned under their multi-task manner. \n Question: What are the baselines?",
            "output": [
                "Vanilla ST baseline encoder pre-training, in which the ST encoder is initialized from an ASR model decoder pre-training, in which the ST decoder is initialized from an MT model encoder-decoder pre-training, where both the encoder and decoder are pre-trained many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models Triangle+pre-train: BIBREF18 DBLP:conf/naacl/AnastasopoulosC18 proposed a triangle multi-task strategy for speech translation"
            ]
        },
        {
            "id": "task460-476b5a819a37423b8d43beeb9e65e294",
            "input": "We collected Japanese-Vietnamese parallel data from TED talks extracted from WIT3's corpus BIBREF15 .  \n Question: did they collect their own data?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-e77e63cc2d5140029062f99d6479c56d",
            "input": "The DeepMine database consists of three parts. The first one contains fixed common phrases to perform text-dependent speaker verification. The second part consists of random sequences of words useful for text-prompted speaker verification, and the last part includes phrases with word- and phoneme-level transcription, useful for text-independent speaker verification using a random phrase (similar to Part4 of RedDots). DeepMine Database Parts ::: Part1 - Text-dependent (TD)\nThis part contains a set of fixed phrases which are used to verify speakers in text-dependent mode. Each speaker utters 5 Persian phrases, and if the speaker can read English, 5 phrases selected from Part1 of the RedDots database are also recorded.\n\nWe have created three experimental setups with different numbers of speakers in the evaluation set. Similar to the text-dependent case, three experimental setups with different number of speaker in the evaluation set are defined (corresponding to the rows in Table TABREF16). However, different strategy is used for defining trials: Depending on the enrollment condition (1- to 3-sess), trials are enrolled on utterances of all words from 1 to 3 different sessions (i.e. 3 to 9 utterances). Further, we consider two conditions for test utterances: seq test utterance with only 3 or 4 words and full test utterances with all words (i.e. same words as in enrollment but in different order). Based on the recording sessions, we created two experimental setups for speaker verification. In the first one, respondents with at least 17 recording sessions are included to the evaluation set, respondents with 16 sessions to the development and the rest of respondents to the background set (can be used as training data). In the second setup, respondents with at least 8 sessions are included to the evaluation set, respondents with 6 or 7 sessions to the development and the rest of respondents to the background set.  two experimental setups for speaker verification \n Question: what evaluation protocols are provided?",
            "output": [
                "three experimental setups with different numbers of speakers in the evaluation set three experimental setups with different number of speaker in the evaluation set are defined  first one, respondents with at least 17 recording sessions are included to the evaluation set, respondents with 16 sessions to the development and the rest of respondents to the background set second setup, respondents with at least 8 sessions are included to the evaluation set, respondents with 6 or 7 sessions to the development and the rest of respondents to the background set"
            ]
        },
        {
            "id": "task460-45afe3f03ca64a7a885ff022974cb28b",
            "input": "To better demonstrate the effectiveness of the proposed model, we compare with baselines and show the results in Table TABREF12 . \n Question: What was the score of the proposed model?",
            "output": [
                "Best results authors obtain is EM 51.10 and F1 63.11"
            ]
        },
        {
            "id": "task460-328e741e04ae4406a44a193fd1dae9c7",
            "input": "In this section, we present the experimental setting for this task\n\nCorpus: SEAME (South East Asia Mandarin-English), a conversational Mandarin-English code-switching speech corpus consists of spontaneously spoken interviews and conversations BIBREF8 .  \n Question: What languages are explored in the work?",
            "output": [
                "Mandarin English"
            ]
        },
        {
            "id": "task460-841294276adc4865ba1b5ccb2fc6c0cc",
            "input": "We extend the Amazon Conversational Bot Toolkit (CoBot) BIBREF6 which is a flexible event-driven framework. CoBot provides ASR results and natural language processing pipelines through the Alexa Skills Kit (ASK) BIBREF7. Gunrock corrects ASR according to the context (asr) and creates a natural language understanding (NLU) (nlu) module where multiple components analyze the user utterances. A dialog manager (DM) (dm) uses features from NLU to select topic dialog modules and defines an individual dialog flow. Each dialog module leverages several knowledge bases (knowledge). Then a natural language generation (NLG) (nlg) module generates a corresponding response. While we provide an overview of the system in the following sections, for detailed system implementation details, please see the technical report BIBREF1. \n Question: Do they specify the model they use for Gunrock?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-29f5fc8b69bf4fac8856f457ccbe1ba3",
            "input": "Comparing the natural and artificial sources of our parallel data wrt. several linguistic and distributional properties, we observe that (see Fig. FIGREF21 - FIGREF22 ):\n\nartificial sources are on average shorter than natural ones: when using BT, cases where the source is shorter than the target are rarer; cases when they have the same length are more frequent.\n\nautomatic word alignments between artificial sources tend to be more monotonic than when using natural sources, as measured by the average Kendall INLINEFORM0 of source-target alignments BIBREF22 : for French-English the respective numbers are 0.048 (natural) and 0.018 (artificial); for German-English 0.068 and 0.053.  The intuition is that properties (i) and (ii) should help translation as compared to natural source, while property (iv) should be detrimental. \n Question: what is their explanation for the effectiveness of back-translation?",
            "output": [
                "when using BT, cases where the source is shorter than the target are rarer; cases when they have the same length are more frequent automatic word alignments between artificial sources tend to be more monotonic than when using natural sources"
            ]
        },
        {
            "id": "task460-f1e7e9fc6ea54b1f9833d5ff59e40e95",
            "input": "Datasets\nFor training English-Hindi NMT systems, we use the IITB English-Hindi parallel corpus BIBREF22 ( INLINEFORM0 sentences from the training set) and the ILCI English-Hindi parallel corpus ( INLINEFORM1 sentences). The ILCI (Indian Language Corpora Initiative) multilingual parallel corpus BIBREF23 spans multiple Indian languages from the health and tourism domains. We use the 520-sentence dev-set of the IITB parallel corpus for validation. For each child task, we use INLINEFORM2 sentences from ILCI corpus as the test set. \n Question: Which dataset(s) do they experiment with?",
            "output": [
                "IITB English-Hindi parallel corpus BIBREF22 ILCI English-Hindi parallel corpus"
            ]
        },
        {
            "id": "task460-f090888c8b9b4dd9b2dc09c395fbf5d2",
            "input": "Results ::: CoinCollector\nIn this setting, we compare the number of actions played in the environment (frames) and the score achieved by the agent (i.e. +1 reward if the coin is collected). In Go-Explore we also count the actions used to restore the environment to a selected cell, i.e. to bring the agent to the state represented in the selected cell. This allows a one-to-one comparison of the exploration efficiency between Go-Explore and algorithms that use a count-based reward in text-based games. Importantly, BIBREF8 showed that DQN and DRQN, without such counting rewards, could never find a successful trajectory in hard games such as the ones used in our experiments. Figure FIGREF17 shows the number of interactions with the environment (frames) versus the maximum score obtained, averaged over 10 games of the same difficulty. As shown by BIBREF8, DRQN++ finds a trajectory with the maximum score faster than to DQN++. On the other hand, phase 1 of Go-Explore finds an optimal trajectory with approximately half the interactions with the environment. Moreover, the trajectory length found by Go-Explore is always optimal (i.e. 30 steps) whereas both DQN++ and DRQN++ have an average length of 38 and 42 respectively. In case of Game CoinCollector,  In CookingWorld, we compared models in the three settings mentioned earlier, namely, single, joint, and zero-shot. In all experiments, we measured the sum of the final scores of all the games and their trajectory length (number of steps). Table TABREF26 summarizes the results in these three settings. Phase 1 of Go-Explore on single games achieves a total score of 19,530 (sum over all games), which is very close to the maximum possible points (i.e. 19,882), with 47,562 steps. A winning trajectory was found in 4,279 out of the total of 4,440 games. This result confirms again that the exploration strategy of Go-Explore is effective in text-based games. Next, we evaluate the effectiveness and the generalization ability of the simple imitation learning policy trained using the extracted trajectories in phase 1 of Go-Explore in the three settings mentioned above. In this setting, each model is trained from scratch in each of the 4,440 games based on the trajectory found in phase 1 of Go-Explore (previous step). As shown in Table TABREF26, the LSTM-DQN BIBREF7, BIBREF8 approach without the use of admissible actions performs poorly. One explanation for this could be that it is difficult for this model to explore both language and game strategy at the same time; it is hard for the model to find a reward signal before it has learned to model language, since almost none of its actions will be admissible, and those reward signals are what is necessary in order to learn the language model. As we see in Table TABREF26, however, by using the admissible actions in the $\\epsilon $-greedy step the score achieved by the LSTM-DQN increases dramatically (+ADM row in Table TABREF26). DRRN BIBREF10 achieves a very high score, since it explicitly learns how to rank admissible actions (i.e. a much simpler task than generating text). Finally, our approach of using a Seq2Seq model trained on the single trajectory provided by phase 1 of Go-Explore achieves the highest score among all the methods, even though we do not use admissible actions in this phase. However, in this experiment the Seq2Seq model cannot perfectly replicate the provided trajectory and the total score that it achieves is in fact 9.4% lower compared to the total score achieved by phase 1 of Go-Explore. Figure FIGREF61 (in Appendix SECREF60) shows the score breakdown for each level and model, where we can see that the gap between our model and other methods increases as the games become harder in terms of skills needed. In this setting the 4,440 games are split into training, validation, and test games. The split is done randomly but in a way that different difficulty levels (recipes 1, 2 and 3), are represented with equal ratios in all the 3 splits, i.e. stratified by difficulty. As shown in Table TABREF26, the zero-shot performance of the RL baselines is poor, which could be attributed to the same reasons why RL baselines under-perform in the Joint case. Especially interesting is that the performance of DRRN is substantially lower than that of the Go-Explore Seq2Seq model, even though the DRRN model has access to the admissible actions at test time, while the Seq2Seq model (as well as the LSTM-DQN model) has to construct actions token-by-token from the entire vocabulary of 20,000 tokens. On the other hand, Go-Explore Seq2Seq shows promising results by solving almost half of the unseen games. Figure FIGREF62 (in Appendix SECREF60) shows that most of the lost games are in the hardest set, where a very long sequence of actions is required for winning the game. These results demonstrate both the relative effectiveness of training a Seq2Seq model on Go-Explore trajectories, but they also indicate that additional effort needed for designing reinforcement learning algorithms that effectively generalize to unseen games. \n Question: How better does new approach behave than existing solutions?",
            "output": [
                " On the other hand, phase 1 of Go-Explore finds an optimal trajectory with approximately half the interactions with the environment Moreover, the trajectory length found by Go-Explore is always optimal (i.e. 30 steps) whereas both DQN++ and DRQN++ have an average length of 38 and 42 respectively. Especially interesting is that the performance of DRRN is substantially lower than that of the Go-Explore Seq2Seq model"
            ]
        },
        {
            "id": "task460-3861c55ba28a4c2f8e61ab2ac3c8f892",
            "input": "We distributed Dryhootch chosen surveys among 1,200 users (305 users are self claimed PTSD sufferers and rest of them are randomly chosen from previous 2,423 users) and received 210 successful responses.  Dryhootch defines the intensity of PTSD into four categories based on the weekly survey results of all three clinical survey tools (DOSPERT, BSSS and VIAS ) \n Question: Which clinically validated survey tools are used?",
            "output": [
                "DOSPERT, BSSS and VIAS"
            ]
        },
        {
            "id": "task460-c53d6aba82554d8cb9fff7a5e030b29e",
            "input": "In addition, we include state-of-the-art models : RegSum BIBREF0 and GCN+PADG BIBREF3. We outperform both in terms of ROUGE-1. For ROUGE-2 scores we achieve better results than GCN+PADG but without any use of domain-specific hand-crafted features and a much smaller and simpler model. Finally, RegSum achieves a similar ROUGE-2 score but computes sentence saliences based on word scores, incorporating a rich set of word-level and domain-specific features. \n Question: How better are state-of-the-art results than this model? ",
            "output": [
                "we achieve better results than GCN+PADG but without any use of domain-specific hand-crafted features  RegSum achieves a similar ROUGE-2 score"
            ]
        },
        {
            "id": "task460-e1a6bcb6a048429982f97c6387cffc69",
            "input": "Data was collected from a 10% uniform sample of Twitter posts made during 2013, specifically the Gardenhose API. Control documents were also selected. These documents did not contain any of `caused', `causing', or `causes', nor any bidirectional words, and are further matched temporally to obtain the same number of control documents as causal documents in each fifteen-minute period during 2013. Control documents were otherwise selected randomly; causal synonyms may be present. \n Question: What is the source of the \"control\" corpus?",
            "output": [
                "Randomly selected from a Twitter dump, temporally matched to causal documents"
            ]
        },
        {
            "id": "task460-24a7de9f2ede471b8c6bcbf7986d2a8e",
            "input": "For each candidate mention span, we retrieve the top 10 entities according to the Freebase API. We then create a lattice in which the nodes correspond to mention-entity pairs, scored by their Freebase API scores, and the edges encode the fact that no joint assignment of entities to mentions can contain overlapping spans. We take the top 10 paths through the lattice as possible entity disambiguations. For each possibility, we generate $n$ -best paraphrases that contains the entity mention spans. In the end, this process creates a total of $10n$ paraphrases.  \n Question: How many paraphrases are generated per question?",
            "output": [
                "10*n paraphrases, where n depends on the number of paraphrases that contain the entity mention spans"
            ]
        },
        {
            "id": "task460-e8cf4b9b889a4dd28b5cca2aabafcde2",
            "input": "In contrast, our models are trained only on the 76K questions in the training set. \n Question: Do the authors also try the model on other datasets?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-d4d62ecf2ec84147b4292f18bafe3eba",
            "input": "We use the word embeddings for Vietnamese that created by Kyubyong Park and Edouard Grave at al:\n\nKyubyong Park: In his project, he uses two methods including fastText and word2vec to generate word embeddings from wikipedia database backup dumps. Edouard Grave et al BIBREF11: They use fastText tool to generate word embeddings from Wikipedia. \n Question: What word embeddings were used?",
            "output": [
                "Kyubyong Park Edouard Grave et al BIBREF11"
            ]
        },
        {
            "id": "task460-01f070bd2d624044810ac7f59a0c0853",
            "input": "As the baseline, we simply judge the input token as IOCs on the basis of the spelling features described in BIBREF12 . \n Question: What is used a baseline?",
            "output": [
                "As the baseline, we simply judge the input token as IOCs on the basis of the spelling features described in BIBREF12"
            ]
        },
        {
            "id": "task460-d831836c4cab46c6962cac2162d286a0",
            "input": "At the same time, some information about inflected word forms in the context can be useful, but it is lost during lemmatization, and this leads to the decreased score. Arguably, this means that lemmatization brings along both advantages and disadvantages for WSD with ELMo.  \n Question: Do the authors mention any downside of lemmatizing input before training ELMo?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-6732d7115f1e4bc0bec90592fc9a1758",
            "input": "Statistical analysis of tweets ::: Unigram, Bigram an Trigram word frequency analysis\nThree forms of tokens of words have been considered for the study viz. unigram, bigram and trigram. \n Question: Which word frequencies reflect on the psychology of the twitter users, according to the authors?",
            "output": [
                "unigram, bigram and trigram"
            ]
        },
        {
            "id": "task460-6a188e5e4d134a0693cbbe35c7add647",
            "input": "In Table TABREF1, we summarize the quantitative results of the above previous studies. It includes the model basis, feature description, languages classified and the used dataset along with accuracy obtained. The table also lists the overall results of our proposed models (at the top). The languages used by various authors along with their acronyms are English (En), Spanish (Es), French (Fr), German (De), Russian (Ru), Italian (It), Bengali (Ben), Hindi (Hi) and Telegu (Tel). \n Question: Is the performance compared against a baseline model?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-187b1c4dbbd04995bb7a6544ce1903c7",
            "input": "First, we propose a class of recurrent-like neural networks for NLP tasks that satisfy the differential equation DISPLAYFORM0\n\nwhere DISPLAYFORM0\n\nand where INLINEFORM0 and INLINEFORM1 are learned functions. INLINEFORM2 corresponds to traditional RNNs, with INLINEFORM3 . For INLINEFORM4 , this takes the form of RNN cells with either nested internal memories or dependencies that extend temporally beyond the immediately previous hidden state.  \n Question: What novel class of recurrent-like networks is proposed?",
            "output": [
                "A network, whose learned functions satisfy a certain equation. The  network contains RNN cells with either nested internal memories or dependencies that extend temporally beyond the immediately previous hidden state."
            ]
        },
        {
            "id": "task460-5862904fe16f4babb6faa2cfadc31d3c",
            "input": "On inspection, the drop in performance between translated and original Japanese was often a result of translations that were reasonable but not consistent with the labels. For example, when translating the first example in Figure FIGREF2, both machine translations map “UTF8min風邪”, which means cold (the illness), into “UTF8min寒さ”, which means cold (low temperature). Another example is where the Japanese pseudo-tweet “UTF8min花粉症の時期はすごい疲れる。” was provided alongside an English pseudo-tweet “Allergy season is so exhausting.”. Here, the Japanese word for hay fever “UTF8min花粉症。” has been manually mapped to the less specific word “allergies” in English; the machine translation maps back to Japanese using the word for “allergies” i.e. “UTF8minアレルギー” in the katakana alphabet (katakana is used to express words derived from foreign languages), since there is no kanji character for the concept of allergies. \n Question: Is there any explanation why some choice of language pair is better than the other?",
            "output": [
                "translations that were reasonable but not consistent with the labels"
            ]
        },
        {
            "id": "task460-7e75be91099f4e688d941f4018dda76e",
            "input": "We extract data from the WMT'14 English-French (En-Fr) and English-German (En-De) datasets.  \n Question: Are this techniques used in training multilingual models, on what languages?",
            "output": [
                "English to French and English to German"
            ]
        },
        {
            "id": "task460-34b1d42adae7435294990d7228914e76",
            "input": "Questions: We make use of the 7,787 science exam questions of the Aristo Reasoning Challenge (ARC) corpus BIBREF31 , which contains standardized 3rd to 9th grade science questions from 12 US states from the past decade.  \n Question: How was the dataset collected?",
            "output": [
                "from 3rd to 9th grade science questions collected from 12 US states"
            ]
        },
        {
            "id": "task460-f28074c30e5d43c1bfe7e29282acc5f4",
            "input": "Since, we there was no publicly available standard Nepali NER dataset and did not receive any dataset from the previous researchers, we had to create our own dataset. This dataset contains the sentences collected from daily newspaper of the year 2015-2016.  \n Question: What is the source of their dataset?",
            "output": [
                "daily newspaper of the year 2015-2016"
            ]
        },
        {
            "id": "task460-edbf9dc5d29a4b45b21f4b561525b5c7",
            "input": "We used the annotated dataset reported by degen2015investigating, a dataset of the utterances from the Switchboard corpus of telephone dialogues BIBREF21 that contain the word some. The dataset consists of 1,362 unique utterances with a noun phrase containing some (some-NP). For each example with a some-NP, degen2015investigating collected inference strength ratings from at least 10 participants recruited on Amazon's Mechanical Turk. Participants saw both the target utterance and ten utterances from the preceding discourse context. They then rated the similarity between the original utterance like (UNKREF8) and an utterance in which some was replaced with some, but not all like (UNKREF9), on a 7-point Likert scale with endpoints labeled “very different meaning” (1) and “same meaning” (7). Low similarity ratings thus indicate low inference strength, and high similarity ratings indicate high inference strength. \n Question: Which dataset do they use?",
            "output": [
                "the annotated dataset reported by degen2015investigating, a dataset of the utterances from the Switchboard corpus of telephone dialogues BIBREF21 that contain the word some"
            ]
        },
        {
            "id": "task460-2965fd486a0d4eee9b1a1ef0ed770d7a",
            "input": "We explore how to utilize additional commonsense knowledge (i.e. rationales) as the input to the task. Like we mentioned in Section SECREF6, we search relevant sentences from the OMCS corpus as the additional distant rationales, and ground truth rationale sentences for dev/test data. The inputs are no longer the concept-sets themselves, but in a form of “[rationales$|$concept-set]” (i.e. concatenating the rationale sentences and original concept-set strings). \n Question: Are the models required to also generate rationales?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-a5fb539c354a49ed9e2311db3382dfdc",
            "input": "Overall, this confirms our initial intuition that combining the best performing popularity-based approach with the best similarity-based approach should result in the highest accuracy (i.e., INLINEFORM7 for INLINEFORM8 ). \n Question: which algorithm was the highest performer?",
            "output": [
                "A hybrid model consisting of best performing popularity-based approach with the best similarity-based approach"
            ]
        },
        {
            "id": "task460-803808e28bf54a14b48e1bf93c1c454b",
            "input": "In this regard, we observed that HDGAN produced relatively better visual results on the CUB and Oxford datasets while AttnGAN produced far more impressive results than the rest on the more complex COCO dataset. In terms of inception score (IS), which is the metric that was applied to majority models except DC-GAN, the results in Table TABREF48 show that StackGAN++ only showed slight improvement over its predecessor, StackGAN, for text-to-image synthesis. In addition, the results in Table TABREF48 also show that DM-GAN BIBREF53 has the best performance, followed by Obj-GAN BIBREF81. Notice that both DM-GAN and Obj-GAN are most recently developed methods in the field (both published in 2019), indicating that research in text to image synthesis is continuously improving the results for better visual perception and interception. \n Question: What is the conclusion of comparison of proposed solution?",
            "output": [
                "HDGAN produced relatively better visual results on the CUB and Oxford datasets while AttnGAN produced far more impressive results than the rest on the more complex COCO dataset In terms of inception score (IS), which is the metric that was applied to majority models except DC-GAN, the results in Table TABREF48 show that StackGAN++ only showed slight improvement over its predecessor text to image synthesis is continuously improving the results for better visual perception and interception"
            ]
        },
        {
            "id": "task460-9506c36322374711a45e07e51912893f",
            "input": "The Word2Vec architecture has inspired a great deal of research in the bio/cheminformatics domains. The Word2Vec algorithm has been successfully applied for determining protein classes BIBREF44 and protein-protein interactions (PPI) BIBREF56. \n Question: Is there any concrete example in the paper that shows that this approach had huge impact on drug discovery?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-fff5c38843744ffa99d869fe9407a89d",
            "input": "To verify the reliability of the transformed semantic space, we propose an evaluation benchmark on the basis of word similarity datasets. Given an enriched space INLINEFORM0 and a similarity dataset INLINEFORM1 , we compute the similarity of each word pair INLINEFORM2 as the cosine similarity of their corresponding transformed vectors INLINEFORM3 and INLINEFORM4 from the two spaces, where INLINEFORM5 and INLINEFORM6 for LS and INLINEFORM7 and INLINEFORM8 for CCA. A high performance on this benchmark shows that the mapping has been successful in placing semantically similar terms near to each other whereas dissimilar terms are relatively far apart in the space. We repeat the computation for each pair in the reverse direction. \n Question: How is performance measured?",
            "output": [
                "To verify the reliability of the transformed semantic space, we propose an evaluation benchmark on the basis of word similarity datasets. Given an enriched space INLINEFORM0 and a similarity dataset INLINEFORM1 , we compute the similarity of each word pair INLINEFORM2 as the cosine similarity of their corresponding transformed vectors INLINEFORM3 and INLINEFORM4 from the two spaces, where INLINEFORM5 and INLINEFORM6 for LS and INLINEFORM7 and INLINEFORM8 for CCA. "
            ]
        },
        {
            "id": "task460-0e445cd4fd774a2bb71373bccfdee598",
            "input": "The above pseudocode is agnostic with respect to the choice of fragmentation and environment functions; task-specific choices are described in more detail for each experiment below.\n\nDiscussion \n Question: Which languages do they test on?",
            "output": [
                "Answer with content missing: (Applications section) We use Wikipedia articles\nin five languages\n(Kinyarwanda, Lao, Pashto, Tok Pisin, and a subset of English) as well as the Na dataset of Adams\net al. (2017).\nSelect:\nKinyarwanda, Lao, Pashto, Tok Pisin, and a subset of English"
            ]
        },
        {
            "id": "task460-b7a5395bc2e64b6b8d8e01b9260bbb75",
            "input": "We collect three years of online news articles from June 2016 to June 2019. \n Question: What unlabeled corpus did they use?",
            "output": [
                "three years of online news articles from June 2016 to June 2019"
            ]
        },
        {
            "id": "task460-d11f71df1d204d5588ec5b2d2763a33c",
            "input": "In this section, we compare our model with state-of-the-art systems, including those with different degrees of supervision. The baselines include: (1) Procrustes BIBREF11, which learns a linear mapping through Procrustes Analysis BIBREF36. (2) GPA BIBREF37, an extension of Procrustes Analysis. (3) GeoMM BIBREF38, a geometric approach which learn a Mahalanobis metric to refine the notion of similarity. (4) GeoMM$_{semi}$, iterative GeoMM with weak supervision. (5) Adv-C-Procrustes BIBREF11, which refines the mapping learned by Adv-C with iterative Procrustes, which learns the new mapping matrix by constructing a bilingual lexicon iteratively. (6) Unsup-SL BIBREF13, which integrates a weak unsupervised mapping with a robust self-learning. (7) Sinkhorn-BT BIBREF28, which combines sinkhorn distance BIBREF29 and back-translation. \n Question: What are current state-of-the-art methods that consider the two tasks independently?",
            "output": [
                "Procrustes GPA GeoMM GeoMM$_{semi}$ Adv-C-Procrustes Unsup-SL Sinkhorn-BT"
            ]
        },
        {
            "id": "task460-742e5070332d4c60a8fd133d615db28d",
            "input": "Data was collected from a 10% uniform sample of Twitter posts made during 2013, specifically the Gardenhose API. Control documents were also selected. These documents did not contain any of `caused', `causing', or `causes', nor any bidirectional words, and are further matched temporally to obtain the same number of control documents as causal documents in each fifteen-minute period during 2013. Control documents were otherwise selected randomly; causal synonyms may be present. \n Question: how do they collect the comparable corpus?",
            "output": [
                "Randomly from a Twitter dump"
            ]
        },
        {
            "id": "task460-bf05c21bf95e4d879f8a6aaf3246d159",
            "input": "The basic concept is to use a phone-discriminative model to produce frame-level phonetic features, and then use these features to enhance RNN LID systems that were originally built with raw acoustic features. The initial step is therefore feature combination, with the phonetic feature used as auxiliary information to assist acoustic RNN LID. This is improved further, as additional research identified that a simpler model using only the phonetic feature as the RNN LID input provides even better performance. We call this RNN model based on phonetic features the phonetic temporal neural LID approach, or PTN LID. As well as having a simplified model structure, the PTN offers deeper insight into the LID task by rediscovering the value of the phonetic temporal property in language discrimination.  \n Question: What is the main contribution of the paper? ",
            "output": [
                "Proposing an improved RNN model, the phonetic temporal neural LID approach, based on phonetic features that results in better performance"
            ]
        },
        {
            "id": "task460-a1982577edfd4601acfd4612b9e664bb",
            "input": "The embedding-wise convolution is to apply a convolution filter w $\\in \\mathbb {R}^{\\mathcal {\\\\}k*d}$ to a window of $k$ word embeddings to generate a new feature, i.e., summarizing a local context of $k$ words. By applying the convolutional filter to all possible windows in the sentence, a feature map $c$ will be generated. \n Question: What supports the claim that injected CNN into recurent units will enhance ability of the model to catch local context and reduce ambiguities?",
            "output": [
                "word embeddings to generate a new feature, i.e., summarizing a local context"
            ]
        },
        {
            "id": "task460-4d1e402c1be34555889ca39b10245eae",
            "input": "Curriculum Plausibility ::: Conversational Attributes ::: Specificity\nA notorious problem for neural dialogue generation model is that the model is prone to generate generic responses. Curriculum Plausibility ::: Conversational Attributes ::: Repetitiveness\nRepetitive responses are easy to generate in current auto-regressive response decoding, where response generation loops frequently, whereas diverse and informative responses are much more complicated for neural dialogue generation. Curriculum Plausibility ::: Conversational Attributes ::: Continuity\nA coherent response not only responds to the given query, but also triggers the next utterance. Curriculum Plausibility ::: Conversational Attributes ::: Model Confidence\nDespite the heuristic dialogue attributes, we further introduce the model confidence as an attribute, which distinguishes the easy-learnt samples from the under-learnt samples in terms of the model learning ability. Curriculum Plausibility ::: Conversational Attributes ::: Query-relatedness\nA conversation is considered to be coherent if the response correlates well with the given query. \n Question: What five dialogue attributes were analyzed?",
            "output": [
                "Model Confidence Continuity Query-relatedness Repetitiveness Specificity"
            ]
        },
        {
            "id": "task460-d42207b1c87b4129a32ec2af937f7385",
            "input": "To evaluate our model capability with processing the Chinese language, we also tested the performance of LCF-ATEPC on four Chinese comment datasets BIBREF35, BIBREF36, BIBREF29 (Car, Phone, Notebook, Camera). \n Question: In what four Chinese review datasets does LCF-ATEPC achieves state of the art?",
            "output": [
                "Car, Phone, Notebook, Camera"
            ]
        },
        {
            "id": "task460-737f6fc997e540bf95127bb4f930d368",
            "input": " To evaluate real-world performance of our selected classifier (i.e., performance in the absence of model and parameter bias), we perform classification of the holdout set. On this set, our classifier had an accuracy and F1-score of 89.6% and 89.2%, respectively.  \n Question: What was their system's performance?",
            "output": [
                "accuracy and F1-score of 89.6% and 89.2%, respectively"
            ]
        },
        {
            "id": "task460-43d75f661f70477cbaa7b2291c2d5001",
            "input": "The experiment settings from this paper and evaluation protocol for the Mboshi corpus (Boundary F-scores using the ZRC speech reference) are the same from BIBREF8. Table presents the results for bilingual UWS and multilingual leveraging.  \n Question: Does the paper report the accuracy of the model?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-344db0cc0577464a9176ab7ee3e6c85b",
            "input": "n this paper we provide a novel real-time and adaptive cryptocurrency price prediction platform based on Twitter sentiments. The integrative and modular platform copes with the three aforementioned challenges in several ways. Firstly, it provides a Spark-based architecture which handles the large volume of incoming data in a persistent and fault tolerant way. Secondly, the proposed platform offers an approach that supports sentiment analysis based on VADER which can respond to large amounts of natural language processing queries in real time. Thirdly, the platform supports a predictive approach based on online learning in which a machine learning model adapts its weights to cope with new prices and sentiments. Finally, the platform is modular and integrative in the sense that it combines these different solutions to provide novel real-time tool support for bitcoin price prediction that is more scalable, data-rich, and proactive, and can help accelerate decision-making, uncover new opportunities and provide more timely insights based on the available and ever-larger financial data volume and variety. \n Question: Which elements of the platform are modular?",
            "output": [
                "handling large volume incoming data, sentiment analysis on tweets and predictive online learning"
            ]
        },
        {
            "id": "task460-c529970c71a948219bb90b439128fc63",
            "input": "As in the Sequicity framework, we report entity match rate, BLEU score and Success F1 score.  \n Question: What were the evaluation metrics used?",
            "output": [
                "entity match rate BLEU score Success F1 score"
            ]
        },
        {
            "id": "task460-b7780217b89b4f788fe09ac1bbe12c9d",
            "input": "In ConMask, we use a similar idea to select the most related words given some relationship and mask irrelevant words by assigning a relationship-dependent similarity score to words in the given entity description. ConMask selects words that are related to the given relationship to mitigate the inclusion of irrelevant and noisy words. \n Question: Can the model add new relations to the knowledge graph, or just new entities?",
            "output": [
                "The model does not add new relations to the knowledge graph."
            ]
        },
        {
            "id": "task460-506a2ba8dcdb483a981ff56870eaee94",
            "input": "We employ BIBREF73 's weighted lexicon of terms that uses the dataset of 75,394 Facebook users who shared their status, age and gender. Utilizing these two weighted lexicon of terms, we are predicting the demographic information (age or gender) of INLINEFORM0 (denoted by INLINEFORM1 ) using following equation: INLINEFORM2\n\nwhere INLINEFORM0 is the lexicon weight of the term, and INLINEFORM1 represents the frequency of the term in the user generated INLINEFORM2 , and INLINEFORM3 measures total word count in INLINEFORM4 . \n Question: How do this framework facilitate demographic inference from social media?",
            "output": [
                "Demographic information is predicted using weighted lexicon of terms."
            ]
        },
        {
            "id": "task460-c386a316b0c24910a5d4ef0b7d5071c2",
            "input": "Evaluation Metrics. To evaluate the strategy formulation ability, we introduce a measure called Coverage( INLINEFORM0 ), defined as the fraction of total query data instances, for which LiLi has successfully formulated strategies that lead to winning. If LiLi wins on all episodes for a given dataset, INLINEFORM1 is 1.0. To evaluate the predictive performance, we use Avg. MCC and avg. +ve F1 score. \n Question: What metrics are used to establish that this makes chatbots more knowledgeable and better at learning and conversation? ",
            "output": [
                "Coverage Avg. MCC and avg. +ve F1 score"
            ]
        },
        {
            "id": "task460-199e32a023fc4dc1a3bbe3bc536fb73d",
            "input": "Following the previous works, we report the precision ($P.$), recall ($R.$) and $F_1$ scores for target recognition and targeted sentiment.  \n Question: How is the effectiveness of the model evaluated?",
            "output": [
                "precision ($P.$), recall ($R.$) and $F_1$ scores for target recognition and targeted sentiment"
            ]
        },
        {
            "id": "task460-796ce2d8cfb54bf1b3fc71df99797add",
            "input": "To compare the conversions between USVC and PitchNet, we employed an automatic evaluation score and a human evaluation score. The automatic score roughly followed the design in BIBREF13. The pitch tracker of librosa package BIBREF18 was employed to extract pitch information of the input and output audio. Then the output pitch was compared to the input pitch using the normalized cross correlation (NCC) which would give a score between 0 and 1. The higher the score is, the better the output pitch matches the input pitch. We conducted the evaluation on USVC (our) and PitchNet. The evaluated automatic scores on conversion and reconstruction tasks are shown in Tab. TABREF14. Our method performed better both on conversion and reconstruction. The scores of reconstruction are higher than conversion since both models were trained using a reconstruction loss. However, the score of our method on conversion is even higher than the score of USVC (Our) on reconstruction. Mean Opinion Score (MOS) was used as a subjective metric to evaluate the quality of the converted audio. Two questions were asked: (1) what is quality of the audio? (naturalness) (2) How well does the converted version match the original? (similarity) A score of 1-5 would be given to answer the questions. The evaluation was conducted on USVC (Our) and PitchNet. Besides, the converted samples provided by BIBREF0 was also included to give a more convincing evaluation. As shown by Tab. TABREF15, the naturalness and similarity of our method are both higher than the other two ones. Our implementation of USVC performed slightly lower than the original author's because we cannot fully reproduce the results of them. Next we qualitatively analyze the influence of input pitch in our method. We used different pitch as input to observe how the output pitch would change along with the input pitch. The input pitch was multiplied by 0.7, 1.0 and 1.3 respectively. And the output pitch was also extracted by the pitch tracker of the librosa package. Fig. FIGREF16 plots the pitch of input audio and output audio with different pitch as input while keeping the target singer the same. As shown by Fig. FIGREF16, the output pitch changes significantly along with the input pitch. The examples are also presented at our website. \n Question: How is the quality of singing voice measured?",
            "output": [
                "To compare the conversions between USVC and PitchNet, we employed an automatic evaluation score and a human evaluation score."
            ]
        },
        {
            "id": "task460-f670a51ebfe04506aacdbc4b91ceb9d5",
            "input": "Five attributes, specifying certain details of clinical significance, are defined to characterize the answer types of INLINEFORM4 : (1) the time the patient has been experiencing the symptom, (2) activities that trigger the symptom (to occur or worsen), (3) the extent of seriousness, (4) the frequency occurrence of the symptom, and (5) the location of symptom. For each symptom/attribute, it can take on different linguistic expressions, defined as entities. Note that if the queried symptom or attribute is not mentioned in the dialogue, the groundtruth output is “No Answer”, as in BIBREF6 . \n Question: What labels do they create on their dataset?",
            "output": [
                "(1) the time the patient has been experiencing the symptom, (2) activities that trigger the symptom (to occur or worsen), (3) the extent of seriousness, (4) the frequency occurrence of the symptom, and (5) the location of symptom No Answer"
            ]
        },
        {
            "id": "task460-b3a14610b6654770a13f1abf62b65141",
            "input": "In order to find the object type, SimplerVoice, first, builds an ontology-based knowledge tree. We propose to use 2 methods to generate the suitable verbs for the target object: heuristics-based, and n-grams model. \n Question: Which model do they use to generate key messages?",
            "output": [
                "ontology-based knowledge tree heuristics-based n-grams model"
            ]
        },
        {
            "id": "task460-046b7a73e8cd410a842a3ab654ada802",
            "input": "Table TABREF15 shows a comparison of the results on SimCluster versus K-means algorithm. Here our SimCluster algorithm improves the F1-scores from 0.412 and 0.417 in the two domains to 0.442 and 0.441. The ARI scores also improve from 0.176 and 0.180 to 0.203 and 0.204. \n Question: Do they use the same distance metric for both the SimCluster and K-means algorithm?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-f7014a710c2640e79c2b0b119001694c",
            "input": "Notably, this increase is observed after the conclusion of the US presidential primaries and during the period of the Democratic and Republican National Conventions and does not reduce even after the conclusion of the US presidential elections held on November 8. \n Question: What other political events are included in the database?",
            "output": [
                "US presidential primaries Democratic and Republican National Conventions"
            ]
        },
        {
            "id": "task460-6e0401e4974944ec8af2d50f9ed224f6",
            "input": "Each annotator annotated 90 reference sentences (i.e. from the training corpus) with which style they thought the sentence was from. The accuracy on this baseline task for annotators A1, A2, and A3 was 80%, 88%, and 80% respectively, giving us an upper expected bound on the human evaluation. \n Question: How they perform manual evaluation, what is criteria?",
            "output": [
                "accuracy"
            ]
        },
        {
            "id": "task460-184167a369244243b00e3f87afbe71eb",
            "input": "To better analyze model generalization to an unseen, new domain as well as model leveraging the out-of-domain sources, we propose a new architecture which is an extension of the ARED model. In order to better select, aggregate and control the semantic information, a Refinement Adjustment LSTM-based component (RALSTM) is introduced to the decoder side. The proposed model can learn from unaligned data by jointly training the sentence planning and surface realization to produce natural language sentences.  \n Question: What is the difference of the proposed model with a standard RNN encoder-decoder?",
            "output": [
                "Introduce a \"Refinement Adjustment LSTM-based component\" to the decoder"
            ]
        },
        {
            "id": "task460-afc8b6b9d6a34d018809ae4dfad5093a",
            "input": "Our resulting model obtains state of the art results on both the VQA dataset and the bAbI-10k text question-answering dataset, proving the framework can be generalized across input domains. \n Question: Does the DMN+ model establish state-of-the-art ?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-06eeb9a68f3b446d950ec462e3214647",
            "input": "Motivated by this, we introduce resolution mode variables $\\Pi = \\lbrace \\pi _1, \\ldots , \\pi _n\\rbrace $ , where for each mention $j$ the variable $\\pi _j \\in \\lbrace str, prec, attr\\rbrace $ indicates in which mode the mention should be resolved. In our model, we define three resolution modes — string-matching (str), precise-construct (prec), and attribute-matching (attr) — and $\\Pi $ is deterministic when $D$ is given (i.e. $P(\\Pi |D)$ is a point distribution). We determine $\\pi _j$ for each mention $m_j$ in the following way:\n\n$\\pi _j = str$ , if there exists a mention $m_i, i < j$ such that the two mentions satisfy the String Match sieve, the Relaxed String Match sieve, or the Strict Head Match A sieve in the Stanford multi-sieve system BIBREF1 .\n\n$\\pi _j = prec$ , if there exists a mention $m_i, i < j$ such that the two mentions satisfy the Speaker Identification sieve, or the Precise Constructs sieve.\n\n$\\pi _j = attr$ , if there is no mention $m_i, i < j$ satisfies the above two conditions. \n Question: What are resolution model variables?",
            "output": [
                "Variables in the set {str, prec, attr} indicating in which mode the mention should be resolved."
            ]
        },
        {
            "id": "task460-830e3f01483945319d84d08018196504",
            "input": "RKS approach proposed in BIBREF21, BIBREF22, explicitly maps data vectors to a space where linear separation is possible. The RKS method provides an approximate kernel function via explicit mapping. \n Question: What is the Random Kitchen Sink approach?",
            "output": [
                "Random Kitchen Sink method uses a kernel function to map data vectors to a space where linear separation is possible."
            ]
        },
        {
            "id": "task460-108c08d2099b47519a19f4fba5aa4b96",
            "input": "The results in Table TABREF7 show that SRCC performs much better in knowledge extraction. The two documents' contents contain the same idea expressed by terms in a different order that John had asked Mary to marry him before she left. It is obvious that cosine similarity cannot recognize this association, but SRCC has successfully recognized it and produced a similarity value of -0.285714. \n Question: How do they evaluate knowledge extraction performance?",
            "output": [
                "SRCC"
            ]
        },
        {
            "id": "task460-347740aee907448d9681d553498a648a",
            "input": "We introduce a new crowdsourced dataset of 23,700 queries, including 22,500 in-scope queries covering 150 intents, which can be grouped into 10 general domains.  \n Question: Where does the data come from?",
            "output": [
                "crowsourcing platform"
            ]
        },
        {
            "id": "task460-5edd5f17ea404918b201f65edd660d36",
            "input": "To enable sentence-level classification we need to obtain a sentence representation from the word vectors INLINEFORM0 . We achieved this by using a BiLSTM with max pooling, which was shown to be a good universal sentence encoding mechanism BIBREF13 . \n Question: Which model architecture do they use to obtain representations?",
            "output": [
                "BiLSTM with max pooling"
            ]
        },
        {
            "id": "task460-2f4619b0bb4a46098b90f112d4d9ee09",
            "input": "The incomplete dataset used for training is composed of lower-cased incomplete data obtained by manipulating the original corpora. \n Question: Do they test their approach on a dataset without incomplete data?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-c6d1ae2411fa4ab4b8eabe9fe6737152",
            "input": "Baselines. We compare our approach (FacTweet) to the following set of baselines: LR + Bag-of-words: We aggregate the tweets of a feed and we use a bag-of-words representation with a logistic regression (LR) classifier.\n\nTweet2vec: We use the Bidirectional Gated recurrent neural network model proposed in BIBREF20. We keep the default parameters that were provided with the implementation. To represent the tweets, we use the decoded embedding produced by the model. With this baseline we aim at assessing if the tweets' hashtags may help detecting the non-factual accounts.\n\nLR + All Features (tweet-level): We extract all our features from each tweet and feed them into a LR classifier. Here, we do not aggregate over tweets and thus view each tweet independently.\n\nLR + All Features (chunk-level): We concatenate the features' vectors of the tweets in a chunk and feed them into a LR classifier.\n\nFacTweet (tweet-level): Similar to the FacTweet approach, but at tweet-level; the sequential flow of the tweets is not utilized. We aim at investigating the importance of the sequential flow of tweets.\n\nTop-$k$ replies, likes, or re-tweets: Some approaches in rumors detection use the number of replies, likes, and re-tweets to detect rumors BIBREF21. Thus, we extract top $k$ replied, liked or re-tweeted tweets from each account to assess the accounts factuality. We tested different $k$ values between 10 tweets to the max number of tweets from each account. Figure FIGREF24 shows the macro-F1 values for different $k$ values. It seems that $k=500$ for the top replied tweets achieves the highest result. Therefore, we consider this as a baseline. \n Question: What baselines were used in this work?",
            "output": [
                "LR + Bag-of-words Tweet2vec LR + All Features (tweet-level) LR + All Features (chunk-level) FacTweet (tweet-level) Top-$k$ replies, likes, or re-tweets"
            ]
        },
        {
            "id": "task460-05ea1d9e0d6d4c5f94abd409a4a0b202",
            "input": "To build the large ancient-modern Chinese dataset, we collected 1.7K bilingual ancient-modern Chinese articles from the internet. More specifically, a large part of the ancient Chinese data we used come from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era. \n Question: Where does the ancient Chinese dataset come from?",
            "output": [
                "ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era"
            ]
        },
        {
            "id": "task460-e13514dac4a74f60b78917b092e8716d",
            "input": "It is a well known fact that NLP applications in industrial settings often have to deal with the noisy data. There are different kinds of possible noise namely non-canonical text such as spelling mistakes, typographic errors, colloquialisms, abbreviations, slang, internet jargon, emojis, embedded metadata (such as hashtags, URLs, mentions), non standard syntactic constructions and spelling variations, grammatically incorrect text, mixture of two or more languages to name a few. Such noisy data is a hallmark of user generated text content and commonly found on social media, chats, online reviews, web forums to name a few. \n Question: What kind is noise is present in typical industrial data?",
            "output": [
                " non-canonical text such as spelling mistakes, typographic errors, colloquialisms, abbreviations, slang, internet jargon, emojis, embedded metadata (such as hashtags, URLs, mentions), non standard syntactic constructions and spelling variations, grammatically incorrect text, mixture of two or more languages"
            ]
        },
        {
            "id": "task460-8273cc4810e541b4941da40ed0f90156",
            "input": "Notably, disabling the first layer in the RTE task gives a significant boost, resulting in an absolute performance gain of 3.2%. However, effects of this operation vary across tasks, and for QNLI and MNLI, it produces a performance drop of up to -0.2%. \n Question: How much is performance improved by disabling attention in certain heads?",
            "output": [
                "disabling the first layer in the RTE task gives a significant boost, resulting in an absolute performance gain of 3.2%  this operation vary across tasks"
            ]
        },
        {
            "id": "task460-386e58a194fe469eb28997e9b475b009",
            "input": "We use 300 Dimensional Glove Common Crawl Embeddings (840B Token) BIBREF11 and fine tune them for the task \n Question: What embedding algorithm and dimension size are used?",
            "output": [
                "300 Dimensional Glove"
            ]
        },
        {
            "id": "task460-2876824585c646c9b492adf870a1e658",
            "input": "For experiments, we train and evaluate our models in the civil law system of mainland China. We collect and construct a large-scale real-world data set of INLINEFORM0 case documents that the Supreme People's Court of People's Republic of China has made publicly available \n Question: what is the size of the real-world civil case dataset?",
            "output": [
                "100 000 documents"
            ]
        },
        {
            "id": "task460-cb7ae25641b1465b8b1e28e3ceacdec9",
            "input": " In Section SECREF16, we first provide more details about the experimental setting that we followed.  As explained in Section SECREF3, we used BabelNet BIBREF29 as our reference taxonomy. BabelNet is a large-scale full-fledged taxonomy consisting of heterogeneous sources such as WordNet BIBREF36, Wikidata BIBREF37 and WiBi BIBREF38, making it suitable to test our hypothesis in a general setting.  To test our proposed category induction model, we consider all BabelNet categories with fewer than 50 known instances. This is motivated by the view that conceptual neighborhood is mostly useful in cases where the number of known instances is small. For each of these categories, we split the set of known instances into 90% for training and 10% for testing. To tune the prior probability $\\lambda _A$ for these categories, we hold out 10% from the training set as a validation set. \n Question: What experiments they perform to demonstrate that their approach leads more accurate region based representations?",
            "output": [
                " To test our proposed category induction model, we consider all BabelNet categories with fewer than 50 known instances. This is motivated by the view that conceptual neighborhood is mostly useful in cases where the number of known instances is small. For each of these categories, we split the set of known instances into 90% for training and 10% for testing."
            ]
        },
        {
            "id": "task460-f983ebbd5e9c4a288e9123ecfbb0d711",
            "input": "Our approach improves LR by 5.17% (Accuracy) and 18.38% (AUC), and MLP by 10.71% (Accuracy) and 30.27% (AUC) on average. Such significant improvements clearly demonstrate that our approach is effective at improving model performance. \n Question: How are the accuracy merits of the approach demonstrated?",
            "output": [
                "significant improvements clearly demonstrate that our approach is effective at improving model performance"
            ]
        },
        {
            "id": "task460-b73c2af4a8b34e8d837023445d3bfd64",
            "input": "Causal documents were chosen to contain one occurrence only of the exact unigrams: `caused', `causing', or `causes'. \n Question: What are the selection criteria for \"causal statements\"?",
            "output": [
                "Presence of only the exact unigrams 'caused', 'causing', or 'causes'"
            ]
        },
        {
            "id": "task460-246e463a9f49462aabbd4ea4bf1fc9d1",
            "input": "First, in order to deal with the unseen relations, we propose to break the relation names into word sequences for question-relation matching. Second, noticing that original relation names can sometimes help to match longer question contexts, we propose to build both relation-level and word-level relation representations. Third, we use deep bidirectional LSTMs (BiLSTMs) to learn different levels of question representations in order to match the different levels of relation information. Finally, we propose a residual learning method for sequence matching, which makes the model training easier and results in more abstract (deeper) question representations, thus improves hierarchical matching. \n Question: What they use in their propsoed framework?",
            "output": [
                "break the relation names into word sequences  relation-level and word-level relation representations bidirectional LSTMs (BiLSTMs)  residual learning method"
            ]
        },
        {
            "id": "task460-5de4ae113fb94e75a9ef3ad8496cf1e7",
            "input": "In this part, we show the performance of our proposed models—HAKE and ModE—against existing state-of-the-art methods, including TransE BIBREF8, DistMult BIBREF9, ComplEx BIBREF17, ConvE BIBREF18, and RotatE BIBREF7. \n Question: What are state-of-the art models for this task?",
            "output": [
                "TransE DistMult ComplEx ConvE RotatE"
            ]
        },
        {
            "id": "task460-98d3cc52627b4562853b4fa042e0e428",
            "input": " In this work, we consider words that can indicate the characteristics of the neighbor words as contextual keywords and develop an approach to generate features from the automatically extracted contextual keywords. \n Question: What contextual features are used?",
            "output": [
                "The words that can indicate the characteristics of the neighbor words as contextual keywords and generate it from the automatically extracted contextual keywords."
            ]
        },
        {
            "id": "task460-f9c959bb1ad74a5f911d30c0c6e6b64a",
            "input": "Thus we propose two kinds of relation-specific meta information: relation meta and gradient meta corresponding to afore mentioned two perspectives respectively. In our proposed framework MetaR, relation meta is the high-order representation of a relation connecting head and tail entities. Gradient meta is the loss gradient of relation meta which will be used to make a rapid update before transferring relation meta to incomplete triples during prediction. \n Question: What meta-information is being transferred?",
            "output": [
                "high-order representation of a relation, loss gradient of relation meta"
            ]
        },
        {
            "id": "task460-db4cf4425ea14098aa241dff141e18fd",
            "input": "The tweetLID workshop shared task requires systems to identify the language of tweets written in Spanish (es), Portuguese (pt), Catalan (ca), English (en), Galician (gl) and Basque (eu).  \n Question: What shared task does this system achieve SOTA in?",
            "output": [
                "tweetLID workshop shared task"
            ]
        },
        {
            "id": "task460-1f00acc6fafd48d4a8c445722802b612",
            "input": "Sememes are minimum semantic units of word meanings, and the meaning of each word sense is typically composed of several sememes, as is illustrated in Figure 1 . \n Question: What is a sememe?",
            "output": [
                "Sememes are minimum semantic units of word meanings, and the meaning of each word sense is typically composed of several sememes"
            ]
        },
        {
            "id": "task460-877b152fcf3d4898a73c026417f709b2",
            "input": "The first one is a Turkish news-web corpus containing 423M words and 491M tokens, namely the BOUN Web Corpus BIBREF9 , BIBREF10 . The second one is composed of 21M Turkish tweets with 241M words and 293M tokens, where we combined 1M tweets from TS TweetS by Sezer-2013 and 20M Turkish Tweets by Bolat and Amasyalı. \n Question: What data was used to build the word embeddings?",
            "output": [
                "Turkish news-web corpus  TS TweetS by Sezer-2013 and 20M Turkish Tweets by Bolat and Amasyalı"
            ]
        },
        {
            "id": "task460-085ea81dd05b4391ac8587861c457a35",
            "input": "Even among researchers familiar with the definitions outlined above, there was still a low level of agreement (Krippendorff's INLINEFORM0 ). \n Question: How was reliability measured?",
            "output": [
                "level of agreement (Krippendorff's INLINEFORM0 )"
            ]
        },
        {
            "id": "task460-93c5c12828f24835a674294e1b38b17c",
            "input": "We identify many specific syntactic features that make sentences harder to classify, and many that have little effect. For instance, sentences involving unusual or marked argument structures are no harder than the average sentence, while sentences with long distance dependencies are hard to learn. We also find features of sentences that accentuate or minimize the differences between models. Specifically, the transformer models seem to learn long-distance dependencies much better than the recurrent model, yet have no advantage on sentences with morphological violations. We identify many specific syntactic features that make sentences harder to classify, and many that have little effect. For instance, sentences involving unusual or marked argument structures are no harder than the average sentence, while sentences with long distance dependencies are hard to learn. We also find features of sentences that accentuate or minimize the differences between models. Specifically, the transformer models seem to learn long-distance dependencies much better than the recurrent model, yet have no advantage on sentences with morphological violations. \n Question: Which models are best for learning long-distance movement?",
            "output": [
                "the transformer models"
            ]
        },
        {
            "id": "task460-e317badf5205403f92b82f2016421f26",
            "input": "We crowdsource sentence specificity for evaluation for three target domains: Twitter, Yelp reviews and movie reviews. \n Question: What domains do they experiment with?",
            "output": [
                "Twitter, Yelp reviews and movie reviews"
            ]
        },
        {
            "id": "task460-70f1cd88239a4251ae6ec7358a98588a",
            "input": "Figure FIGREF18 shows the confusion matrices of labels between annotators as heatmaps. \n Question: How is the annotation experiment evaluated?",
            "output": [
                "confusion matrices of labels between annotators"
            ]
        },
        {
            "id": "task460-8a661ecef3ce4ee7b91e6af166e82104",
            "input": "The final annotated dataset consists of 1000 conversations composed of 6833 sentences and 88047 tokens.  \n Question: What is the size of the dataset?",
            "output": [
                "1000 conversations composed of 6833 sentences and 88047 tokens"
            ]
        },
        {
            "id": "task460-b4e1588e95524e898b341732b4c8a0e0",
            "input": "Chatino is a group of languages spoken in Oaxaca, Mexico. Together with the Zapotec language group, the Chatino languages form the Zapotecan branch of the Otomanguean language family.  \n Question: Which language family does Chatino belong to?",
            "output": [
                "the Otomanguean language family"
            ]
        },
        {
            "id": "task460-75a783d58e814c5582b0827be9846dbc",
            "input": "We adopt the annotation machinery of BIBREF5 implemented using Amazon's Mechanical Turk, and annotate each predicate by 2 trained workers independently, while a third consolidates their annotations into a final set of roles and arguments.  \n Question: How was the previous dataset annotated?",
            "output": [
                "the annotation machinery of BIBREF5"
            ]
        },
        {
            "id": "task460-d2e6abfee49945449f2607d13a3cce9c",
            "input": "We collected a simplified dataset from Simple English Wikipedia that are freely available, which has been previously used for many text simplification methods BIBREF0 , BIBREF10 , BIBREF3 . \n Question: what language does this paper focus on?",
            "output": [
                "English"
            ]
        },
        {
            "id": "task460-fa7f7a0b902c4aa3ab3df82f1822c305",
            "input": "In BIBREF14 , several word embedding models trained in a large Portuguese corpus are evaluated.  Using the word2vec model available in a public repository BIBREF14 , the proposal involves the analysis of the most similar analogies generated before and after the application of the BIBREF3 .  \n Question: What were the word embeddings trained on?",
            "output": [
                "large Portuguese corpus"
            ]
        },
        {
            "id": "task460-2ea5ea7d133f422c84428a4e4c987b57",
            "input": "Finally, we performed hyperparameter tuning by adjusting the number of coattention blocks, the batch size, and the number of epochs trained and ensembled our three best networks. \n Question: What hyperparameters have been tuned?",
            "output": [
                "number of coattention blocks, the batch size, and the number of epochs trained and ensembled our three best networks"
            ]
        },
        {
            "id": "task460-38428ce9d8544edab0980f5c0da1cfe9",
            "input": "Dialogue Collection: before the formal data collection starts, we required the workers to make a small number of dialogues and gave them feedback about the dialogue quality. Then, well-trained workers were paired to converse according to the given goals. The workers were also asked to annotate both user states and system states.\n\nDialogue Annotation: we used some rules to automatically annotate dialogue acts according to user states, system states, and dialogue histories. To evaluate the quality of the annotation of dialogue acts and states, three experts were employed to manually annotate dialogue acts and states for 50 dialogues. The results show that our annotations are of high quality. Finally, each dialogue contains a structured goal, a task description, user states, system states, dialogue acts, and utterances. \n Question: How was the corpus annotated?",
            "output": [
                "The workers were also asked to annotate both user states and system states we used some rules to automatically annotate dialogue acts according to user states, system states, and dialogue histories"
            ]
        },
        {
            "id": "task460-7f6eb54f3b074dc0b28f84987acc950c",
            "input": "A simple, yet recurrent text model is the well-known word co-occurrence network. After optional textual pre-processing steps, in a co-occurrence network each different word becomes a node and edges are established via co-occurrence in a desired window. A common strategy connects only adjacent words in the so called word adjacency networks.\n\nWhile the co-occurrence representation yields good results in classification scenarios, some important features are not considered in the model. For example, long-range syntactical links, though less frequent than adjacent syntactical relationships, might be disregarded from a simple word adjacency approach BIBREF12. \n Question: What is the reason that traditional co-occurrence networks fail in establishing links between similar words whenever they appear distant in the text?",
            "output": [
                "long-range syntactical links, though less frequent than adjacent syntactical relationships, might be disregarded from a simple word adjacency approach"
            ]
        },
        {
            "id": "task460-b81cdb9fd76644f8b3692a7ec738b57b",
            "input": "The choice of two methods for our empirical study is motivated by the best performance achieved by Logistic Regression in question-question similarity at SemEval 2017 (best system BIBREF37 and second best system BIBREF38 ), and the high performance achieved by neural networks on larger datasets such as SNLI BIBREF13 , BIBREF39 , BIBREF40 , BIBREF41  \n Question: What machine learning and deep learning methods are used for RQE?",
            "output": [
                "Logistic Regression neural networks"
            ]
        },
        {
            "id": "task460-301eb81aff7a4d96b2ecf624f874bc86",
            "input": "The resulting dataset consists of 22,880 users, 41,094 blogs, and 561,003 posts. Table TABREF2 presents additional statistics of our dataset. \n Question: How many users do they look at?",
            "output": [
                "22,880 users"
            ]
        },
        {
            "id": "task460-6bf775537808485a94a65eecfae5db17",
            "input": "Architecture-based methods either try to induce task-specific architecture during pre-training (task-specific methods), or aim at building a general pre-training architecture to fit all downstream tasks (task-agnostic methods) \n Question: How architecture-based method handle obstacles in NLG?",
            "output": [
                "task-specific architecture during pre-training (task-specific methods) aim at building a general pre-training architecture to fit all downstream tasks (task-agnostic methods)"
            ]
        },
        {
            "id": "task460-0862233fe2a54a239363b3792ef44238",
            "input": "Our experiments are run using the English$\\rightarrow $Italian/German portions of the MuST-C corpus BIBREF25, which is extracted from TED talks, using the same train/validation/test split as provided with the corpus (see Table TABREF18). As additional data, we use a mix of public and proprietary data for about 16 million sentence pairs for English-Italian (En-It) and $4.4$ million WMT14 sentence pairs for the English-German (En-De). \n Question: What dataset do they use?",
            "output": [
                "English$\\rightarrow $Italian/German portions of the MuST-C corpus As additional data, we use a mix of public and proprietary data for about 16 million sentence pairs for English-Italian (En-It) and $4.4$ million WMT14 sentence pairs for the English-German (En-De)"
            ]
        },
        {
            "id": "task460-810c3517566548869f3123c3ec171391",
            "input": "The training objective is to minimize the negative likelihood of the aligned unanswerable question $\\tilde{q}$ given the answerable question $q$ and its corresponding paragraph $p$ that contains the answer $a$ : L=-(q,q,p,a)DP(q|q,p,a;) where $\\mathcal {D}$ is the training corpus and $\\theta $ denotes all the parameters. Sequence-to-sequence and pair-to-sequence models are trained with the same objective. \n Question: What is the training objective of their pair-to-sequence model?",
            "output": [
                "is to minimize the negative likelihood of the aligned unanswerable question $\\tilde{q}$ given the answerable question $q$ and its corresponding paragraph $p$ that contains the answer "
            ]
        },
        {
            "id": "task460-82442641ea88418dabbf5ef28629cff3",
            "input": "To model an answer which is a collection of spans, the multi-span head uses the $\\mathtt {BIO}$ tagging format BIBREF8: $\\mathtt {B}$ is used to mark the beginning of a span, $\\mathtt {I}$ is used to mark the inside of a span and $\\mathtt {O}$ is used to mark tokens not included in a span. In this way, we get a sequence of chunks that can be decoded to a final answer - a collection of spans. \n Question: How they use sequence tagging to answer multi-span questions?",
            "output": [
                "To model an answer which is a collection of spans, the multi-span head uses the $\\mathtt {BIO}$ tagging format BIBREF8: $\\mathtt {B}$ is used to mark the beginning of a span, $\\mathtt {I}$ is used to mark the inside of a span and $\\mathtt {O}$ is used to mark tokens not included in a span"
            ]
        },
        {
            "id": "task460-6b75c5a1a74d467da460d6edbf4c08cd",
            "input": "$Coherence@k$ has been shown to have high correlation with human interpretability of topics learned via various topic modeling methods BIBREF7 . Hence, we can expect interpretable embeddings by maximizing it. For evaluating the interpretability, we use $Coherence@k$ (Equation 6 ) , automated and manual word intrusion tests. \n Question: How do they evaluate interpretability?",
            "output": [
                "For evaluating the interpretability, we use $Coherence@k$ (Equation 6 ) , automated and manual word intrusion tests."
            ]
        },
        {
            "id": "task460-b7f01303cfac47089d496b7758c5721b",
            "input": "Our RoBERTa baseline performs quite well on HotpotQA (77.0 F1), despite processing each paragraph separately, which prohibits inter-paragraph reasoning. We fine-tune a pre-trained model to take a question and several paragraphs and predicts the answer, similar to the single-hop QA model from BIBREF21.  We use $\\textsc {RoBERTa}_{\\textsc {LARGE}}$ BIBREF24 as our pre-trained initialization.  \n Question: What is the strong baseline that this work outperforms?",
            "output": [
                "RoBERTa baseline"
            ]
        },
        {
            "id": "task460-8293ed10a24c42f895ab2e59eaaa4700",
            "input": "By analyzing large numbers of dialogue act sequences correlated with specific outcomes, various rules can be derived, i.e. \"Continuing to request information late in a conversation often leads to customer dissatisfaction.\" This can then be codified into a best practice pattern rules for automated systems, such as \"A request for information act should be issued early in a conversation, followed by an answer, informative statement, or apology towards the end of the conversation.\" Our analysis helps zone in on how the use of certain dialogue acts may be likely to result in different outcomes. The weights we observe vary in the amount of insight provided: for example, offering extra help at the end of a conversation, or thanking the customer yields more satisfied customers, and more resolved problems (with ratios of above 6:1). However, some outcomes are much more subtle: for example, asking yes-no questions early-on in a conversation is highly associated with problem resolution (ratio 3:1), but asking them at the end of a conversation has as similarly strong association with unsatisfied customers. Giving elaborate answers that are not a simple affirmative, negative, or response acknowledgement (i.e. Answer (Other)) towards the middle of a conversation leads to satisfied customers that are not frustrated. Likewise, requesting information towards the end of a conversation (implying that more information is still necessary at the termination of the dialogue) leads to unsatisfied and unresolved customers, with ratios of at least 4:1. \n Question: Which patterns and rules are derived?",
            "output": [
                "A request for information act should be issued early in a conversation, followed by an answer, informative statement, or apology towards the end of the conversation  offering extra help at the end of a conversation, or thanking the customer yields more satisfied customers, and more resolved problems  asking yes-no questions early-on in a conversation is highly associated with problem resolution (ratio 3:1), but asking them at the end of a conversation has as similarly strong association with unsatisfied customers Giving elaborate answers that are not a simple affirmative, negative, or response acknowledgement (i.e. Answer (Other)) towards the middle of a conversation leads to satisfied customers that are not frustrated requesting information towards the end of a conversation (implying that more information is still necessary at the termination of the dialogue) leads to unsatisfied and unresolved customers"
            ]
        },
        {
            "id": "task460-f570eaba8c4b49648e26dc39e8758899",
            "input": "The method is evaluated on the BERTbase model, which has 12 layers, 12 self-attention heads with a hidden size of 768.\n\nTo accelerate the training speed, two-phase training BIBREF1 is adopted. The first phase uses a maximal sentence length of 128, and 512 for the second phase. The numbers of training steps of two phases are 50K and 40K for the BERTBase model. We used AdamW BIBREF13 optimizer with a learning rate of 1e-4, a $\\beta _1$ of 0.9, a $\\beta _2$ of 0.999 and a L2 weight decay rate of $0.01$. The first 10% of the total steps are used for learning rate warming up, followed by the linear decay schema. We used a dropout probability of 0.1 on all layers. The data used for pre-training is the same as BERT, i.e., English Wikipedia (2500M words) and BookCorpus (800M words) BIBREF14. \n Question: Do they train their model starting from a checkpoint?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-ba5e1d469d174261a13085f1e6976c59",
            "input": "For the emotion recognition from text, we manually transcribe all utterances of our AMMER study. To exploit existing and available data sets which are larger than the AMMER data set, we develop a transfer learning approach. We use a neural network with an embedding layer (frozen weights, pre-trained on Common Crawl and Wikipedia BIBREF36), a bidirectional LSTM BIBREF37, and two dense layers followed by a soft max output layer. This setup is inspired by BIBREF38. We use a dropout rate of 0.3 in all layers and optimize with Adam BIBREF39 with a learning rate of $10^{-5}$ (These parameters are the same for all further experiments). We build on top of the Keras library with the TensorFlow backend. We consider this setup our baseline model. \n Question: What is the baseline method for the task?",
            "output": [
                "For the emotion recognition from text they use described neural network as baseline.\nFor audio and face there is no baseline."
            ]
        },
        {
            "id": "task460-4cdf1c60441d4bdb86e5cd04607025a4",
            "input": "We collected Japanese fictional stories from the Web to construct the dataset. \n Question: How is the dataset created?",
            "output": [
                "We collected Japanese fictional stories from the Web"
            ]
        },
        {
            "id": "task460-37deea6b01a6448c8714e7839806916c",
            "input": "The results are shown in Fig. FIGREF87 for different numbers of topics, where we can see that the proposed model outperforms all the baselines, being the svi version the one that performs best. In order to assess the computational advantages of the stochastic variational inference (svi) over the batch algorithm, the log marginal likelihood (or log evidence) was plotted against the number of iterations. Fig. FIGREF88 shows this comparison. Not surprisingly, the svi version converges much faster to higher values of the log marginal likelihood when compared to the batch version, which reflects the efficiency of the svi algorithm. \n Question: what are the advantages of the proposed model?",
            "output": [
                "he proposed model outperforms all the baselines, being the svi version the one that performs best. the svi version converges much faster to higher values of the log marginal likelihood when compared to the batch version, which reflects the efficiency of the svi algorithm."
            ]
        },
        {
            "id": "task460-039191addf024ff38ab997265d80debd",
            "input": "Given a statement and articles, workers are asked to judge whether the statement can be derived from the articles at three grades: True, Likely (i.e. Answerable), or Unsure (i.e. Unanswerable). If a worker selects Unsure, we ask workers to tell us why they are unsure from two choices (“Not stated in the article” or “Other”). If a worker selects True or Likely in the judgement task, we first ask which sentences in the given articles are justification explanations for a given statement, similarly to HotpotQA BIBREF2. The “summary” text boxes (i.e. NLDs) are then initialized with these selected sentences. \n Question: How was the dataset annotated?",
            "output": [
                "True, Likely (i.e. Answerable), or Unsure (i.e. Unanswerable) why they are unsure from two choices (“Not stated in the article” or “Other”) The “summary” text boxes"
            ]
        },
        {
            "id": "task460-8f671301df3b40788a4be6bb797a7bc7",
            "input": "Let us take a closer look at what we mean by agreement between the two parse trees. Essentially, if we have two words in the English sentence denoted by i and i', aligned to words j and j' in the parallel Hindi sentence respectively, we can expect a dependency edge between i and i' in the English parse tree to correspond to an edge between j and j' in the Hindi parse tree, and vice versa. For an edge in the English parse tree, we term the corresponding edge or path in the Hindi parse tree as the projection or projected path of the English edge on the Hindi parse tree, and similarly there are projected paths from Hindi to English. The dual decomposition inference algorithm tries to bring the parse trees in the two languages through its constraints. \n Question: How does enforcing agreement between parse trees work across different languages?",
            "output": [
                "we can expect a dependency edge between i and i' in the English parse tree to correspond to an edge between j and j' in the Hindi parse tree dual decomposition inference algorithm tries to bring the parse trees in the two languages through its constraints"
            ]
        },
        {
            "id": "task460-19f1f32f563c475592f836395b3a31fa",
            "input": "Our model introduces a multi-turns inference mechanism to process multi-perspective matching features.  \n Question: Which matching features do they employ?",
            "output": [
                "Matching features from matching sentences from various perspectives."
            ]
        },
        {
            "id": "task460-cc18814b143d4b56a4d8228419faa192",
            "input": "We use the publicly available dataset KVRET BIBREF5 in our experiments.  This dataset includes dialogues in 3 domains: calendar, weather, navigation (POI) which is suitable for our mix-domain dialogue experiments.  \n Question: Which domains did they explored?",
            "output": [
                "calendar weather navigation"
            ]
        },
        {
            "id": "task460-6478168c12724c45828dc72448c08497",
            "input": "We use English translations of the Chinese source texts in the WMT 2017 English–Chinese test set BIBREF18 for all experiments presented in this article: \n Question: What languages do they investigate for machine translation?",
            "output": [
                "English  Chinese "
            ]
        },
        {
            "id": "task460-7e58eaad5291423cb3dbfee9aabac3af",
            "input": "For this reason as a baseline algorithm for English dataset we refer to results from BIBREF0, and as for Russian dataset, we used the probabilistic language model, described in BIBREF8. \n Question: Which languages are used in the paper?",
            "output": [
                "English Russian"
            ]
        },
        {
            "id": "task460-3e22cef983e64e2a804098347d4b3d4d",
            "input": "We carry out experiments on multiple language pairs including German-English, French-English, and Japanese-English.  \n Question: Which languages are used in the multi-lingual caption model?",
            "output": [
                "German-English, French-English, and Japanese-English"
            ]
        },
        {
            "id": "task460-9c64cceecb9a45718127b7fe54cf1ce9",
            "input": "Stanford - Twitter Sentiment Corpus (STS Corpus): STS Corpus contains 1,600K training tweets collected by a crawler from BIBREF0 . BIBREF0 constructed a test set manually with 177 negative and 182 positive tweets. The Stanford test set is small. However, it has been widely used in different evaluation tasks BIBREF0 BIBREF5 BIBREF13 .\n\nSanders - Twitter Sentiment Corpus: This dataset consists of hand-classified tweets collected by using search terms: INLINEFORM0 , #google, #microsoft and #twitter. We construct the dataset as BIBREF14 for binary classification.\n\nHealth Care Reform (HCR): This dataset was constructed by crawling tweets containing the hashtag #hcr BIBREF15 . Task is to predict positive/negative tweets BIBREF14 . \n Question: Which datasets did they use?",
            "output": [
                "Stanford - Twitter Sentiment Corpus (STS Corpus) Sanders - Twitter Sentiment Corpus Health Care Reform (HCR)"
            ]
        },
        {
            "id": "task460-56b130205d7142a6b5e8b70c5ecd351b",
            "input": "In addition, we note that our approach can still lead to erroneous facts or even hallucinations. An interesting perspective might be to further constrain the model on the data structure in order to prevent inaccurate of even contradictory descriptions. \n Question: What future possible improvements are listed?",
            "output": [
                "rther constrain the model on the data structure in order to prevent inaccurate of even contradictory descriptions"
            ]
        },
        {
            "id": "task460-2d86b5b3b1f04067883fc5532497f006",
            "input": "Most of the results seem to match with the views of some experts such as the political pundits of the Washington Post. \n Question: Who are the experts?",
            "output": [
                "political pundits of the Washington Post"
            ]
        },
        {
            "id": "task460-ecc0c0a405e54f12b4e5b6ddfe3a16b3",
            "input": "In this study, we experiment four different classification models including logistic regression (LR), recurrent neural network (RNN) BIBREF35, convolutional neural network (CNN) BIBREF36 and Google BERT BIBREF37.  \n Question: Which document classifiers do they experiment with?",
            "output": [
                "logistic regression (LR), recurrent neural network (RNN) BIBREF35, convolutional neural network (CNN) BIBREF36 and Google BERT BIBREF37"
            ]
        },
        {
            "id": "task460-b302b995d1d340508c833d7cbad6ef84",
            "input": "Traditional text-to-speech (TTS) systems are composed of complex pipelines BIBREF0 , these often include accoustic frontends, duration model, acoustic prediction model and vocoder models. Neural text-to-speech systems have garnered large research interest in the past 2 years. The first to fully explore this avenue of research was Google's tacotron BIBREF1 system The architecture of our model utilizes RNN-based Seq2Seq model for generating mel spectrogram from text. The architecture is similar to that of Tacotron 2 BIBREF4 .  Direct comparison of model parameters between ours and the open-source tacotron 2, our model contains 4.5 million parameters, whereas the Tacotron 2 contains around 13 million parameters with default setting \n Question: How do they measure the size of models?",
            "output": [
                "Direct comparison of model parameters"
            ]
        },
        {
            "id": "task460-96a3b242a4ea4bf595116d590bfb0ab0",
            "input": "convertible.pl: implementing DCG rules for 1st and 3rd steps in the three-steps conversion, as well as other rules including lexicon. \n Question: What DCGs are used?",
            "output": [
                "Author's own DCG rules are defined from scratch."
            ]
        },
        {
            "id": "task460-6b1ec8edba3f4b468d70eb5c56d19b85",
            "input": "We utilize the VATEX dataset for video captioning, which contains over 41,250 videos and 825,000 captions in both English and Chinese. Among the captions, there are over 206,000 English-Chinese parallel translation pairs. It covers 600 human activities and a variety of video content. Each video is paired with 10 English and 10 Chinese diverse captions. We follow the official split with 25,991 videos for training, 3,000 videos for validation and 6,000 public test videos for final testing. \n Question: How big is the dataset used?",
            "output": [
                "over 41,250 videos and 825,000 captions in both English and Chinese. over 206,000 English-Chinese parallel translation pairs"
            ]
        },
        {
            "id": "task460-a49ea0df24e74a809f1b59955dd4aa4c",
            "input": " For a given query $q = \\langle s, r, ? \\rangle $ , we identify mentions in $S_q$ of the entities in $C_q \\cup \\lbrace s\\rbrace $ and create one node per mention. This process is based on the following heuristic:\n\nwe consider mentions spans in $S_q$ exactly matching an element of $C_q \\cup \\lbrace s\\rbrace $ . Admittedly, this is a rather simple strategy which may suffer from low recall.\n\nwe use predictions from a coreference resolution system to add mentions of elements in $C_q \\cup \\lbrace s\\rbrace $ beyond exact matching (including both noun phrases and anaphoric pronouns). In particular, we use the end-to-end coreference resolution by BIBREF16 .\n\nwe discard mentions which are ambiguously resolved to multiple coreference chains; this may sacrifice recall, but avoids propagating ambiguity. \n Question: How did they detect entity mentions?",
            "output": [
                "Exact matches to the entity string and predictions from a coreference resolution system"
            ]
        },
        {
            "id": "task460-21d59105f15049e2baf774f58922bfa9",
            "input": "The joke corpus in this dataset contains thousands of unique jokes of different categories (sci-fi, sports, etc) and types (puns, limerick, etc). \n Question: Where did the real production data come from?",
            "output": [
                " jokes of different categories (sci-fi, sports, etc) and types (puns, limerick, etc)"
            ]
        },
        {
            "id": "task460-e1385c023725403080a92623618f90f9",
            "input": "An important feature when suggesting an article INLINEFORM0 to an entity INLINEFORM1 is the novelty of INLINEFORM2 w.r.t the already existing entity profile INLINEFORM3 Given an entity INLINEFORM0 and the already added news references INLINEFORM1 up to year INLINEFORM2 , the novelty of INLINEFORM3 at year INLINEFORM4 is measured by the KL divergence between the language model of INLINEFORM5 and articles in INLINEFORM6 . We combine this measure with the entity overlap of INLINEFORM7 and INLINEFORM8 . The novelty value of INLINEFORM9 is given by the minimal divergence value. Low scores indicate low novelty for the entity profile INLINEFORM10 .\n\n \n Question: What features are used to represent the novelty of news articles to entity pages?",
            "output": [
                "KL-divergences of language models for the news article and the already added news references"
            ]
        },
        {
            "id": "task460-89f867226fa44061baaf2564213e35c5",
            "input": " Our final survey based classification results showed an accuracy of 96% in detecting PTSD and mean squared error of 1.2 in estimating its intensity given we have four intensity, No PTSD, Low Risk PTSD, Moderate Risk PTSD and High Risk PTSD with a score of 0, 1, 2 and 3 respectively.  \n Question: How is the intensity of the PTSD established?",
            "output": [
                "Given we have four intensity, No PTSD, Low Risk PTSD, Moderate Risk PTSD and High Risk PTSD with a score of 0, 1, 2 and 3 respectively, the estimated intensity  is established as mean  squared error."
            ]
        },
        {
            "id": "task460-91cd80618d9b4cc68e06abc7f604bfa9",
            "input": "The second, Continuous Bag-of-Words (CBOW), the model is given the sequence of words without the middle one and attempts to predict this omitted word.  \n Question: Which word embeddings are analysed?",
            "output": [
                "Continuous Bag-of-Words (CBOW)"
            ]
        },
        {
            "id": "task460-b34a8aad759d42ddb3e91e7e112552ab",
            "input": "The decoder is the second LSTM network that uses the information obtained from the encoder to generate the sequence's story. The first input $x_0$ to the decoder is the image for which the text is being generated. The last hidden state from the encoder $h_e^{(t)}$ is used to initialize the first hidden state of the decoder $h_d^{(0)}$ . With this strategy, we provide the decoder with the context of the whole sequence and the content of the current image (i.e. global and local information) to generate the corresponding text that will contribute to the overall story. \n Question: How is the sequential nature of the story captured?",
            "output": [
                "we provide the decoder with the context of the whole sequence and the content of the current image (i.e. global and local information) to generate the corresponding text that will contribute to the overall story"
            ]
        },
        {
            "id": "task460-8cdc37491cc74d35b1bf49c64f6fca84",
            "input": "We evaluate our model and baseline models on three versions of the dataset. The first one (Inc) only considers the original data, containing naturally incorrect tweets, and achieves accuracy of 80$\\%$ against BERT's 72$\\%$. The second version (Corr) considers the corrected tweets, and shows higher accuracy given that it is less noisy.  \n Question: Should their approach be applied only when dealing with incomplete data?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-81d587e6b46b4ad8967bfaa6840f662e",
            "input": "Specifically, for each multiple-choice question $(q,A) \\in Q_\\mathit {tr}$ and each choice $a \\in A$ , we use all non-stopword tokens in $q$ and $a$ as an ElasticSearch query against S. We take the top 200 hits, run Open IE v4, and aggregate the resulting tuples over all $a \\in A$ and over all questions in $Q_\\mathit {tr}$ to create the tuple KB (T). \n Question: What OpenIE method was used to generate the extractions?",
            "output": [
                "for each multiple-choice question $(q,A) \\in Q_\\mathit {tr}$ and each choice $a \\in A$ , we use all non-stopword tokens in $q$ and $a$ as an ElasticSearch query against S take the top 200 hits, run Open IE v4, and aggregate the resulting tuples over all $a \\in A$ and over all questions in $Q_\\mathit {tr}$"
            ]
        },
        {
            "id": "task460-817a1048514049a38d8bdc5b1d8f3cc3",
            "input": "In light of this, we suggest that care is needed when applying graph embeddings in NLP pipelines, and work needed to develop robust methods to debias such embeddings. \n Question: Do they propose any solution to debias the embeddings?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-8eb8afcf74d34218884f4df43d2db8b8",
            "input": " 29,794 The Wikipedia dataset consists of articles from English Wikipedia, with quality class labels assigned by the Wikipedia community. Wikipedia articles are labelled with one of six quality classes, in descending order of quality: Featured Article (“FA”), Good Article (“GA”), B-class Article (“B”), C-class Article (“C”), Start Article (“Start”), and Stub Article (“Stub”). We randomly sampled 5,000 articles from each quality class and removed all redirect pages, resulting in a dataset of 29,794 articles. \n Question: How large is their data set?",
            "output": [
                "a sample of  29,794 wikipedia articles and 2,794 arXiv papers "
            ]
        },
        {
            "id": "task460-d104cf00896a45e9a4b1888268cd1dac",
            "input": "In order to evaluate the models trained on generated data, we manually annotated a named entities dataset comprising 53453 tokens and 2566 sentences selected from over 250 news texts from ilur.am. \n Question: what is the source of the news sentences?",
            "output": [
                "ilur.am"
            ]
        },
        {
            "id": "task460-cf2659b0437746cba9e8b0150fb827e4",
            "input": "The DBpedia SPARQL endpoint is used for query answering and reasoning. \n Question: What is the reasoning method that is used?",
            "output": [
                "SPARQL"
            ]
        },
        {
            "id": "task460-e2544420e038477cbe6ee4234ebe8d9b",
            "input": "We tested the system on two datasets, different in size and complexity of the addressed language.\n\nExperimental Evaluation ::: Datasets ::: NLU-Benchmark dataset\nThe first (publicly available) dataset, NLU-Benchmark (NLU-BM), contains $25,716$ utterances annotated with targeted Scenario, Action, and involved Entities. Experimental Evaluation ::: Datasets ::: ROMULUS dataset\nThe second dataset, ROMULUS, is composed of $1,431$ sentences, for each of which dialogue acts, semantic frames, and corresponding frame elements are provided. This dataset is being developed for modelling user utterances to open-domain conversational systems for robotic platforms that are expected to handle different interaction situations/patterns – e.g., chit-chat, command interpretation. \n Question: Which publicly available NLU dataset is used?",
            "output": [
                "ROMULUS dataset NLU-Benchmark dataset"
            ]
        },
        {
            "id": "task460-9193ec3212b84fb8b78bd1bf4f01c1e9",
            "input": "We compile three Chinese text corpora from online data for three domains, namely, “hotel\", “mobile phone (mobile)\", and “travel\". All texts are about user reviews.  \n Question: What are the sources of the data?",
            "output": [
                "User reviews written in Chinese collected online for hotel, mobile phone, and travel domains"
            ]
        },
        {
            "id": "task460-7adde40fe26a4561a096cd2866dcac87",
            "input": "To do so, we constructed a new dataset with 23,700 queries that are short and unstructured, in the same style made by real users of task-oriented systems.  \n Question: What is the size of this dataset?",
            "output": [
                "23,700 "
            ]
        },
        {
            "id": "task460-70b04e5a53fa4455b85ef098409a88df",
            "input": "In BIBREF8 a refined collection of tweets gathered from twitter is presented. Their dataset which is labeled for named entity recognition task contains 8,257 tweets. There are 12,784 entities in total in this dataset. Table TABREF19 shows statistics related to each named entity in training, development and test sets.\n\n \n Question: Which social media platform is explored?",
            "output": [
                "twitter "
            ]
        },
        {
            "id": "task460-a3b4ab494dbc4e2d811fff8d73f24293",
            "input": "we apply our domain adaptation method to a neural captioning model and show performance improvement over other standard methods on several datasets and metrics.  \n Question: Did they only experiment with captioning task?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-2369c29b12b2404985a9c03a26dcbfc6",
            "input": "Just as introduced in sec:introduction, it is intractable to compute similarity exactly, as involving INLINEFORM0 computation. Hence, we consider the monte-carlo approximation: DISPLAYFORM0\n\nwhere INLINEFORM0 is a list of entity pairs sampled from INLINEFORM1 . We use sequential sampling to gain INLINEFORM6 , which means we first sample INLINEFORM7 given INLINEFORM8 from INLINEFORM9 , and then sample INLINEFORM10 given INLINEFORM11 and INLINEFORM12 from INLINEFORM13 . \n Question: Which sampling method do they use to approximate similarity between the conditional probability distributions over entity pairs?",
            "output": [
                "monte-carlo sequential sampling"
            ]
        },
        {
            "id": "task460-65f660925f024e5f981dfb727081bda4",
            "input": "mcdonald:11 established that, when no treebank annotations are available in the target language, training on multiple source languages outperforms training on one (i.e., multi-source model transfer outperforms single-source model transfer). Following guo:16, for each target language, we train the parser on six other languages in the Google universal dependency treebanks version 2.0 (de, en, es, fr, it, pt, sv, excluding whichever is the target language), and we use gold coarse POS tags. \n Question: How does the model work if no treebank is available?",
            "output": [
                "train the parser on six other languages in the Google universal dependency treebanks version 2.0 (de, en, es, fr, it, pt, sv, excluding whichever is the target language), and we use gold coarse POS tags"
            ]
        },
        {
            "id": "task460-9209b4f5001941a0891e888f19f4ba64",
            "input": "Our official scores (column Ens Test in Table TABREF19 ) have placed us second (EI-Reg, EI-Oc), fourth (V-Reg) and fifth (V-Oc) on the SemEval AIT-2018 leaderboard. \n Question: What were the scores of their system?",
            "output": [
                "column Ens Test in Table TABREF19"
            ]
        },
        {
            "id": "task460-421c53bd910f449a9bd598425b379ba9",
            "input": "The corpus used for sentiment analysis is the IMDb dataset of movie reviews by BIBREF11 while that for NER is Groningen Meaning Bank (GMB) by BIBREF12, containing 47,959 sentence samples.  \n Question: What Named Entity Recognition dataset is used?",
            "output": [
                "Groningen Meaning Bank"
            ]
        },
        {
            "id": "task460-dfc6f1e855f34548adb73cce2319551b",
            "input": "We ensemble BERT and E-BERT by (a) mean-pooling their outputs (AVG) or (b) concatenating the entity and its name with a slash symbol (CONCAT), e.g.: Jean_Marais / Jean Mara ##is. \n Question: What are the two ways of ensembling BERT and E-BERT?",
            "output": [
                "mean-pooling their outputs (AVG) concatenating the entity and its name with a slash symbol (CONCAT)"
            ]
        },
        {
            "id": "task460-05ebe2c34d954160a5a42025f51a0700",
            "input": "In the first task, we use the SST-2 (Stanford Sentiment Treebank, version 2) dataset BIBREF25 to conduct sentiment analysis experiments. SST-2 is a sentence binary classification dataset with train/dev/test splits provided and two types of sentence labels, i.e., positive and negative. Here we experiment with the Snips dataset BIBREF26, which is widely used in SLU research. This dataset contains test spoken utterances (text) classified into one of 7 intents. \n Question: Which real-world datasets did they use?",
            "output": [
                "SST-2 (Stanford Sentiment Treebank, version 2) Snips"
            ]
        },
        {
            "id": "task460-3c6545604a27417397ac3c3554325ff3",
            "input": "Each of the methods we explore improve in % gendered words, % male bias, and F1 over the baseline Transformer generation model, but we find combining all methods in one – the ALL model is the most advantageous. \n Question: What baseline is used to compare the experimental results against?",
            "output": [
                "Transformer generation model"
            ]
        },
        {
            "id": "task460-eb6e8f5d6cc945b892ff5757e4c13bfe",
            "input": "To evaluate our sampling strategy, we compare it with the pretrained model and fine-tuned model on 16 different corpora. The results show that our approach outperforms those baselines on all corpora: it achieves 1.6% lower phone error rate on average. \n Question: By how much do they, on average, outperform the baseline multilingual model on 16 low-resource tasks?",
            "output": [
                "1.6% lower phone error rate on average"
            ]
        },
        {
            "id": "task460-fc09b975c59347ceb9283c5ca87a311a",
            "input": "To tackle this issue, we present a new evaluation dataset that covers a wide range of monotonicity reasoning that was created by crowdsourcing and collected from linguistics publications (Section \"Dataset\" ). \n Question: Do they release MED?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-c1880aa990dc4db896ddd853854b8a65",
            "input": "We created our own causal explanation dataset by collecting 3,268 random Facebook status update messages. \n Question: What types of social media did they consider?",
            "output": [
                "Facebook status update messages"
            ]
        },
        {
            "id": "task460-73e3c846c78d473eb2405fa82ead1955",
            "input": "In this paper, we propose a simple and effective method for learning affective events that only requires a very small seed lexicon and a large raw corpus. As illustrated in Figure FIGREF1, our key idea is that we can exploit discourse relations BIBREF4 to efficiently propagate polarity from seed predicates that directly report one's emotions (e.g., “to be glad” is positive). \n Question: How does their model learn using mostly raw data?",
            "output": [
                "by exploiting discourse relations to propagate polarity from seed predicates to final sentiment polarity"
            ]
        },
        {
            "id": "task460-df0d7a4ed25c4da39cd19b19bf75c5ca",
            "input": "Then, we created test sets with varying levels of perturbation operations - $\\lbrace 20\\%,40\\%,60\\%\\rbrace $. \n Question: Are recurrent neural networks trained on perturbed data?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-e37d2159aadf418b8d5456d134137b91",
            "input": "The baseline applies linear logistic regression to a set of stock technical signals to predict the following day’s stock return sign (+/‐). No sentiment features are provided to the baseline model. \n Question: What is the baseline machine learning prediction approach?",
            "output": [
                "linear logistic regression to a set of stock technical signals"
            ]
        },
        {
            "id": "task460-6e2b557c2de547cfa4b24df2e2b13e1a",
            "input": "Accordingly, marcheggiani2017 presented a neural model putting syntax aside for dependency-based SRL and obtain favorable results, which overturns the inherent belief that syntax is indispensable in SRL task BIBREF11 . \n Question: Are there syntax-agnostic SRL models before?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-78a5b63fce854a01809d00d9b00ec190",
            "input": "The experiment settings from this paper and evaluation protocol for the Mboshi corpus (Boundary F-scores using the ZRC speech reference) are the same from BIBREF8. Table presents the results for bilingual UWS and multilingual leveraging. For the former, we reach our best result by using as aligned information the French, the original aligned language for this dataset.  For the multilingual selection experiments, we experimented combining the languages from top to bottom as they appear Table (ranked by performance; e.g. 1-3 means the combination of FR(1), EN(2) and PT(3)). We observe that the performance improvement is smaller than the one observed in previous work BIBREF10, which we attribute to the fact that our dataset was artificially augmented. Lastly, following the methodology from BIBREF8, we extract the most confident alignments (in terms of ANE) discovered by the bilingual models. Table presents the top 10 most confident (discovered type, translation) pairs. Looking at the pairs the bilingual models are most confident about, we observe there are some types discovered by all the bilingual models (e.g. Mboshi word itua, and the concatenation oboá+ngá). \n Question: How is the performance of the model evaluated?",
            "output": [
                "The experiment settings from this paper and evaluation protocol for the Mboshi corpus (Boundary F-scores using the ZRC speech reference) are the same from BIBREF8. For the multilingual selection experiments, we experimented combining the languages from top to bottom as they appear Table (ranked by performance; e.g. 1-3 means the combination of FR(1), EN(2) and PT(3)).  Lastly, following the methodology from BIBREF8, we extract the most confident alignments (in terms of ANE) discovered by the bilingual models."
            ]
        },
        {
            "id": "task460-5f45efc61c8445c99e1059cb45a247b0",
            "input": "We used the following affective resources relying on different emotion models.\n\nEmolex: it contains 14,182 words associated with eight primary emotion based on the Plutchik model BIBREF10 , BIBREF11 .\n\nEmoSenticNet(EmoSN): it is an enriched version of SenticNet BIBREF12 including 13,189 words labeled by six Ekman's basic emotion BIBREF13 , BIBREF14 .\n\nDictionary of Affect in Language (DAL): includes 8,742 English words labeled by three scores representing three dimensions: Pleasantness, Activation and Imagery BIBREF15 .\n\nAffective Norms for English Words (ANEW): consists of 1,034 English words BIBREF16 rated with ratings based on the Valence-Arousal-Dominance (VAD) model BIBREF17 .\n\nLinguistic Inquiry and Word Count (LIWC): this psycholinguistic resource BIBREF18 includes 4,500 words distributed into 64 emotional categories including positive (PosEMO) and negative (NegEMO). \n Question: What affective-based features are used?",
            "output": [
                "affective features provided by different emotion models such as Emolex, EmoSenticNet, Dictionary of Affect in Language, Affective Norms for English Words and Linguistics Inquiry and Word Count"
            ]
        },
        {
            "id": "task460-081ee1fe1df6440aaa15a54c1e87bb67",
            "input": "We recognise features that add ambiguity to the supporting facts, for example when information is only expressed implicitly by using an Ellipsis. As opposed to redundant words, we annotate Restrictivity and Factivity modifiers, words and phrases whose presence does change the meaning of a sentence with regard to the expected answer, and occurrences of intra- or inter-sentence Coreference in supporting facts (that is relevant to the question). Lastly, we mark ambiguous syntactic features, when their resolution is required in order to obtain the answer. Concretely, we mark argument collection with con- and disjunctions (Listing) and ambiguous Prepositions, Coordination Scope and Relative clauses/Adverbial phrases/Appositions. We recognise features that add ambiguity to the supporting facts, for example when information is only expressed implicitly by using an Ellipsis. As opposed to redundant words, we annotate Restrictivity and Factivity modifiers, words and phrases whose presence does change the meaning of a sentence with regard to the expected answer, and occurrences of intra- or inter-sentence Coreference in supporting facts (that is relevant to the question). Lastly, we mark ambiguous syntactic features, when their resolution is required in order to obtain the answer. Concretely, we mark argument collection with con- and disjunctions (Listing) and ambiguous Prepositions, Coordination Scope and Relative clauses/Adverbial phrases/Appositions. \n Question: What features are absent from MRC gold standards that can result in potential lexical ambiguity?",
            "output": [
                "Restrictivity  Factivity  Coreference "
            ]
        },
        {
            "id": "task460-e1c64ab9bf8e4102967f042f90d1370a",
            "input": "Our WordDecoding (WDec) model achieves F1 scores that are $3.9\\%$ and $4.1\\%$ higher than HRL on the NYT29 and NYT24 datasets respectively. Similarly, our PtrNetDecoding (PNDec) model achieves F1 scores that are $3.0\\%$ and $1.3\\%$ higher than HRL on the NYT29 and NYT24 datasets respectively. \n Question: Which one of two proposed approaches performed better in experiments?",
            "output": [
                "WordDecoding (WDec) model"
            ]
        },
        {
            "id": "task460-43be252811f54b6388ff27ec04ffe269",
            "input": "First, it provides an annotation interface that allows users to define content elements, upload documents, and annotate documents Our platform is able to ingest documents in a variety of formats, including PDFs and Microsoft Word, and converts these formats into plain text before presenting them to the annotators. \n Question: What type of documents are supported by the annotation platform?",
            "output": [
                "Variety of formats supported (PDF, Word...), user can define content elements of document"
            ]
        },
        {
            "id": "task460-107d6134282e4cecaf5468aaadaa44f3",
            "input": "Depechemood is a lexicon-based emotion detection method gathered from crowd-annotated news BIBREF24. Drawing on approximately 23.5K documents with average of 500 words per document from rappler.com, researchers asked subjects to report their emotions after reading each article. They then multiplied the document-emotion matrix and word-document matrix to derive emotion-word matrix for these words.  \n Question: How id Depechemood trained?",
            "output": [
                "By multiplying crowd-annotated document-emotion matrix with emotion-word matrix. "
            ]
        },
        {
            "id": "task460-de07077716e644038b4d675c07e65444",
            "input": "We evaluate our model in a simulated binning task in which the robot is tasked to place a cube into a bowl as outlined by the verbal command. Each environment contains between three and five objects differentiated by their size (small, large), shape (round, square) and color (red, green, blue, yellow, pink), totalling in 20 different objects. Depending on the generated scenario, combinations of these three features are necessary to distinguish the targets from each other, allowing for tasks of varying complexity. \n Question: What simulations are performed by the authors to validate their approach?",
            "output": [
                "a simulated binning task in which the robot is tasked to place a cube into a bowl as outlined by the verbal command"
            ]
        },
        {
            "id": "task460-7cf007ae92374b93bbd661387f2955c3",
            "input": "Non-contextual properties of a word. These features are derived directly from the word and capture the general tendency of a word being echoed in explanations. \n Question: What non-contextual properties do they refer to?",
            "output": [
                "These features are derived directly from the word and capture the general tendency of a word being echoed in explanations."
            ]
        },
        {
            "id": "task460-3d2cdda392ad4e2d8f96620ea6e45477",
            "input": "We define the STransE score function $\\mathcal {R}$1 as follows:\n\n$ f_r(h, t) & = & \\Vert \\textbf {W}_{r,1}\\textbf {h} + \\textbf {r} - \\textbf {W}_{r,2}\\textbf {t}\\Vert _{\\ell _{1/2}} $\n\nusing either the $\\ell _1$ or the $\\ell _2$ -norm (the choice is made using validation data; in our experiments we found that the $\\ell _1$ norm gave slightly better results). \n Question: What scoring function does the model use to score triples?",
            "output": [
                "$ f_r(h, t) & = & \\Vert \\textbf {W}_{r,1}\\textbf {h} + \\textbf {r} - \\textbf {W}_{r,2}\\textbf {t}\\Vert _{\\ell _{1/2}} $"
            ]
        },
        {
            "id": "task460-a1752261d6cf434fa0e1fdce36f4e984",
            "input": " To compensate for the exclusion of slot-specific parameters, we incorporate better feature representation of user utterance and dialogue states using syntactic information and convolutional neural networks (CNN). \n Question: What network architecture do they use for SIM?",
            "output": [
                "convolutional neural networks (CNN)"
            ]
        },
        {
            "id": "task460-58d9431e1735477fb4bc2d280dba8dfe",
            "input": "Then we extract these features from two sources of context texts, specifically the title of the news article that the comment was posted for and the screen name of the user who posted the comment. \n Question: What context do they use?",
            "output": [
                "title of the news article screen name of the user"
            ]
        },
        {
            "id": "task460-8b5f904e2f674518801d38221eefcedb",
            "input": "In order to make the corpus collection easier and faster, we adopted a semi-automatic procedure based on sequential neural models BIBREF19, BIBREF20. Instead, since tokens are transcribed at morpheme level, we split Arabish tokens into characters, and Arabic tokens into morphemes, and we treated each token itself as a sequence. Our model learns thus to map Arabish characters into Arabic morphemes. With this model we automatically transcribed into Arabic morphemes, roughly, 5,000 additional tokens, corresponding to the second annotation block.  Manual transcription plus a \n Question: How does the semi-automatic construction process work?",
            "output": [
                "Automatic transcription of 5000 tokens through sequential neural models trained on the annotated part of the corpus"
            ]
        },
        {
            "id": "task460-413559c2683847e9b9ff4011e7192ea0",
            "input": "The gain from disabling a single head is different for different tasks, ranging from the minimum absolute gain of 0.1% for STS-B, to the maximum of 1.2% for MRPC (see fig:disableheadsall). Furthermore, disabling a whole layer, that is, all 12 heads in a given layer, also improves the results. fig:disablelayers shows the resulting model performance on the target GLUE tasks when different layers are disabled. \n Question: In which certain heads was attention disabled in experiments?",
            "output": [
                "single head disabling a whole layer, that is, all 12 heads in a given layer"
            ]
        },
        {
            "id": "task460-7ad79c63bd6943079af553b2dcba05cc",
            "input": "The experimental results are shown in Table 1, where it can be seen that autoencoders outperformed MLP and CNN outperformed autoencoders with the highest achieved accuracy of 82.6%. \n Question: What was their performance on the dataset?",
            "output": [
                "accuracy of 82.6%"
            ]
        },
        {
            "id": "task460-1a37cc4450ca49d9a0cc3f9331c2783e",
            "input": "From the chart, it is clear that there is a higher chance of fake news coming from unverified accounts. Notice that accounts spreading viral tweets with fake news have, on average, a larger ratio of friends/followers.  Figure FIGREF24 shows that, in contrast to other kinds of viral tweets, those containing fake news were created more recently. \n Question: What are the characteristics of the accounts that spread fake news?",
            "output": [
                "Accounts that spread fake news are mostly unverified, recently created and have on average high friends/followers ratio"
            ]
        },
        {
            "id": "task460-2edd9dce522041c6bdc659b501121798",
            "input": "However, what we did was more than this, and we also achieved great results on the second-level and third-level senses for the sake of our self-demand for high-quality, finally achieving agreement of 0.85 and Kappa value of 0.83 for these two deeper levels of senses. \n Question: How high is the inter-annotator agreement?",
            "output": [
                "agreement of 0.85 and Kappa value of 0.83"
            ]
        },
        {
            "id": "task460-5b9a9fe2743647a88721e6448f30b1bd",
            "input": " In terms of this, rather than starting from Twitter API Search, we look into the archived snapshots of two major news websites (CNN, NBC), and then extract the tweet blocks that are embedded in the news articles. \n Question: How do they determine if tweets have been used by journalists?",
            "output": [
                " we look into the archived snapshots of two major news websites (CNN, NBC), and then extract the tweet blocks that are embedded in the news articles"
            ]
        },
        {
            "id": "task460-c23cfe3018f944199f817c4fdaaf4768",
            "input": "The current release of Katecheo uses a Bi-Directional Attention Flow, or BiDAF, model for reading comprehension BIBREF6 . Future releases of Katecheo will include the ability to swap out the reading comprehension model for newer architectures based on, e.g., BERT BIBREF8 or XLNet BIBREF9 or custom trained models.\n\nArchitecture and Configuration \n Question: what pretrained models were used?",
            "output": [
                "BiDAF BERT "
            ]
        },
        {
            "id": "task460-11b0730ead244b5291c9f859f65e7ad4",
            "input": "Similar to BIBREF9, we experiment with SVM with RBF kernel, with features that represent (1) the simple characteristics of the argument tree and (2) the linguistic characteristics of the claim. \n Question: What models that rely only on claim-specific linguistic features are used as baselines?",
            "output": [
                "SVM with RBF kernel"
            ]
        },
        {
            "id": "task460-fd31c8c459e746dfba6114256d466e5d",
            "input": "Dialogue act (DA) characterizes the type of a speaker's intention in the course of producing an utterance and is approximately equivalent to the illocutionary act of BIBREF0 or the speech act of BIBREF1. The recognition of DA is essential for modeling and automatically detecting discourse structure, especially in developing a human-machine dialogue system. It is natural to predict the Answer acts following an utterance of type Question, and then match the Question utterance to each QA-pair in the knowledge base. The predicted DA can also guide the response generation process BIBREF2. For instance, system generates a Greeting type response to former Greeting type utterance. DA recognition is aimed to assign a label to each utterance in a conversation. It can be formulated as a supervised classification problem. There are two trends to solve this problem: 1) as a sequence labeling problem, it will predict the labels for all utterances in the whole dialogue history BIBREF13, BIBREF14, BIBREF9; 2) as a sentence classification problem, it will treat utterance independently without any context history BIBREF5, BIBREF15.  \n Question: What is dialogue act recognition?",
            "output": [
                "DA recognition is aimed to assign a label to each utterance in a conversation. It can be formulated as a supervised classification problem. "
            ]
        },
        {
            "id": "task460-7336d3e0ce7e43ff89bc80fed4e9e590",
            "input": "We evaluate the proposed architecture on two publicly available datasets: the Adverse Drug Events (ADE) dataset BIBREF6 and the CoNLL04 dataset BIBREF7. We show that our architecture is able to outperform the current state-of-the-art (SOTA) results on both the NER and RE tasks in the case of ADE. In the case of CoNLL04, our proposed architecture achieves SOTA performance on the NER task and achieves near SOTA performance on the RE task. On both datasets, our results are SOTA when averaging performance across both tasks. \n Question: Do they repot results only on English data?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-b22b64fbd01141fe957ff0f8becc85bb",
            "input": "Interesting prior work on quantifying social norm violation has taken a heavily data-driven focus BIBREF8 , BIBREF9 .  \n Question: Does this paper propose a new task that others can try to improve performance on?",
            "output": [
                "No, there has been previous work on recognizing social norm violation."
            ]
        },
        {
            "id": "task460-a6ba78521f0f4b158111b7d7f0ab0851",
            "input": "Grammatical error correction (GEC) is a challenging task due to the variability of the type of errors and the syntactic and semantic dependencies of the errors on the surrounding context. Most of the grammatical error correction systems use classification and rule-based approaches for correcting specific error types. However, these systems use several linguistic cues as features. The standard linguistic analysis tools like part-of-speech (POS) taggers and parsers are often trained on well-formed text and perform poorly on ungrammatical text. This introduces further errors and limits the performance of rule-based and classification approaches to GEC. As a consequence, the phrase-based statistical machine translation (SMT) approach to GEC has gained popularity because of its ability to learn text transformations from erroneous text to correct text from error-corrected parallel corpora without any additional linguistic information.  We model our GEC system based on the phrase-based SMT approach. However, traditional phrase-based SMT systems treat words and phrases as discrete entities. We take advantage of continuous space representation by adding two neural network components that have been shown to improve SMT systems BIBREF3 , BIBREF4 . To train NNJM, we use the publicly available implementation, Neural Probabilistic Language Model (NPLM) BIBREF14 . The latest version of Moses can incorporate NNJM trained using NPLM as a feature while decoding. Similar to NNGLM, we use the parallel text used for training the translation model in order to train NNJM. We use a source context window size of 5 and a target context window size of 4. We select a source context vocabulary of 16,000 most frequent words from the source side. The target context vocabulary and output vocabulary is set to the 32,000 most frequent words. We use a single hidden layer to speed up training and decoding with an input embedding dimension of 192 and 512 hidden layer nodes.  \n Question: Do they use pretrained word representations in their neural network models?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-efb578bed5b04e078e6e02198d906cee",
            "input": "GANN is novel neural network model for APC task aimed to solve the shortcomings of traditional RNNs and CNNs. The GANN applied the Gate Truncation RNN (GTR) to learn informative aspect-dependent sentiment clue representations. GANN obtained the state-of-the-art APC performance on the Chinese review datasets. \n Question: What was previous state-of-the-art on four Chinese reviews datasets?",
            "output": [
                "GANN obtained the state-of-the-art APC performance on the Chinese review datasets"
            ]
        },
        {
            "id": "task460-5603940fcb724a7c8bec09327d4cc466",
            "input": "Even if LangID-High does not present a more accurate result, it does present a more compact one: LangID-High is 15.4 MB, while the combined wFST high resource models are 197.5 MB. \n Question: how is model compactness measured?",
            "output": [
                "Using file size on disk"
            ]
        },
        {
            "id": "task460-94de3a22585545f3b0805f7817c8a8e7",
            "input": "we take the CORD-19 dataset BIBREF2, which contains over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses. We develop sentence classification methods to identify all sentences narrating radiological findings from COVID-19.  We conduct experiments to verify the effectiveness of our method. From the CORD-19 dataset, our method successfully discovers a set of clinical findings that are closely related with COVID-19. \n Question: How large is the collection of COVID-19 literature?",
            "output": [
                "45,000 scholarly articles, including over 33,000 with full text"
            ]
        },
        {
            "id": "task460-80779489f90c410baec087000b88993a",
            "input": "We also explored clustering 12742 STRENGTH sentences directly using CLUTO BIBREF19 and Carrot2 Lingo BIBREF20 clustering algorithms.  \n Question: What clustering algorithms were used?",
            "output": [
                "CLUTO Carrot2 Lingo"
            ]
        },
        {
            "id": "task460-0a6278015a774c4eb4e8668eaf24572b",
            "input": "our method still can improve the state-of-the-art accuracy BIBREF7 from 60.32% to 60.34% Although we only have 57% of testing questions can benefit from the basic questions, our method still can improve the state-of-the-art accuracy BIBREF7 from 60.32% to 60.34%,  \n Question: What accuracy do they approach with their proposed method?",
            "output": [
                "our method still can improve the state-of-the-art accuracy BIBREF7 from 60.32% to 60.34%"
            ]
        },
        {
            "id": "task460-78f0c424bf504a1e95e636ee7b7b7479",
            "input": "Our attempt at language pre-training fell short of our expectations in all but one tested dataset. Our pre-training was unsuccessful in improving accuracy, even when applied to networks larger than those reported. \n Question: Does pre-training on general text corpus improve performance?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-a50fdbfe8af04fb198634b56f9410b55",
            "input": "To overcome the size issue of the student reflection dataset, we first explore the effect of incorporating domain transfer into a recent abstractive summarization model: pointer networks with coverage mechanism (PG-net)BIBREF0.  \n Question: What is the recent abstractive summarization method in this paper?",
            "output": [
                "pointer networks with coverage mechanism (PG-net)"
            ]
        },
        {
            "id": "task460-c03b4d0607d64fd4a71a86d2bace3447",
            "input": "The National Gang Threat Assessment Report confirms that at least tens of thousands of gang members are using social networking websites such as Twitter and video sharing websites such as YouTube in their daily life BIBREF0 . They are very active online; the 2007 National Assessment Center's survey of gang members found that 25% of individuals in gangs use the Internet for at least 4 hours a week BIBREF4 . \n Question: Do the authors provide evidence that 'most' street gang members use Twitter to intimidate others?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-4a3238ff37894729aa8a715fa6b5f9f6",
            "input": "As a benchmark, we will use the results of the systems by Burckhardt et al. BIBREF22 , Liu et al. BIBREF18 , Dernoncourt et al. BIBREF9 and Yang et al. BIBREF10 on the i2b2 dataset and the performance of Burckhardt et al. on the nursing corpus. \n Question: What is their baseline?",
            "output": [
                "Burckhardt et al. BIBREF22 Liu et al. BIBREF18 Dernoncourt et al. BIBREF9 Yang et al. BIBREF10"
            ]
        },
        {
            "id": "task460-4c79a879e579489d87282df01be704d0",
            "input": "We test mgru on two well-known datasets, the Penn Treebank and Text8. \n Question: Which dataset do they train their models on?",
            "output": [
                "Penn Treebank Text8"
            ]
        },
        {
            "id": "task460-a9e67f269f3f4b66bde884475747e837",
            "input": "Our system learns Perceptron models BIBREF37 using the Machine Learning machinery provided by the Apache OpenNLP project with our own customized (local and clustering) features.  The local features constitute our baseline system on top of which the clustering features are added. \n Question: what are the baselines?",
            "output": [
                "Perceptron model using the local features."
            ]
        },
        {
            "id": "task460-47c5b575061c4569832dd91bf1e56340",
            "input": "We illustrate the concept by discussing some instantiations that we have recently experimented with. The design also provides for a clear “relevance place” at which an opportunity arises for semantic negotiation, namely, before the final decision is made. An example of this is shown in the example below.   As SECREF12 shows, even in cases where a decision can be reached quickly, there can be an explicit mutual confirmation step, before the (silent) decision signal is sent. A third setting that we have explored BIBREF19 brings conceptual negotiation more clearly into the foreground. In that game, the players are presented with images of birds of particular species and are tasked with coming up with a description of common properties. Again, the final answer has to be approved by both participants. As SECREF13 shows, this can lead to an explicit negotiation of conceptual content. \n Question: Do the authors perform experiments using their proposed method?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-729c503385c247f9841808c842e7bb3a",
            "input": "The first part is described as Task A, the purpose of which is to identify the boundaries of timexes and assign them to one of the following classes: date, time, duration, set. We chose the best 3 results from each word embeddings group (EE, EP, EC) from Table TABREF19 presenting F1-scores for all models. Then we evaluated these results using more detailed measures for timexes, presented in BIBREF27 . \n Question: What experiments are presented?",
            "output": [
                "identify the boundaries of timexes and assign them to one of the following classes: date, time, duration, set  Then we evaluated these results using more detailed measures for timexes"
            ]
        },
        {
            "id": "task460-c560678234e744dd937a73d7ea69c120",
            "input": "The retweeting behavior of MEPs is captured by their retweet network. Each MEP active on Twitter is a node in this network. An edge in the network between two MEPs exists when one MEP retweeted the other. The weight of the edge is the number of retweets between the two MEPs We measure the cohesion of a political group INLINEFORM0 as the average retweets, i.e., the ratio of the number of retweets between the MEPs in the group INLINEFORM1 to the number of MEPs in the group INLINEFORM2  \n Question: Do they authors account for differences in usage of Twitter amongst MPs into their model?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-716adb8e80db49bdaea1b8e178167248",
            "input": "We train $\\mathcal {F}$ and $\\mathcal {G}$ jointly and introduce two regularizers. Formally, we hope that $\\mathcal {G}(\\mathcal {F}(X))$ is similar to $X$ and $\\mathcal {F}(\\mathcal {G}(Y))$ is similar to $Y$. We implement this constraint as a cycle consistency loss. As a result, the proposed model has two learning objectives: i) an adversarial loss ($\\ell _{adv}$) for each model as in the baseline. ii) a cycle consistency loss ($\\ell _{cycle}$) on each side to avoid $\\mathcal {F}$ and $\\mathcal {G}$ from contradicting each other. \n Question: What regularizers were used to encourage consistency in back translation cycles?",
            "output": [
                "an adversarial loss ($\\ell _{adv}$) for each model as in the baseline a cycle consistency loss ($\\ell _{cycle}$) on each side"
            ]
        },
        {
            "id": "task460-f680bfdf4d6a444d92800e42c13ad5d3",
            "input": "In this paper, we present an extraction-then-synthesis framework for machine reading comprehension shown in Figure 1 , in which the answer is synthesized from the extraction results. We build an evidence extraction model to predict the most important sub-spans from the passages as evidence, and then develop an answer synthesis model which takes the evidence as additional features along with the question and passage to further elaborate the final answers. \n Question: Which framework they propose in this paper?",
            "output": [
                " extraction-then-synthesis framework"
            ]
        },
        {
            "id": "task460-77be53aa323449d8a4abba329f7b2528",
            "input": "Our model consists of two neural network modules, i.e. an extractor and abstractor. The extractor encodes a source document and chooses sentences from the document, and then the abstractor paraphrases the summary candidates.  The extractor is based on the encoder-decoder framework. We adapt BERT for the encoder to exploit contextualized representations from pre-trained transformers. We use LSTM Pointer Network BIBREF22 as the decoder to select the extracted sentences based on the above sentence representations.  Our abstractor is practically identical to the one proposed in BIBREF8. \n Question: What's the method used here?",
            "output": [
                "Two neural networks: an extractor based on an encoder (BERT) and a decoder (LSTM Pointer Network BIBREF22) and an abstractor identical to the one proposed in BIBREF8."
            ]
        },
        {
            "id": "task460-19545cd341044408b0646ae664c22c57",
            "input": "69.10%/78.38% \n Question: how much of improvement the adaptation model can get?",
            "output": [
                " 69.10%/78.38%"
            ]
        },
        {
            "id": "task460-5c3e9c461698410b92eb909e914e3e68",
            "input": "As shown in Table TABREF39 , the proposed framework consistently obtains the best scores on all of the four datasets. Compared with the winning systems of SemEval ABSA, our framework achieves 5.0%, 1.6%, 1.4%, 1.3% absolute gains on INLINEFORM0 , INLINEFORM1 , INLINEFORM2 and INLINEFORM3 respectively.\n\nOur framework can outperform RNCRF, a state-of-the-art model based on dependency parsing, on all datasets. We also notice that RNCRF does not perform well on INLINEFORM0 and INLINEFORM1 (3.7% and 3.9% inferior than ours). We find that INLINEFORM2 and INLINEFORM3 contain many informal reviews, thus RNCRF's performance degradation is probably due to the errors from the dependency parser when processing such informal texts. \n Question: By how much do they outperform state-of-the-art methods?",
            "output": [
                "Compared with the winning systems of SemEval ABSA, our framework achieves 5.0%, 1.6%, 1.4%, 1.3% absolute gains on INLINEFORM0 , INLINEFORM1 , INLINEFORM2 and INLINEFORM3 respectively."
            ]
        },
        {
            "id": "task460-21fc1dd04fbd4cb98baf0b8e3dac5aa1",
            "input": "Probing Methodology and Modeling ::: Task Definition and Modeling ::: Baselines and Sanity Checks.\nWhen creating synthetic datasets, it is important to ensure that systematic biases, or annotation artifacts BIBREF41, are not introduced into the resulting probes and that the target datasets are sufficiently challenging (or good, in the sense of BIBREF42). To test for this, we use several of the MCQA baseline models first introduced in BIBREF0, which take inspiration from the LSTM-based models used in BIBREF43 for NLI and various partial-input baselines based on these models. \n Question: How do they control for annotation artificats?",
            "output": [
                " we use several of the MCQA baseline models first introduced in BIBREF0"
            ]
        },
        {
            "id": "task460-7c4aa807b3f9467385ea0b2a1a13b5c6",
            "input": "We collect data by using the Twitter API for saved data, available for public use. For our experiments we collect 3200 tweets filtered by keywords like “fire”, “earthquake”, “theft”, “robbery”, “drunk driving”, “drunk driving accident” etc. Later, we manually label tweets with and labels for classification as stage one. Our dataset contains 1313 tweet with positive label and 1887 tweets with a negative label . We create another dataset with the positively labeled tweets and provide them with category labels like “fire”, “accident”, “earthquake” etc. \n Question: Are the tweets specific to a region?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-5839cb66274344b6a89f460f37875b8a",
            "input": "Hence we have determined our targets as Galatasaray (namely Target-1) and Fenerbahçe (namely, Target-2) which are two of the most popular football clubs in Turkey. \n Question: Which sports clubs are the targets?",
            "output": [
                "Galatasaray Fenerbahçe"
            ]
        },
        {
            "id": "task460-c94e6bd1d2f249088e30f2c19f8df60f",
            "input": "However, a significant bottleneck that has been overlooked by previous efforts is the input vocabulary size and its corresponding word embedding matrix, often accounting for a significant proportion of all model parameters. For instance, the embedding table of the BERTBASE model, comprising over 30K WordPiece tokens BIBREF14, accounts for over $21\\%$ of the model size. While there has been existing work on reducing NLP model vocabulary sizes BIBREF15, distillation techniques cannot utilize these, since they require the student and teacher models to share the same vocabulary and output space. This profoundly limits their potential to further reduce model sizes. \n Question: Why are prior knowledge distillation techniques models are ineffective in producing student models with vocabularies different from the original teacher models?  ",
            "output": [
                "While there has been existing work on reducing NLP model vocabulary sizes BIBREF15, distillation techniques cannot utilize these, since they require the student and teacher models to share the same vocabulary and output space. This profoundly limits their potential to further reduce model sizes."
            ]
        },
        {
            "id": "task460-20bcbbd9f87945898b3dd9f515d5ea03",
            "input": "The whole dataset consists of 81,826 samples annotated by native English speakers. 80% of them are used as training set. 10% of them are used as validation set while the rest is used as test set. 3000 hard samples are selected from the test set. \n Question: What is the difference between the full test set and the hard test set?",
            "output": [
                "3000 hard samples are selected from the test set"
            ]
        },
        {
            "id": "task460-fbb7f280583a4b839ade6f0034dd486c",
            "input": "Training and testing are done in alternating steps: In each epoch, for training, we first present to an LSTM network 1000 samples in a given language, which are generated according to a certain discrete probability distribution supported on a closed finite interval. We then freeze all the weights in our model, exhaustively enumerate all the sequences in the language by their lengths, and determine the first $k$ shortest sequences whose outputs the model produces inaccurately.  experimented with 1, 2, 3, and 36 hidden units for $a^n b^n$ ; 2, 3, 4, and 36 hidden units for $a^n b^n c^n$ ; and 3, 4, 5, and 36 hidden units for $a^n b^n c^n d^n$ .  Following the traditional approach adopted by BIBREF7 , BIBREF12 , BIBREF9 and many other studies, we train our neural network as follows. At each time step, we present one input character to our model and then ask it to predict the set of next possible characters, based on the current character and the prior hidden states. Given a vocabulary $\\mathcal {V}^{(i)}$ of size $d$ , we use a one-hot representation to encode the input values; therefore, all the input vectors are $d$ -dimensional binary vectors. The output values are $(d+1)$ -dimensional though, since they may further contain the termination symbol $\\dashv $ , in addition to the symbols in $\\mathcal {V}^{(i)}$ . The output values are not always one-hot encoded, because there can be multiple possibilities for the next character in the sequence, therefore we instead use a $k$ -hot representation to encode the output values. Our objective is to minimize the mean-squared error (MSE) of the sequence predictions. \n Question: What training settings did they try?",
            "output": [
                "Training and testing are done in alternating steps: In each epoch, for training, we first present to an LSTM network 1000 samples in a given language, which are generated according to a certain discrete probability distribution supported on a closed finite interval. We then freeze all the weights in our model, exhaustively enumerate all the sequences in the language by their lengths, and determine the first $k$ shortest sequences whose outputs the model produces inaccurately.  experimented with 1, 2, 3, and 36 hidden units for $a^n b^n$ ; 2, 3, 4, and 36 hidden units for $a^n b^n c^n$ ; and 3, 4, 5, and 36 hidden units for $a^n b^n c^n d^n$ .  Following the traditional approach adopted by BIBREF7 , BIBREF12 , BIBREF9 and many other studies, we train our neural network as follows. At each time step, we present one input character to our model and then ask it to predict the set of next possible characters, based on the current character and the prior hidden states. Given a vocabulary $\\mathcal {V}^{(i)}$ of size $d$ , we use a one-hot representation to encode the input values; therefore, all the input vectors are $d$ -dimensional binary vectors. The output values are $(d+1)$ -dimensional though, since they may further contain the termination symbol $\\dashv $ , in addition to the symbols in $\\mathcal {V}^{(i)}$ . The output values are not always one-hot encoded, because there can be multiple possibilities for the next character in the sequence, therefore we instead use a $k$ -hot representation to encode the output values. Our objective is to minimize the mean-squared error (MSE) of the sequence predictions."
            ]
        },
        {
            "id": "task460-880ee418843746029d2e6d4fa33edca4",
            "input": "To identify legally sound answers, we recruit seven experts with legal training to construct answers to Turker questions.  \n Question: Who were the experts used for annotation?",
            "output": [
                "Individuals with legal training"
            ]
        },
        {
            "id": "task460-f86787ff5f5e4abab8be7273f338525a",
            "input": "Table IV shows the result of our model for sentiment classification against other models. We compare our model performance with the approaches of BIBREF0 BIBREF5 on STS Corpus. BIBREF0 reported the results of Maximum Entropy (MaxEnt), NB, SVM on STS Corpus having good performance in previous time. The model of BIBREF5 is a state-of-the-art so far by using a CharSCNN. As can be seen, 86.63 is the best prediction accuracy of our model so far for the STS Corpus.\n\nFor Sanders and HCR datasets, we compare results with the model of BIBREF14 that used a ensemble of multiple base classifiers (ENS) such as NB, Random Forest (RF), SVM and Logistic Regression (LR). The ENS model is combined with bag-of-words (BoW), feature hashing (FH) and lexicons. The model of BIBREF14 is a state-of-the-art on Sanders and HCR datasets. Our models outperform the model of BIBREF14 for the Sanders dataset and HCR dataset.\n\n \n Question: What was the baseline?",
            "output": [
                "We compare our model performance with the approaches of BIBREF0 BIBREF5 on STS Corpus. BIBREF0 reported the results of Maximum Entropy (MaxEnt), NB, SVM on STS Corpus having good performance in previous time. The model of BIBREF5 is a state-of-the-art so far by using a CharSCNN. we compare results with the model of BIBREF14 that used a ensemble of multiple base classifiers (ENS) such as NB, Random Forest (RF), SVM and Logistic Regression (LR). The ENS model is combined with bag-of-words (BoW), feature hashing (FH) and lexicons. The model of BIBREF14 is a state-of-the-art on Sanders and HCR datasets. "
            ]
        },
        {
            "id": "task460-f7b5f3e7c6044396b669a23dd56ab215",
            "input": "As fig:fit shows, estimated test accuracy is highly correlated with actual test accuracy for various datasets, with worst-case values $\\mu <1\\%$ and $\\sigma <5\\%$ . Note that the number of free parameters is small ($||\\le 6$) compared to the number of points (42–49 model-data configurations), demonstrating the appropriateness of the proposed function for modeling the complex error landscape. \n Question: What is proof that proposed functional form approximates well generalization error in practice?",
            "output": [
                "estimated test accuracy is highly correlated with actual test accuracy for various datasets appropriateness of the proposed function for modeling the complex error landscape"
            ]
        },
        {
            "id": "task460-02b6fa802d8d4a37801792b5a34358fe",
            "input": "Second, we compute three metrics on the extracted information:\n\n$\\bullet $ Relation Generation (RG) estimates how well the system is able to generate text containing factual (i.e., correct) records. We measure the precision and absolute number (denoted respectively RG-P% and RG-#) of unique relations $r$ extracted from $\\hat{y}_{1:T}$ that also appear in $s$.\n\n$\\bullet $ Content Selection (CS) measures how well the generated document matches the gold document in terms of mentioned records. We measure the precision and recall (denoted respectively CS-P% and CS-R%) of unique relations $r$ extracted from $\\hat{y}_{1:T}$ that are also extracted from $y_{1:T}$.\n\n$\\bullet $ Content Ordering (CO) analyzes how well the system orders the records discussed in the description. We measure the normalized Damerau-Levenshtein distance BIBREF36 between the sequences of records extracted from $\\hat{y}_{1:T}$ that are also extracted from $y_{1:T}$. \n Question: Which qualitative metric are used for evaluation?",
            "output": [
                " Relation Generation (RG)  Content Selection (CS)  Content Ordering (CO)"
            ]
        },
        {
            "id": "task460-9685e4c695f845578f2220751a903933",
            "input": "In Table TABREF1, we summarize the quantitative results of the above previous studies. In Table TABREF1, we summarize the quantitative results of the above previous studies. \n Question: What is the accuracy reported by state-of-the-art methods?",
            "output": [
                "Answer with content missing: (Table 1)\nPrevious state-of-the art on same dataset: ResNet50 89% (6 languages), SVM-HMM 70% (4 languages)"
            ]
        },
        {
            "id": "task460-8e3985bd6c0543fe990dc860b299534b",
            "input": "This dataset was collected through Mechanical Turk using 100 simulated environments with a corresponding topological map and, to the best of our knowledge, it is the first of its kind for behavioral navigation.  We created a new dataset for the problem of following navigation instructions under the behavioral navigation framework of BIBREF5 . This dataset was created using Amazon Mechanical Turk and 100 maps of simulated indoor environments, each with 6 to 65 rooms. To the best of our knowledge, this is the first benchmark for comparing translation models in the context of behavioral robot navigation.\n\nAs shown in Table TABREF16 , the dataset consists of 8066 pairs of free-form natural language instructions and navigation plans for training. This training data was collected from 88 unique simulated environments, totaling 6064 distinct navigation plans (2002 plans have two different navigation instructions each; the rest has one). The dataset contains two test set variants:\n\nWhile the dataset was collected with simulated environments, no structure was imposed on the navigation instructions while crowd-sourcing data. Thus, many instructions in our dataset are ambiguous. Moreover, the order of the behaviors in the instructions is not always the same. For instance, a person said “turn right and advance” to describe part of a route, while another person said “go straight after turning right” in a similar situation. The high variability present in the natural language descriptions of our dataset makes the problem of decoding instructions into behaviors not trivial. See Appendix A of the supplementary material for additional details on our data collection effort.\n\n \n Question: How were the navigation instructions collected?",
            "output": [
                "using Amazon Mechanical Turk using simulated environments with topological maps"
            ]
        },
        {
            "id": "task460-ff12563b766e47799c82a8a4329bd0bf",
            "input": "This work presented MVCNN, a novel CNN architecture for sentence classification. It combines multichannel initialization – diverse versions of pretrained word embeddings are used – and variable-size filters – features of multigranular phrases are extracted with variable-size convolution filters.  \n Question: How is MVCNN compared to CNN?",
            "output": [
                "MVCNN, a novel CNN architecture for sentence classification. It combines multichannel initialization – diverse versions of pretrained word embeddings are used – and variable-size filters – features of multigranular phrases are extracted with variable-size convolution filters. "
            ]
        },
        {
            "id": "task460-ef9c0aab10f14dbb91664a4931dc45eb",
            "input": "In our experiments, we used WordNet 3.0 BIBREF9 as our external knowledge base INLINEFORM0 . For word embeddings, we experimented with two popular models: (1) GloVe embeddings trained by BIBREF10 on Wikipedia and Gigaword 5 (vocab: 400K, dim: 300), and (2) w2v-gn, Word2vec BIBREF5 trained on the Google News dataset (vocab: 3M, dim: 300). Our coverage enhancement starts by transforming the knowledge base INLINEFORM0 into a vector space representation that is comparable to that of the corpus-based space INLINEFORM1 . To this end, we use two techniques for learning low-dimensional feature spaces from knowledge graphs: DeepWalk and node2vec. DeepWalk uses a stream of short random walks in order to extract local information for a node from the graph. By treating these walks as short sentences and phrases in a special language, the approach learns latent representations for each node. Similarly, node2vec learns a mapping of nodes to continuous vectors that maximizes the likelihood of preserving network neighborhoods of nodes. Thanks to a flexible objective that is not tied to a particular sampling strategy, node2vec reports improvements over DeepWalk on multiple classification and link prediction datasets. For both these systems we used the default parameters and set the dimensionality of output representation to 100. Also, note than nodes in the semantic graph of WordNet represent synsets. Hence, a polysemous word would correspond to multiple nodes. In our experiments, we use the MaxSim assumption of BIBREF11 in order to map words to synsets. \n Question: What other embedding models are tested?",
            "output": [
                "GloVe embeddings trained by BIBREF10 on Wikipedia and Gigaword 5 (vocab: 400K, dim: 300) w2v-gn, Word2vec BIBREF5 trained on the Google News dataset (vocab: 3M, dim: 300) DeepWalk  node2vec"
            ]
        },
        {
            "id": "task460-2f369c96b05e4ebaa3d01aa075b2eef9",
            "input": "We use 300 Dimensional Glove Common Crawl Embeddings (840B Token) BIBREF11 and fine tune them for the task. \n Question: What data are the embeddings trained on?",
            "output": [
                "Common Crawl "
            ]
        },
        {
            "id": "task460-4fa993f28e9848d581f448612f6b4ea3",
            "input": "Table TABREF14 shows the effectiveness of our model (multi-task transformer) over the baseline transformer BIBREF8 .  \n Question: what are the baselines?",
            "output": [
                "the baseline transformer BIBREF8"
            ]
        },
        {
            "id": "task460-8fc5ff7ee74e409f83fe03347374d0f9",
            "input": "Email classifier using machine learning ::: Machine learning approach ::: Feature selection\nNgrams are a continuous sequence of n items from a given sample of text. From title, body and OCR text words are selected. Ngrams of 3 nearby words are extracted with Term Frequency-Inverse Document Frequency (TF-IDF) vectorizing, then features are filtered using chi squared the feature scoring method. Email classifier using machine learning ::: Machine learning approach ::: Random forest\nRandom Forest is a bagging Algorithm, an ensemble learning method for classification that operates by constructing a multitude of decision trees at training time and outputting the class that has highest mean majority vote of the classesBIBREF14. Email classifier using machine learning ::: Machine learning approach ::: XGBoost\nXGBoost is a decision-tree-based ensemble Machine Learning algorithm that uses a gradient boosting framework. It is used commonly in the classification problems involving unstructured dataBIBREF5. Email classifier using machine learning ::: Machine learning approach ::: Hierarchical Model\nSince the number of target labels are high, achieving the higher accuracy is difficult, while keeping all the categories under same feature selection method. Some categories performs well with lower TF-IDF vectorizing range and higher n grams features even though they showed lower accuracy in the overall single model. Therefore, hierarchical machine learning models are built to classify 31 categories in the first classification model and remaining categories are named as low-accu and predicted as one category. In the next model, predicted low-accu categories are again classified into 47 categories. Comparatively this hierarchical model works well since various feature selection methods are used for various categoriesBIBREF5. \n Question: What are all machine learning approaches compared in this work?",
            "output": [
                "Feature selection Random forest XGBoost Hierarchical Model"
            ]
        },
        {
            "id": "task460-d3b46aed72ec4c598b629c27bc9b56ce",
            "input": "To test if the learned representations can separate phonetic categories, we use a minimal pair ABX discrimination task BIBREF19 , BIBREF20 . For the within-speaker task, all the phones triplets belong to the same speaker (e.g. $x$3 ) Finally the scores for every pair of central phones are averaged and subtracted from 1 to yield the reported within-talker ABX error rate. For the across-speaker task, $x$4 and $x$5 belong to the same speaker, and $x$6 to a different one (e.g. $x$7 ). The scores for a given minimal pair are first averaged across all of the pairs of speakers for which this contrast can be made. As above, the resulting scores are averaged over all contexts over all pairs of central phones and converted to an error rate. \n Question: What is the metric that is measures in this paper?",
            "output": [
                "error rate in a minimal pair ABX discrimination task"
            ]
        },
        {
            "id": "task460-aeb34bb3a17b4377a38a093e492de500",
            "input": "The utterance is concatenated with a special symbol marking the end of the sequence. We initialize our word embeddings using 300-dimensional GloVe BIBREF30 and then fine-tune them during training. \n Question: Do they use pretrained word vectors for dialogue context embedding?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-97ebb83d180544a6a4627adf3b2556e8",
            "input": "In this study, we concentrate our effort on re-assessing keyphrase extraction performance on three increasingly sophisticated levels of document preprocessing described below. \n Question: what keyphrase extraction models were reassessed?",
            "output": [
                "Answer with content missing: (LVL1, LVL2, LVL3) \n- Stanford CoreNLP\n- Optical Character Recognition (OCR) system, ParsCIT \n- further abridge the input text from level 2 preprocessed documents to the following: title, headers, abstract, introduction, related work, background and conclusion."
            ]
        },
        {
            "id": "task460-eff140c821a346f790a9c1a9454f3a5a",
            "input": "One-Vs-The-Rest strategy was adopted for the task of multi-class classification and I reported F-score, micro-F, macro-F and weighted-F scores using 10-fold cross-validation.  \n Question: What metrics are considered?",
            "output": [
                "F-score micro-F macro-F weighted-F "
            ]
        },
        {
            "id": "task460-1b2bcc70464449c987d9dac2b35dc932",
            "input": " We used state-of-the-art features that have shown to be useful in ID: some of them are language-independent (e.g., punctuation marks, positive and negative emoticons, quotations, personal pronouns, tweet's length, named entities) while others are language-dependent relying on dedicated lexicons (e.g., negation, opinion lexicons, opposition words). \n Question: What text-based features are used?",
            "output": [
                "language-independent (e.g., punctuation marks, positive and negative emoticons, quotations, personal pronouns, tweet's length, named entities)  language-dependent relying on dedicated lexicons (e.g., negation, opinion lexicons, opposition words)"
            ]
        },
        {
            "id": "task460-1a3b670be75b4570802ea59c88437f71",
            "input": "Co-Reference Resolution: To support multi-turn interactions, it is sometimes necessary to use co-reference resolution techniques for effective retrieval. In Macaw, we identify all the co-references from the last request of user to the conversation history. The same co-reference resolution outputs can be used for different query generation components. This can be a generic or action-specific component.\n\nQuery Generation: This component generates a query based on the past user-system interactions. The query generation component may take advantage of co-reference resolution for query expansion or re-writing.\n\nRetrieval Model: This is the core ranking component that retrieves documents or passages from a large collection. Macaw can retrieve documents from an arbitrary document collection using the Indri python interface BIBREF9, BIBREF10. We also provide the support for web search using the Bing Web Search API. Macaw also allows multi-stage document re-ranking.\n\nResult Generation: The retrieved documents can be too long to be presented using some interfaces. Result generation is basically a post-processing step ran on the retrieved result list. In case of question answering, it can employ answer selection or generation techniques, such as machine reading comprehension models. For example, Macaw features the DrQA model BIBREF11 for question answering. \n Question: What functionality does Macaw provide?",
            "output": [
                "Co-Reference Resolution Query Generation Retrieval Model Result Generation"
            ]
        },
        {
            "id": "task460-9aa5fede971848df90260760b7122513",
            "input": "Paraphrases can be obtained by translating an English string into a foreign language and then back-translating it into English.  \n Question: It looks like learning to paraphrase questions, a neural scoring model and a answer selection model cannot be trained end-to-end. How are they trained?",
            "output": [
                "using multiple pivot sentences"
            ]
        },
        {
            "id": "task460-b6267f159a2c42f7a3ad37653ce60bda",
            "input": "The NLG model is a seq2seq model with attention as described in section SECREF2. \n Question: Do they use attention?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-fbf7cb8aebab483eb63df65c5974b543",
            "input": "Our method is purely text-based, and ignores the publication date and the source of the article. It combines task-specific embeddings, produced by a two-level attention-based deep neural network model, with manually crafted features (stylometric, lexical, grammatical, and semantic), into a kernel-based SVM classifier. \n Question: what types of features were used?",
            "output": [
                "stylometric, lexical, grammatical, and semantic"
            ]
        },
        {
            "id": "task460-0101289471d4493d8492ff2120a34f1f",
            "input": "This paper used the real-time method to randomly collect 10% of publicly available English tweets using several pre-defined DDEO-related queries (Table TABREF6 ) within a specific time frame.  \n Question: Do they evaluate only on English data?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-283c5345f73d4aef9631ddd1b227fe4e",
            "input": "We compare the results of our model to a baseline multi-task architecture inspired by yang2016multi. In our baseline model there are no explicit connections between tasks - the only shared parameters are in the hidden layer. \n Question: What is the baseline?",
            "output": [
                "The baseline is a multi-task architecture inspired by another paper."
            ]
        },
        {
            "id": "task460-b81722a6f3464725a5c746f98f804c12",
            "input": "Further, we employ BLEU score to evaluate the accuracy of generations BIBREF21, and the number of distinct n-gram to evaluate the diversity of generations BIBREF6.  \n Question: How do they measure the diversity of inferences?",
            "output": [
                "by number of distinct n-grams"
            ]
        },
        {
            "id": "task460-1cd551acfb154e388c358cf65dab95b6",
            "input": "We decided to explore the remaining space for improvement on the CBT by testing humans on a random subset of 50 named entity and 50 common noun validation questions that the psr ensemble could not answer correctly. \n Question: How do they show there is space for further improvement?",
            "output": [
                " by testing humans on a random subset of 50 named entity and 50 common noun validation questions that the psr ensemble could not answer correctly"
            ]
        },
        {
            "id": "task460-461663862d414f23b2acca9156450cf7",
            "input": "Given a set of evaluation documents, each having a known correct label from a closed set of labels (often referred to as the “gold-standard”), and a predicted label for each document from the same set, the document-level accuracy is the proportion of documents that are correctly labeled over the entire evaluation collection. This is the most frequently reported metric and conveys the same information as the error rate, which is simply the proportion of documents that are incorrectly labeled (i.e. INLINEFORM0 ). There are two distinct ways in which results are generally summarized per-language: (1) precision, in which documents are grouped according to their predicted language; and (2) recall, in which documents are grouped according to what language they are actually written in. It is also common practice to report an F-score INLINEFORM0 , which is the harmonic mean of precision and recall. \n Question: what evaluation methods are discussed?",
            "output": [
                "document-level accuracy precision recall F-score"
            ]
        },
        {
            "id": "task460-12e6212623ed48398778eb1902b0f16c",
            "input": "We propose a novel problem of relationship recommendation (RSR). Different from the reciprocal recommendation problem on DSNs, our RSR task operates on regular social networks (RSN), estimating long-term and serious relationship compatibility based on social posts such as tweets. \n Question: Is this a task other people have worked on?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-a49170c0e215457aa346b23f830d77ab",
            "input": "Tourist: I can't go straight any further.\n\nGuide: ok. turn so that the theater is on your right.\n\nGuide: then go straight\n\nTourist: That would be going back the way I came\n\nGuide: yeah. I was looking at the wrong bank\n\nTourist: I'll notify when I am back at the brooks brothers, and the bank.\n\nTourist: ACTION:TURNRIGHT\n\nGuide: make a right when the bank is on your left\n\nTourist: ACTION:FORWARD ACTION:FORWARD ACTION:TURNRIGHT\n\nTourist: Making the right at the bank.\n\nTourist: ACTION:FORWARD ACTION:FORWARD\n\nTourist: I can't go that way.\n\nTourist: ACTION:TURNLEFT\n\nTourist: Bank is ahead of me on the right\n\nTourist: ACTION:FORWARD ACTION:FORWARD ACTION:TURNLEFT\n\nGuide: turn around on that intersection\n\nTourist: I can only go to the left or back the way I just came.\n\nTourist: ACTION:TURNLEFT\n\nGuide: you're in the right place. do you see shops on the corners?\n\nGuide: If you're on the corner with the bank, cross the street\n\nTourist: I'm back where I started by the shop and the bank.\n\nTourist: ACTION:TURNRIGHT \n Question: What language do the agents talk in?",
            "output": [
                "English"
            ]
        },
        {
            "id": "task460-eff8aaf8789445cd832d861183b081b4",
            "input": "Inflectional realization defines the inflected forms of a lexeme/lemma. As a computational task, often referred to as simply “morphological inflection,\" inflectional realization is framed as a mapping from the pairing of a lemma with a set of morphological tags to the corresponding word form. For example, the inflectional realization of SJQ Chatino verb forms entails a mapping of the pairing of the lemma lyu1 `fall' with the tag-set 1;SG;PROG to the word form nlyon32. Morphological analysis is the task of creating a morphosyntactic description for a given word. It can be framed in a context-agnostic manner (as in our case) or within a given context, as for instance for the SIGMORPHON 2019 second shared task BIBREF11. \n Question: How does morphological analysis differ from morphological inflection?",
            "output": [
                "Morphological analysis is the task of creating a morphosyntactic description for a given word  inflectional realization is framed as a mapping from the pairing of a lemma with a set of morphological tags to the corresponding word form"
            ]
        },
        {
            "id": "task460-560e8171ecf449de87359198d9662081",
            "input": "Our pool of annotators is selected after several short training rounds, with up to 15 predicates per round, in which they received extensive personal feedback. \n Question: How are workers trained?",
            "output": [
                "extensive personal feedback"
            ]
        },
        {
            "id": "task460-5eff5aff75cc4c7eab06b0a80df8200f",
            "input": "We used the Reuters-8 dataset without stop words from BIBREF27 aiming at single-label classification, which is a preprocessed format of the Reuters-21578. \n Question: Which dataset has been used in this work?",
            "output": [
                "Reuters-8 dataset without stop words"
            ]
        },
        {
            "id": "task460-b3ad81e2d544412ab606d90488f6bdfc",
            "input": " The RelNet model achieves a mean error of 0.285% across tasks which is better than the results of the EntNet model BIBREF17 . The RelNet model is able to achieve 0% test error on 11 of the tasks, whereas the EntNet model achieves 0% error on 7 of the tasks. The RelNet model achieves a mean error of 0.285% across tasks which is better than the results of the EntNet model BIBREF17 . The RelNet model is able to achieve 0% test error on 11 of the tasks, whereas the EntNet model achieves 0% error on 7 of the tasks. \n Question: What are the relative improvements observed over existing methods?",
            "output": [
                "The RelNet model achieves a mean error of 0.285% across tasks which is better than the results of the EntNet model BIBREF17 . The RelNet model is able to achieve 0% test error on 11 of the tasks, whereas the EntNet model achieves 0% error on 7 of the tasks."
            ]
        },
        {
            "id": "task460-f75904f93af841fe813d5d3ff6985bb3",
            "input": " the input fusion layer to allow interactions between input facts and a novel attention based GRU that allows for logical reasoning over ordered inputs. \n Question: How does the model circumvent the lack of supporting facts during training?",
            "output": [
                "the input fusion layer to allow interactions between input facts and a novel attention based GRU that allows for logical reasoning over ordered inputs. "
            ]
        },
        {
            "id": "task460-94fdbc3f24f64dd388f15cb931b380a2",
            "input": "We leveraged an existing, annotated Twitter dataset that was constructed based on a hierarchical model of depression-related symptoms BIBREF12 , BIBREF13 . The dataset contains 9,473 annotations for 9,300 tweets. \n Question: What dataset is used for this study?",
            "output": [
                "BIBREF12 , BIBREF13"
            ]
        },
        {
            "id": "task460-546277b5adfa461ab72ee91c35392c85",
            "input": "For example, the performance of even the best QA models degrades substantially on our hyponym probes (by 8-15%) when going from 1-hop links to 2-hops. Further, the accuracy of even our best models on the WordNetQA probe drops by 14-44% under our cluster-based analysis, which assesses whether a model knows several facts about each individual concept, rather than just being good at answering isolated questions.  \n Question: After how many hops does accuracy decrease?",
            "output": [
                "1-hop links to 2-hops"
            ]
        },
        {
            "id": "task460-c6c962cb5ec94ecaac30c591b0e47fe9",
            "input": "The initial analyses of the embedding matrices for both the UK and France revealed that in general, words were grouped by context or influence on the electricity consumption. For instance, we observed that winter words were together and far away from summer ones.  \n Question: Is there any example where geometric property is visible for context similarity between words?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-099c13e30c584a0d9c621afbbe6cdaab",
            "input": "Out of 43 teams our system ranked 421st with an official F1-score of 0.2905 on the test set. Although our model outperforms the baseline in the validation set in terms of F1-score, we observe important drops for all metrics compared to the test set, showing that the architecture seems to be unable to generalize well. \n Question: What were their results on the test set?",
            "output": [
                "an official F1-score of 0.2905 on the test set"
            ]
        },
        {
            "id": "task460-95581e662b374da4b75ee42f2219e540",
            "input": "The baseline is a hierarchical phrase-based system BIBREF29 with a 4-gram language model, with feature weights tuned using MIRA BIBREF30 . Therefore, an appropriate additional baseline would be to mark translation rules with these indicator functions but without the scores, akin to identifying rules with phrases in them (Baseline + SegOn). \n Question: Which translation systems do they compare against?",
            "output": [
                "hierarchical phrase-based system BIBREF29 appropriate additional baseline would be to mark translation rules with these indicator functions but without the scores, akin to identifying rules with phrases in them (Baseline + SegOn)"
            ]
        },
        {
            "id": "task460-a165ffcc74044ba18cbc3745dbcf9e1f",
            "input": "While $\\beta $-VAE offers regularizing the ELBO via an additional coefficient $\\beta \\in {\\rm I\\!R}^+$, a simple extension BIBREF16 of its objective function incorporates an additional hyperparameter $C$ to explicitly control the magnitude of the KL term,\n\nwhere $C\\!\\! \\in \\!\\! {\\rm I\\!R}^+$ and $| . |$ denotes the absolute value. \n Question: How does explicit constraint on the KL divergence term that authors propose looks like?",
            "output": [
                "Answer with content missing: (Formula 2) Formula 2 is an answer: \n\\big \\langle\\! \\log p_\\theta({x}|{z}) \\big \\rangle_{q_\\phi({z}|{x})}  -  \\beta |D_{KL}\\big(q_\\phi({z}|{x}) || p({z})\\big)-C|"
            ]
        },
        {
            "id": "task460-3f54a785f2e9470b9a43403a2c1969a4",
            "input": "To evaluate the proposed LRC framework and the AutoJudge model, we carry out a series of experiments on the divorce proceedings, a typical yet complex field of civil cases.  \n Question: what civil field is the dataset about?",
            "output": [
                "divorce "
            ]
        },
        {
            "id": "task460-cd43fb64404c42479dc5081f6490cfc4",
            "input": "Table 1 shows an example of difficult and easy examples according to our definition of difficulty. The underlined text demarcates the (consensus) reference label provided by domain experts. In the difficult examples, crowd workers marked text distinct from these reference annotations; whereas in the easy cases they reproduced them with reasonable fidelity. The difficult sentences usually exhibit complicated structure and feature jargon. \n Question: Is an instance a sentence or an IE tuple?",
            "output": [
                "sentence"
            ]
        },
        {
            "id": "task460-db420a8f079a4d859376b11e6d3efe86",
            "input": "We collect more human-written scenes for each concept-set in dev and test set through crowd-sourcing via the Amazon Mechanical Turk platform. Each input concept-set is annotated by at least three different humans. The annotators are also required to give sentences as the rationales, which further encourage them to use common sense in creating their scenes. \n Question: Are the rationales generated after the sentences were written?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-d8a2d032135747b6a86d65b4f4ffba62",
            "input": "Since our objective is to compare character-based and word-based approaches, we have also implemented a simple word-level encoder for tweets. The input tweet is first split into tokens along white-spaces. A more sophisticated tokenizer may be used, but for a fair comparison we wanted to keep language specific preprocessing to a minimum. The encoder is essentially the same as tweet2vec, with the input as words instead of characters. A lookup table stores word vectors for the $V$ (20K here) most common words, and the rest are grouped together under the `UNK' token. \n Question: what is the word level baseline they compare to?",
            "output": [
                "a simple word-level encoder with the input as words instead of characters"
            ]
        },
        {
            "id": "task460-d12014b26d864eb6ad195136a732b2b6",
            "input": "We employ a multi-class Naive Bayes classifier as the second stage classification mechanism, for categorizing tweets appropriately, depending on the type of emergencies they indicate. \n Question: What classifier is used for emergency categorization?",
            "output": [
                "multi-class Naive Bayes"
            ]
        },
        {
            "id": "task460-877edccac20345d888635d765f6d36e1",
            "input": "We evaluated SDNet on CoQA dataset \n Question: Is the model evaluated on other datasets?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-0cc0d14950f14e9d896d1ce59d4a5246",
            "input": "One of the several formats into which FHIR can be serialized is RDF. However, because RDF was designed as an abstract information model and FHIR was designed for operational use in a healthcare setting, there is the potential for a slight mismatch between the models \n Question: What are the differences between FHIR and RDF?",
            "output": [
                "One of the several formats into which FHIR can be serialized is RDF there is the potential for a slight mismatch between the models"
            ]
        },
        {
            "id": "task460-f0136972fcd94c16b48812962574591f",
            "input": "We collect utterances from the $\\mathbf {C}$hinese $\\mathbf {A}$rtificial $\\mathbf {I}$ntelligence $\\mathbf {S}$peakers (CAIS), and annotate them with slot tags and intent labels. The training, validation and test sets are split by the distribution of intents, where detailed statistics are provided in the supplementary material. Since the utterances are collected from speaker systems in the real world, intent labels are partial to the PlayMusic option. We adopt the BIOES tagging scheme for slots instead of the BIO2 used in the ATIS, since previous studies have highlighted meaningful improvements with this scheme BIBREF30 in the sequence labeling field. \n Question: What is the domain of their collected corpus?",
            "output": [
                "speaker systems in the real world"
            ]
        },
        {
            "id": "task460-551e4fc9663245978138ab2634e38923",
            "input": "Our search space consists of two stackable cells, one for the model encoder and one for the decoder Each cell contains NASNet-style blocks, which receive two hidden state inputs and produce new hidden states as outputs  Our search space contains five branch-level search fields (input, normalization, layer, output dimension and activation), one block-level search field (combiner function) and one cell-level search field (number of cells). \n Question: What is in the model search space?",
            "output": [
                "Our search space consists of two stackable cells, one for the model encoder and one for the decoder  Each cell contains NASNet-style blocks, which receive two hidden state inputs and produce new hidden states as outputs Our search space contains five branch-level search fields (input, normalization, layer, output dimension and activation), one block-level search field (combiner function) and one cell-level search field (number of cells)."
            ]
        },
        {
            "id": "task460-5abc02927ca54e009e5ec08521d56712",
            "input": "More specifically, we explore three regularization terms to address the problem: (1) a regularization term associated with neutral features; (2) the maximum entropy of class distribution regularization term; and (3) the KL divergence between reference and predicted class distribution. \n Question: What are the three regularization terms?",
            "output": [
                "a regularization term associated with neutral features the maximum entropy of class distribution regularization term the KL divergence between reference and predicted class distribution"
            ]
        },
        {
            "id": "task460-2b4e24bc00854fdcb3802962306e1fb5",
            "input": "As a convention, the metric of joint goal accuracy is used to compare our model to previous work. \n Question: What are the performance metrics used?",
            "output": [
                "joint goal accuracy"
            ]
        },
        {
            "id": "task460-f1ea097fd89044a69e9c6a84244f0530",
            "input": "In Sec. \"Materials and Methods\" we discuss our materials and methods, including the dataset we studied, how we preprocessed that data and extracted a `causal' corpus and a corresponding `control' corpus, and the details of the statistical and language analysis tools we studied these corpora with. \n Question: Do they use expert annotations, crowdsourcing, or only automatic methods to analyze the corpora?",
            "output": [
                "Only automatic methods"
            ]
        },
        {
            "id": "task460-1a9ece496dc845108c894aa1ca9cdf7d",
            "input": "We thus also conducted human studies on Amazon MTurk to evaluate the generated responses with pairwise comparison for dialogue quality. We compare our models with an advanced decoding algorithm MMI BIBREF2 and two models, namely LSTM BIBREF0 and VHRED BIBREF7, both with additive attention. To our best knowledge, LSTM and VHRED were the primary models with which F1's were reported on the Ubuntu dataset. Following BIBREF5 (BIBREF5), we employ two criteria: Plausibility and Content Richness. The first criterion measures whether the response is plausible given the context, while the second gauges whether the response is diverse and informative.  \n Question: How is human evaluation performed, what was the criteria?",
            "output": [
                "Through Amazon MTurk annotators to determine plausibility and content richness of the response"
            ]
        },
        {
            "id": "task460-32ae07b2269a4ec3b7c1ead3eb02cd53",
            "input": "Compared to using external models for confidence modeling, an advantage of the proposed method is that the base model does not change: the binary classification loss just provides additional supervision. For supervised open IE systems, the confidence score of an assertion is typically computed based on its extraction likelihood given by the model BIBREF3 , BIBREF5  We follow the evaluation metrics described by Stanovsky:2016:OIE2016: area under the precision-recall curve (AUC) and F1 score. \n Question: How does this compare to traditional calibration methods like Platt Scaling?",
            "output": [
                "No reliability diagrams are provided and no explicit comparison is made between confidence scores or methods."
            ]
        },
        {
            "id": "task460-fed4147ae25541cda52f372791ebb5f5",
            "input": "Thus, to limit the number of different inputs to the classifier, we wish to reduce the number of distinct word recognition outputs that an attacker can induce, not just the number of words on which the model is “fooled”. We denote this property of a model as its sensitivity. \n Question: What does the \"sensitivity\" quantity denote?",
            "output": [
                "the number of distinct word recognition outputs that an attacker can induce"
            ]
        },
        {
            "id": "task460-65ff1b9f61464803b3c971acc78853df",
            "input": "We compare our best F1 score with other state-of-the-art approaches in table TABREF39 , which shows our model has competitive advantage in dealing with drug-drug interaction extraction. \n Question: What is the performance of their model?",
            "output": [
                "Answer with content missing: (Table II) Proposed model has F1 score of  0.7220."
            ]
        },
        {
            "id": "task460-77a43f7352ff427a8914c76c30a729aa",
            "input": "With the above hyperparameter setting, the hybrid NER model achieved a F1 score of $0.995$ on synthesized queries and $0.948$ on clinical notes while the i2b2 NER model achieved a F1 score of $0.441$ on synthesized queries and $0.927$ on clinical notes (See Table TABREF23). \n Question: what were their performance results?",
            "output": [
                " the hybrid NER model achieved a F1 score of $0.995$ on synthesized queries and $0.948$ on clinical notes while the i2b2 NER model achieved a F1 score of $0.441$ on synthesized queries and $0.927$ on clinical notes"
            ]
        },
        {
            "id": "task460-5733c1b374c5488ea10d1a855ac81ad0",
            "input": "Posts in FBFans dataset are used for this analysis. We calculate the like statistics of each distinct author from these 32,595 posts. \n Question: What is the size of the Chinese data?",
            "output": [
                "32,595 posts"
            ]
        },
        {
            "id": "task460-1e07c78cc5dc4b399ad88491db3d1850",
            "input": "To this goal, we select a diverse set of eight source languages from different language families – Basque, French, German, Hungarian, Italian, Navajo, Turkish, and Quechua – and three target languages – English, Spanish and Zulu.  \n Question: What are the tree target languages studied in the paper?",
            "output": [
                "English, Spanish and Zulu"
            ]
        },
        {
            "id": "task460-2f14ecf796a34271bf9f32942b2dacf4",
            "input": "jiant System Overview ::: jiant Components\nTasks: Tasks have references to task data, methods for processing data, references to classifier heads, and methods for calculating performance metrics, and making predictions. \n Question: Does jiant involve datasets for the 50 NLU tasks?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-10b831b56f994a5b933ace68617507fb",
            "input": "Hashtag prediction for social media has been addressed earlier, for example in BIBREF15 , BIBREF16 . BIBREF15 also use a neural architecture, but compose text embeddings from a lookup table of words. \n Question: Is this hashtag prediction task an established task, or something new?",
            "output": [
                "established task"
            ]
        },
        {
            "id": "task460-a1d7c3ee2323490fbf1c5cfb2a470842",
            "input": "Results of the DQN-based agent are presented in fig: scenario comparison. Each plot depicts the average reward (across 5 seeds) of all representations methods. It can be seen that the NLP representation outperforms the other methods.  NLP representations remain robust to changes in the environment as well as task-nuisances in the state.  \n Question: What result from experiments suggest that natural language based agents are more robust?",
            "output": [
                "Average reward across 5 seeds show that NLP representations are robust to changes in the environment as well task-nuisances"
            ]
        },
        {
            "id": "task460-87fbbdb3974b4f2993e45aad61a5e058",
            "input": "We train an LSTM with and without attention on the training set. After training, we take the best model in terms of BLEU score BIBREF16 on the development set and calculate the BLEU score on the test set.  \n Question: How is the generative model evaluated?",
            "output": [
                "Comparing BLEU score of model with and without attention"
            ]
        },
        {
            "id": "task460-b3de6ee5a8a44ffda6923474757b4f96",
            "input": "Next, we compared our VTN model with an RNN-based seq2seq VC model called ATTS2S BIBREF8. This model is based on the Tacotron model BIBREF32 with the help of context preservation loss and guided attention loss to stabilize training and maintain linguistic consistency after conversion. We followed the configurations in BIBREF8 but used mel spectrograms instead of WORLD features. \n Question: What is the baseline model?",
            "output": [
                "a RNN-based seq2seq VC model called ATTS2S based on the Tacotron model"
            ]
        },
        {
            "id": "task460-0d8f112324c044cb88182285501d82e6",
            "input": "To report the result for answer selection, MAP and MRR are used; however, the answer triggering task is evaluated by F1-score. The result for MULT Method is reported in Table. 1. \n Question: Is accuracy the only metric they used to compare systems?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-6b516e525872402cb5a0f01973d8bdfe",
            "input": "By choosing English (En) as the pivot language, we perform pivot alignments for identical English segments on Europarl Fr-En and En-De parallel corpora BIBREF18 , constructing a multi-parallel corpus of Fr-En-De. Then each of the Fr*-De and Fr-De* pseudo parallel corpora is established from the multi-parallel data by applying the pivot language-based translation described in the previous section. \n Question: How do they align the synthetic data?",
            "output": [
                "By choosing English (En) as the pivot language, we perform pivot alignments for identical English segments on Europarl Fr-En and En-De parallel corpora BIBREF18 , constructing a multi-parallel corpus of Fr-En-De. Then each of the Fr*-De and Fr-De* pseudo parallel corpora is established from the multi-parallel data by applying the pivot language-based translation described in the previous section."
            ]
        },
        {
            "id": "task460-038ce233dd1241fe93e1c0827670c7b0",
            "input": "Despite the greater similarity between our task and the 2013 ShARe/CLEF Task 1, we use the clinical notes from the CE task in 2010 i2b2/VA on account of 1) the data from 2010 i2b2/VA being easier to access and parse, 2) 2013 ShARe/CLEF containing disjoint entities and hence requiring more complicated tagging schemes. \n Question: where did they obtain the annotated clinical notes from?",
            "output": [
                "clinical notes from the CE task in 2010 i2b2/VA"
            ]
        },
        {
            "id": "task460-d6d1efdf93ec4d1a8b5cb1d16937f574",
            "input": "We address this issue by We address this issue by examining and reviewing publicly available datasets for abusive content detection, which we provide access to on a new dedicated website, hatespeechdata.com. We address this issue by examining and reviewing publicly available datasets for abusive content detection, which we provide access to on a new dedicated website, hatespeechdata.com. \n Question: What is open website for cataloguing abusive language data?",
            "output": [
                "hatespeechdata.com"
            ]
        },
        {
            "id": "task460-ec943fc328af4c8ca18f5490a12cde4a",
            "input": "For each test set, we use the corresponding training questions $Q_\\mathit {tr}$ to retrieve domain-relevant sentences from S. Specifically, for each multiple-choice question $(q,A) \\in Q_\\mathit {tr}$ and each choice $a \\in A$ , we use all non-stopword tokens in $q$ and $a$ as an ElasticSearch query against S. We take the top 200 hits, run Open IE v4, and aggregate the resulting tuples over all $a \\in A$ and over all questions in $Q_\\mathit {tr}$ to create the tuple KB (T). \n Question: What method was used to generate the OpenIE extractions?",
            "output": [
                "for each multiple-choice question $(q,A) \\in Q_\\mathit {tr}$ and each choice $a \\in A$ , we use all non-stopword tokens in $q$ and $a$ as an ElasticSearch query against S take the top 200 hits, run Open IE v4, and aggregate the resulting tuples over all $a \\in A$ and over all questions in $Q_\\mathit {tr}$"
            ]
        },
        {
            "id": "task460-7acbaf485a6349f69b82c1eda7f0309e",
            "input": "We build the iSQuAD and iNewsQA datasets based on SQuAD v1.1 BIBREF0 and NewsQA BIBREF1. iMRC: Making MRC Interactive ::: Evaluation Metric\nSince iMRC involves both MRC and RL, we adopt evaluation metrics from both settings. First, as a question answering task, we use $\\text{F}_1$ score to compare predicted answers against ground-truth, as in previous works. When there exist multiple ground-truth answers, we report the max $\\text{F}_1$ score. Second, mastering multiple games remains quite challenging for RL agents. Therefore, we evaluate an agent's performance during both its training and testing phases. During training, we report training curves averaged over 3 random seeds. During test, we follow common practice in supervised learning tasks where we report the agent's test performance corresponding to its best validation performance . \n Question: What are the models evaluated on?",
            "output": [
                "They evaluate F1 score and agent's test performance on their own built interactive datasets (iSQuAD and iNewsQA)"
            ]
        },
        {
            "id": "task460-cffbb28ef680418ba7fcaac5ec797ab7",
            "input": "Visual Question Generation (VQG) is another emerging topic which aims to ask questions given an image. Motivated by this, BIBREF10 proposed open-ended VQG that aims to generate natural and engaging questions about an image. \n Question: Do they survey visual question generation work?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-66d3e171d0de4e908104a3684d6ec7e3",
            "input": "To test the proposed methods ability to generate unsupervised words, it was necessary to devise a method of measuring word relevance. Topic modeling was used based on the assumption that words found in the same topic are more relevant to one another then words from different topics BIBREF14. The complete 200k headline dataset BIBREF11 was modeled using a Naïve Bayes Algorithm BIBREF15 to create a word-category co-occurrence model. The top 200 most relevant words were then found for each category and used to create the topic table SECREF12. It was assumed that each category represented its own unique topic.\n\nThe number of relevant output words as a function of the headline’s category label were measured, and can be found in figure SECREF4. The results demonstrate that the proposed method could correctly identify new words relevant to the input topic at a signal to noise ratio of 4 to 1. \n Question: How do they determine similarity between predicted word and topics?",
            "output": [
                "number of relevant output words as a function of the headline’s category label"
            ]
        },
        {
            "id": "task460-d136498871604232bd344feac2fb2a10",
            "input": "BIBREF6 introduce the KG-A2C, which uses a knowledge graph based state-representation to aid in the section of actions in a combinatorially-sized action-space—specifically they use the knowledge graph to constrain the kinds of entities that can be filled in the blanks in the template action-space. They test their approach on Zork1, showing the combination of the knowledge graph and template action selection resulted in improvements over existing methods. They note that their approach reaches a score of 40 which corresponds to a bottleneck in Zork1 where the player is eaten by a “grue” (resulting in negative reward) if the player has not first lit a lamp. \n Question: What are the baselines?",
            "output": [
                "a score of 40"
            ]
        },
        {
            "id": "task460-376f5eb98c5940a387c7a80ad89a8f72",
            "input": "In this work, we use MLP (modified to handle our data representation) as the base classifier. \n Question: Which classification algorithm do they use for s2sL?",
            "output": [
                "MLP"
            ]
        },
        {
            "id": "task460-78fbcc156c2d4325b1422be9e6390cce",
            "input": "We show the final results for opinion recommendation, comparing our proposed model with the following state-of-the-art baseline systems:\n\nRS-Average is the widely-adopted baseline (e.g., by Yelp.com), using the averaged review scores as the final score.\n\nRS-Linear estimates the rating score that a user would give by INLINEFORM0 BIBREF49 , where INLINEFORM1 and INLINEFORM2 are the the training deviations of the user INLINEFORM3 and the product INLINEFORM4 , respectively.\n\nRS-Item applies INLINEFORM0 NN to estimate the rating score BIBREF50 . We choose the cosine similarity between INLINEFORM1 to measure the distance between product.\n\nRS-MF is a state-of-the-art recommendation model, which uses matrix factorisation to predict rating score BIBREF8 , BIBREF41 , BIBREF25 .\n\nSum-Opinosis uses a graph-based framework to generate abstractive summarisation given redundant opinions BIBREF51 .\n\nSum-LSTM-Att is a state-of-the-art neural abstractive summariser, which uses an attentional neural model to consolidate information from multiple text sources, generating summaries using LSTM decoding BIBREF44 , BIBREF3 .\n\nAll the baseline models are single-task models, without considering rating and summarisation prediction jointly. The results are shown in Table TABREF46 .  \n Question: What are the baselines?",
            "output": [
                "RS-Average  RS-Linear RS-Item RS-MF Sum-Opinosis Sum-LSTM-Att"
            ]
        },
        {
            "id": "task460-1a1dcedc7b7a44d998241179b58bcf08",
            "input": "Stanford - Twitter Sentiment Corpus (STS Corpus): STS Corpus contains 1,600K training tweets collected by a crawler from BIBREF0 . BIBREF0 constructed a test set manually with 177 negative and 182 positive tweets. The Stanford test set is small. However, it has been widely used in different evaluation tasks BIBREF0 BIBREF5 BIBREF13 .\n\nSanders - Twitter Sentiment Corpus: This dataset consists of hand-classified tweets collected by using search terms: INLINEFORM0 , #google, #microsoft and #twitter. We construct the dataset as BIBREF14 for binary classification.\n\nHealth Care Reform (HCR): This dataset was constructed by crawling tweets containing the hashtag #hcr BIBREF15 . Task is to predict positive/negative tweets BIBREF14 . \n Question: Which three Twitter sentiment classification datasets are used for experiments?",
            "output": [
                "Stanford - Twitter Sentiment Corpus (STS Corpus) Sanders - Twitter Sentiment Corpus Health Care Reform (HCR)"
            ]
        },
        {
            "id": "task460-7fee876edd8f4b95b23ccb873a9627ce",
            "input": "According to the context given, the raters were instructed to evaluate the quality of the responses based on three criteria: (1) grammatical correctness—whether or not the response is fluent and free of grammatical mistakes; (2) contextual coherence—whether or not the response is context sensitive to the previous dialog history; (3) emotional appropriateness—whether or not the response conveys the right emotion and feels as if it had been produced by a human. For each criterion, the raters gave scores of either 0, 1 or 2, where 0 means bad, 2 means good, and 1 indicates neutral. \n Question: How is human evaluation performed?",
            "output": [
                "(1) grammatical correctness (2) contextual coherence (3) emotional appropriateness"
            ]
        },
        {
            "id": "task460-402542e1236247fcb59057a2bf81715a",
            "input": "To address our next question, we assessed our pipeline when learning relations independently (i.e., individually) versus learning relations jointly within the RDN, displayed in Table TABREF22 . \n Question: What do they learn jointly?",
            "output": [
                "relations"
            ]
        },
        {
            "id": "task460-3b063ca72b0c4ae19a05ffc4be8bef6b",
            "input": "Another approach for generating query and image representations is treating images as a black box. Given the statistics of our dataset (3B query, image pairs with 220M unique queries and 900M unique images), we know that different queries co-occur with the same images. Intuitively, if a query $q_1$ co-occurs with many of the same images as query $q_2$ , then $q_1$ and $q_2$ are likely to be semantically similar, regardless of the visual content of the shared images. Thus, we can use a method that uses only co-occurrence statistics to better understand how well we can capture relationships between queries. \n Question: Could you learn such embedding simply from the image annotations and without using visual information?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-6e8aa287d6af4b96ac6704360b9234aa",
            "input": "Our prediction task is thus a straightforward binary classification task at the word level. We develop the following five groups of features to capture properties of how a word is used in the explanandum (see Table TABREF18 for the full list):\n\n[itemsep=0pt,leftmargin=*,topsep=0pt]\n\nNon-contextual properties of a word. These features are derived directly from the word and capture the general tendency of a word being echoed in explanations.\n\nWord usage in an OP or PC (two groups). These features capture how a word is used in an OP or PC. As a result, for each feature, we have two values for the OP and PC respectively.\n\nHow a word connects an OP and PC. These features look at the difference between word usage in the OP and PC. We expect this group to be the most important in our task.\n\nGeneral OP/PC properties. These features capture the general properties of a conversation. They can be used to characterize the background distribution of echoing.\n\nTable TABREF18 further shows the intuition for including each feature, and condensed $t$-test results after Bonferroni correction. Specifically, we test whether the words that were echoed in explanations have different feature values from those that were not echoed. In addition to considering all words, we also separately consider stopwords and content words in light of Figure FIGREF8. Here, we highlight a few observations:\n\n[itemsep=0pt,leftmargin=*,topsep=0pt]\n\nAlthough we expect more complicated words (#characters) to be echoed more often, this is not the case on average. We also observe an interesting example of Simpson's paradox in the results for Wordnet depth BIBREF38: shallower words are more likely to be echoed across all words, but deeper words are more likely to be echoed in content words and stopwords. \n Question: What features are proposed?",
            "output": [
                "Non-contextual properties of a word Word usage in an OP or PC (two groups) How a word connects an OP and PC General OP/PC properties"
            ]
        },
        {
            "id": "task460-7b7d0d4538ff4b8784596c7858fdec78",
            "input": "While confirming the effectiveness of BT, our study also proposed significantly cheaper ways to improve the baseline performance, using a slightly modified copy of the target, instead of its full BT.  \n Question: why are their techniques cheaper to implement?",
            "output": [
                "They use a slightly modified copy of the target to create the pseudo-text instead of full BT to make their technique cheaper"
            ]
        },
        {
            "id": "task460-e4fb6fcce57d4626bf8473b3ea10ac0c",
            "input": "We examine a deep-learning approach to sequence labeling using a vanilla recurrent neural network (RNN) with word embeddings, as well as a joint inference, structured prediction approach using Stanford's knowledge base construction framework DeepDive BIBREF1 . \n Question: Which structured prediction approach do they adopt for temporal entity extraction?",
            "output": [
                "DeepDive BIBREF1"
            ]
        },
        {
            "id": "task460-59275d587bc9439fb295c7b5b9cf1d1e",
            "input": "TextCat is the most well-known Perl implementation of the out-of-place method, it lists models for 76 languages in its off-the-shelf configuration; the program is not actively maintained. TextCat is the most well-known Perl implementation of the out-of-place method, it lists models for 76 languages in its off-the-shelf configuration; the program is not actively maintained. TextCat is not the only example of an off-the-shelf implementation of the out-of-place method: other implementations include libtextcat with 76 language models, JTCL with 15 languages, and mguesser with 104 models for different language-encoding pairs. The main issue addressed by later implementations is classification speed: TextCat is implemented in Perl and is not optimized for speed, whereas implementations such as libtextcat and mguesser have been specifically written to be fast and efficient. whatlang-rs uses an algorithm based on character trigrams and refers the user to the BIBREF7 article. It comes pre-trained with 83 languages.\n\nis the language identifier embedded in the Google Chrome web browser. It uses a NB classifier, and script-specific classification strategies. assumes that all the input is in UTF-8, and assigns the responsibility of encoding detection and transcoding to the user. uses Unicode information to determine the script of the input. also implements a number of pre-processing heuristics to help boost performance on its target domain (web pages), such as stripping character sequences like .jpg. The standard implementation supports 83 languages, and an extended model is also available, that supports 160 languages.\n\nis a Java library that implements a language identifier based on a NB classifier trained over character . The software comes with pre-trained models for 53 languages, using data from Wikipedia. makes use of a range of normalization heuristics to improve the performance on particular languages, including: (1) clustering of Chinese/Japanese/Korean characters to reduce sparseness; (2) removal of “language-independent” characters, and other text normalization; and (3) normalization of Arabic characters.\n\nis a Python implementation of the method described by BIBREF150 , which exploits training data for the same language across multiple different sources of text to identify sequences of characters that are strongly predictive of a given language, regardless of the source of the text. This feature set is combined with a NB classifier, and is distributed with a pre-trained model for 97 languages prepared using data from 5 different text sources. BIBREF151 provide an empirical comparison of to , and and find that it compares favorably both in terms of accuracy and classification speed. There are also implementations of the classifier component (but not the training portion) of in Java, C, and JavaScript.\n\nBIBREF153 uses a vector-space model with per-feature weighting on character sequences. One particular feature of is that it uses discriminative training in selecting features, i.e. it specifically makes use of features that are strong evidence against a particular language, which is generally not captured by NB models. Another feature of is that it uses inter-string smoothing to exploit sentence-level locality in making language predictions, under the assumption that adjacent sentences are likely to be in the same language. BIBREF153 reports that this substantially improves the accuracy of the identifier. Another distinguishing feature of is that it comes pre-trained with data for 1400 languages, which is the highest number by a large margin of any off-the-shelf system.\n\nwhatthelang is a recent language identifier written in Python, which utilizes the FastText NN-based text classification algorithm. It supports 176 languages.\n\nimplements an off-the-shelf classifier trained using Wikipedia data, covering 122 languages. Although not described as such, the actual classification algorithm used is a linear model, and is thus closely related to both NB and a cosine-based vector space model.\n\nIn addition to the above-mentioned general-purpose language identifiers, there have also been efforts to produce pre-trained language identifiers targeted specifically at Twitter messages. is a Twitter-specific tool with built-in models for 19 languages. It uses a document representation based on tries BIBREF401 . The algorithm is a LR classifier using all possible substrings of the data, which is important to maximize the available information from the relatively short Twitter messages.\n\nBIBREF152 provides a comparison of 8 off-the-shelf language identifiers applied without re-training to Twitter messages. One issue they report is that comparing the accuracy of off-the-shelf systems is difficult because of the different subset of languages supported by each system, which may also not fully cover the languages present in the target data. The authors choose to compare accuracy over the full set of languages, arguing that this best reflects the likely use-case of applying an off-the-shelf system to new data. They find that the best individual systems are , and , but that slightly higher accuracy can be attained by a simple voting-based ensemble classifier involving these three systems.\n\nIn addition to this, commercial or other closed-source language identifiers and language identifier services exist, of which we name a few. The Polyglot 3000 and Lextek Language Identifier are standalone language identifiers for Windows. Open Xerox Language Identifier is a web service with available REST and SOAP APIs. \n Question: what are the off-the-shelf systems discussed in the paper?",
            "output": [
                "Answer with content missing: (Names of many identifiers missing) TextCat, ChromeCLD, LangDetect, langid.py, whatlang, whatthelang, YALI, LDIG, Polyglot 3000, Lextek Language Identifier and Open Xerox Language Identifier."
            ]
        },
        {
            "id": "task460-c96c19840939453bb9d2d495650a7052",
            "input": " Generally, providing an incorrect speaker and/or audience information decreases the BLEU scores, while providing the correct information substantially improves it - we see an increase of up to 2.3 BLEU over the baseline. We see that the baseline system severely under-predicts the feminine form of verbs as compared to the reference. The “He said” conditions further decreases the number of feminine verbs, while the “I said” conditions bring it back to the baseline level. Finally, the “She said” prefixes substantially increase the number of feminine-marked verbs, bringing the proportion much closer to that of the reference (though still under-predicting some of the feminine cases). \n Question: How is it demonstrated that the correct gender and number information is injected using this system?",
            "output": [
                " correct information substantially improves it - we see an increase of up to 2.3 BLEU over the baseline Finally, the “She said” prefixes substantially increase the number of feminine-marked verbs, bringing the proportion much closer to that of the reference"
            ]
        },
        {
            "id": "task460-d0e4e01268fd4bda8b12c54834fb990d",
            "input": "The accuracy of our model is 7.8% higher than the best result achieved by LSVM. The results show that this model can perform better than state-of-the-art baselines including hybrid CNN BIBREF15 and LSTM with attention BIBREF16 by 3.1% on the validation set and 1% on the test set. \n Question: What are state of the art methods authors compare their work with? ",
            "output": [
                "ISOT dataset: LLVM\nLiar dataset: Hybrid CNN and LSTM with attention"
            ]
        },
        {
            "id": "task460-14ae8c5295b14546b5ba5cd6c726e27b",
            "input": "Instead of reading in the tokenized input text, our model reads raw utf-8 bytes. For English text in the ASCII range, this is equivalent to processing characters as individual tokens. Non-ASCII characters (e.g. accented characters, or non-Latin scripts) are typically two or three utf-8 bytes. We use a standard “transformer decoder” (a stack of transformer layers with a causal attention mask) to process the sequence $x_{0:i-1}$ and predict the following byte $x_i$. The model's prediction is an estimate of the probability distribution over all possible 256 byte values. Our input byte embedding matrix has dimensionality 256. \n Question: How many characters are accepted as input of the language model?",
            "output": [
                "input byte embedding matrix has dimensionality 256"
            ]
        },
        {
            "id": "task460-a5dcad41489a49d79ecc66bac98b1539",
            "input": "We showed in BIBREF22 that Pearson correlation is badly suited for STS. Instead, we compute the Spearman's rank correlation between the cosine-similarity of the sentence embeddings and the gold labels. \n Question: What metrics are used for the STS tasks?",
            "output": [
                " Spearman's rank correlation between the cosine-similarity of the sentence embeddings and the gold labels"
            ]
        },
        {
            "id": "task460-f211a7e45c4a4f9c8a6e754470c901cb",
            "input": "We evaluate three models of the image attention mechanism INLINEFORM0 of equation EQREF11 . Soft attention Hard Stochastic attention Local Attention \n Question: Which attention mechanisms do they compare?",
            "output": [
                "Soft attention Hard Stochastic attention Local Attention"
            ]
        },
        {
            "id": "task460-e4f92e7d92d34e599f5aa7b7cbb4514d",
            "input": "Our approach to FEVER is to fix the most obvious shortcomings of the baseline approaches to retrieval and entailment, and to train a sharp entailment classifier that can be used to filter a broad set of retrieved potential evidence. For the entailment classifier we compare Decomposable Attention BIBREF2 , BIBREF3 as implemented in the official baseline, ESIM BIBREF4 , and a transformer network with pre-trained weights BIBREF5 . The transformer network naturally supports out-of-vocabulary words and gives substantially higher performance than the other methods. \n Question: What baseline do they compare to?",
            "output": [
                "For the entailment classifier we compare Decomposable Attention BIBREF2 , BIBREF3 as implemented in the official baseline, ESIM BIBREF4 , and a transformer network with pre-trained weights BIBREF5 ."
            ]
        },
        {
            "id": "task460-7a04be812414455f97ca1039307c505e",
            "input": "We also did a qualitative study on the Starbucks (SBUX) stock movement during this event. Figure FIGREF12 is the daily percentage change of SBUX and NASDAQ index between April 11th and April 20th. SBUX did not follow the upward trend of the whole market before April 17th, and then its change on April 20th, INLINEFORM0 , is quite significant from historical norms. We collected the historical 52 week stock prices prior to this event and calculated the daily stock price change. The distribution of the daily price change of the previous 52 weeks is Figure FIGREF13 with a mean INLINEFORM1 and standard deviation INLINEFORM2 .  \n Question: How does the method measure the impact of the event on market prices?",
            "output": [
                "We collected the historical 52 week stock prices prior to this event and calculated the daily stock price change. The distribution of the daily price change of the previous 52 weeks is Figure FIGREF13 with a mean INLINEFORM1 and standard deviation INLINEFORM2 . "
            ]
        },
        {
            "id": "task460-f07fe62f390244469649ba7dceeafcec",
            "input": "When compared with baselines that randomly choose one of the neighbors, or assume that the fact with the lowest score is incorrect, we see that outperforms both of these with a considerable gap, obtaining an accuracy of $42\\%$ and $55\\%$ in detecting errors. \n Question: Can this adversarial approach be used to directly improve model accuracy?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-49584c55c6114be9a941d26bfaed3213",
            "input": "We also consider the following baselines. BOW-Tags represents locations using a bag-of-words representation, using the same tag weighting as the embedding model. BOW-KL(Tags) uses the same representation but after term selection, using the same KL-based method as the embedding model. BOW-All combines the bag-of-words representation with the structured information, encoded as proposed in BIBREF7 . GloVe uses the objective from the original GloVe model for learning location vectors, i.e. this variant differs from EGEL-Tags in that instead of INLINEFORM1 we use the number of co-occurrences of tag INLINEFORM2 near location INLINEFORM3 , measured as INLINEFORM4 . \n Question: what are the existing approaches?",
            "output": [
                "BOW-Tags BOW-KL(Tags) BOW-All GloVe"
            ]
        },
        {
            "id": "task460-9c4c3f05211b40fda337a1cc3f18deb3",
            "input": "he immediate impact of the explicit constraint is avoiding the collapse issue ($D_{KL}=0$) by setting a non-zero positive constraint ($C\\ge 0$) on the KL term ($|D_{KL}\\big (q_\\phi ({z}|{x}) || p({z})\\big )-C|$). \n Question: Why does proposed term help to avoid posterior collapse?",
            "output": [
                "by setting a non-zero positive constraint ($C\\ge 0$) on the KL term ($|D_{KL}\\big (q_\\phi ({z}|{x}) || p({z})\\big )-C|$)"
            ]
        },
        {
            "id": "task460-b7005cd395434c249ef9c7594f21abc2",
            "input": "We calculate the probability of a sentence with a long-short term memory (LSTM, hochreiter1997long) LM, i.e., a special type of RNN LM, which has been trained on a large corpus. We train our LSTM LMs on the English Gigaword corpus BIBREF15 , which consists of news data. \n Question: what language models do they use?",
            "output": [
                "LSTM LMs"
            ]
        },
        {
            "id": "task460-40209810658f4f1fa3573a35ec7f8a6f",
            "input": "To create training data for unanswerable question generation, we use (plausible) answer spans in paragraphs as pivots to align pairs of answerable questions and unanswerable questions. In this way, we obtain the data with which the models can learn to ask unanswerable questions by editing answerable ones with word exchanges, negations, etc. \n Question: How do they ensure the generated questions are unanswerable?",
            "output": [
                "learn to ask unanswerable questions by editing answerable ones with word exchanges, negations, etc"
            ]
        },
        {
            "id": "task460-cb336f68fda148e3afc26ba260ecec71",
            "input": "Our main contributions are (1) the publication of Turkish corpus for coarse-grained and fine-grained NER, and TC research, (2) six different versions of corpus according to noise reduction methodology and entity types, (3) an analysis of the corpus and (4) benchmark comparisons for NER and TC tasks against human annotators.  \n Question: Did they experiment with the dataset on some tasks?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-5be3ef06952443a282cf3a1e167e603c",
            "input": "Specifically, Roget's Thesaurus BIBREF38 , BIBREF39 is used to derive the concepts and concept word-groups to be used as the external lexical resource for our proposed method. By using the concept word-groups, we have trained the GloVe algorithm with the proposed modification given in Section SECREF4 on a snapshot of English Wikipedia measuring 8GB in size, with the stop-words filtered out. \n Question: Do they report results only on English data?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-a3a79384fbeb411caba42ff9e5132425",
            "input": "For the task-oriented system, although there are some objective evaluation metrics, such as the number of turns in a dialogue, the ratio of task completion, etc., there is no gold standard for automatically evaluating two (or more) dialogue systems when considering the satisfaction of the human and the fluency of the generated dialogue. \n Question: What problems are found with the evaluation scheme?",
            "output": [
                "no gold standard for automatically evaluating two (or more) dialogue systems when considering the satisfaction of the human and the fluency of the generated dialogue"
            ]
        },
        {
            "id": "task460-06f38816c58b4d6ca9229490534ffdd7",
            "input": "A key benefit is that the RNN infers a latent representation of state, obviating the need for state labels. \n Question: Does the latent dialogue state heklp their model?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-c96aa556899d49ec8bef5f8ac3ecc232",
            "input": "In Figure FIGREF15 , we plot the zero-resource German and Japanese test set accuracy as a function of the number of steps taken, with and without adversarial training. The plot shows that the variation in the test accuracy is reduced with adversarial training, which suggests that the cross-lingual performance is more consistent when adversarial training is applied. \n Question: Do any of the evaluations show that adversarial learning improves performance in at least two different language families?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-087a566369ce429cb612bf7fed1070a9",
            "input": "We experiment with two image captioning models: the Show&Tell model BIBREF0 and the LRCN1u model BIBREF1. Both models follow the basic encoder-decoder architecture design that uses a CNN encoder to condense the visual information into an image embedding, which in turn conditions an LSTM decoder to generate a natural language caption. The main difference between the two models is the way they condition the decoder. The Show&Tell model feeds the image embedding as the “predecessor word embedding” to the first produced word, while the LRCN1u model concatenates the image features with the embedded previous word as the input to the sequence model at each time step. \n Question: Which existing models are evaluated?",
            "output": [
                "Show&Tell and LRCN1u"
            ]
        },
        {
            "id": "task460-1c900f00284a4641b7a57a86434b6be4",
            "input": "In its general form, a type-logical grammar consists of following components: \n Question: Does Grail accept Prolog inputs?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-7a62cd98dcc5449aae968a6a54f40c93",
            "input": "Incorporating Document Features ::: Learning Phase ::: Feature Extraction ::: Document-unaware Features\nOrdinal position: It is shown that inclusion of sentence, in summary, is relevant to its position in the document or even in a paragraph. Intuitively, sentences at the beginning or the end of a text are more likely to be included in the summary. Depending on how it is defined, this feature might be document-unaware or not. For example, in BIBREF29 and BIBREF37 it is defined as $\\frac{5}{5}$ for the first sentence, $\\frac{4}{5}$ for the second, and so on to $\\frac{1}{5}$ for fifth and zero for remaining sentences. In another research conducted by Wong et al. BIBREF9, it is defined as $\\frac{1}{sentence\\ number}$. With such a definition, we may have several sentences, for example, with position=$\\frac{1}{5}$ in the training set, these may not have the same sense of position. While a sentence position=$\\frac{1}{5}$ means “among the firsts” in a document with 40 sentences, it has a totally different meaning of “in the middle”, in another document containing 10 sentences. Thus, a useful feature formula should involve differences of documents which may change the meaning of information within it. In our experiments, we used the definition of BIBREF9. A document-aware version of position will be introduced in (SECREF6).\n\nLength of sentence: the intuition behind this feature is that sentences of too long or too short length are less likely to be included in the summary. Like sentence position, this feature is also subject to the wrong definition that makes it document-unaware. For example, in BIBREF9 it is defined as a number of words in a sentence. Such a definition does not take into account that a sentence with, say 15 words may be considered long if all other sentences of document have fewer words. Another sentence with the same number of words may be regarded as short, because other sentences in that document have more than 15 words. This might occur due to different writing styles. However, we included this in our experiments to compare its effect with that of its document-aware counterpart, which will be listed in (SECREF6).\n\nThe Ratio of Nouns: is defined in BIBREF30 as the number of nouns divided by total number of words in the sentence, after stop-words are removed. Three other features, Ratio of Verbs, Ratio of Adjectives, and Ratio of Adverbs are defined in the same manner and proved to have a positive effect on ranking performance. From our perspective, however, a sentence with a ratio of nouns =0.5, for example, in a document containing many nouns, must be discriminated in the training set from another sentence with the same ratio of nouns, that appeared in another document having fewer nouns. This feature does not represent how many nouns are there in the document, which is important in sentence ranking. The same discussion goes on to justify the need to consider the number of verbs, adjectives, and adverbs in the document. The impact of these features is examined in our experiments and compared to that of their document-aware counterparts.\n\nThe Ratio of Numerical entities: assuming that sentences containing more numerical data are probably giving us more information, this feature may help us in ranking BIBREF31, BIBREF32. For calculation, we count the occurrences of numbers and digits proportional to the length of sentence. This feature must be less weighted if almost all sentences of a document have numerical data. However, it does not count numbers and digits in other sentences of the document.\n\nCue Words: if a sentence contains special phrases such as “in conclusion”, “overall”, “to summarize”, “in a nutshell” and so forth, its selection as a part of the summary is more probable than others. The number of these phrases is counted for this feature.\n\nIncorporating Document Features ::: Learning Phase ::: Feature Extraction ::: Document-aware Features\nCosine position: As mentioned in (SECREF5) a good definition of position should take into account document length. A well-known formula used in the literature BIBREF38, BIBREF7 is\n\nin which index is an integer representing the order of sentences and T is the total number of sentences in document. This feature ranges from 0 to 1, the closer to the beginning or to the end, the higher value this feature will take. $\\alpha $ is a tuning parameter. As it increases, the value of this feature will be distributed more equally over sentences. In this manner, equal values of this feature in the training set represent a uniform notion of position in a document, so it becomes document-aware.\n\nRelative Length: the intuition behind this feature is explained in (SECREF5). A discussion went there that a simple count of words does not take into account that a sentence with a certain number of words may be considered long or short, based on the other sentences appeared the document. Taking this into consideration, we divided the number of words in the sentence by the average length of sentences in the document. More formally, the formula is:\n\nin which n is number of sentences in the document and $s_i$ is the i’th sentence of it. Values greater than 1 could be interpreted as long and vice versa.\n\nTF-ISF: this feature counts the frequency of terms in a document and assigns higher values to sentences having more frequent terms. It also discounts terms which appear in more sentences. Since it is well explained in the literature, we have not included details and formula which are in references BIBREF34 and BIBREF39. Nonetheless, the aspect that matters in our discussion is that both frequency and inverse sentence frequency are terms which involve properties of context, and consequently are document-aware.\n\nPOS features: Here we introduce another way to include the ratio of part of speech (POS) units in features and keep them document-normalized. To do this, the number of occurrences of each POS unit should be divided by the number of them in the document, instead of that occurring in a sentence. The formal definition of the new document-aware features are as follows:\n\nIncorporating Document Features ::: Learning Phase ::: Feature Extraction ::: Explicit Document Features\nIn order to further investigate how effective are document specific features in sentence ranking, we defined several features for documents. These features are then calculated for each document and repeated in the feature vector of every sentence of that document. Their formal definition is described below and their effect is examined in the result and discussion section (SECREF5):\n\nDocument sentences: An important property of a document that affects summarization is the total number of sentences participating in sentence ranking. As this number grows, a summarizer should be more selective and precise. Also, some sentence features such as cue words, maybe more weighted for longer documents. In addition, the main contextual information is probably more distributed over sentences. In such a case even lower values of other features should be considered important.\n\nDocument words: the number of words in the document is another notion of document length. Since the number of sentences alone is not enough to represent document length, this feature should also be considered.\n\nTopical category: different topics such as political, economic, etc. have different writing styles and this might affect sentence ranking. For instance, numerical entities may appear more in economic or sport reports than in religious or social news. Therefore the weight of this attribute should be more or less, based on a document’s category. So it needs to be included. \n Question: What features of the document are integrated into vectors of every sentence?",
            "output": [
                "Ordinal position Length of sentence The Ratio of Nouns The Ratio of Numerical entities Cue Words Cosine position Relative Length TF-ISF POS features Document sentences Document words Topical category Ratio of Verbs, Ratio of Adjectives, and Ratio of Adverbs"
            ]
        },
        {
            "id": "task460-a65a8f448b394439b4a0cfc47cbb59d6",
            "input": "Table TABREF19 displays the performance of the 4 baselines on the ReviewQA's test set. These results are the performance achieved by our own implementation of these 4 models. \n Question: What tasks were evaluated?",
            "output": [
                "ReviewQA's test set"
            ]
        },
        {
            "id": "task460-a4e1cbd3474043309c665591c29a651e",
            "input": "We also propose an effective model to integrate clinical named entity information into pre-trained language model. In this section, we present an effective model for the question answering based clinical text structuring (QA-CTS). As shown in Fig. FIGREF8, paragraph text $X$ is first passed to a clinical named entity recognition (CNER) model BIBREF12 to capture named entity information and obtain one-hot CNER output tagging sequence for query text $I_{nq}$ and paragraph text $I_{nt}$ with BIEOS (Begin, Inside, End, Outside, Single) tag scheme. $I_{nq}$ and $I_{nt}$ are then integrated together into $I_n$. Meanwhile, the paragraph text $X$ and query text $Q$ are organized and passed to contextualized representation model which is pre-trained language model BERT BIBREF26 here to obtain the contextualized representation vector $V_s$ of both text and query. Afterwards, $V_s$ and $I_n$ are integrated together and fed into a feed forward network to calculate the start and end index of answer-related text. \n Question: How they introduce domain-specific features into pre-trained language model?",
            "output": [
                "integrate clinical named entity information into pre-trained language model"
            ]
        },
        {
            "id": "task460-b6df02432cea4fbb94d9d4085ed0e6ee",
            "input": "We compare our model with several baseline models as described below. We note that the results of the first two are previously reported in BIBREF2.\n\nHasty Student BIBREF2 is a heuristics-based simple model which ignores the recipe and gives an answer by examining only the question and the answer set using distances in the visual feature space.\n\nImpatient Reader BIBREF19 is a simple neural model that takes its name from the fact that it repeatedly computes attention over the recipe after observing each image in the query.\n\nBiDAF BIBREF14 is a strong reading comprehension model that employs a bi-directional attention flow mechanism to obtain a question-aware representation and bases its predictions on this representation. Originally, it is a span-selection model from the input context. BiDAF w/ static memory is an extended version of the BiDAF model which resembles our proposed PRN model in that it includes a memory unit for the entities. \n Question: What are previously reported models?",
            "output": [
                "Hasty Student Impatient Reader BiDAF BiDAF w/ static memory"
            ]
        },
        {
            "id": "task460-aad3d1108d4a4527916773413ecf29b8",
            "input": "Second, scrapping Tweets directly from Twitter website. Using the second option, the daily Tweets for stocks of interest from 2015 January to 2017 June were downloaded. For this reason, two companies from the same industry, Tesla and Ford are investigated on how Twitter sentiment could impact the stock price. To translate each tweet into a sentiment score, the Stanford coreNLP software was used. \n Question: Which tweets are used to output the daily sentiment signal?",
            "output": [
                "Tesla and Ford are investigated on how Twitter sentiment could impact the stock price"
            ]
        },
        {
            "id": "task460-48f479e829b3410daad017a3291bc9eb",
            "input": "There are three main components to the model: 1) input encoder 2) dynamic memory, and 3) output module. We will describe these three modules in details. The input encoder and output module implementations are similar to the Entity Network BIBREF17 and main novelty lies in the dynamic memory. We describe the operations executed by the network for a single example consisting of a document with $T$ sentences, where each sentence consists of a sequence of words represented with $K$ -dimensional word embeddings $\\lbrace e_1, \\ldots , e_N\\rbrace $ , a question on the document represented as another sequence of words and an answer to the question. \n Question: How is knowledge stored in the memory?",
            "output": [
                "entity memory and relational memory."
            ]
        },
        {
            "id": "task460-5cd6706b7931481a841ab656df445b05",
            "input": "To further demonstrate the effectiveness of the additional task-specific BiRNN layers in our architecture, we conducted an ablation study using the CoNLL04 dataset. We trained and evaluated in the same manner described above, using the same hyperparameters, with the following exceptions:\n\nWe used either (i) zero NER-specific BiRNN layers, (ii) zero RE-specific BiRNN layers, or (iii) zero task-specific BiRNN layers of any kind.\n\nWe increased the number of shared BiRNN layers to keep the total number of model parameters consistent with the number of parameters in the baseline model.\n\nWe average the results for each set of hyperparameter across three trials with random weight initializations. \n Question: What were the variables in the ablation study?",
            "output": [
                "(i) zero NER-specific BiRNN layers, (ii) zero RE-specific BiRNN layers, or (iii) zero task-specific BiRNN layers of any kind"
            ]
        },
        {
            "id": "task460-c06bf7aa0bee45779ebc0ef602bf8582",
            "input": "At each training step, we train the model with a batch of original examples along with adversarial examples with balanced labels. The adversarial examples account for around 10% in a batch. During training, we generate adversarial examples with the current model as the target and update the model parameters with the hybrid batch iteratively. \n Question: What is discriminator in this generative adversarial setup?",
            "output": [
                " current model"
            ]
        },
        {
            "id": "task460-3e73513a83444f97af0145c339ddeca3",
            "input": "Therefore, in order to enhance the prediction model's robustness, we will adopt differential privacy (DP) method. DP is a system for sharing information about a dataset publicly by describing groups' patterns within the dataset while withholding information about individuals in the dataset. DP can be achieved if the we are willing to add random noise to the result. For example, rather than simply reporting the sum, we can inject noise from a Laplace or gaussian distribution, producing a result that’s not quite exact, that masks the contents of any given row.  It intuitively requires that the mechanism that outputs information about an underlying dataset is robust to one sample's any change, thus protecting privacy. A mechanism ${f}$ is a random function that takes a dataset $\\mathcal {N}$ as input, and outputs a random variable ${f}(\\mathcal {N})$. For example, suppose $\\mathcal {N}$ is a news articles dataset, then the function that outputs compound score of articles in $\\mathcal {N}$ plus noise from the standard normal distribution is a mechanism [7]. \n Question: How does the differential privacy mechanism work?",
            "output": [
                "A mechanism ${f}$ is a random function that takes a dataset $\\mathcal {N}$ as input, and outputs a random variable ${f}(\\mathcal {N})$."
            ]
        },
        {
            "id": "task460-f458b117032341df8f906a3034c7e2de",
            "input": "In this section, we consider 3 categories of data augmentation techniques that are effectively applicable to video datasets. To further increase training data size and diversity, we can create new audios via superimposing each original audio with additional noisy audios in time domain. To obtain diverse noisy audios, we use AudioSet, which consists of 632 audio event classes and a collection of over 2 million manually-annotated 10-second sound clips from YouTube videos BIBREF28. We consider applying the frequency and time masking techniques – which are shown to greatly improve the performance of end-to-end ASR models BIBREF23 – to our hybrid systems. Similarly, they can be applied online during each epoch of LF-MMI training, without the need for realignment.\n\nConsider each utterance (i.e. after the audio segmentation in Section SECREF5), and we compute its log mel spectrogram with $\\nu $ dimension and $\\tau $ time steps:\n\nFrequency masking is applied $m_F$ times, and each time the frequency bands $[f_0$, $f_0+ f)$ are masked, where $f$ is sampled from $[0, F]$ and $f_0$ is sampled from $[0, \\nu - f)$.\n\nTime masking is optionally applied $m_T$ times, and each time the time steps $[t_0$, $t_0+ t)$ are masked, where $t$ is sampled from $[0, T]$ and $t_0$ is sampled from $[0, \\tau - t)$. Data augmentation ::: Speed and volume perturbation\nBoth speed and volume perturbation emulate mean shifts in spectrum BIBREF18, BIBREF19. To perform speed perturbation of the training data, we produce three versions of each audio with speed factors $0.9$, $1.0$, and $1.1$. The training data size is thus tripled. For volume perturbation, each audio is scaled with a random variable drawn from a uniform distribution $[0.125, 2]$. \n Question: What are the best within-language data augmentation methods?",
            "output": [
                "Frequency masking Time masking Additive noise Speed and volume perturbation"
            ]
        },
        {
            "id": "task460-0fae96a0bbad40c2b98ba4739ee1d145",
            "input": "The train and test data have divided into 70-30 ratio and we got these results as shown in Table TABREF17 for the individual dataset and the combination of both. The pre-trained network was already trained and we used the target data Queensland flood which provided 96% accuracy with 0.118 Test loss in only 11 seconds provided we used only 70% of training labeled data. The second target data is Alberta flood with the same configuration of train-test split which provided 95% accuracy with 0.118 Test loss in just 19 seconds. \n Question: What were the model's results on flood detection?",
            "output": [
                "Queensland flood which provided 96% accuracy Alberta flood with the same configuration of train-test split which provided 95% accuracy"
            ]
        },
        {
            "id": "task460-c1f951474ac843b8acfc53e556ad61d7",
            "input": "Creating human-labeled training datasets for machine learning often looks like content analysis, a well-established methodology in the humanities and the social sciences (particularly literature, communication studies, and linguistics), which also has versions used in the life, ecological, and medical sciences. Content analysis has taken many forms over the past century, from more positivist methods that formally establish structural ways of evaluating content to more interpretivist methods that embrace ambiguity and multiple interpretations, such as grounded theory BIBREF16. Today, structured content analysis (also called “closed coding”) is used to turn qualitative or unstructured data of all kinds into structured and/or quantitative data, including media texts, free-form survey responses, interview transcripts, and video recordings. Projects usually involve teams of “coders” (also called “annotators”, “labelers”, or “reviewers”), with human labor required to “code”, “annotate”, or “label” a corpus of items. \n Question: In what sense is data annotation similar to structured content analysis? ",
            "output": [
                "structured content analysis (also called “closed coding”) is used to turn qualitative or unstructured data of all kinds into structured and/or quantitative data Projects usually involve teams of “coders” (also called “annotators”, “labelers”, or “reviewers”), with human labor required to “code”, “annotate”, or “label” a corpus of items."
            ]
        },
        {
            "id": "task460-c44b07246556469bb9a06361215a43cb",
            "input": "We present LOG-Cad, a neural network-based description generator (Figure FIGREF1 ) to directly solve this task. Given a word with its context, our generator takes advantage of the target word's embedding, pre-trained from massive text (global contexts), while also encoding the given local context, combining both to generate a natural language description. \n Question: Do they use pretrained word embeddings?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-794b6d72a0c24a04bb1785fc5b493f68",
            "input": "In Figure FIGREF32 , we visualize the syntactic distance estimated by the Parsing Network, while reading three different sequences from the PTB test set. We observe that the syntactic distance tends to be higher between the last character of a word and a space, which is a reasonable breakpoint to separate between words.  The model autonomously discovered to avoid inter-word attention connection, and use the hidden states of space (separator) tokens to summarize previous information. This is strong proof that the model can understand the latent structure of data. \n Question: How do they show their model discovers underlying syntactic structure?",
            "output": [
                "By visualizing syntactic distance estimated by the parsing network"
            ]
        },
        {
            "id": "task460-70d118d50c0e4247ada0d7818a543163",
            "input": "Specifically, we take the CORD-19 dataset BIBREF2, which contains over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses. We develop sentence classification methods to identify all sentences narrating radiological findings from COVID-19.  \n Question: What is the CORD-19 dataset?",
            "output": [
                "which contains over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses"
            ]
        },
        {
            "id": "task460-8bb6096c40a042118b0e1419a91ee8ac",
            "input": "Following BIBREF3, BLEU scores and the slot error rate (ERR) are reported. BLEU score evaluates how natural the generated utterance is compared with human readers. ERR measures the exact matching of the slot tokens in the candidate utterances. $\\text{ERR}=(p+q)/M$, where $M$ is the total number of slots in the dialog act, and $p$, $q$ is the number of missing and redundant slots in the given realisation. For each dialog act, we generate five utterances and select the top one with the lowest ERR as the final output. \n Question: What automatic metrics are used to measure performance of the system?",
            "output": [
                "BLEU scores and the slot error rate (ERR)"
            ]
        },
        {
            "id": "task460-5cb60ded6542404aaff4d4345fd163fc",
            "input": "We propose a listening comprehension model for the task defined above, the Attention-based Multi-hop Recurrent Neural Network (AMRNN) framework, and show that this model is able to perform reasonably well for the task. In the proposed approach, the audio of the stories is first transcribed into text by ASR, and the proposed model is developed to process the transcriptions for selecting the correct answer out of 4 choices given the question.  \n Question: What approach does this work propose for the new task?",
            "output": [
                "We propose a listening comprehension model for the task defined above, the Attention-based Multi-hop Recurrent Neural Network (AMRNN) framework, and show that this model is able to perform reasonably well for the task. In the proposed approach, the audio of the stories is first transcribed into text by ASR, and the proposed model is developed to process the transcriptions for selecting the correct answer out of 4 choices given the question. "
            ]
        },
        {
            "id": "task460-d20117f3ee884e92a257d4a373d5ba3b",
            "input": "Maximum matching (MM) is one of the most popular fundamental and structural segmentation algorithms for word segmentation BIBREF19 .  However, it does not solve the problem of ambiguous words and unknown words that do not exist in the dictionary. \n Question: What are the limitations of existing Vietnamese word segmentation systems?",
            "output": [
                " ambiguous words unknown words"
            ]
        },
        {
            "id": "task460-addc2f40a04648c4a2476a24ea410a74",
            "input": "We formalize this intuition in a model called NextSum, which selects the next summary sentence based not only on properties of the source text, but also on the previously selected sentences in the summary. \n Question: How does nextsum work?",
            "output": [
                "selects the next summary sentence based not only on properties of the source text, but also on the previously selected sentences in the summary"
            ]
        },
        {
            "id": "task460-1e90ca18229a4f10974ba8dc5b9693c6",
            "input": "We include ten subreddits in the five domains of abuse, social, anxiety, PTSD, and financial, as detailed in tab:data-spread, and our analysis focuses on the domain level.  \n Question: What categories does the dataset come from?",
            "output": [
                "abuse, social, anxiety, PTSD, and financial"
            ]
        },
        {
            "id": "task460-ed0df55feed44f0981ef2280a56fca52",
            "input": "We construct three datasets based on IMDB reviews and Yelp reviews. The IMDB dataset is binarised and split into a training and test set, each with 25K reviews (2K reviews from the training set are reserved for development). For Yelp, we binarise the ratings, and create 2 datasets, where we keep only reviews with $\\le $ 50 tokens (yelp50) and $\\le $200 tokens (yelp200). \n Question: What datasets do they use?",
            "output": [
                "three datasets based on IMDB reviews and Yelp reviews"
            ]
        },
        {
            "id": "task460-b95a5eda7d724c809cdda5bff415957c",
            "input": "French and Russian, and Arabic can be regarded as high resource languages whereas Hindi has far less data and can be considered as low resource. \n Question: Is the system tested on low-resource languages?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-33ed825622fa407299540772ee3fccb7",
            "input": "Since the ground-truth reference captions in ShapeWorld are randomly sampled, we take the sampled captions accompanying the test images as a proxy for optimal caption diversity, and compare it with the empirical output diversity of the evaluated model on these test images. Practically, we look at language constructions used and compute the corresponding diversity score as the ratio of observed number versus optimal number \n Question: How is diversity measured?",
            "output": [
                "diversity score as the ratio of observed number versus optimal number"
            ]
        },
        {
            "id": "task460-102107ca608a42a28106509c7a1fd477",
            "input": "In this case, the decoding function is a linear projection, which is $= f_{\\text{de}}()=+ $ , where $\\in ^{d_\\times d_}$ is a trainable weight matrix and $\\in ^{d_\\times 1}$ is the bias term. A family of bijective transformation was designed in NICE BIBREF17 , and the simplest continuous bijective function $f:^D\\rightarrow ^D$ and its inverse $f^{-1}$ is defined as:\n\n$$h: \\hspace{14.22636pt} _1 &= _1, & _2 &= _2+m(_1) \\nonumber \\\\ h^{-1}: \\hspace{14.22636pt} _1 &= _1, & _2 &= _2-m(_1) \\nonumber $$ (Eq. 15)\n\nwhere $_1$ is a $d$ -dimensional partition of the input $\\in ^D$ , and $m:^d\\rightarrow ^{D-d}$ is an arbitrary continuous function, which could be a trainable multi-layer feedforward neural network with non-linear activation functions. It is named as an `additive coupling layer' BIBREF17 , which has unit Jacobian determinant. To allow the learning system to explore more powerful transformation, we follow the design of the `affine coupling layer' BIBREF24 :\n\n$$h: \\hspace{5.69046pt} _1 &= _1, & _2 &= _2 \\odot \\text{exp}(s(_1)) + t(_1) \\nonumber \\\\ h^{-1}: \\hspace{5.69046pt} _1 &= _1, & _2 &= (_2-t(_1)) \\odot \\text{exp}(-s(_1)) \\nonumber $$ (Eq. 16)\n\nwhere $s:^d\\rightarrow ^{D-d}$ and $t:^d\\rightarrow ^{D-d}$ are both neural networks with linear output units.\n\nThe requirement of the continuous bijective transformation is that, the dimensionality of the input $$ and the output $$ need to match exactly. \n Question: What are the two decoding functions?",
            "output": [
                "a linear projection and a bijective function with continuous transformation though  ‘affine coupling layer’ of (Dinh et al.,2016). "
            ]
        },
        {
            "id": "task460-f5dba8ee3e0841718f3efaf9ea4551d7",
            "input": "The English online magazine of ISIS was named Dabiq and first appeared on the dark web on July 2014 and continued publishing for 15 issues. This publication was followed by Rumiyah which produced 13 English language issues through September 2017. Looking through both Dabiq and Rumiyah, many issues of the magazines contain articles specifically addressing women, usually with “ to our sisters ” incorporated into the title. \n Question: Do they report results only on English data?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-e067c55229754057a114763fd9cfe718",
            "input": "One of the most popular metrics for QG, BLEU BIBREF21 provides a set of measures to compare automatically generated texts against one or more references. In particular, BLEU-N is based on the count of overlapping n-grams between the candidate and its corresponding reference(s). Therefore, we adopt Self-BLEU, originally proposed by BIBREF33, as a measure of diversity for the generated text sequences.  Therefore, given a question-context pair as input to a QA model, two type of metrics can be computed:\n\nn-gram based score: measuring the average overlap between the retrieved answer and the ground truth.\n\nprobability score: the confidence of the QA model for its retrieved answer; this corresponds to the probability of being the correct answer assigned by the QA model to the retrieved answer. \n Question: What automated metrics authors investigate?",
            "output": [
                "BLEU Self-BLEU n-gram based score probability score"
            ]
        },
        {
            "id": "task460-4aea4ffd9673445eb420d67cfa8cbc3c",
            "input": "oss analysis. To conduct a more thorough evaluation, we further investigate model behaviours in terms of both reconstruc \n Question: How do they evaluate generated text quality?",
            "output": [
                "Loss analysis. To conduct a more thorough evaluation, we further investigate model behaviours in terms of both reconstruction loss and KL loss, as shown in Figure FIGREF14. These plots were obtained based on the E2E training set using the inputless setting."
            ]
        },
        {
            "id": "task460-01b64cb47c384dcc83acac5aed0036c7",
            "input": "We trained and evaluated our algorithm on the Microsoft COCO (MS-COCO) 2014 Captions dataset BIBREF21 . We report results on the Karpathy validation and test splits BIBREF8 , which are commonly used in other image captioning publications. The dataset contains 113K training images with 5 human annotated captions for each image. The Karpathy test and validation sets contain 5K images each. We evaluate our models using the CIDEr-D BIBREF22 , SPICE BIBREF23 , BLEU BIBREF24 , METEOR BIBREF25 , and ROUGE-L BIBREF26 metrics. While it has been shown experimentally that BLEU and ROUGE have lower correlation with human judgments than the other metrics BIBREF23 , BIBREF22 , the common practice in the image captioning literature is to report all the mentioned metrics. \n Question: What are the common captioning metrics?",
            "output": [
                "the CIDEr-D BIBREF22 , SPICE BIBREF23 , BLEU BIBREF24 , METEOR BIBREF25 , and ROUGE-L BIBREF26 metrics"
            ]
        },
        {
            "id": "task460-6488c736e7b94735b38e6b09c3aded85",
            "input": "This model consists of two identical RNN encoders with no shared parameters, as well as a standard RNN decoder. For each target sentence, two versions of the source sentence are used: the sequential (standard) version and the linearized parse (lexicalized or unlexicalized). \n Question: What kind of encoders are used for the parsed source sentence?",
            "output": [
                "RNN encoders"
            ]
        },
        {
            "id": "task460-3cd58662d4644dbfaf341ce43e296297",
            "input": "The focus of this publication is on representing information that is valuable for these tasks but that hitherto has largely been ignored in machine learning approaches centering around simplified language, specifically, text structure (e.g., paragraphs, lines), typography (e.g., font type, font style), and image (content, position, and dimensions) information. Information on physical page segmentation (for PDFs only), paragraph segmentation, and line segmentation was added as part of a textspan element in the textstructure layer \n Question: Which information about text structure is included in the corpus?",
            "output": [
                "paragraphs lines Information on physical page segmentation (for PDFs only), paragraph segmentation, and line segmentation"
            ]
        },
        {
            "id": "task460-a872655dd30d4297a0481cbb7fe785e1",
            "input": " As BIBREF0 discussed, the text-only UMT is fundamentally an ill-posed problem, since there are potentially many ways to associate target with source sentences. Intuitively, since the visual content and language are closely related, the image can play the role of a pivot “language\" to bridge the two languages without paralleled corpus, making the problem “more well-defined\" by reducing the problem to supervised learning. \n Question: Why is this work different from text-only UNMT?",
            "output": [
                "the image can play the role of a pivot “language\" to bridge the two languages without paralleled corpus"
            ]
        },
        {
            "id": "task460-01bd1584cf5d4dd18da63e270a1042a8",
            "input": "For a fair comparison, we trained each model on the same corpus of 10 million sentences gathered from Wikipedia. \n Question: On which dataset(s) do they compute their word embeddings?",
            "output": [
                "10 million sentences gathered from Wikipedia"
            ]
        },
        {
            "id": "task460-d729ed3a9fc04a3c948a5f63129fdc38",
            "input": "Our main results are presented in Table TABREF6 on the test set of the regulatory filings and in Table TABREF7 on the test set of the property lease agreements; F$_1$, precision, and recall are computed in the manner described above. \n Question: What evaluation metric were used for presenting results? ",
            "output": [
                "F$_1$, precision, and recall"
            ]
        },
        {
            "id": "task460-d87d5557e4f14a53aeda0d32db3fd436",
            "input": "In this work, we build a novel regression model, based on linguistic, content, behavioral and topic features to detect Arabic Twitter bots to understand the impact of bots in spreading religious hatred in Arabic Twitter space.  \n Question: Do they propose a new model to better detect Arabic bots specifically?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-b2bad5ffc30c472fbb3df7a9519c7b86",
            "input": "For the input, we build a parallel monolingual corpus by translating the mixed language sequence using Google NMT to English ( INLINEFORM0 ) and Mandarin ( INLINEFORM1 ) sequences. \n Question: What parallel corpus did they use?",
            "output": [
                "Parallel monolingual corpus in English and Mandarin"
            ]
        },
        {
            "id": "task460-246f987408754125b9f9be459ffa4a49",
            "input": "For instance, a message can be regarded as harmless on its own, but when taking previous threads into account it may be seen as abusive, and vice versa. This aspect makes detecting abusive language extremely laborious even for human annotators; therefore it is difficult to build a large and reliable dataset BIBREF10 . \n Question: What examples of the difficulties presented by the context-dependent nature of online aggression do they authors give?",
            "output": [
                "detecting abusive language extremely laborious it is difficult to build a large and reliable dataset"
            ]
        },
        {
            "id": "task460-22d8e717f8ae45c18ac18189acd7ee31",
            "input": "We compare the above model to a similar model, where rather than explicitly represent $K$ features as input, we have $K$ features in the form of a genre embedding, i.e. we learn a genre specific embedding for each of the gothic, scifi, and philosophy genres, as studied in BIBREF8 and BIBREF7. \n Question: Is this style generator compared to some baseline?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-4ff539ecf1b4406d82735c70e54b72c9",
            "input": "Causal documents were chosen to contain one occurrence only of the exact unigrams: `caused', `causing', or `causes'. \n Question: How do they extract causality from text?",
            "output": [
                "They identify documents that contain the unigrams 'caused', 'causing', or 'causes'"
            ]
        },
        {
            "id": "task460-442914c7a3f34de4962c3f6042a850ea",
            "input": "We adapted BERTNLU from ConvLab-2.  We implemented a rule-based model (RuleDST) and adapted TRADE (Transferable Dialogue State Generator) BIBREF19 in this experiment.  We adapted a vanilla policy trained in a supervised fashion from ConvLab-2 (SL policy).  \n Question: What are the benchmark models?",
            "output": [
                "BERTNLU from ConvLab-2 a rule-based model (RuleDST)  TRADE (Transferable Dialogue State Generator)  a vanilla policy trained in a supervised fashion from ConvLab-2 (SL policy)"
            ]
        },
        {
            "id": "task460-73c2ba16b41b48ffbfe3f5b62d752480",
            "input": "We use a new dataset of GD statements from 1970 to 2016, the UN General Debate Corpus (UNGDC), to examine the international development agenda in the UN BIBREF3 .  \n Question: Is the dataset multilingual?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-ab3d5637d30e48aa84d3c8995facc682",
            "input": "For the datasets, we use Karpathy and Fei-Fei's split for MS-COCO dataset BIBREF10 . \n Question: What dataset/corpus is this work evaluated over?",
            "output": [
                "Karpathy and Fei-Fei's split for MS-COCO dataset BIBREF10"
            ]
        },
        {
            "id": "task460-61ec4c96f4764900a5647d5b384d7867",
            "input": "Method ::: Passage Ranking Model\nThe key component of our framework is the Ranker model, which is provided with a question $q$ and $K$ passages $\\mathcal {P} = \\lbrace p_1, p_2 ... p_K\\rbrace $ from a pool of candidates, and outputs a chain of selected passages. Method ::: Cooperative Reasoner\nTo alleviate the noise in the distant supervision signal $\\mathcal {C}$, in addition to the conditional selection, we further propose a cooperative Reasoner model, also implemented with the MatchLSTM architecture (see Appendix SECREF6), to predict the linking entity from the selected passages. \n Question: What are two models' architectures in proposed solution?",
            "output": [
                "Reasoner model, also implemented with the MatchLSTM architecture Ranker model"
            ]
        },
        {
            "id": "task460-72d3592fda674203947356c3ae9f2492",
            "input": "We chose Conditional Copy (CC) model as our baseline, which is the best model in Wiseman.  \n Question: What is the strong baseline?",
            "output": [
                "Conditional Copy (CC) model "
            ]
        },
        {
            "id": "task460-a386ea7a49fb4c4985968325cba1b8c1",
            "input": "Hence, in this paper, we propose a cascaded multimodal STT with two components: (i) an English ASR system trained on the How2 dataset and (ii) a transformer-based BIBREF0 visually grounded MMT system. \n Question: What dataset was used in this work?",
            "output": [
                "How2"
            ]
        },
        {
            "id": "task460-294f8ef599e14522b04228b805d86cc7",
            "input": "Among commercial NLP toolkits (e.g., BIBREF14, BIBREF15, BIBREF16), we selected BIBREF17 and BIBREF18 for our experiments, which, to the best of our knowledge, are the only publicly accessible commercial APIs for the task of entity-level sentiment analysis that is agnostic to the text domain. We also report results of TensiStrength BIBREF13, TwitterNLP BIBREF6, BIBREF19, CogComp-NLP BIBREF20, and Stanford NLP NER BIBREF21. \n Question: Which toolkits do they use?",
            "output": [
                "BIBREF17 BIBREF18 TensiStrength BIBREF13 TwitterNLP BIBREF6 BIBREF19 CogComp-NLP BIBREF20 Stanford NLP NER BIBREF21"
            ]
        },
        {
            "id": "task460-80699f8689dc4b65b903c1633567a782",
            "input": "Suppose there are INLINEFORM0 mentions in documents on average, among these global models, NCEL not surprisingly has the lowest time complexity INLINEFORM1 since it only considers adjacent mentions, where INLINEFORM2 is the number of sub-GCN layers indicating the iterations until convergence. \n Question: Do they only use adjacent entity mentions or use more than that in some cases (next to adjacent)?",
            "output": [
                "NCEL considers only adjacent mentions."
            ]
        },
        {
            "id": "task460-855434a1ac42463e83e1f1266e7d5262",
            "input": "We describe our rules for WikiSQL here. Our rule for KBQA is simple without using a curated mapping dictionary. The pipeline of rules in SequentialQA is similar to that of WikiSQL. \n Question: Are the rules dataset specific?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-33f9ba7156a2457f8d3d7ad11093acbc",
            "input": "a) Parallel Scan Inference b) Vectorized Parsing c) Semiring Matrix Operations Torch-Struct aims for computational and memory efficiency. Implemented naively, dynamic programming algorithms in Python are prohibitively slow. As such Torch-Struct provides key primitives to help batch and vectorize these algorithms to take advantage of GPU computation and to minimize the overhead of backpropagating through chart-based dynamic programmming. \n Question: What general-purpose optimizations are included?",
            "output": [
                "Parallel Scan Inference Vectorized Parsing Semiring Matrix Operations"
            ]
        },
        {
            "id": "task460-f8f1f521d06049ceb9821aaca5cf07c4",
            "input": "We focus on social media posts from the website Twitter, which are an excellent testing ground for character based models due to the noisy nature of text. Heavy use of slang and abundant misspellings means that there are many orthographically and semantically similar tokens, and special characters such as emojis are also immensely popular and carry useful semantic information. In our moderately sized training dataset of 2 million tweets, there were about 0.92 million unique word types. \n Question: Does the paper clearly establish that the challenges listed here exist in this dataset and task?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-41c40444f7714401913b41b23456e559",
            "input": "Single-Turn: This dataset consists of single-turn instances of statements and responses from the MiM chatbot developed at Constellation AI BIBREF5. Multi-Turn: This dataset is taken from the ConvAI2 challenge and consists of various types of dialogue that have been generated by human-computer conversations. \n Question: what datasets did they use?",
            "output": [
                "Single-Turn Multi-Turn"
            ]
        },
        {
            "id": "task460-0e8ea7a5a31344c69dea93c08b556a0f",
            "input": "We analyze which discourse phenomena are hard to capture using monolingual data only. Among the four phenomena in the test sets we use (deixis, lexical cohesion, VP ellipsis and ellipsis which affects NP inflection) we find VP ellipsis to be the hardest phenomenon to be captured using round-trip translations. \n Question: what phenomena do they mention is hard to capture?",
            "output": [
                "Four discourse phenomena - deixis, lexical cohesion, VP ellipsis, and ellipsis which affects NP inflection."
            ]
        },
        {
            "id": "task460-89ea33bdb22042d9b82ca2ef4580100b",
            "input": "The experiment settings from this paper and evaluation protocol for the Mboshi corpus (Boundary F-scores using the ZRC speech reference) are the same from BIBREF8. Table presents the results for bilingual UWS and multilingual leveraging. For the former, we reach our best result by using as aligned information the French, the original aligned language for this dataset. Languages closely related to French (Spanish and Portuguese) ranked better, while our worst result used German. English also performs notably well in our experiments. We believe this is due to the statistics features of the resulting text. We observe in Table that the English portion of the dataset contains the smallest vocabulary among all languages. Since we train our systems in very low-resource settings, vocabulary-related features can impact greatly the system's capacity to language-model, and consequently the final quality of the produced alignments. Even in high-resource settings, it was already attested that some languages are more difficult to model than others BIBREF9. \n Question: Is the model evaluated against any baseline?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-f4afded9df0949b2860565b8fa37d0e9",
            "input": "DrQA is a CRC baseline coming with the CoQA dataset. Note that this implementation of DrQA is different from DrQA for SQuAD BIBREF8 in that it is modified to support answering no answer questions by having a special token unknown at the end of the document.  DrQA+CoQA is the above baseline pre-tuned on CoQA dataset and then fine-tuned on $(\\text{RC})_2$ .  BERT is the vanilla BERT model directly fine-tuned on $(\\text{RC})_2$ . We use this baseline for ablation study on the effectiveness of pre-tuning. BERT+review first tunes BERT on domain reviews using the same objectives as BERT pre-training and then fine-tunes on $(\\text{RC})_2$ . We use this baseline to show that a simple domain-adaptation of BERT is not good. BERT+CoQA first fine-tunes BERT on the supervised CoQA data and then fine-tunes on $(\\text{RC})_2$ . We use this baseline to show that pre-tuning is very competitive even compared with models trained from large-scale supervised data. \n Question: What is the baseline model used?",
            "output": [
                "The baseline models used are DrQA modified to support answering no answer questions, DrQA+CoQA which is pre-tuned on CoQA dataset, vanilla BERT, BERT+review tuned on domain reviews, BERT+CoQA tuned on the supervised CoQA data"
            ]
        },
        {
            "id": "task460-84bb964983be4016b3ff8bfaf2f27941",
            "input": "We will compare to the more recent cross-lingual language model XLM BIBREF12, as well as the state-of-the-art CoNLL 2018 shared task results with predicted tokenisation and segmentation in an updated version of the paper. In French, no extensive work has been done due to the limited availability of NER corpora. We compare our model with the strong baselines settled by BIBREF49, who trained both CRF and BiLSTM-CRF architectures on the FTB and enhanced them using heuristics and pretrained word embeddings. In the TRANSLATE-TRAIN setting, we report the best scores from previous literature along with ours. BiLSTM-max is the best model in the original XNLI paper, mBERT which has been reported in French in BIBREF52 and XLM (MLM+TLM) is the best-presented model from BIBREF50. \n Question: What is the state of the art?",
            "output": [
                "POS and DP task: CONLL 2018\nNER task: (no extensive work) Strong baselines CRF and BiLSTM-CRF\nNLI task: mBERT or XLM (not clear from text)"
            ]
        },
        {
            "id": "task460-680fc1186b12450aa81be773739b4605",
            "input": "Using the LDA model, each person in the dataset is with a topic probability vector $X_i$ . Assume $x_{ik}\\in X_{i}$ denotes the likelihood that the $\\emph {i}^{th}$ tweet account favors $\\emph {k}^{th}$ topic in the dataset. Our topic based features can be calculated as below. Global Outlier Standard Score measures the degree that a user's tweet content is related to a certain topic compared to the other users. Local Outlier Standard Score measures the degree of interest someone shows to a certain topic by considering his own homepage content only. Three baseline classification methods: Support Vector Machines (SVM), Adaboost, and Random Forests are adopted to evaluate our extracted features. \n Question: How do they detect spammers?",
            "output": [
                "Extract features from the LDA model and use them in a binary classification task"
            ]
        },
        {
            "id": "task460-af8586f903fb4282bea97ac161a08e27",
            "input": "Since labels are unknown on INLINEFORM1 , EGL computes the expectation of the gradient norm over all possible labelings. BIBREF3 interprets EGL as “expected model change”. In the following section, we formalize the intuition for EGL and show that it follows naturally from reducing the variance of an estimator. Eq. ( EQREF7 ) indicates that to reduce INLINEFORM0 on test data, we need to minimize the expected variance INLINEFORM1 over the test set. A practical issue is that we do not know INLINEFORM0 in advance. We could instead substitute an estimate INLINEFORM1 from a pre-trained model, where it is reasonable to assume the INLINEFORM2 to be close to the true INLINEFORM3 . The batch selection then works by taking the samples that have largest gradient norms, DISPLAYFORM0\n\nFor RNNs, the gradients for each potential label can be obtained by back-propagation. Another practical issue is that EGL marginalizes over all possible labelings, but in speech recognition, the number of labelings scales exponentially in the number of timesteps. Therefore, we only marginalize over the INLINEFORM0 most probable labelings. They are obtained by beam search decoding, as in BIBREF7 . The EGL method in BIBREF3 is almost the same as Eq. ( EQREF8 ), except the gradient's norm is not squared in BIBREF3 . \n Question: How do they calculate variance from the model outputs?",
            "output": [
                "reducing the variance of an estimator  EGL method in BIBREF3 is almost the same as Eq. ( EQREF8 ), except the gradient's norm is not squared in BIBREF3"
            ]
        },
        {
            "id": "task460-220dee4c45234a42ad3293c20ad8acdd",
            "input": "Our AV in-cabin data-set includes 30 hours of multimodal data collected from 30 passengers (15 female, 15 male) in 20 rides/sessions. 10 types of passenger intents are identified and annotated as: Set/Change Destination, Set/Change Route (including turn-by-turn instructions), Go Faster, Go Slower, Stop, Park, Pull Over, Drop Off, Open Door, and Other (turn music/radio on/off, open/close window/trunk, change AC/temp, show map, etc.). Relevant slots are identified and annotated as: Location, Position/Direction, Object, Time-Guidance, Person, Gesture/Gaze (this, that, over there, etc.), and None. In addition to utterance-level intent types and their slots, word-level intent keywords are annotated as Intent as well.  \n Question: What intents does the paper explore?",
            "output": [
                "Set/Change Destination Set/Change Route Go Faster Go Slower Stop Park Pull Over Drop Off Open Door Other "
            ]
        },
        {
            "id": "task460-f22537301a174accadd3575199aa16ef",
            "input": "Throughout this article, we use two different embedding spaces. The first is the widely used representation built on GoogleNews BIBREF8 . The second is taken from BIBREF2 , and was trained on a Reddit dataset BIBREF9 . \n Question: Which embeddings do they detect biases in?",
            "output": [
                "Word embeddings trained on GoogleNews and Word embeddings trained on Reddit dataset"
            ]
        },
        {
            "id": "task460-3f24b1d6e61a48309479d5f741ffbb32",
            "input": "Our analysis found that gang members have a penchant for using just a small set of emoji symbols that convey their anger and violent behavior through their tweets. Figure FIGREF24 illustrates the emoji distribution for the top 20 most frequent emojis used by gang member profiles in our dataset. The fuel pump emoji was the most frequently used emoji by the gang members, which is often used in the context of selling or consuming marijuana. The pistol emoji is the second most frequent in our dataset, which is often used with the guardsman emoji or the police cop emoji in an `emoji chain'. Figure FIGREF28 presents some prototypical `chaining' of emojis used by gang members. The chains may reflect their anger at law enforcement officers, as a cop emoji is often followed up with the emoji of a weapon, bomb, or explosion. We found that 32.25% of gang members in our dataset have chained together the police and the pistol emoji, compared to just 1.14% of non-gang members. Moreover, only 1.71% of non-gang members have used the hundred points emoji and pistol emoji together in tweets while 53% of gang members have used them. A variety of the angry face emoji such as devil face emoji and imp emoji were also common in gang member tweets. \n Question: What are the differences in the use of emojis between gang member and the rest of the Twitter population?",
            "output": [
                "32.25% of gang members in our dataset have chained together the police and the pistol emoji, compared to just 1.14% of non-gang members only 1.71% of non-gang members have used the hundred points emoji and pistol emoji together in tweets while 53% of gang members have used them gang members have a penchant for using just a small set of emoji symbols that convey their anger and violent behavior"
            ]
        },
        {
            "id": "task460-7148615e739b421692737e20a6a49896",
            "input": "The analysis of results from Tables TABREF17 , TABREF18 and TABREF19 show that 12 of 15 best results were obtained using new word embeddings. The evaluation results presented in Table TABREF20 (the chosen best embeddings models from Table TABREF19 ) prove that the best group of word embeddings is EC. The highest type F1-score was obtained for EC1 model, built using binary FastText Skip-gram method utilising subword information, with vector dimension equal to 300 and negative sampling equal to 10. The ability of the model to provide vector representation for the unknown words seems to be the most important. \n Question: What conclusions are drawn from these experiments?",
            "output": [
                "best results were obtained using new word embeddings best group of word embeddings is EC The highest type F1-score was obtained for EC1 model, built using binary FastText Skip-gram method utilising subword information ability of the model to provide vector representation for the unknown words seems to be the most important"
            ]
        },
        {
            "id": "task460-f8b6589fda48459988517a86fa2e1ea7",
            "input": "For evaluating the proposed model on behavior related data, we employ the Couples Therapy Corpus (CoupTher) BIBREF21 and Cancer Couples Interaction Dataset (Cancer) BIBREF22. These are the targeted conditions under which a behavior-gated language model can offer improved performance. We utilize the Couple's Therapy Corpus as an in-domain experimental corpus since our behavior classification model is also trained on the same.  \n Question: On which dataset is model trained?",
            "output": [
                "Couples Therapy Corpus (CoupTher) BIBREF21"
            ]
        },
        {
            "id": "task460-eb2eab1bc1494156b050630ed40f0942",
            "input": "We perform experiments (§ SECREF5 ) on two English-Japanese translation corpora to evaluate the method's utility in improving translation accuracy and reducing the time required for training. \n Question: What language pairs did they experiment with?",
            "output": [
                "English-Japanese"
            ]
        },
        {
            "id": "task460-621ce4883dd44b99bfbd8e7469992be8",
            "input": "This study focuses on Switchboard-300, a standard 300-hour English conversational speech recognition task. \n Question: How big is Switchboard-300 database?",
            "output": [
                "300-hour English conversational speech"
            ]
        },
        {
            "id": "task460-bf1a3873f18941cb91167a65fec91c85",
            "input": "We use the same baseline as used by lang2011unsupervised which has been shown to be difficult to outperform. This baseline assigns a semantic role to a constituent based on its syntactic function, i.e. the dependency relation to its head. \n Question: What baseline model is used?",
            "output": [
                "same baseline as used by lang2011unsupervised"
            ]
        },
        {
            "id": "task460-700ccd43c9194245ad771c6f31845f74",
            "input": "In recent years, there has been a rapid growth in the usage of social media. People post their day-to-day happenings on regular basis. BIBREF0 propose four tasks for detecting drug names, classifying medication intake, classifying adverse drug reaction and detecting vaccination behavior from tweets. We participated in the Task2 and Task4. \n Question: Was the system only evaluated over the second shared task?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-b761c45a9f0e46f5a4584e28d6e7e9db",
            "input": "We use the following subset of GLUE tasks BIBREF4 for fine-tuning:\n\nMRPC: the Microsoft Research Paraphrase Corpus BIBREF13\n\nSTS-B: the Semantic Textual Similarity Benchmark BIBREF14\n\nSST-2: the Stanford Sentiment Treebank, two-way classification BIBREF15\n\nQQP: the Quora Question Pairs dataset\n\nRTE: the Recognizing Textual Entailment datasets\n\nQNLI: Question-answering NLI based on the Stanford Question Answering Dataset BIBREF3\n\nMNLI: the Multi-Genre Natural Language Inference Corpus, matched section BIBREF16 \n Question: What subset of GLUE tasks is used?",
            "output": [
                "MRPC STS-B SST-2 QQP RTE QNLI MNLI"
            ]
        },
        {
            "id": "task460-7768e32c3c0f48b18455e2c0c837a2ac",
            "input": "We define the Effective Word Score of score x as\n\nEFWS(x) = N(+x) - N(-x),\n\nwhere N(x) is the number of words in the tweet with polarity score x. \n Question: How is effective word score calculated?",
            "output": [
                "We define the Effective Word Score of score x as\n\nEFWS(x) = N(+x) - N(-x),\n\nwhere N(x) is the number of words in the tweet with polarity score x."
            ]
        },
        {
            "id": "task460-71d70fd9fa8b45e58c90a930290d86cc",
            "input": "The SemEval-2010 benchmark dataset BIBREF0 is composed of 244 scientific articles collected from the ACM Digital Library (conference and workshop papers). \n Question: how many articles are in the dataset?",
            "output": [
                "244"
            ]
        },
        {
            "id": "task460-ad48dea5996d4757b2878df72d42c18a",
            "input": "Each testing instance was paired with 99 randomly sampled negative instances. Each recommendation model ranks the 100 instances according to its predicted results. The ranked list is judged by Hit Ratio (HR) BIBREF49 and Normalized Discount Cumulative Gain (NDCG) BIBREF50 at the position 10. HR@10 is a recall-based metric, measuring the percentage of the testing item being correctly recommended in the top-10 position. NDCG@10 is a ranked evaluation metric which considers the position of the correct hit in the ranked result. Overall, our AMRAN outperforms all baselines, achieving 0.657 HR@10 and 0.410 NDCG@10. It improves HR@10 by 5.3% and NDCG@10 by 3% over the best baseline (i.e., $NAIS_{concat}$). \n Question: What is the model accuracy?",
            "output": [
                "Overall, our AMRAN outperforms all baselines, achieving 0.657 HR@10 and 0.410 NDCG@10."
            ]
        },
        {
            "id": "task460-f095c431bf9a4c35876175a1315c7605",
            "input": "For reliable multi-hop reasoning using OpenIE tuples, we can add inter-tuple connections to the support graph search, controlled by a small number of rules over the OpenIE predicates. Learning such rules for the Science domain is an open problem and potential avenue of future work. \n Question: Is their method capable of multi-hop reasoning?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-eca378557f7d4804a43e60cf18466d91",
            "input": "After adversarial modifications, the performance of the original target models (those without the “-adv” suffix) drops dramatically (e.g. the overall accuracy of BERT on Quora drops from 94.6% to 24.1%), revealing that the target models are vulnerable to our adversarial examples. \n Question: How much dramatically results drop for models on generated adversarial examples?",
            "output": [
                "BERT on Quora drops from 94.6% to 24.1%"
            ]
        },
        {
            "id": "task460-25ad2fab024b4d64aff75fc6fb4864c0",
            "input": "Both beat the non-attention baseline by a significant margin. For the attention baseline, we use the standard parametrized attention BIBREF2 . \n Question: Which baseline methods are used?",
            "output": [
                "standard parametrized attention and a non-attention baseline"
            ]
        },
        {
            "id": "task460-ae5d124f74244606b4b74757807de83b",
            "input": "Therefore, we just implement a simple one-layer RNN with LSTM cell to classify pre-processed sentences into ironic sentences and non-ironic sentences because LSTM networks are widely used in irony detection. \n Question: How did the authors find ironic data on twitter?",
            "output": [
                "They developed a classifier to find ironic sentences in twitter data"
            ]
        },
        {
            "id": "task460-1dcf31b5e6d94370af91bab5b34caa5a",
            "input": "We address this tension by training vector space models to represent the data, in which each unique word in a large corpus is represented by a vector (embedding) in high-dimensional space. The geometry of the resulting vector space captures many semantic relations between words.  We address this tension by training vector space models to represent the data, in which each unique word in a large corpus is represented by a vector (embedding) in high-dimensional space. The geometry of the resulting vector space captures many semantic relations between words.  \n Question: Do they model semantics ",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-c8255b9da2384e868852d5ba76a84695",
            "input": "Table TABREF34 shows the inference speed of our method when implementing the sequnece modeling layer with the LSTM-based, CNN-based, and Transformer-based architecture, respectively.  From the table, we can see that our method has a much faster inference speed than Lattice-LSTM when using the LSTM-based sequence modeling layer, and it was also much faster than LR-CNN, which used an CNN architecture to implement the sequence modeling layer. And as expected, our method with the CNN-based sequence modeling layer showed some advantage in inference speed than those with the LSTM-based and Transformer-based sequence model layer. \n Question:  What percentage of improvement in inference speed is obtained by the proposed method over the newest state-of-the-art methods?",
            "output": [
                "Across 4 datasets, the best performing proposed model (CNN) achieved an average of 363% improvement over the state of the art method (LR-CNN)"
            ]
        },
        {
            "id": "task460-af211caf89b24fcea77dbc12eb99b595",
            "input": " This baseline has an F1 of 41.59% and accuracy of 71.22%. \n Question: what are their evaluation metrics?",
            "output": [
                "F1 accuracy"
            ]
        },
        {
            "id": "task460-8c22ec32737f43288a99ab55db45d2bd",
            "input": "The vector gate is inspired by BIBREF11 and BIBREF12 , but is different to the former in that the gating mechanism acts upon each dimension of the word and character-level vectors, and different to the latter in that it does not rely on external sources of information for calculating the gating mechanism. \n Question: Where do they employ feature-wise sigmoid gating?",
            "output": [
                "gating mechanism acts upon each dimension of the word and character-level vectors"
            ]
        },
        {
            "id": "task460-77126d5c98ff488c8ade6df43d402519",
            "input": "The Software Ontology (SWO) BIBREF5 is included because its set of CQs is of substantial size and it was part of Ren et al.'s set of analysed CQs. The CQ sets of Dem@Care BIBREF8 and OntoDT BIBREF9 were included because they were available. CQs for the Stuff BIBREF6 and African Wildlife (AWO) BIBREF7 ontologies were added to the set, because the ontologies were developed by one of the authors (therewith facilitating in-depth domain analysis, if needed), they cover other topics, and are of a different `type' (a tutorial ontology (AWO) and a core ontology (Stuff)), thus contributing to maximising diversity in source selection. \n Question: How many domains of ontologies do they gather data from?",
            "output": [
                "5 domains: software, stuff, african wildlife, healthcare, datatypes"
            ]
        },
        {
            "id": "task460-2a93797013db47e3a83cbf7b68099e2e",
            "input": "The competition is divided into five subtasks which involve standard classification, ordinal classification and distributional estimation. For a more detailed description see BIBREF0 . \n Question: What were the five English subtasks?",
            "output": [
                " five subtasks which involve standard classification, ordinal classification and distributional estimation. For a more detailed description see BIBREF0"
            ]
        },
        {
            "id": "task460-8625c6be50034e9681baad15a1550471",
            "input": "When we include the LM embeddings in our system overall performance increases from 90.87% to 91.93% INLINEFORM0 for the CoNLL 2003 NER task, a more then 1% absolute F1 increase, and a substantial improvement over the previous state of the art. We also establish a new state of the art result (96.37% INLINEFORM1 ) for the CoNLL 2000 Chunking task. \n Question: what results do they achieve?",
            "output": [
                "91.93% F1 score on CoNLL 2003 NER task and 96.37% F1 score on CoNLL 2000 Chunking task"
            ]
        },
        {
            "id": "task460-83b5c8fcfe1949ffbca27ac0b4f843fb",
            "input": "We tried many configurations of our network models, but report results with only three configurations of Transformers.\n\nTransformer Type 1: This network is a small to medium-sized network consisting of 4 Transformer layers. Each layer utilizes 8 attention heads with a depth of 512 and a feed-forward depth of 1024.\n\nTransformer Type 2: The second model is small in size, using 2 Transformer layers. The layers utilize 8 attention heads with a depth of 256 and a feed-forward depth of 1024.\n\nTransformer Type 3: The third type of model is minimal, using only 1 Transformer layer. This network utilizes 8 attention heads with a depth of 256 and a feed-forward depth of 512. \n Question: What neural configurations are explored?",
            "output": [
                "tried many configurations of our network models, but report results with only three configurations Transformer Type 1 Transformer Type 2 Transformer Type 3"
            ]
        },
        {
            "id": "task460-47e0032ea2ed4dd982feadb0a92be0d9",
            "input": "For answer retrieval, a dataset is created by INLINEFORM4 , which gives INLINEFORM5 accuracy and INLINEFORM6 coverage, respectively. \n Question: Do they employ their indexing-based method to create a sample of a QA Wikipedia dataset?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-c893cf57ce47491b85d27376dfcc8d10",
            "input": "  In a second step, we will train a neural monolingual translation system, that translates from the output of the PBMT system INLINEFORM0 to a better target sentence INLINEFORM1 . \n Question: Do they train the NMT model on PBMT outputs?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-0099c40935d64e7eb1e3d9c64191ca47",
            "input": "We extract unique users and tweets from the combined result set to generate a dataset of about 60,000 unique tweets, pertaining to 51,104 unique users. \n Question: How many followers did they analyze?",
            "output": [
                "51,104"
            ]
        },
        {
            "id": "task460-0bf5e92fb74e4ab7b538fc9fd7358b4a",
            "input": "We need good amount of data to try deep learning state-of-the-art algorithms. There are lot of open datasets available for names, locations, organisations, but not for topics as defined in Abstract above. Also defining and inferring topics is an individual preference and there are no fix set of rules for its definition. But according to our definition, we can use wikipedia titles as our target topics. English wikipedia dataset has more than 18 million titles if we consider all versions of them till now. We had to clean up the titles to remove junk titles as wikipedia title almost contains all the words we use daily.  After doing some more cleaning we were left with 10 million titles. We have a dump of 15 million English news articles published in past 4 years. Further, we reduced number of articles by removing duplicate and near similar articles. We used our pre-trained doc2vec models and cosine similarity to detect almost similar news articles. \n Question: How large is the dataset they used?",
            "output": [
                "English wikipedia dataset has more than 18 million a dump of 15 million English news articles "
            ]
        },
        {
            "id": "task460-177e38f92dd04bb8b662e576e513b8e2",
            "input": "We have training and testing sets in three different languages: English, Chinese and Korean. When fine-tuning, we simply adopted the official training script of BERT, with default hyperparameters, to fine-tune each model until training loss converged. \n Question: what does the model learn in zero-shot setting?",
            "output": [
                "we simply adopted the official training script of BERT, with default hyperparameters, to fine-tune each model until training loss converged"
            ]
        },
        {
            "id": "task460-9b3e5cbc1fc4426eb778d1444d666e0b",
            "input": ". While automated evaluation metrics like ROUGE measure lexical similarity between machine and human summaries, humans can better measure how coherent and readable a summary is. Our evaluation study investigates whether tuning the PG-net model increases summary coherence, by asking evaluators to select which of three summaries for the same document they like most: the PG-net model trained on CNN/DM; the model trained on student reflections; and finally the model trained on CNN/DM and tuned on student reflections. 20 evaluators were recruited from our institution and asked to each perform 20 annotations. Summaries are presented to evaluators in random order. Evaluators are then asked to select the summary they feel to be most readable and coherent. \n Question: Who were the human evaluators used?",
            "output": [
                "20 evaluators were recruited from our institution and asked to each perform 20 annotations"
            ]
        },
        {
            "id": "task460-35ca76738cd1476680b1ca64ad2139be",
            "input": "As shown in Equation ( EQREF6 ), tone prediction sub-network ( INLINEFORM0 ) takes video and pinyin sequence as inputs and predict corresponding tone sequence. Video context vectors INLINEFORM0 and pinyin context vectors INLINEFORM1 are fused when predicting a tone character at each decoder step. The video encoder is the same as in Section SECREF7 and the pinyin encoder is: DISPLAYFORM0 The input video sequence is first fed into the VGG model BIBREF9 to extract visual feature. The output of conv5 of VGG is appended with global average pooling BIBREF10 to get the 512-dim feature vector. Then the 512-dim feature vector is fed into video encoder. \n Question: What visual information characterizes tones?",
            "output": [
                "video sequence is first fed into the VGG model BIBREF9 to extract visual feature"
            ]
        },
        {
            "id": "task460-d36b36c6aac8467b964d49c141f2fe6f",
            "input": "To test this hypothesis we first trained UG-WGAN in English, Chinese and German following the procedure described in Section \"UG-WGAN\" . For this experiment we trained UG-WGAN on the English and Russian language following the procedure described in Section \"UG-WGAN\" .  To do so we trained 6 UG-WGAN models on the following languages: English, Russian, Arabic, Chinese, German, Spanish, French. \n Question: What are the languages they consider in this paper?",
            "output": [
                "The languages considered were English, Chinese, German, Russian, Arabic, Spanish, French"
            ]
        },
        {
            "id": "task460-76f1b709c8b4464fa4fa1ace5ef0169b",
            "input": "We only used models of dimension 300 and, for Word2Vec, Wang2Vec, and FastText, skip-gram architectured models. \n Question: what word embedding techniques did they experiment with?",
            "output": [
                "Word2Vec, Wang2Vec, and FastText"
            ]
        },
        {
            "id": "task460-3e2e25f7ce194623ba6cdece660fb193",
            "input": "The baselines are: (a) trained on S-SQuAD, (b) trained on T-SQuAD and then fine-tuned on S-SQuAD, and (c) previous best model trained on S-SQuAD BIBREF5 by using Dr.QA BIBREF20 . The proposed approach (row (f)) outperforms previous best model (row (c)) by 2% EM score and over 1.5% F1 score. \n Question: What was the previous best model?",
            "output": [
                "(c) previous best model trained on S-SQuAD BIBREF5 by using Dr.QA BIBREF20 "
            ]
        },
        {
            "id": "task460-9c9d858cf456471b8fcff8978e38d439",
            "input": "We downloaded 76 videos from a tutorial website about an image editing program .  \n Question: What is the source of the triples?",
            "output": [
                "a tutorial website about an image editing program "
            ]
        },
        {
            "id": "task460-69d93d8855b34d8fbf88fcd776d854ab",
            "input": "Excluding the embedding weights, our model requires 100k parameters , while BIBREF8 requires 250k parameters. \n Question: how much was the parameter difference between their model and previous methods?",
            "output": [
                "our model requires 100k parameters , while BIBREF8 requires 250k parameters"
            ]
        },
        {
            "id": "task460-97091748a3584835b9e420e7723f8130",
            "input": "Input of the model is the concatenation of word embedding and another embedding indicating whether this word is predicate: $ \\mathbf {x}_t = [\\mathbf {W}_{\\text{emb}}(w_t), \\mathbf {W}_{\\text{mask}}(w_t = v)]. $ \n Question: What's the input representation of OpenIE tuples into the model?",
            "output": [
                "word embeddings"
            ]
        },
        {
            "id": "task460-ffb6ab40ed1e4bd5b44c9305419d732b",
            "input": "We use an off-the-shelf parser, in this case Stanford CoreNLP BIBREF11 , to create binary constituency parses. \n Question: How do they obtain parsed source sentences?",
            "output": [
                "Stanford CoreNLP BIBREF11 "
            ]
        },
        {
            "id": "task460-eb5c9268dc994b6f8b32e2938eb8e901",
            "input": "Most of the above-discussed systems either shows high performance on (a) Twitter dataset or (b) Facebook dataset (given in the TRAC-2018), but not on both English code-mixed datasets. This may be due to the text style or level of complexities of both datasets. \n Question: How have the differences in communication styles between Twitter and Facebook increase the complexity of the problem?",
            "output": [
                "Systems do not perform well both in Facebook and Twitter texts"
            ]
        },
        {
            "id": "task460-18798f256a7845fc8fe1f731acaea776",
            "input": "\n In this section, we describe our approach to classify user reactions into one of eight types of discourse: agreement, answer, appreciation, disagreement, elaboration, humor, negative reaction, or question, or as none of the given labels, which we call “other”, using linguistically-infused neural network models. \n Question: What are the nine types?",
            "output": [
                "agreement answer appreciation disagreement elaboration humor negative reaction question other"
            ]
        },
        {
            "id": "task460-b9738a1743cc458a81d6c7f7d839f474",
            "input": "We evaluate the generations with the ROUGE BIBREF29 and METEOR BIBREF30 metrics using the true sentences as targets. \n Question: What metrics are used for evaluation?",
            "output": [
                "ROUGE BIBREF29 and METEOR BIBREF30"
            ]
        },
        {
            "id": "task460-6b6e044d0dc2476baf73dfd51344abf5",
            "input": "More specifically, in our model we use the context of the word to predict its label and by doing so our model learns label-aware context for each word in the sentence. In order to improve the interactivity between the word representation and its context, we increase the mutual information between the word representations and its context. \n Question: How does their model utilize contextual information for each work in the given sentence in a multi-task setting? setting?",
            "output": [
                "we use the context of the word to predict its label and by doing so our model learns label-aware context for each word in the sentence"
            ]
        },
        {
            "id": "task460-d36cf0db7c07427d912d5817fe0e27e1",
            "input": "The straight line in figures FIGREF39, FIGREF43 and FIGREF51 is the result of a supervised LDA algorithm which is used as a baseline.  \n Question: Was performance of the weakly-supervised model compared to the performance of a supervised model?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-94349418f0a94bcc8bdc0d1c6549c70b",
            "input": "The corpus includes 2000 sentences.  \n Question: How long is the dataset?",
            "output": [
                "2000"
            ]
        },
        {
            "id": "task460-659571f97d304e0e85ec0f65d02679a6",
            "input": "A selection of results from this study are that:\n\nNon-English code is a large-scale phenomena.\n\nTransliteration is common in identifiers for all languages.\n\nLanguages clusters into three distinct groups based on how speakers use identifiers/comments/transliteration.\n\nNon-latin script users write comments in their L1 script but write identifiers in English.\n\nRight-to-left (RTL) language scripts, such as Arabic, have no observed prevalence on GitHub identifiers, implying that existing coders who speak RTL languages have substantial barriers in using their native script in code. \n Question: What are results of public code repository study?",
            "output": [
                "Non-English code is a large-scale phenomena. Transliteration is common in identifiers for all languages. Languages clusters into three distinct groups based on how speakers use identifiers/comments/transliteration. Non-latin script users write comments in their L1 script but write identifiers in English. Right-to-left (RTL) language scripts, such as Arabic, have no observed prevalence on GitHub identifiers"
            ]
        },
        {
            "id": "task460-d83d21ad1f3b40e59415c713a3ff1ba4",
            "input": "Word embeddings have risen in popularity for NLP applications due to the success of models designed specifically for the big data setting.  Note that “big” datasets are not always available, particularly in computational social science NLP applications, where the data of interest are often not obtained from large scale sources such as the internet and social media, but from sources such as press releases BIBREF11 , academic journals BIBREF10 , books BIBREF12 , and transcripts of recorded speech BIBREF13 , BIBREF14 , BIBREF15  have proposed a model-based method for training interpretable corpus-specific word embeddings for computational social science, using mixed membership representations, Metropolis-Hastings-Walker sampling, and NCE. Experimental results for prediction, supervised learning, and case studies on state of the Union addresses and NIPS articles, indicate that high-quality embeddings and topics can be obtained using the method. The results highlight the fact that big data is not always best, as domain-specific data can be very valuable, even when it is small. \n Question: Why is big data not appropriate for this task?",
            "output": [
                "Training embeddings from small-corpora can increase the performance of some tasks"
            ]
        },
        {
            "id": "task460-0e08c7a0b59d42cf93d1dc0dcd23fdba",
            "input": "Our dataset is annotated based on Chinese pathology reports provided by the Department of Gastrointestinal Surgery, Ruijin Hospital. It contains 17,833 sentences, 826,987 characters and 2,714 question-answer pairs. \n Question: How big is dataset of pathology reports collected from Ruijing Hospital?",
            "output": [
                "17,833 sentences, 826,987 characters and 2,714 question-answer pairs"
            ]
        },
        {
            "id": "task460-609dc1c72cfe46d898dd04cbe0651fec",
            "input": "We chose three different datasets for the experiments: SNLI, MultiNLI and SICK. The Stanford Natural Language Inference (SNLI) corpus BIBREF4 is a dataset of 570k human-written sentence pairs manually labeled with the labels entailment, contradiction, and neutral.  The Multi-Genre Natural Language Inference (MultiNLI) corpus BIBREF5 consisting of 433k human-written sentence pairs labeled with entailment, contradiction and neutral. SICK BIBREF6 is a dataset that was originally constructed to test compositional distributional semantics (DS) models.  \n Question: Which datasets were used?",
            "output": [
                "SNLI, MultiNLI and SICK"
            ]
        },
        {
            "id": "task460-3c12e5543679479784fb00a83d498ce5",
            "input": "Although MCDN doesn't obtain the highest precision, it increases F1-score by 10.2% and 3% compared with the existing best systems $LS \\cup KLD \\cup CONN$ and $KLD \\cup LS \\cup LS_{inter}$. \n Question: What performance did proposed method achieve, how much better is than previous state-of-the-art?",
            "output": [
                "increases F1-score by 10.2% and 3% compared with the existing best systems $LS \\cup KLD \\cup CONN$ and $KLD \\cup LS \\cup LS_{inter}$"
            ]
        },
        {
            "id": "task460-6219bf3f148744279d8403920be8b716",
            "input": "In particular, for the NLG task, our single model outperformed competing models in terms of both Rouge-L and Bleu-1. \n Question: How do they measure the quality of summaries?",
            "output": [
                "Rouge-L Bleu-1"
            ]
        },
        {
            "id": "task460-b05c020c340d4e5485017bc4412493ff",
            "input": "The stories from each scenario were distributed among four different annotators.  \n Question: How many subjects have been used to create the annotations?",
            "output": [
                " four different annotators"
            ]
        },
        {
            "id": "task460-51b56e1db09d42e29e8994fe45532be6",
            "input": "Recent attempts can be divided into two categories: (i) those which tries to incorporate additional information to further improve the performance of knowledge graph embedding, e.g., entity types or concepts BIBREF13, relations paths BIBREF17, textual descriptions BIBREF11, BIBREF12 and logical rules BIBREF23; (ii) those which tries to design more complicated strategies, e.g., deep neural network models BIBREF24. \n Question: What are recent works on knowedge graph embeddings authors mention?",
            "output": [
                "entity types or concepts BIBREF13 relations paths BIBREF17  textual descriptions BIBREF11, BIBREF12 logical rules BIBREF23 deep neural network models BIBREF24"
            ]
        },
        {
            "id": "task460-5aeb197202614503b3c529ed5e02fcde",
            "input": "First, we consider WeedsPrec BIBREF8 which captures the features of INLINEFORM0 which are included in the set of a broader term's features, INLINEFORM1 : DISPLAYFORM0\n\nSecond, we consider invCL BIBREF11 which introduces a notion of distributional exclusion by also measuring the degree to which the broader term contains contexts not used by the narrower term. Although most unsupervised distributional approaches are based on the DIH, we also consider the distributional SLQS model based on on an alternative informativeness hypothesis BIBREF10 , BIBREF4 . For completeness, we also include cosine similarity as a baseline in our evaluation. \n Question: Which distributional methods did they consider?",
            "output": [
                "WeedsPrec BIBREF8 invCL BIBREF11 SLQS model cosine similarity"
            ]
        },
        {
            "id": "task460-c622f8fd2c6243cd811338128aeb2b0f",
            "input": "We start the training process by applying the rule INLINEFORM4 to a set of natural language questions INLINEFORM5 . The resulting dataset is considered as the training data to initialize both the semantic parser and the question generator. Afterwards, both models are improved following the back-translation protocol that target sequences should follow the real data distribution, yet source sequences can be generated with noises. \n Question: How is the back-translation model trained?",
            "output": [
                " applying the rule INLINEFORM4 to a set of natural language questions INLINEFORM5 both models are improved following the back-translation protocol that target sequences should follow the real data distribution"
            ]
        },
        {
            "id": "task460-30d0d170972d4a8e8912beadf27208ed",
            "input": "Same as BIBREF5 , in the first sub-network, pinyin sequence is predicted from the video. Different from BIBREF5 , which predicts pinyin characters from video, pinyin is taken as a whole in CSSMCM, also known as syllables. As we know, Mandarin Chinese is a syllable-based language and syllables are their logical unit of pronunciation. \n Question: What syntactic structure is used to model tones?",
            "output": [
                "syllables"
            ]
        },
        {
            "id": "task460-79b27f71b2764741ae3669c763d30d5f",
            "input": "More than 2,100 texts were paired with 15 questions each, resulting in a total number of approx. 32,000 annotated questions. For 13% of the questions, the workers did not agree on one of the 4 categories with a 3 out of 5 majority, so we did not include these questions in our dataset.\n\nThe distribution of category labels on the remaining 87% is shown in Table TABREF10 . 14,074 (52%) questions could be answered. Out of the answerable questions, 10,160 could be answered from the text directly (text-based) and 3,914 questions required the use of commonsense knowledge (script-based). After removing 135 questions during the validation, the final dataset comprises 13,939 questions, 3,827 of which require commonsense knowledge (i.e. 27.4%). This ratio was manually verified based on a random sample of questions. \n Question: what dataset statistics are provided?",
            "output": [
                "More than 2,100 texts were paired with 15 questions each, resulting in a total number of approx. 32,000 annotated questions. 13% of the questions are not answerable.  Out of the answerable questions, 10,160 could be answered from the text directly (text-based) and 3,914 questions required the use of commonsense knowledge (script-based).  The final dataset comprises 13,939 questions, 3,827 of which require commonsense knowledge (i.e. 27.4%)."
            ]
        },
        {
            "id": "task460-3212f61f3e8c49d99b1f21ebcb1dc384",
            "input": "It was collected using crowdsourcing BIBREF4. The data collection was done using an Android application. \n Question: what is the source of the data?",
            "output": [
                "Android application"
            ]
        },
        {
            "id": "task460-198fd21a6b5e457db5ba3207e28936ec",
            "input": "To help the human representative quickly determine the cause of the escalation, we generate a visualization of the user's turns using the attention weights to highlight the turns influential in the escalation decision. \n Question: Do they explain model predictions solely on attention weights?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-07adf8230b7e4e688970740e3ceeef18",
            "input": "We mainly concern with the two following structures of the embedding space.\n\nSemantic similarity structure: Semantically similar entities are close to each other in the embedding space, and vice versa. Semantic direction structure: There exist semantic directions in the embedding space, by which only one semantic aspect changes while all other aspects stay the same. It can be identified by a vector difference, such as the subtraction between two embedding vectors. \n Question: What are the uncanny semantic structures of the embedding space?",
            "output": [
                "Semantic similarity structure Semantic direction structure"
            ]
        },
        {
            "id": "task460-7969b25aa28c4757a6750e1816f847a7",
            "input": "The type and correctness of all the question answer pairs are verified by at least two annotators. \n Question: What was the inter-annotator agreement?",
            "output": [
                "correctness of all the question answer pairs are verified by at least two annotators"
            ]
        },
        {
            "id": "task460-9f495a16aea7408c9dd2869d9b2cf0e1",
            "input": "Figure 1 shows a simple linear model with rank constraint. \n Question: What are their baseline methods?",
            "output": [
                "simple linear model with rank constraint Hierarchical softmax N-gram features"
            ]
        },
        {
            "id": "task460-e815146e4d324de9bee874beb8044217",
            "input": "We then evaluate the ranks of the target entities $t$ with the commonly used mean rank (MR), mean reciprocal rank (MRR), as well as Hits@1, Hits@3, and Hits@10. \n Question: what was the evaluation metrics studied in this work?",
            "output": [
                "mean rank (MR), mean reciprocal rank (MRR), as well as Hits@1, Hits@3, and Hits@10"
            ]
        },
        {
            "id": "task460-98f90629b56b46498b002d131b981ddc",
            "input": "In our experiments, the RNN takes in spectrograms of utterances, passing them through two 2D-convolutional layers, followed by seven bi-directional recurrent layers and a fully-connected layer with softmax activation. All recurrent layers are batch normalized. At each timestep, the softmax activations give a probability distribution over the characters. CTC loss BIBREF8 is then computed from the timestep-wise probabilities. \n Question: Which model do they use for end-to-end speech recognition?",
            "output": [
                "RNN"
            ]
        },
        {
            "id": "task460-6890274b649d4f708983674a3ad3d949",
            "input": "We obtained 9,892 stories of sexual harassment incidents that was reported on Safecity. Those stories include a text description, along with tags of the forms of harassment, e.g. commenting, ogling and groping. A dataset of these stories was published by Karlekar and Bansal karlekar2018safecity. In addition to the forms of harassment, we manually annotated each story with the key elements (i.e. “harasser\", “time\", “location\", “trigger\"), because they are essential to uncover the harassment patterns. An example is shown in Figure FIGREF3. Furthermore, we also assigned each story classification labels in five dimensions (Table TABREF4). The detailed definitions of classifications in all dimensions are explained below. \n Question: What is the size of the dataset?",
            "output": [
                " 9,892 stories of sexual harassment incidents"
            ]
        },
        {
            "id": "task460-3c30d0728b2c4302bc27535419c9dbbd",
            "input": "The reviews we used have been extracted from TripAdvisor and originally proposed in BIBREF10 , BIBREF11 .  \n Question: Where are the hotel reviews from?",
            "output": [
                "TripAdvisor"
            ]
        },
        {
            "id": "task460-d3056ff30ae8442e83dfad6728e3769c",
            "input": "SIGHAN Bakeoff defines two types of evaluation settings, closed test limits all the data for learning should not be beyond the given training set, while open test does not take this limitation BIBREF21. \n Question: What is meant by closed test setting?",
            "output": [
                "closed test limits all the data for learning should not be beyond the given training set, while open test does not take this limitation"
            ]
        },
        {
            "id": "task460-5c46b7be9ff4459ca484bf8679623b06",
            "input": "To quantitatively evaluate the performance of our bidirectional recurrent based approach, we adopt METEOR metric because of its robust performance. \n Question: what metrics were used for evaluation?",
            "output": [
                "METEOR"
            ]
        },
        {
            "id": "task460-1b107ad92d4b419fbd2c511f12f2cb07",
            "input": "To test whether topic models can be used for dating poetry or attributing authorship, we perform supervised classification experiments with Random Forest Ensemble classifiers.  \n Question: What is the algorithm used for the classification tasks?",
            "output": [
                "Random Forest Ensemble classifiers"
            ]
        },
        {
            "id": "task460-71dd65a4242947d8a12352e5a0f659fc",
            "input": "Experimental Protocol ::: Datasets ::: Training Dataset\n(i) TTS System dataset: We trained our TTS system with a mixture of neutral and newscaster style speech. For a total of 24 hours of training data, split in 20 hours of neutral (22000 utterances) and 4 hours of newscaster styled speech (3000 utterances).\n\n(ii) Embedding selection dataset: As the evaluation was carried out only on the newscaster speaking style, we restrict our linguistic search space to the utterances associated to the newscaster style: 3000 sentences.\n\nExperimental Protocol ::: Datasets ::: Evaluation Dataset\nThe systems were evaluated on two datasets:\n\n(i) Common Prosody Errors (CPE): The dataset on which the baseline Prostron model fails to generate appropriate prosody. This dataset consists of complex utterances like compound nouns (22%), “or\" questions (9%), “wh\" questions (18%). This set is further enhanced by sourcing complex utterances (51%) from BIBREF24.\n\n(ii) LFR: As demonstrated in BIBREF25, evaluating sentences in isolation does not suffice if we want to evaluate the quality of long-form speech. Thus, for evaluations on LFR we curated a dataset of news samples. The news style sentences were concatenated into full news stories, to capture the overall experience of our intended use case. \n Question: What dataset is used for train/test of this method?",
            "output": [
                "Training datasets: TTS System dataset and embedding selection dataset. Evaluation datasets: Common Prosody Errors dataset and LFR dataset."
            ]
        },
        {
            "id": "task460-c182f474a5ae41338afe00c64b6430f7",
            "input": "We now seek to know if a pre-trained multi-BERT has ability to solve RC tasks in the zero-shot setting. \n Question: What model is used as a baseline?  ",
            "output": [
                "pre-trained multi-BERT"
            ]
        },
        {
            "id": "task460-d424f623cb9f48cab783cbc931daa015",
            "input": "Wizard interface: the interface shown to participants with the Wizard role provides possible actions on the right-hand side of the browser window. These actions could be verbal, such as sending a message, or non-verbal, such as switching on/off a button to activate a robot.  Dialogue structure: we introduced structured dialogues through a Finite State Machine (FSM) that controls the current dialogue state and offers multiple suitable and relevant state transitions (actions) to the Wizard depending on the point in the interaction, the state of the world and the history. A graph of dialogue states, transitions and utterances is loaded when the system is initialised, and each chat room has its own dialogue state, which changes through actions. System-changing actions: actions trigger transitions between the states in the FSM. We differentiate two types of actions:\n\nVerbal actions, such as the dialogue options available at that moment. The Wizard can select one of several predefined messages to send, or type their own message if needed. Free text messages do not change the dialogue state in the FSM, so it is important to minimise their use by providing enough dialogue options to the Wizard. Predefined messages can also trigger other associated events such as pop-ups or follow-up non-verbal actions.\n\nNon-verbal actions, such as commands to trigger events. These can take any form, but we used buttons to control robots in our data collection.\n\nSubmitting an action would change the dialogue state in the FSM, altering the set of actions available in the subsequent turn visible to the Wizard. Some dialogue options are only possible at certain states, in a similar way as to how non-verbal actions are enabled or disabled depending on the state.  \n Question: How does framework made sure that dialogue will not breach procedures?",
            "output": [
                "The Wizard can select one of several predefined messages to send, or type their own message if needed. Free text messages do not change the dialogue state in the FSM, so it is important to minimise their use by providing enough dialogue options to the Wizard. Predefined messages can also trigger other associated events such as pop-ups or follow-up non-verbal actions."
            ]
        },
        {
            "id": "task460-cd3228cda29e47548ebeb71762722520",
            "input": " In particular, the best-performing systems at SemEval-2015 and SemEval-2016 used deep convolutional networks BIBREF53 , BIBREF54  \n Question: What is the current SOTA for sentiment analysis on Twitter at the time of writing?",
            "output": [
                "deep convolutional networks BIBREF53 , BIBREF54"
            ]
        },
        {
            "id": "task460-fa6615e251d14fe6af13f17c78fdee83",
            "input": "We use a graph to model the social media context, relating tweets to one another, authors to tweets and other authors. Figure FIGREF7 shows the graph, composed of three types of nodes: tweets (T), users (U) and the “world” (W). Edges are created between nodes and weighted as follows: T-T the unigram cosine similarity between tweets, T-U weighted 100 between a tweet and its author, U-U weighted 1 between two users in a “follows” relationship and U-W weighted 0.001 to ensure a connected graph for the mad algorithm. \n Question: What information is contained in the social graph of tweet authors?",
            "output": [
                " the graph, composed of three types of nodes: tweets (T), users (U) and the “world” (W). Edges are created between nodes and weighted as follows: T-T the unigram cosine similarity between tweets, T-U weighted 100 between a tweet and its author, U-U weighted 1 between two users in a “follows” relationship and U-W weighted 0.001 to ensure a connected graph for the mad algorithm."
            ]
        },
        {
            "id": "task460-2dc5c339fd6e4dbcb8bc3281c6147a8f",
            "input": "By choosing English (En) as the pivot language, we perform pivot alignments for identical English segments on Europarl Fr-En and En-De parallel corpora BIBREF18 , constructing a multi-parallel corpus of Fr-En-De. Then each of the Fr*-De and Fr-De* pseudo parallel corpora is established from the multi-parallel data by applying the pivot language-based translation described in the previous section. For automatic translation, we utilize a pre-trained and publicly released NMT model for En $\\rightarrow $ De and train another NMT model for En $\\rightarrow $ Fr using the WMT'15 En-Fr parallel corpus BIBREF19 . \n Question: Where do they collect the synthetic data?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-615301a686db4bed910536a9f1364b90",
            "input": "We achieve significant gains over our strong baseline by leveraging decompositions from our best decomposition model, trained with USeq2Seq on FastText pseudo-decompositions; we find a 3.1 F1 gain on the original dev set, 11 F1 gain on the multi-hop dev set, and 10 F1 gain on the out-of-domain dev set.  \n Question: How large is the improvement over the baseline?",
            "output": [
                "3.1 F1 gain on the original dev set 11 F1 gain on the multi-hop dev set 10 F1 gain on the out-of-domain dev set."
            ]
        },
        {
            "id": "task460-8d14121408fa4185865f00310d78871c",
            "input": "Contextualized word embeddings, sentence embeddings, such as deep contextualized word representations BIBREF20 , BERT BIBREF22 , encode the complex characteristics and meanings of words in various context by jointly training a bidirectional language model.  The second method is to use sentence embeddings, BERT. It is used to a generate single 786-dimensional sentence embedding from 10k-dimensional one-hot vector or distribution over previous words and then merge into a single context vector with two different merging methods. Using this approach, we can obtain a more dense, informative, fixed-length vectors to encode conversational-context information, $e^k_{context}$ to be used in next $k$ -th utterance prediction. We use contextual gating mechanism in our decoder network to combine the conversational-context embeddings with speech and word embeddings effectively. Our gating is contextual in the sense that multiple embeddings compute a gate value that is dependent on the context of multiple utterances that occur in a conversation.  Let $e_w = e_w(y_{u-1})$ be our previous word embedding for a word $y_{u-1}$ , and let $e_s = e_s(x^k_{1:T})$ be a speech embedding for the acoustic features of current $k$ -th utterance $x^k_{1:T}$ and $e_c = e_c(s_{k-1-n:k-1})$ be our conversational-context embedding for $n$ -number of preceding utterances ${s_{k-1-n:k-1}}$ . Then using a gating mechanism:\n\n$$g = \\sigma (e_c, e_w, e_s)$$ (Eq. 15)\n\nwhere $\\sigma $ is a 1 hidden layer DNN with $\\texttt {sigmoid}$ activation, the gated embedding $e$ is calcuated as\n\n$$e = g \\odot (e_c, e_w, e_s) \\\\ h = \\text{LSTM}(e)$$ (Eq. 16)\n\nand fed into the LSTM decoder hidden layer.  The output of the decoder $h$ is then combined with conversational-context embedding $e_c$ again with a gating mechanism,\n\n$$g = \\sigma (e_C, h) \\\\ \\hat{h} = g \\odot (e_c, h)$$ (Eq. 17)\n\nThen the next hidden layer takes these gated activations, $\\hat{h}$ , and so on. \n Question: How are sentence embeddings incorporated into the speech recognition system?",
            "output": [
                "BERT generates sentence embeddings that represent words in context. These sentence embeddings are merged into a single conversational-context vector that is used to calculate a gated embedding and is later combined with the output of the decoder h to provide the gated activations for the next hidden layer."
            ]
        },
        {
            "id": "task460-3c52ce64e818414f9e369123241ca190",
            "input": "Motivated by phrase based SMT, we retrieve neighbors which have high local, sub-sentence level overlap with the source sentence. We adapt our approach to retrieve n-grams instead of sentences. We represent every sentence by their reduced n-gram set. For every n-gram in INLINEFORM0 , we find the closest n-gram in the training set using the IDF similarity defined above. \n Question: Where do they retrieve neighbor n-grams from in their approach?",
            "output": [
                "represent every sentence by their reduced n-gram set"
            ]
        },
        {
            "id": "task460-dc0375c2addb44b68136cf7db94ca8e9",
            "input": "We used a dataset provided by Shortir, an Indonesian news aggregator and summarizer company. The dataset contains roughly 20K news articles. Each article has the title, category, source (e.g., CNN Indonesia, Kumparan), URL to the original article, and an abstractive summary which was created manually by a total of 2 native speakers of Indonesian \n Question: Did they use a crowdsourcing platform for the summaries?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-32362a4b45454d8b95f73454b6571a80",
            "input": "Using our methodology, we tagged 10,000 Arabic tweet dataset for offensiveness, where offensive tweets account for roughly 19% of the tweets. Further, we labeled tweets as vulgar or hate speech.  \n Question: How many tweets are in the dataset?",
            "output": [
                "10,000 Arabic tweet dataset "
            ]
        },
        {
            "id": "task460-d8b253a8e98a477c9f700d51c8bf4794",
            "input": ". Grammatically correct text is needed as the starting point for inserting artificial errors, and we used two different sources: 1) the corrected version of the same FCE training set on which the system is trained (450K tokens), and 2) example sentences extracted from the English Vocabulary Profile (270K tokens). \n Question: Which languages are explored in this paper?",
            "output": [
                "English "
            ]
        },
        {
            "id": "task460-ed4751e6d02c4c66a3bec255629281a3",
            "input": "In this paper, we first highlight the importance of TV and radio broadcast as a source of data for ASR, and the potential impact it can have. We then perform a statistical analysis of gender representation in a data set composed of four state-of-the-art corpora of French broadcast, widely used within the speech community. Finally we question the impact of such a representation on the systems developed on this data, through the perspective of an ASR system. \n Question: What tasks did they use to evaluate performance for male and female speakers?",
            "output": [
                "ASR"
            ]
        },
        {
            "id": "task460-c204f32621104186ba7ee6c581e3784a",
            "input": "Our experiments are performed on an actual endangered language, Mboshi (Bantu C25), a language spoken in Congo-Brazzaville, using the bilingual French-Mboshi 5K corpus of BIBREF17. On the Mboshi side, we consider alphabetic representation with no tonal information. On the French side,we simply consider the default segmentation into words. \n Question: What is the dataset used in the paper?",
            "output": [
                "French-Mboshi 5K corpus"
            ]
        },
        {
            "id": "task460-61ae590082454f22a0ffbc16355df42c",
            "input": "We identified some limitations during the process, which we describe in this section.\n\nWhen deciding publisher partisanship, the number of people from whom we computed the score was small. For example, de Stentor is estimated to reach 275K readers each day on its official website. Deciding the audience leaning from 55 samples was subject to sampling bias. Besides, the scores differ very little between publishers. None of the publishers had an absolute score higher than 1, meaning that even the most partisan publisher was only slightly partisan. Deciding which publishers we consider as partisan and which not is thus not very reliable.\n\nThe article-level annotation task was not as well-defined as on a crowdsourcing platform. We included the questions as part of an existing survey and didn't want to create much burden to the annotators. Therefore, we did not provide long descriptive text that explained how a person should annotate an article. We thus run under the risk of annotator bias. This is one of the reasons for a low inter-rater agreement. \n Question: What limitations are mentioned?",
            "output": [
                "deciding publisher partisanship, risk annotator bias because of short description text provided to annotators"
            ]
        },
        {
            "id": "task460-8071785c04024bc5a77bf9025a1fad37",
            "input": "We used the Universal Dependencies Treebank UD v2.1 BIBREF0 for our experiments. We picked four low-resource/high-resource language pairs, each from a different family: Danish/Swedish (da/sv), Russian/Bulgarian (ru/bg), Finnish/Hungarian (fi/hu), Spanish/Portuguese (es/pt). Picking languages from different families would ensure that we obtain results that are on average consistent across languages. \n Question: What languages are explored?",
            "output": [
                "Danish/Swedish (da/sv), Russian/Bulgarian (ru/bg), Finnish/Hungarian (fi/hu), Spanish/Portuguese (es/pt)"
            ]
        },
        {
            "id": "task460-ca387573e8d1401ca25853a3597b3636",
            "input": "All corpora provide datasets/splits for answer selection, whereas only (WikiQA, SQuAD) and (WikiQA, SelQA) provide datasets for answer extraction and answer triggering, respectively. SQuAD is much larger in size although questions in this corpus are often paraphrased multiple times. On the contrary, SQuAD's average candidates per question ( INLINEFORM0 ) is the smallest because SQuAD extracts answer candidates from paragraphs whereas the others extract them from sections or infoboxes that consist of bigger contexts. Although InfoboxQA is larger than WikiQA or SelQA, the number of token types ( INLINEFORM1 ) in InfoboxQA is smaller than those two, due to the repetitive nature of infoboxes.\n\nAll corpora show similar average answer candidate lengths ( INLINEFORM0 ), except for InfoboxQA where each line in the infobox is considered a candidate. SelQA and SQuAD show similar average question lengths ( INLINEFORM1 ) because of the similarity between their annotation schemes. It is not surprising that WikiQA's average question length is the smallest, considering their questions are taken from search queries. InfoboxQA's average question length is relatively small, due to the restricted information that can be asked from the infoboxes. InfoboxQA and WikiQA show the least question-answer word overlaps over questions and answers ( INLINEFORM2 and INLINEFORM3 in Table TABREF2 ), respectively. In terms of the F1-score for overlapping words ( INLINEFORM4 ), SQuAD gives the least portion of overlaps between question-answer pairs although WikiQA comes very close. \n Question: How do they analyze contextual similaries across datasets?",
            "output": [
                "They compare the tasks that the datasets are suitable for, average number of answer candidates per question, number of token types, average answer candidate lengths, average question lengths, question-answer word overlap."
            ]
        },
        {
            "id": "task460-ac55a35e14444774b61c8a4e47c85ada",
            "input": "27,534 messages were left after filtering. This is the data set that is used for experimentation. \n Question: What is the size of the labelled dataset?",
            "output": [
                "27,534 messages "
            ]
        },
        {
            "id": "task460-f6d710f1c6154e81a3dcd7d731097d34",
            "input": "In this work, we propose to augment the integer linear programming (ILP)-based summarization framework with a low-rank approximation of the co-occurrence matrix, and further evaluate the approach on a broad range of datasets exhibiting high lexical diversity. \n Question: What do they constrain using integer linear programming?",
            "output": [
                "low-rank approximation of the co-occurrence matrix"
            ]
        },
        {
            "id": "task460-337e439de041476197808b8d970e2a32",
            "input": "Note that the BoW/SVM model being a linear predictor relying directly on word frequency statistics, it lacks expressive power in comparison to the CNN model which additionally learns intermediate hidden layer representations and convolutional filters. Moreover the CNN model can take advantage of the semantic similarity encoded in the distributed word2vec representations, while for the BoW/SVM model all words are “equidistant” in the bag-of-words semantic space \n Question: According to the authors, why does the CNN model exhibit a higher level of explainability?",
            "output": [
                "CNN model which additionally learns intermediate hidden layer representations and convolutional filters. Moreover the CNN model can take advantage of the semantic similarity encoded in the distributed word2vec representations"
            ]
        },
        {
            "id": "task460-76de2fbe9b9a4a00b285983c837d2d44",
            "input": "BIBREF5 adopt a machine learning approach for NER. Their NER system extracts medical problems, tests and treatments from discharge summaries and progress notes. The dataset used is the i2b2 2010 challenge dataset.  \n Question: Does the paper explore extraction from electronic health records?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-f6a34e72765a4b61ba4af2848920ae7f",
            "input": "More specifically, we observe the impact of: (i) pre-trained word embeddings BIBREF11, BIBREF12, recurrent BIBREF13 and transformer-based sentence encoders BIBREF14 as question representation strategies; (ii) distinct convolutional neural networks used for visual feature extraction BIBREF15, BIBREF16, BIBREF17; and (iii) standard fusion strategies, as well as the importance of two main attention mechanisms BIBREF18, BIBREF19. \n Question: What type of experiments are performed?",
            "output": [
                "pre-trained word embeddings BIBREF11, BIBREF12 recurrent BIBREF13 transformer-based sentence encoders BIBREF14 distinct convolutional neural networks standard fusion strategies  two main attention mechanisms BIBREF18, BIBREF19"
            ]
        },
        {
            "id": "task460-6518732fa76b496eaaac6972c9ffcf01",
            "input": "The GENIA Corpus BIBREF3 contains biomedical abstracts from the PubMed database. We use GENIA technical term annotations 3.02, which cover linguistic expressions to entities of interest in molecular biology, e.g. proteins, genes and cells. CoNLL2003 BIBREF14 is a standard NER dataset based on the Reuters RCV-1 news corpus. It covers named entities of type person, location, organization and misc.\n\nFor testing the overall annotation performance, we utilize CoNLL2003-testA and a 50 document split from GENIA.  \n Question: what standard dataset were used?",
            "output": [
                "The GENIA Corpus  CoNLL2003"
            ]
        },
        {
            "id": "task460-628f9cf1376041868df2f388d2dc704f",
            "input": "We focus here on the Europarl domain, for which we have ample data in several languages, and use as in-domain training data the Europarl corpus BIBREF5 for two translation directions: English INLINEFORM0 German and English INLINEFORM1 French. \n Question: what language pairs are explored?",
            "output": [
                "English-German, English-French."
            ]
        },
        {
            "id": "task460-ac9cb4831ada42c7bc71c39be997bbc7",
            "input": "Surprisingly, using the best QA model (bert-large-wwm) does not lead to the best correlations with human judgments. On CNN/DM, bert-large-wwm slightly underperforms bert-base and bert-large. \n Question: What models are evaluated with QAGS?",
            "output": [
                "bert-large-wwm bert-base bert-large"
            ]
        },
        {
            "id": "task460-6b5fc202c7214211b9b9e8a7583f1a3c",
            "input": "We used dataset, HEOT obtained from one of the past studies done by Mathur et al. where they annotated a set of cleaned tweets obtained from twitter for the conversations happening in Indian subcontinent. A labelled dataset for a corresponding english tweets were also obtained from a study conducted by Davidson et al. \n Question: How is the dataset collected?",
            "output": [
                "A labelled dataset for a corresponding english tweets were also obtained from a study conducted by Davidson et al HEOT obtained from one of the past studies done by Mathur et al"
            ]
        },
        {
            "id": "task460-de52c26197a54125acc2dce57a5d9c47",
            "input": "Routines were developed to simulate input data based on the authors experience with real healthcare data. The reasons for this choice were twofold: One, healthcare data can be high in incidental complexity, requiring one-off code to handle unusual inputs, but not necessarily in such a way as to significantly alter the fundamental engineering choices in a semantic enrichment engine such as this one. Two, healthcare data is strictly regulated, and the process for obtaining access to healthcare data for research can be cumbersome and time-consuming.\n\nA simplified set of input data, in a variety of different formats that occur frequently in a healthcare setting, was used for simulation. \n Question: What type of simulations of real-time data feeds are used for validaton?",
            "output": [
                "simplified set of input data, in a variety of different formats that occur frequently in a healthcare setting"
            ]
        },
        {
            "id": "task460-75aebd6e57f34c01885d21b73d5af395",
            "input": "Our neural network model mainly consists of three parallel LSTM BIBREF21 layers. \n Question: What architecture has the neural network?",
            "output": [
                "three parallel LSTM BIBREF21 layers"
            ]
        },
        {
            "id": "task460-a3cd247f777a4403ab5c7aff3b1d20d9",
            "input": "For estimating the sense of the word INLINEFORM0 in a sentence, we search for such a synset INLINEFORM1 that maximizes the cosine similarity to the sentence vector: DISPLAYFORM0 \n Question: What measure of semantic similarity is used?",
            "output": [
                "cosine similarity"
            ]
        },
        {
            "id": "task460-7fa401ad1eb84fc6b2474b82ca98d95b",
            "input": "Since BERT has already achieved the state-of-the-art performance of question-answering, in this section we compare our proposed model with state-of-the-art question answering models (i.e. QANet BIBREF39) and BERT-Base BIBREF26. As BERT has two versions: BERT-Base and BERT-Large, due to the lack of computational resource, we can only compare with BERT-Base model instead of BERT-Large. \n Question: What are strong baseline models in specific tasks?",
            "output": [
                "state-of-the-art question answering models (i.e. QANet BIBREF39) and BERT-Base BIBREF26"
            ]
        },
        {
            "id": "task460-a3b3b039d1da4017ad57b6182148764a",
            "input": "As we mention in the Section 2, CAEVO is the current state-of-the-art system for feature-based temporal event relation extraction BIBREF10 . It's widely used as the baseline for evaluating TB-Dense data. We adopt it as our baseline for evaluating CaTeRS and RED datasets. \n Question: What were the traditional linguistic feature-based models?",
            "output": [
                "CAEVO"
            ]
        },
        {
            "id": "task460-c1dbf90e17f94d79a240b0eb70e9baa3",
            "input": "This corpus has multiple versions, and we choose the following two versions as their test dataset has significantly larger number of instances of multiple relation tuples with overlapping entities. (i) The first version is used by BIBREF6 (BIBREF6) (mentioned as NYT in their paper) and has 24 relations. We name this version as NYT24. (ii) The second version is used by BIBREF11 (BIBREF11) (mentioned as NYT10 in their paper) and has 29 relations. \n Question: Are there datasets with relation tuples annotated, how big are datasets available?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-6a20323313974853ae4d6e357c3ef574",
            "input": "Furthermore, we do not restrict ourselves to a test set of sequences of fixed lengths during testing. Rather, we exhaustively enumerate all the sequences in a language by their lengths and then go through the sequences in the test set one by one until our network errs $k$ times, thereby providing a more fine-grained evaluation criterion of its generalization capabilities. To study the effect of various length distributions on the learning capability and speed of LSTM models, we experimented with four discrete probability distributions supported on bounded intervals (Figure 2 ) to sample the lengths of sequences for the languages.  \n Question: Are the unobserved samples from the same distribution as the training data?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-35cdf05ea5a44aeabf63d24d644e92cc",
            "input": "We evaluate the performance of our proposed method using two criteria: i) rank-correlation BIBREF25 to evaluate visual grounding and ii) accuracy to evaluate question answering. Intuitively, rank-correlation measures the similarity between human and model attention maps under a rank-based metric. A high rank-correlation means that the model is `looking at' image areas that agree to the visual information used by a human to answer the same question. In terms of accuracy of a predicted answer INLINEFORM0 is evaluated by: DISPLAYFORM0\n\n \n Question: How do they measure the correlation between manual groundings and model generated ones?",
            "output": [
                "rank-correlation BIBREF25"
            ]
        },
        {
            "id": "task460-3faf15f923154390a591c72f9d8642eb",
            "input": "We evaluated the phrase compositionality models on the adjective–noun and noun–noun phrase similarity tasks compiled by Mitchell2010, using the same evaluation scheme as in the original work. Spearman's INLINEFORM0 between phrasal similarities derived from our compositional functions and the human annotators (computed individually per annotator and then averaged across all annotators) was the evaluation measure. \n Question: How do they score phrasal compositionality?",
            "output": [
                "Spearman's INLINEFORM0 between phrasal similarities derived from our compositional functions and the human annotators"
            ]
        },
        {
            "id": "task460-4d0fe3a7ab184df984cd55a9fd14363c",
            "input": "With BERT, two fully unsupervised tasks are performed. The Masked Language Model and the Next Sentence Prediction (NSP).\n\nFor this study, the NSP is used as a proxy for the relevance of response. \n Question: was bert used?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-28616db0824b4ee6a25f2d55dbbd3388",
            "input": "In the instances where a model thinks all historical samples should be considered equally important in a sequential analysis task, we must look elsewhere for a computationally inexpensive means to understand what happened at the stopping point. \n Question: Can their method of creating more informative visuals be applied to tasks other than turn taking in conversations?",
            "output": [
                "computationally inexpensive means to understand what happened at the stopping point"
            ]
        },
        {
            "id": "task460-eb2e0df498bb40338939a925457a2cc6",
            "input": "In addition, in order to estimate annotation reliability and provide for better evaluation, every question in the test set is answered by at least two additional experts. \n Question: Are the answers double (and not triple) annotated?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-7ad93a432fa54a089532d41d317ab4d4",
            "input": "We create the graph using all data, and training set tweets have an initial language label distribution. A naïve approach to building the tweet-tweet subgraph requires O( INLINEFORM0 ) comparisons, measuring the similarity of each tweet with all others. Instead, we performed INLINEFORM1 -nearest-neighbour classification on all tweets, represented as a bag of unigrams, and compared each tweet and the top- INLINEFORM2 neighbours. We use Junto (mad) BIBREF5 to propagate labels from labelled to unlabelled nodes. Upon convergence, we renormalise label scores for initially unlabelled nodes to find the value of INLINEFORM4 . \n Question: How are labels propagated using this approach?",
            "output": [
                "We use Junto (mad) BIBREF5 to propagate labels from labelled to unlabelled nodes. "
            ]
        },
        {
            "id": "task460-c868a66001c24848817e79491053c9a9",
            "input": "At training time, we use the plan-to-DFS mapping to perform the correct sequence of traversals, and train a neural classifier to act as a controller, choosing which action to perform at each step. \n Question: How is neural planning component trained?",
            "output": [
                "plan-to-DFS mapping to perform the correct sequence of traversals, and train a neural classifier to act as a controller"
            ]
        },
        {
            "id": "task460-5df9bd3220f44526bbf7a23d55e3ae5d",
            "input": "We observe interesting hidden correlation in data. Fig. FIGREF24 has Topic 2 as selected topic. Topic 2 contains top-4 co-occurring keywords \"vegan\", \"yoga\", \"job\", \"every_woman\" having the highest term frequency. We can infer different things from the topic that \"women usually practice yoga more than men\", \"women teach yoga and take it as a job\", \"Yogi follow vegan diet\". We would say there are noticeable correlation in data i.e. `Yoga-Veganism', `Women-Yoga'. Women-Yoga \n Question: What other interesting correlations are observed?",
            "output": [
                "Women-Yoga"
            ]
        },
        {
            "id": "task460-a798d9253a39461685c9f94b31a827e9",
            "input": "We have collected more than 3,934,610 million tweets so far. \n Question: How big is the dataset?",
            "output": [
                "more than 3,934,610 million tweets"
            ]
        },
        {
            "id": "task460-ac208ae561e04ea0b3d8167e1c63b44a",
            "input": "The question generation model provides each candidate answer with a score by measuring semantic relevance between the question and the generated question based on the semantics of the candidate answer.  \n Question: Where is a question generation model used?",
            "output": [
                "The question generation model provides each candidate answer with a score by measuring semantic relevance between the question and the generated question based on the semantics of the candidate answer. "
            ]
        },
        {
            "id": "task460-865ceae8d6824035bda867eb2c45eeab",
            "input": "An octave convolutional layer BIBREF0 factorizes the output feature maps of a convolutional layer into two groups. The resolution of the low-frequency feature maps is reduced by an octave – height and width dimensions are divided by 2. In this work, we explore spatial reduction by up to 3 octaves – dividing by $2^t$, where $t=1,2,3$ – and for up to 4 groups. We refer to such a layer as a multi-octave convolutional (MultiOctConv) layer, and an example with three groups and reductions of one and two octaves is depicted in Fig. FIGREF1. \n Question: How is octave convolution concept extended to multiple resolutions and octaves?",
            "output": [
                "The resolution of the low-frequency feature maps is reduced by an octave – height and width dimensions are divided by 2. In this work, we explore spatial reduction by up to 3 octaves – dividing by $2^t$, where $t=1,2,3$ – and for up to 4 groups. We refer to such a layer as a multi-octave convolutional (MultiOctConv) layer,"
            ]
        },
        {
            "id": "task460-a4a4b382084c48fb9a303a0b8dbfd79e",
            "input": "In this paper, we present an extraction-then-synthesis framework for machine reading comprehension shown in Figure 1 , in which the answer is synthesized from the extraction results. We build an evidence extraction model to predict the most important sub-spans from the passages as evidence, and then develop an answer synthesis model which takes the evidence as additional features along with the question and passage to further elaborate the final answers. \n Question: What two components are included in their proposed framework?",
            "output": [
                "evidence extraction and answer synthesis"
            ]
        },
        {
            "id": "task460-6edaaed4de62446d8cee5fc8b6d7d096",
            "input": "Our model contains five independent decoders, one for each image in the sequence. This allows each decoder to learn a specific language model for each position of the sequence. \n Question: Do the decoder LSTMs all have the same weights?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-6e2450856271476db048c7c7d8dc866d",
            "input": "We trained a logistic regression baseline model (line 1 in Table TABREF10 ) using character ngrams and word unigrams using TF*IDF weighting BIBREF13 , to provide a baseline since HAR has no reported results. For the SR and HATE datasets, the authors reported their trained best logistic regression model's results on their respective datasets. \n Question: what was the baseline?",
            "output": [
                "logistic regression"
            ]
        },
        {
            "id": "task460-21c38654f7b049819be5c05fbbcddea3",
            "input": "Our study focused primarily on English tweets, since this was the language of our diagnostic training sample. \n Question: Do the authors report results only on English datasets?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-7016325035f54d2a878b03905d9e581b",
            "input": "The PolyResponse restaurant search is currently available in 8 languages and for 8 cities around the world: English (Edinburgh), German (Berlin), Spanish (Madrid), Mandarin (Taipei), Polish (Warsaw), Russian (Moscow), Korean (Seoul), and Serbian (Belgrade). \n Question: In what 8 languages is PolyResponse engine used for restourant search and booking system?",
            "output": [
                "English, German, Spanish, Mandarin, Polish, Russian, Korean and Serbian"
            ]
        },
        {
            "id": "task460-cbda103c7e1f45ae9f154ba842a2e0c6",
            "input": "According to the above analysis According to the above analysis, we proposed a weighted version of DIRL to address the problem caused by the shift of $\\rm {P}(\\rm {Y})$ to DIRL. The key idea of this framework is to first align $\\rm {P}(\\rm {Y})$ across domains before performing domain-invariant learning, and then take account the shift of $\\rm {P}(\\rm {Y})$ in the label prediction procedure. Specifically, it introduces a class weight $\\mathbf {w}$ to weigh source domain examples by class. Based on the weighted source domain, the domain shift problem is resolved in two steps.  The motivation behind this practice is to adjust data distribution of the source domain or the target domain to alleviate the shift of $\\rm {P}(\\rm {Y})$ across domains before applying DIRL. Consider that we only have labels of source domain data, we choose to adjust data distribution of the source domain. To achieve this purpose, we introduce a trainable class weight $\\mathbf {w}$ to reweigh source domain examples by class when performing DIRL, with $\\mathbf {w}_i > 0$. Specifically, we hope that:\n\nand we denote $\\mathbf {w}^*$ the value of $\\mathbf {w}$ that makes this equation hold.  We shall see that when $\\mathbf {w}=\\mathbf {w}^*$, DIRL is to align $\\rm {P}_S(G(\\rm {X})|\\rm {Y})$ with $\\rm {P}_T(G(\\rm {X})|\\rm {Y})$ without the shift of $\\rm {P}(\\rm {Y})$. According to our analysis, we know that due to the shift of $\\rm {P}(\\rm {Y})$, there is a conflict between the training objects of the supervised learning $\\mathcal {L}_{sup}$ and the domain-invariant learning $\\mathcal {L}_{inv}$. And the conflict degree will decrease as $\\rm {P}_S(\\rm {Y})$ getting close to $\\rm {P}_T(\\rm {Y})$. Therefore, during model training, $\\mathbf {w}$ is expected to be optimized toward $\\mathbf {w}^*$ since it will make $\\rm {P}(\\rm {Y})$ of the weighted source domain close to $\\rm {P}_T(\\rm {Y})$, so as to solve the conflict. \n Question: How are different domains weighted in WDIRL?",
            "output": [
                "To achieve this purpose, we introduce a trainable class weight $\\mathbf {w}$ to reweigh source domain examples by class when performing DIRL, with $\\mathbf {w}_i > 0$"
            ]
        },
        {
            "id": "task460-fca9e69f9ec64bc38b61e8495123b5ae",
            "input": "Stanford - Twitter Sentiment Corpus (STS Corpus): STS Corpus contains 1,600K training tweets collected by a crawler from BIBREF0 . BIBREF0 constructed a test set manually with 177 negative and 182 positive tweets. The Stanford test set is small. However, it has been widely used in different evaluation tasks BIBREF0 BIBREF5 BIBREF13 .\n\nSanders - Twitter Sentiment Corpus: This dataset consists of hand-classified tweets collected by using search terms: INLINEFORM0 , #google, #microsoft and #twitter. We construct the dataset as BIBREF14 for binary classification.\n\nHealth Care Reform (HCR): This dataset was constructed by crawling tweets containing the hashtag #hcr BIBREF15 . Task is to predict positive/negative tweets BIBREF14 . Table IV shows the result of our model for sentiment classification against other models. We compare our model performance with the approaches of BIBREF0 BIBREF5 on STS Corpus.  For Sanders and HCR datasets, we compare results with the model of BIBREF14 that used a ensemble of multiple base classifiers (ENS) such as NB, Random Forest (RF), SVM and Logistic Regression (LR).  \n Question: Are results reported only on English datasets?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-91eb0fd2d0b1445cabd87e9fea3e0040",
            "input": "For this purpose, we construct a corpus of 172,815 articles from Tagalog Wikipedia which we call WikiText-TL-39 BIBREF18. \n Question: What other datasets are used?",
            "output": [
                "WikiText-TL-39"
            ]
        },
        {
            "id": "task460-bd8bd0796837498f8693209bc9b6a2a7",
            "input": "It is interesting to observe that the baseline model amplifies the bias in the training data set as measured by INLINEFORM0 and INLINEFORM1 . From measurements using the described bias metrics, our method effectively mitigates bias in language modelling without a significant increase in perplexity. \n Question: how is mitigation of gender bias evaluated?",
            "output": [
                "Using INLINEFORM0 and INLINEFORM1"
            ]
        },
        {
            "id": "task460-56edeb4cbeab4738af60535f9be5f28f",
            "input": "Recently, in the WMT 2018 APE shared task, several adaptations of the transformer architecture have been presented for multi-source APE. pal-EtAl:2018:WMT proposed an APE model that uses three self-attention-based encoders. They introduce an additional joint encoder that attends over a combination of the two encoded sequences from $mt$ and $src$. tebbifakhr-EtAl:2018:WMT, the NMT-subtask winner of WMT 2018 ($wmt18^{nmt}_{best}$), employ sequence-level loss functions in order to avoid exposure bias during training and to be consistent with the automatic evaluation metrics. shin-lee:2018:WMT propose that each encoder has its own self-attention and feed-forward layer to process each input separately. On the decoder side, they add two additional multi-head attention layers, one for $src \\rightarrow mt$ and another for $src \\rightarrow pe$. Thereafter another multi-head attention between the output of those attention layers helps the decoder to capture common words in $mt$ which should remain in $pe$. The APE PBSMT-subtask winner of WMT 2018 ($wmt18^{smt}_{best}$) BIBREF11 also presented another transformer-based multi-source APE which uses two encoders and stacks an additional cross-attention component for $src \\rightarrow pe$ above the previous cross-attention for $mt \\rightarrow pe$. Comparing shin-lee:2018:WMT's approach with the winner system, there are only two differences in the architecture: (i) the cross-attention order of $src \\rightarrow mt$ and $src \\rightarrow pe$ in the decoder, and (ii) $wmt18^{smt}_{best}$ additionally shares parameters between two encoders. Recently, in the WMT 2018 APE shared task, several adaptations of the transformer architecture have been presented for multi-source APE. pal-EtAl:2018:WMT proposed an APE model that uses three self-attention-based encoders. They introduce an additional joint encoder that attends over a combination of the two encoded sequences from $mt$ and $src$. tebbifakhr-EtAl:2018:WMT, the NMT-subtask winner of WMT 2018 ($wmt18^{nmt}_{best}$), employ sequence-level loss functions in order to avoid exposure bias during training and to be consistent with the automatic evaluation metrics. shin-lee:2018:WMT propose that each encoder has its own self-attention and feed-forward layer to process each input separately. On the decoder side, they add two additional multi-head attention layers, one for $src \\rightarrow mt$ and another for $src \\rightarrow pe$. Thereafter another multi-head attention between the output of those attention layers helps the decoder to capture common words in $mt$ which should remain in $pe$. The APE PBSMT-subtask winner of WMT 2018 ($wmt18^{smt}_{best}$) BIBREF11 also presented another transformer-based multi-source APE which uses two encoders and stacks an additional cross-attention component for $src \\rightarrow pe$ above the previous cross-attention for $mt \\rightarrow pe$. Comparing shin-lee:2018:WMT's approach with the winner system, there are only two differences in the architecture: (i) the cross-attention order of $src \\rightarrow mt$ and $src \\rightarrow pe$ in the decoder, and (ii) $wmt18^{smt}_{best}$ additionally shares parameters between two encoders. \n Question: What was previous state of the art model for automatic post editing?",
            "output": [
                "pal-EtAl:2018:WMT proposed an APE model that uses three self-attention-based encoders tebbifakhr-EtAl:2018:WMT, the NMT-subtask winner of WMT 2018 ($wmt18^{nmt}_{best}$), employ sequence-level loss functions in order to avoid exposure bias during training and to be consistent with the automatic evaluation metrics. shin-lee:2018:WMT propose that each encoder has its own self-attention and feed-forward layer to process each input separately.  The APE PBSMT-subtask winner of WMT 2018 ($wmt18^{smt}_{best}$) BIBREF11 also presented another transformer-based multi-source APE which uses two encoders and stacks an additional cross-attention component for $src \\rightarrow pe$ above the previous cross-attention for $mt \\rightarrow pe$."
            ]
        },
        {
            "id": "task460-194e3fa34e87418a8099bd40174826e7",
            "input": "Our user study compares the correctness of three scenarios:\n\nParser correctness - our baseline is the percentage of examples where the top query returned by the semantic parser was correct.\n\nUser correctness - the percentage of examples where the user selected a correct query from the top-7 generated by the parser.\n\nHybrid correctness - correctness of queries returned by a combination of the previous two scenarios. The system returns the query marked by the user as correct; if the user marks all queries as incorrect it will return the parser's top candidate. \n Question: Do they conduct a user study where they show an NL interface with and without their explanation?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-77afc871618549b9a6cdf706875b9d13",
            "input": " While domain transfer is not new, compared to prior summarization studies BIBREF6, BIBREF7, our training (news) and tuning (student reflection) domains are quite dissimilar, and the in-domain data is small. \n Question: Is the student reflection data very different from the newspaper data?  ",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-310a9193b5d84ec0bbf48b85919693b6",
            "input": "The baseline model BIBREF3 is implemented with a recurrent neural network based encoder-decoder framework. \n Question: Do they compare against Noraset et al. 2017?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-7c7d63ab7594455c8cc08ef7fd4b4f39",
            "input": "Given the fact that the research on offensive language detection has to a large extent been focused on the English language, we set out to explore the design of models that can successfully be used for both English and Danish. \n Question: What is the challenge for other language except English",
            "output": [
                "not researched as much as English"
            ]
        },
        {
            "id": "task460-79f10cbcd94441e28a78c0300f4d95b6",
            "input": "We find that all the factors we tested can qualitatively affect how a model generalizes on the question formation task. These factors are the type of recurrent unit, the type of attention, and the choice of sequential vs. tree-based model structure. \n Question: What architectural factors were investigated?",
            "output": [
                "type of recurrent unit type of attention choice of sequential vs. tree-based model structure"
            ]
        },
        {
            "id": "task460-b0aaf052d4814a9288bd5d834a9d5b33",
            "input": "For our study, we consider that a tweet went viral if it was retweeted more than 1000 times. \n Question: What is the threshold for determining that a tweet has gone viral?",
            "output": [
                "1000"
            ]
        },
        {
            "id": "task460-01b5e8f27b8543ea85c647d1b2759353",
            "input": "Clean-labeled Datasets. We use three clean labeled datasets. The first one is the movie sentence polarity dataset from BIBREF19. The other two datasets are laptop and restaurant datasets collected from SemEval-2016 . Noisy-labeled Training Datasets. For the above three domains (movie, laptop, and restaurant), we collected 2,000 reviews for each domain from the same review source. \n Question: What is the dataset used to train the model?",
            "output": [
                " movie sentence polarity dataset from BIBREF19 laptop and restaurant datasets collected from SemEval-201 we collected 2,000 reviews for each domain from the same review source"
            ]
        },
        {
            "id": "task460-ae9257f2e01d4e62a7984b48e71db2e5",
            "input": "In spite of the windowing approach, the class distribution is still skewed, and an accuracy metric would reflect the particular class distribution in our data set. Therefore, we adopt the unweighted average recall (UAR) metric commonly used in emotion classification research. UAR is a reweighted accuracy where the samples of both classes are weighted equally in aggregate. UAR thus simulates a uniform class distribution. To match the objective, our classifiers are trained on appropriately weighted training data. Note that chance performance for UAR is by definition 50%, making results more comparable across different data sets. \n Question: What they use as a metric of finding hot spots in meeting?",
            "output": [
                "unweighted average recall (UAR) metric"
            ]
        },
        {
            "id": "task460-718e2ddeae9346fc8e48774e314274a3",
            "input": "We also looked at where there was complete agreement by all annotators that a triple extraction was incorrect. In total there were 138 of these triples originating from 76 unique sentences.  \n Question: What is the most common error type?",
            "output": [
                "all annotators that a triple extraction was incorrect"
            ]
        },
        {
            "id": "task460-51256460309b4a18801fa8641e352b8f",
            "input": "As a starting point, we used the DIP corpus BIBREF37 , a collection of 49 clusters of 100 web pages on educational topics (e.g. bullying, homeschooling, drugs) with a short description of each topic. \n Question: Which collections of web documents are included in the corpus?",
            "output": [
                "DIP corpus BIBREF37"
            ]
        },
        {
            "id": "task460-0c94a41fa2854ed5803e18b2892f2b10",
            "input": "However, we see that IR methods perform better than the best neural models. We do not find enough evidence to reject the null hypothesis regarding what context from the cited document should be used. \n Question: Which baseline performs best?",
            "output": [
                "IR methods perform better than the best neural models"
            ]
        },
        {
            "id": "task460-a6703d8a546a43afb1d1d045db306b79",
            "input": "Our data has been developed by crawling and pre-processing an OSG web forum. The forum has a great variety of different groups such as depression, anxiety, stress, relationship, cancer, sexually transmitted diseases, etc. \n Question: How did they obtain the OSG dataset?",
            "output": [
                "crawling and pre-processing an OSG web forum"
            ]
        },
        {
            "id": "task460-e1d8a230182d45c0a7401dc2c7035008",
            "input": "Following BIBREF11 , we experiment on the non-anonymized version of . When generating summaries, we follow standard practice of tuning the maximum output length and disallow repeating the same trigram BIBREF27 , BIBREF14 . For this task we train language model representations on the combination of newscrawl and the training data. Table TABREF16 shows that pre-trained embeddings can significantly improve on top of a strong baseline transformer. We also compare to BIBREF26 who use a task-specific architecture compared to our generic sequence to sequence baseline. Pre-trained representations are complementary to their method. \n Question: What other models do they compare to?",
            "output": [
                "BIBREF11  BIBREF26 "
            ]
        },
        {
            "id": "task460-b7fa674e2a1e4c2fac1fb0e0cda8a884",
            "input": "Thus, we can distinguish two main regions or clusters in the maps. The purple background covers most of the map and represents rural regions with small, scattered population. Our analysis shows that this group of cells possesses more specific words in their lexicon. In contrast, the green and yellow cells form a second cluster that is largely concentrated on the center and along the coastline, which correspond to big cities and industrialized areas.  In these cells, the use of standard Spanish language is widespread due probably to school education, media, travelers, etc. \n Question: Do the authors mention any possible confounds in their study?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-489e1d98540c4da1bd49a0e96970ee8a",
            "input": "Each dataset consists of a collection of records with one QA problem per record. For each record, we include some question text, a context document relevant to the question, a set of candidate solutions, and the correct solution. The context document for each record consists of a list of ranked and scored pseudodocuments relevant to the question. Several baselines rely on the retrieved context to extract the answer to a question. For these, we refer to the fraction of instances for which the correct answer is present in the context as Search Accuracy. Naturally, the search accuracy increases as the context size increases, however at the same time reading performance decreases since the task of extracting the answer becomes harder for longer documents. \n Question: Which retrieval system was used for baselines?",
            "output": [
                "The dataset comes with a ranked set of relevant documents. Hence the baselines do not use a retrieval system."
            ]
        },
        {
            "id": "task460-b420a61e529e48fea33c6df8f2362433",
            "input": "For French speech technologies, four corpora containing radio and TV broadcast are the most widely used: ESTER1 BIBREF13, ESTER2 BIBREF14, ETAPE BIBREF15 and REPERE BIBREF16. \n Question: What are four major corpora of French broadcast?",
            "output": [
                "ESTER1 ESTER2 ETAPE REPERE"
            ]
        },
        {
            "id": "task460-3ead4b77283e44b9811006440261367c",
            "input": "Given a news Twitter account, we read its tweets from the account's timeline. Then we sort the tweets by the posting date in ascending way and we split them into $N$ chunks. Each chunk consists of a sorted sequence of tweets labeled by the label of its corresponding account. \n Question: Was the approach used in this work to detect fake news fully supervised?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-c1c4d8d9578e4d55852d54cff0040e24",
            "input": "Future work can be improving the sensationalism scorer and investigating the applications of dynamic balancing methods between RL and MLE in textGANBIBREF23. Our work also raises the ethical questions about generating sensational headlines, which can be further explored. \n Question: What is future work planed?",
            "output": [
                "ethical questions about generating sensational headlines, which can be further explored  improving the sensationalism scorer investigating the applications of dynamic balancing methods between RL and MLE"
            ]
        },
        {
            "id": "task460-eb1e771b87a34c8d87e0ad62fb40c9a1",
            "input": "Methods ::: Combining the two methods\nWe further propose to use the two methods together to combine their strengths. In fact, while the length token acts as a soft constraint to bias NMT to produce short or long translation with respect to the source, actually no length information is given to the network. On the other side, length encoding leverages information about the target length, but it is agnostic of the source length. \n Question: Do they experiment with combining both methods?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-15095b10f7a84e3aaf7b5df19957e975",
            "input": "The corpus of supervisor assessment has 26972 sentences.  \n Question: What is the size of the real-life dataset?",
            "output": [
                "26972"
            ]
        },
        {
            "id": "task460-5ade0ecfa60c4961bf23b55cd9cfc28c",
            "input": "The improvement when the difficult subset with expert annotations is mixed with the remaining crowd annotation is 3.5 F1 score, much larger than when a random set of expert annotations are added. \n Question: How much higher quality is the resulting annotated data?",
            "output": [
                "improvement when the difficult subset with expert annotations is mixed with the remaining crowd annotation is 3.5 F1 score, much larger than when a random set of expert annotations are added"
            ]
        },
        {
            "id": "task460-172c9c6ff17f431184fdef686c5c0df3",
            "input": "We evaluate LinkNBed and baselines on two real world knowledge graphs: D-IMDB (derived from large scale IMDB data snapshot) and D-FB (derived from large scale Freebase data snapshot). \n Question: On what data is the model evaluated?",
            "output": [
                "D-IMDB (derived from large scale IMDB data snapshot) D-FB (derived from large scale Freebase data snapshot)"
            ]
        },
        {
            "id": "task460-8c71f989c1b84ae0924cf6853da4a1cf",
            "input": "We carried out experiments on four Chinese NLP tasks, including Emotion Classification (EC), Named Entity Recognition (NER), Sentence Pair Matching (SPM) and Natural Language Inference (NLI). \n Question: What benchmarks did they experiment on?",
            "output": [
                "Emotion Classification (EC) Named Entity Recognition (NER) Sentence Pair Matching (SPM) Natural Language Inference (NLI)"
            ]
        },
        {
            "id": "task460-334d34200178421c97fc7cbfc372769a",
            "input": "In this work, we use the datasets released by BIBREF1 and HEOT dataset provided by BIBREF0 .  This was then followed by transliteration using the Xlit-Crowd conversion dictionary and translation of each word to English using Hindi to English dictionary.  \n Question: Do all the instances contain code-switching?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-71c35daa82a6412a8de4116f85621f19",
            "input": "In this paper, we have shown that both human and machine translation can alter superficial patterns in data, which requires reconsidering previous findings in cross-lingual transfer learning. \n Question: Does the professional translation or the machine translation introduce the artifacts?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-800fa7ef318a4b888fc3fc17554f467c",
            "input": "We acquired the data in two rounds of annotation. In the first one, we were looking for original and uncommon sentence change suggestions. \n Question: How do they introduce language variation?",
            "output": [
                " we were looking for original and uncommon sentence change suggestions"
            ]
        },
        {
            "id": "task460-c6f507ba8d6047fcaf92c04a04ce10a5",
            "input": "The first dataset is an expanded version of the annotated Wikipedia conversations dataset from BIBREF9. This dataset uses carefully-controlled crowdsourced labels, strictly filtered to ensure the conversations are civil up to the moment of a personal attack. This is a useful property for the purposes of model analysis, and hence we focus on this as our primary dataset. However, we are conscious of the possibility that these strict labels may not fully capture the kind of behavior that moderators care about in practice. We therefore introduce a secondary dataset, constructed from the subreddit ChangeMyView (CMV) that does not use post-hoc annotations. Instead, the prediction task is to forecast whether the conversation will be subject to moderator action in the future. Wikipedia data. BIBREF9's `Conversations Gone Awry' dataset consists of 1,270 conversations that took place between Wikipedia editors on publicly accessible talk pages. The conversations are sourced from the WikiConv dataset BIBREF59 and labeled by crowdworkers as either containing a personal attack from within (i.e., hostile behavior by one user in the conversation directed towards another) or remaining civil throughout. Reddit CMV data. The CMV dataset is constructed from conversations collected via the Reddit API. In contrast to the Wikipedia-based dataset, we explicitly avoid the use of post-hoc annotation. Instead, we use as our label whether a conversation eventually had a comment removed by a moderator for violation of Rule 2: “Don't be rude or hostile to other users”. \n Question: What labels for antisocial events are available in datasets?",
            "output": [
                "The Conversations Gone Awry dataset is labelled as either containing a personal attack from withint (i.e. hostile behavior by one user in the conversation directed towards another) or remaining civil throughout. The Reddit Change My View dataset is labelled with whether or not a coversation eventually had a comment removed by a moderator for violation of Rule 2: \"Don't be rude or hostile to others users.\""
            ]
        },
        {
            "id": "task460-5fd6bf3db48b4f6f9ea1b8da54a3244e",
            "input": "For those willing to have a more tightly-controlled installation of Seshat on their system, we also fully specify the manual installation steps in our online documentation). \n Question: Is this software available to the public?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-ac0079393d0e40989e17ce64bc84a4b4",
            "input": "We seek a function controlled by gates that can mix states across timesteps, but which acts independently on each channel of the state vector. The simplest option, which BIBREF12 term “dynamic average pooling”, uses only a forget gate: DISPLAYFORM0 \n Question: What pooling function is used?",
            "output": [
                "dynamic average pooling"
            ]
        },
        {
            "id": "task460-3558ae73b889493ba368f25eb1017cb7",
            "input": "Consequently, we investigate ways to detect suspicious accounts by considering their tweets in groups (chunks). Our hypothesis is that suspicious accounts have a unique pattern in posting tweet sequences. Since their intention is to mislead, the way they transition from one set of tweets to the next has a hidden signature, biased by their intentions. Given a news Twitter account, we read its tweets from the account's timeline. Then we sort the tweets by the posting date in ascending way and we split them into $N$ chunks. Each chunk consists of a sorted sequence of tweets labeled by the label of its corresponding account. \n Question: How is a \"chunk of posts\" defined in this work?",
            "output": [
                "chunk consists of a sorted sequence of tweets labeled by the label of its corresponding account"
            ]
        },
        {
            "id": "task460-6d0142a86770422fa5cad2cca95d26f2",
            "input": "We operationalized the question by submitting the chosen test data to the same vendor-based transcription pipeline that is used at Microsoft for production data (for model training and internal evaluation purposes), and then comparing the results to ASR system output under the NIST scoring protocol.  \n Question: what standard speech transcription pipeline was used?",
            "output": [
                "pipeline that is used at Microsoft for production data"
            ]
        },
        {
            "id": "task460-6db8e98f11b7489eb559d5664799bc32",
            "input": "In this work, we introduce a new logical inference engine called MonaLog, which is based on natural logic and work on monotonicity stemming from vanBenthemEssays86. Since our logic operates over surface forms, it is straightforward to hybridize our models. We investigate using MonaLog in combination with the language model BERT BIBREF20, including for compositional data augmentation, i.e, re-generating entailed versions of examples in our training sets.  We perform two experiments to test MonaLog. We first use MonaLog to solve the problems in a commonly used natural language inference dataset, SICK BIBREF1, comparing our results with previous systems. Second, we test the quality of the data generated by MonaLog. To do this, we generate more training data (sentence pairs) from the SICK training data using our system, and performe fine-tuning on BERT BIBREF20, a language model based on the transformer architecture BIBREF23, with the expanded dataset.  \n Question: How do they combine MonaLog with BERT?",
            "output": [
                "They use Monalog for data-augmentation to fine-tune BERT on this task"
            ]
        },
        {
            "id": "task460-c7911d23c07c4859b63fccdd3d1c309f",
            "input": "As this corpus of annotated patient notes comprises original healthcare data which contains protected health information (PHI) per The Health Information Portability and Accountability Act of 1996 (HIPAA) BIBREF16 and can be joined to the MIMIC-III database, individuals who wish to access to the data must satisfy all requirements to access the data contained within MIMIC-III. To satisfy these conditions, an individual who wishes to access the database must take a “Data or Specimens Management” course, as well as sign a user agreement, as outlined on the MIMIC-III database webpage, where the latest version of this database will be hosted as “Annotated Clinical Texts from MIMIC” BIBREF17. This corpus can also be accessed on GitHub after completing all of the above requirements. \n Question: Is this dataset publicly available for commercial use?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-4a94e5685c4c4cf1ab03242a19bef28a",
            "input": "Argus Dataset AI2-8grade/CK12 Dataset We consider this dataset as preliminary since it was not reviewed by a human and many hypotheses are apparently unprovable by the evidence we have gathered (i.e. the theoretical top accuracy is much lower than 1.0).  MCTest Dataset We use an official extension of the dataset for RTE evaluation that again textually merges questions and answers. \n Question: what datasets did they use?",
            "output": [
                "Argus Dataset AI2-8grade/CK12 Dataset MCTest Dataset"
            ]
        },
        {
            "id": "task460-f8d02d1315214be0b29c9ce6f0b2e5b6",
            "input": "The encoder is a Convolutional Neural Network (CNN) and the decoder is a Long Short-Term Memory (LSTM) network, as presented in Figure 2 . The image is passed through the encoder generating the image representation that is used by the decoder to know the content of the image and generate the description word by word. \n Question: What model is used to encode the images?",
            "output": [
                "a Convolutional Neural Network (CNN)"
            ]
        },
        {
            "id": "task460-0129ccdd9ea8459fa118b5be0ee33693",
            "input": "In order to obtain the citation data of the Czech apex courts, it was necessary to recognize and extract the references from the CzCDC 1.0. Given that training data for both the reference recognition model BIBREF13, BIBREF34 and the text segmentation model BIBREF33 are publicly available, we were able to conduct extensive error analysis and put together a pipeline to arguably achieve the maximum efficiency in the task. The pipeline described in this part is graphically represented in Figure FIGREF10. At this point, it is necessary to evaluate the performance of the above mentioned part of the pipeline before proceeding further. The evaluation of the performance is summarised in Table TABREF11. It shows that organising the two models into the pipeline boosted the performance of the reference recognition model, leading to a higher F1 measure in the initial recognition of the text spans and their classification. \n Question: How is quality of the citation measured?",
            "output": [
                "it is necessary to evaluate the performance of the above mentioned part of the pipeline before proceeding further. The evaluation of the performance is summarised in Table TABREF11. It shows that organising the two models into the pipeline boosted the performance of the reference recognition model, leading to a higher F1 measure in the initial recognition of the text spans and their classification."
            ]
        },
        {
            "id": "task460-6a4ac05272684982ad3dce88ba4c6d11",
            "input": "Encoders with induced latent structures have been shown to benefit several tasks including document classification, natural language inference BIBREF12, BIBREF13, and machine translation BIBREF11.  \n Question: Is there any evidence that encoders with latent structures work well on other tasks?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-575ecfdd24774098bee28062c8f31e42",
            "input": " More generally, we know little about the types of development issues that different countries prioritise, or whether country-specific factors such as wealth or democracy make countries more likely to push for specific development issues to be put on the global political agenda.  We find that discussion of Topic 2 is not significantly impacted by country-specific factors, such as wealth, population, democracy, levels of ODA, and conflict (although there are regional effects).  \n Question: What are the country-specific drivers of international development rhetoric?",
            "output": [
                "wealth  democracy  population levels of ODA conflict "
            ]
        },
        {
            "id": "task460-c31452dceff649ee905d2d1afee7a35a",
            "input": "The results show that NCEL consistently outperforms various baselines with a favorable generalization ability. \n Question: How effective is their NCEL approach overall?",
            "output": [
                "NCEL consistently outperforms various baselines with a favorable generalization ability"
            ]
        },
        {
            "id": "task460-80280839e3e34ecf82430c73c810bcd7",
            "input": "In our method, we take an image as input and generate a natural question as output. The architecture for our model is shown in Figure FIGREF4 . Our model contains three main modules, (a) Representation Module that extracts multimodal features (b) Mixture Module that fuses the multimodal representation and (c) Decoder that generates question using an LSTM-based language model. \n Question: How/where are the natural question generated?",
            "output": [
                "Decoder that generates question using an LSTM-based language model"
            ]
        },
        {
            "id": "task460-20f8592093b449f29478bde0477c0bf9",
            "input": "In general, the baseline model CAEVO is outperformed by both NN models, and NN model with BERT embedding achieves the greatest performance. \n Question: Do the BERT-based embeddings improve results?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-774060dc984e4bda84baa007f20a847f",
            "input": "Shared Self-attention Layers As our model providing two outputs from one input, there is a bifurcation setting for how much shared part should be determined. Both constituent and dependency parsers share token representation and 8 self-attention layers at most. Assuming that either parser always takes input information flow through 8 self-attention layers as shown in Figure FIGREF4, then the number of shared self-attention layers varying from 0 to 8 may reflect the shared degree in the model. When the number is set to 0, it indicates only token representation is shared for both parsers trained for the joint loss through each own 8 self-attention layers. When the number is set to less than 8, for example, 6, then it means that both parsers first shared 6 layers from token representation then have individual 2 self-attention layers. For different numbers of shared layers, the results are in Table TABREF14. We respectively disable the constituent and the dependency parser to obtain a separate learning setting for both parsers in our model. The comparison in Table TABREF14 indicates that even though without any shared self-attention layers, joint training of our model may significantly outperform separate learning mode. At last, the best performance is still obtained from sharing full 8 self-attention layers. \n Question: How are different network components evaluated?",
            "output": [
                "For different numbers of shared layers, the results are in Table TABREF14. We respectively disable the constituent and the dependency parser to obtain a separate learning setting for both parsers in our model. "
            ]
        },
        {
            "id": "task460-3ba252fc7b5d44ab83ced084f530b37a",
            "input": "Specifically, we design three tasks for seq2seq model pre-training, namely Sentence Reordering (SR), Next Sentence Generation (NSG), and Masked Document Generation (MDG).  \n Question: What is masked document generation?",
            "output": [
                "A task for seq2seq model pra-training that recovers a masked document to its original form."
            ]
        },
        {
            "id": "task460-f35136680df041b780caf12e55cba128",
            "input": "Unlike the SQuAD dataset, which only has one passage given a question, there are several related passages for each question in the MS-MARCO dataset. In addition to annotating the answer, MS-MARCO also annotates which passage is correct.  \n Question: Why MS-MARCO is different from SQuAD?",
            "output": [
                "there are several related passages for each question in the MS-MARCO dataset. MS-MARCO also annotates which passage is correct"
            ]
        },
        {
            "id": "task460-c43dcd6465fa44c5855af4750b5c057e",
            "input": "We apply attention probes to the task of identifying the existence and type of dependency relation between two words. Our first experiment is an exploratory visualization of how word sense affects context embeddings.  \n Question: What linguistic features were probed for?",
            "output": [
                "dependency relation between two words word sense"
            ]
        },
        {
            "id": "task460-04ff28cf7db3458a9710ff728f16b0a4",
            "input": "n the OLID dataset, we use a hierarchical annotation model split into three levels to distinguish between whether language is offensive or not (A), and type (B) and target (C) of the offensive language. Each level is described in more detail in the following subsections and examples are shown in Table TABREF10 . Level A: Offensive language Detection\nLevel A discriminates between offensive (OFF) and non-offensive (NOT) tweets. Level B: Categorization of Offensive Language\nLevel B categorizes the type of offense and two labels are used: targeted (TIN) and untargeted (INT) insults and threats. Level C: Offensive Language Target Identification\nLevel C categorizes the targets of insults and threats as individual (IND), group (GRP), and other (OTH). \n Question: What are the three layers of the annotation scheme?",
            "output": [
                "Level A: Offensive language Detection\n Level B: Categorization of Offensive Language\n Level C: Offensive Language Target Identification\n"
            ]
        },
        {
            "id": "task460-60d8eee40b1d479994fda70cbd026eb4",
            "input": "To develop the dataset, we extracted 200 sentences from Sorani Kurdish books of grades one to three of the primary school in the Kurdistan Region of Iraq. We randomly created 2000 sentences from the extracted sentences. \n Question: How was the dataset collected?",
            "output": [
                "extracted text from Sorani Kurdish books of primary school and randomly created sentences"
            ]
        },
        {
            "id": "task460-40e530eda4e04a8d8d7cf24aa2a7f598",
            "input": "To train our best model we chose the best network from our experiments (6-layer bLSTM with 1024 hidden units), trained it with Adam optimizer and fine-tuned with SGD with momentum using exponential learning rate decay. \n Question: Which architecture is their best model?",
            "output": [
                "6-layer bLSTM with 1024 hidden units"
            ]
        },
        {
            "id": "task460-3022236c5449471d92b8e6dc344d6a93",
            "input": "Through experiments on two large complex cross-domain datasets, SParC BIBREF2 and CoSQL BIBREF6, we carefully compare and analyze the performance of different context modeling methods. \n Question: What are two datasets models are tested on?",
            "output": [
                "SParC BIBREF2 and CoSQL BIBREF6"
            ]
        },
        {
            "id": "task460-674a291d406c4ca8a5806d0ad4cd5b93",
            "input": "To ensure the universality and general applicability of the curriculum, we perform an in-depth investigation on three publicly available conversation corpora, PersonaChat BIBREF12, DailyDialog BIBREF13 and OpenSubtitles BIBREF7, consisting of 140 248, 66 594 and 358 668 real-life conversation samples, respectively. \n Question: What three publicly available coropora are used?",
            "output": [
                "PersonaChat BIBREF12 DailyDialog BIBREF13 OpenSubtitles BIBREF7"
            ]
        },
        {
            "id": "task460-df2f74d15fb84c28b4abca202b85179c",
            "input": " In order to fully leverage gloss information, we propose GlossBERT to construct context-gloss pairs from all possible senses of the target word in WordNet, thus treating WSD task as a sentence-pair classification problem. \n Question: Do they incoprorate WordNet into the model?",
            "output": [
                "construct context-gloss pairs from all possible senses of the target word in WordNet, thus treating WSD task as a sentence-pair classification problem"
            ]
        },
        {
            "id": "task460-fd0ddd66af3c48d6ac3423c0b3ec00f0",
            "input": "Spanish (SPA), in contrast, is morphologically rich, and disposes of much larger verbal paradigms than English. Like English, it is a suffixing language, and it additionally makes use of internal stem changes (e.g., o $\\rightarrow $ ue). We choose Zulu (ZUL), a Bantoid language. In contrast to the first two, it is strongly prefixing. Second, the system pretrained on HUN performing well suggests again that a source language with an agglutinative, as opposed to a fusional, morphology seems to be beneficial as well. Again, TUR and HUN obtain high accuracy, which is an additional indicator for our hypothesis that a source language with an agglutinative morphology facilitates learning of inflection in another language. \n Question: Are agglutinative languages used in the prediction of both prefixing and suffixing languages?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-1e800a3fa55740c6b2fbf6c215d36dc7",
            "input": "Our lexical features include 1-, 2-, and 3-grams in both word and character levels. We also use the number of characters and the number of words as features. We then use all the POS tags with their corresponding tf-idf values as our syntactic features and feature values, respectively. Firstly, we employ 300-dimensional pre-trained word embeddings from GloVe BIBREF29 to compute a tweet embedding as the average of the embeddings of words in the tweet. Secondly, we apply the latent semantic indexing BIBREF30 to capture the underlying semantics of the dataset. Here, each tweet is represented as a vector of 100 dimensions. Thirdly, we also extract tweet representation by applying the Brown clustering algorithm BIBREF31 , BIBREF32 —a hierarchical clustering algorithm which groups the words with similar meaning and syntactical function together. Motivated by the verbal irony by means of polarity contrast, such as “I really love this year's summer; weeks and weeks of awful weather”, we use the number of polarity signals appearing in a tweet as the polarity features. The signals include positive words (e.g., love), negative words (e.g., awful), positive emoji icon and negative emoji icon. We use the sentiment dictionaries provided by BIBREF33 to identify positive and negative words in a tweet. We further use boolean features that check whether or not a negation word is in a tweet (e.g., not, n't). \n Question: What type of lexical, syntactic, semantic and polarity features are used?",
            "output": [
                "Our lexical features include 1-, 2-, and 3-grams in both word and character levels. number of characters and the number of words POS tags 300-dimensional pre-trained word embeddings from GloVe latent semantic indexing tweet representation by applying the Brown clustering algorithm positive words (e.g., love), negative words (e.g., awful), positive emoji icon and negative emoji icon boolean features that check whether or not a negation word is in a tweet"
            ]
        },
        {
            "id": "task460-41fa2e41edcf4228a7e556cc565d1ede",
            "input": " The annotation scheme was developed to capture three aspects of classroom talk that are theorized in the literature as important to discussion quality and learning opportunities: argumentation (the process of systematically reasoning in support of an idea), specificity (the quality of belonging or relating uniquely to a particular subject), and knowledge domain (area of expertise represented in the content of the talk). \n Question: how do they measure discussion quality?",
            "output": [
                "Measuring three aspects: argumentation, specificity and knowledge domain."
            ]
        },
        {
            "id": "task460-cd95737e1ed542f0bc72197348a12392",
            "input": "To enable this, we use a bifocal attention mechanism which computes an attention over fields at a macro level and over values at a micro level. We then fuse these attention weights such that the attention weight for a field also influences the attention over the values within it. Fused Bifocal Attention Mechanism\nIntuitively, when a human writes a description from a table she keeps track of information at two levels. At the macro level, it is important to decide which is the appropriate field to attend to next and at a micro level (i.e., within a field) it is important to know which values to attend to next. To capture this behavior, we use a bifocal attention mechanism as described below. Fused Attention: Intuitively, the attention weights assigned to a field should have an influence on all the values belonging to the particular field. To ensure this, we reweigh the micro level attention weights based on the corresponding macro level attention weights. In other words, we fuse the attention weights at the two levels as: DISPLAYFORM0\n\nwhere INLINEFORM0 is the field corresponding to the INLINEFORM1 -th value, INLINEFORM2 is the macro level context vector. \n Question: What is a bifocal attention mechanism?",
            "output": [
                "At the macro level, it is important to decide which is the appropriate field to attend to next micro level (i.e., within a field) it is important to know which values to attend to next fuse the attention weights at the two levels"
            ]
        },
        {
            "id": "task460-ebd3c75c04cc4deeaec50f7406cc1e41",
            "input": "The task presented here was easy and simple to analyze, however, future work may be done on natural language tasks. If these properties hold it might indicate that a new evaluation paradigm for NLP should be pushed; one that emphasizes performance on uncharacteristic (but structurally sound) inputs in addition to the data typically seen in training. \n Question: Can the findings of this paper be generalized to a general-purpose task?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-3dc8c7aec0774e2ca6c2825581ebbf2d",
            "input": "Notably, automatic translations of TED talks contain more words than the corresponding reference translation, which means that machine-translated texts of this type have also more potential tokens to enter in a coreference relation, and potentially indicating a shining through effect. The same does not happen with the news test set. We see how NMT translations increase the number of mentions about $30\\%$ with respect to human references showing even a more marked explicitation effect than human translations do. \n Question: What translationese effects are seen in the analysis?",
            "output": [
                "potentially indicating a shining through effect explicitation effect"
            ]
        },
        {
            "id": "task460-e3e523a827794d91b034104786aa7bb8",
            "input": "We find that dynamic communities, such as Seahawks or starcraft, have substantially higher rates of monthly user retention than more stable communities (Spearman's INLINEFORM0 = 0.70, INLINEFORM1 0.001, computed with community points averaged over months; Figure FIGREF11 .A, left). Similarly, more distinctive communities, like Cooking and Naruto, exhibit moderately higher monthly retention rates than more generic communities (Spearman's INLINEFORM2 = 0.33, INLINEFORM3 0.001; Figure FIGREF11 .A, right). As with monthly retention, we find a strong positive relationship between a community's dynamicity and the average number of months that a user will stay in that community (Spearman's INLINEFORM0 = 0.41, INLINEFORM1 0.001, computed over all community points; Figure FIGREF11 .B, left). This verifies that the short-term trend observed for monthly retention translates into longer-term engagement and suggests that long-term user retention might be strongly driven by the extent to which a community continually provides novel content. Interestingly, there is no significant relationship between distinctiveness and long-term engagement (Spearman's INLINEFORM2 = 0.03, INLINEFORM3 0.77; Figure FIGREF11 .B, right). Thus, while highly distinctive communities like RandomActsOfMakeup may generate focused commitment from users over a short period of time, such communities are unlikely to retain long-term users unless they also have sufficiently dynamic content. \n Question: How do the various social phenomena examined manifest in different types of communities?",
            "output": [
                "Dynamic communities have substantially higher rates of monthly user retention than more stable communities. More distinctive communities exhibit moderately higher monthly retention rates than more generic communities. There is also a strong positive relationship between a community's dynamicity and the average number of months that a user will stay in that community - a short-term trend observed for monthly retention translates into longer-term engagement and suggests that long-term user retention might be strongly driven by the extent to which a community continually provides novel content.\n"
            ]
        },
        {
            "id": "task460-c9d7f515b3d744a293b2bc0d3da80810",
            "input": "We compare our best F1 score with other state-of-the-art approaches in table TABREF39 , which shows our model has competitive advantage in dealing with drug-drug interaction extraction. \n Question: By how much does their model outperform existing methods?",
            "output": [
                "Answer with content missing: (Table II) Proposed model has F1 score of  0.7220 compared to 0.7148 best state-state-of-the-art result."
            ]
        },
        {
            "id": "task460-b994c6e21cae41d9b441c330bacb8b2e",
            "input": "We adopt the DSTC2 BIBREF20 dataset and Maluuba BIBREF21 dataset to evaluate our proposed model. \n Question: What two benchmark datasets are used?",
            "output": [
                "DSTC2 Maluuba"
            ]
        },
        {
            "id": "task460-b9775ea4b5bd49b4816976293a0c94d5",
            "input": "In this context, we propose a robust multilingual sentiment analysis method, tested in eight different languages: Spanish, English, Italian, Arabic, German, Portuguese, Russian and Swedish. \n Question: What eight language are reported on?",
            "output": [
                "Spanish, English, Italian, Arabic, German, Portuguese, Russian and Swedish"
            ]
        },
        {
            "id": "task460-4d89cc8624e14cd691db5e52ea40e39d",
            "input": "We found interesting patterns which are learned by our model and help understand these monolingual gains. For example, a recurring pattern is that words in English which are translated to the same word, or to semantically close words, in the target language end up closer together after our transformation. For example, in the case of English-Spanish the following pairs were among the pairs whose similarity increased the most by applying our transformation: cellphone-telephone, movie-film, book-manuscript or rhythm-cadence, which are either translated to the same word in Spanish (i.e., teléfono and película in the first two cases) or are already very close in the Spanish space. More generally, we found that word pairs which move together the most tend to be semantically very similar and belong to the same domain, e.g., car-bicycle, opera-cinema, or snow-ice.\n\n \n Question: Why does the model improve in monolingual spaces as well? ",
            "output": [
                "because word pair similarity increases if the two words translate to similar parts of the cross-lingual embedding space"
            ]
        },
        {
            "id": "task460-46e2cb711659491ead5dd7d33ca28c41",
            "input": "Using this annotation model, we create a new large publicly available dataset of English tweets. \n Question: In what language are the tweets?",
            "output": [
                "English"
            ]
        },
        {
            "id": "task460-452b205a80054e1ca3b0c1bcb889f467",
            "input": " First, we analyze the types of structures in code-mixed puns and classify them into two categories namely intra-sequential and intra-word.  \n Question: What are the categories of code-mixed puns?",
            "output": [
                "intra-sequential and intra-word"
            ]
        },
        {
            "id": "task460-c0b4b26852e54bd88b478d0569346353",
            "input": "We evaluated our detection models on three benchmarks: the FCE test data (41K tokens) and the two alternative annotations of the CoNLL 2014 Shared Task dataset (30K tokens) BIBREF3 .  \n Question: Which annotated corpus did they use?",
            "output": [
                " FCE test data (41K tokens) and the two alternative annotations of the CoNLL 2014 Shared Task dataset (30K tokens) "
            ]
        },
        {
            "id": "task460-203c463bbecb43cfbd411d306eb377c6",
            "input": "We test the existing debiasing approaches, CDA and REG, as well but since BIBREF5 reported that results fluctuate substantially with different REG regularization coefficients, we perform hyperparameter tuning and report the best results in Table TABREF12 . \n Question: which existing strategies are compared?",
            "output": [
                "CDA REG"
            ]
        },
        {
            "id": "task460-f7557eee880b45709f3359855ca97cdd",
            "input": "In this way, an attention vector sequence is segmented into several subsequences and each subsequence represents the attention of one word. Then, motivated by the psycholinguistic finding that readers are likely to pay approximate attention to each character in one Chinese word, we devise an appropriate aggregation module to fuse the inner-word character attention. \n Question: How does the fusion method work?",
            "output": [
                "ttention vector sequence is segmented into several subsequences and each subsequence represents the attention of one word we devise an appropriate aggregation module to fuse the inner-word character attention"
            ]
        },
        {
            "id": "task460-b383fb4247334d50a95968234e70d7fa",
            "input": "We have observed that the number of epochs is an important parameter and its increase leads to results that rank our two worst models almost equal, or even better than others. \n Question: How does different parameter settings impact the performance and semantic capacity of resulting model?",
            "output": [
                "number of epochs is an important parameter and its increase leads to results that rank our two worst models almost equal, or even better than others"
            ]
        },
        {
            "id": "task460-afe5119bccbb4dac95bbaeea95789a78",
            "input": "In case of polysemous words, only the first word sense (usually the most common) is taken into account. \n Question: How do they handle polysemous words in their entity library?",
            "output": [
                "only the first word sense (usually the most common) is taken into account"
            ]
        },
        {
            "id": "task460-324de2add30845d5ae32aac416b69fb5",
            "input": "In contrast to other work, we do not show the documents to the workers at all, but provide only a description of the document cluster's topic along with the propositions. This ensures that tasks are small, simple and can be done quickly (see Figure FIGREF4 ). \n Question: How were crowd workers instructed to identify important elements in large document collections?",
            "output": [
                "provide only a description of the document cluster's topic along with the propositions"
            ]
        },
        {
            "id": "task460-740c481e3037458988f4f4b238a133cf",
            "input": "We combine 13k questions gathered from this pipeline with an additional 3k questions with yes/no answers from the NQ training set to reach a total of 16k questions.  \n Question: what is the size of BoolQ dataset?",
            "output": [
                " 16k questions"
            ]
        },
        {
            "id": "task460-c5de0d3c16f44017a6db10ee7a31bb3d",
            "input": "A total of 27 different genres were scraped. \n Question: how many movie genres do they explore?",
            "output": [
                "27 "
            ]
        },
        {
            "id": "task460-cd720474d70b48b9b466e57f44018b75",
            "input": "We carried out two human evaluations using Mechanical Turk to compare the performance of our model and the baseline. \n Question: Is there any human evaluation involved in evaluating this famework?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-ae1880b964f54195b6ccd4d7831bd948",
            "input": "The gated mechanism learn the domain agnostic representations. They together control the information that has to flow through further fully connected output layer after max pooling. The effectiveness of gated architectures rely on the idea of training a gate with sole purpose of identifying a weightage. In the task of sentiment analysis this weightage corresponds to what weights will lead to a decrement in final loss or in other words, most accurate prediction of sentiment. In doing so, the gate architecture learns which words or n-grams contribute to the sentiment the most, these words or n-grams often co-relate with domain independent words. On the other hand the gate gives less weightage to n-grams which are largely either specific to domain or function word chunks which contribute negligible to the overall sentiment. This is what makes gated architectures effective at Domain Adaptation. We see that gated architectures almost always outperform recurrent, attention and linear models BoW, TFIDF, PV. This is largely because while training and testing on same domains, these models especially recurrent and attention based may perform better. However, for Domain Adaptation, as they lack gated structure which is trained in parallel to learn importance, their performance on target domain is poor as compared to gated architectures. As gated architectures are based on convolutions, they exploit parallelization to give significant boost in time complexity as compared to other models.  \n Question: Are there conceptual benefits to using GCNs over more complex architectures like attention?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-9c274abddacb42acbfedb4cb7df62165",
            "input": "We now consider the following problem: given two BabelNet categories $A$ and $B$, predict whether they are likely to be conceptual neighbors based on the sentences from a text corpus in which they are both mentioned. To train such a classifier, we use the distant supervision labels from Section SECREF8 as training data. Once this classifier has been trained, we can then use it to predict conceptual neighborhood for categories for which only few instances are known. To find sentences in which both $A$ and $B$ are mentioned, we rely on a disambiguated text corpus in which mentions of BabelNet categories are explicitly tagged. Such a disambiguated corpus can be automatically constructed, using methods such as the one proposed by BIBREF30 mancini-etal-2017-embedding, for instance. For each pair of candidate categories, we thus retrieve all sentences where they co-occur. Next, we represent each extracted sentence as a vector.  \n Question: How they indentify conceptual neighbours?",
            "output": [
                "Once this classifier has been trained, we can then use it to predict conceptual neighborhood for categories for which only few instances are known."
            ]
        },
        {
            "id": "task460-edb2e1e2c419415e81bb7b09fc37b8d1",
            "input": "We compared our models with the following state-of-the-art baselines:\n\nSequence to Sequence (Seq2Seq): A simple encoder-decoder model which concatenates four sentences to a long sentence with an attention mechanism BIBREF31 .\n\nHierarchical LSTM (HLSTM): The story context is represented by a hierarchical LSTM: a word-level LSTM for each sentence and a sentence-level LSTM connecting the four sentences BIBREF29 . A hierarchical attention mechanism is applied, which attends to the states of the two LSTMs respectively.\n\nHLSTM+Copy: The copy mechanism BIBREF32 is applied to hierarchical states to copy the words in the story context for generation.\n\nHLSTM+Graph Attention(GA): We applied multi-source attention HLSTM where commonsense knowledge is encoded by graph attention.\n\nHLSTM+Contextual Attention(CA): Contextual attention is applied to represent commonsense knowledge. \n Question: Did they compare to Transformer based large language models?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-9b239ec1712c4332aea7cb629b7860be",
            "input": "The improvement over all benchmark datasets also shows that conditional BERT is a general augmentation method for multi-labels sentence classification tasks. \n Question: Does the new objective perform better than the original objective bert is trained on?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-2d8b64c04337499ab1273d835340ef72",
            "input": "Using the distribution of the individual words in a category, we can compile distributions for the entire category, and therefore generate maps for these word categories.  \n Question: Do they build a model to automatically detect demographic, lingustic or psycological dimensons of people?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-2659d6bc273a48a9813693b92b3d0ee1",
            "input": "We use the existing large-scale antonym and synonym pairs previously used by Nguyen:16. Originally, the data pairs were collected from WordNet BIBREF9 and Wordnik. \n Question: What dataset do they use to evaluate their method?",
            "output": [
                "antonym and synonym pairs collected from WordNet BIBREF9 and Wordnik"
            ]
        },
        {
            "id": "task460-7e49fe522bfc4f449d169198628ee8c3",
            "input": "An extrinsic evaluation was carried out on the task of Open IE BIBREF7. \n Question: Is the semantic hierarchy representation used for any task?",
            "output": [
                "Yes, Open IE"
            ]
        },
        {
            "id": "task460-87ab4a1d9d274f4f9c40c3282d5f90ae",
            "input": "Since those sets are expected to reflect social norms, they are referred as Dos and Don'ts hereafter. Analogously, some of the negative words just describe inappropriate behaviour, like slur or misdeal, whereas others are real crimes as murder. \n Question: Do they report results only on English data?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-c108894328e4406fa1b15c9925c55885",
            "input": "For sentiment classification, we systematically study the effect of character-level adversarial attacks on two architectures and four different input formats.  We also consider the task of paraphrase detection. \n Question: What end tasks do they evaluate on?",
            "output": [
                "Sentiment analysis and paraphrase detection under adversarial attacks"
            ]
        },
        {
            "id": "task460-0d444f2357914681a6b6da539aeda91b",
            "input": "We show that by determining and integrating heterogeneous set of features from different modalities – aesthetic features from posted images (colorfulness, hue variance, sharpness, brightness, blurriness, naturalness), choice of profile picture (for gender, age, and facial expression), the screen name, the language features from both textual content and profile's description (n-gram, emotion, sentiment), and finally sociability from ego-network, and user engagement – we can reliably detect likely depressed individuals in a data set of 8,770 human-annotated Twitter users. \n Question: What is the source of the visual data? ",
            "output": [
                "Profile pictures from the Twitter users' profiles."
            ]
        },
        {
            "id": "task460-2057fce83ad047beb8aba30a1ed3fae0",
            "input": "Both systems were optimized on the tst2014 using Minimum error rate training BIBREF20 . A detailed description of the systems can be found in BIBREF21 . \n Question: How is the PBMT system trained?",
            "output": [
                "systems were optimized on the tst2014 using Minimum error rate training BIBREF20"
            ]
        },
        {
            "id": "task460-02012669ac5043adaf361f3761dab1f2",
            "input": "We asked for two distinct paraphrases of each sentence because we believe that a good sentence embedding should put paraphrases close together in vector space.\n\nSeveral modification types were specifically selected to constitute a thorough test of embeddings. In different meaning, the annotators should create a sentence with some other meaning using the same words as the original sentence. Other transformations which should be difficult for embeddings include minimal change, in which the sentence meaning should be significantly changed by using only very small modification, or nonsense, in which words of the source sentence should be shuffled so that it is grammatically correct, but without any sense. \n Question: What annotations are available in the dataset?",
            "output": [
                "For each source sentence, transformation sentences that are transformed according to some criteria (paraphrase, minimal change etc.)"
            ]
        },
        {
            "id": "task460-973173880ff744b7aa42dab6e38a7c76",
            "input": "The results of multilingual training in which the modeling unit is syllables are presented in Table 5. All error rates are the weighted averages of all evaluated speakers. Here, `+ both' represents the result of training with both JNAS and WSJ corpora. The multilingual training is effective in the speaker-open setting, providing a relative WER improvement of 10%. The JNAS corpus was more helpful than the WSJ corpus because of the similarities between Ainu and Japanese language. \n Question: How big are improvements with multilingual ASR training vs single language training?",
            "output": [
                "relative WER improvement of 10%."
            ]
        },
        {
            "id": "task460-249d51732a7b44e1bfc5b5a936e2846f",
            "input": "We compare our approach with several methods BIBREF1 , BIBREF31 , BIBREF11 , BIBREF8 , BIBREF10 , BIBREF39 in two cross-domain settings. Using string kernels, Giménez-Pérez et al. BIBREF10 reported better performance than SST BIBREF31 and KE-Meta BIBREF11 in the multi-source domain setting. In addition, we compare our approach with SFA BIBREF1 , CORAL BIBREF8 and TR-TrAdaBoost BIBREF39 in the single-source setting. Transductive string kernels. We present a simple and straightforward approach to produce a transductive similarity measure suitable for strings. Our transductive kernel classifier (TKC) approach is composed of two learning iterations.  \n Question: What machine learning algorithms are used?",
            "output": [
                "string kernels SST KE-Meta SFA CORAL TR-TrAdaBoost Transductive string kernels transductive kernel classifier"
            ]
        },
        {
            "id": "task460-d994e644a9564590a7c1e6ded36d0dee",
            "input": "Results in Table TABREF13 show ten probes. Again, we see the performance of baseline ELMo-transformer and mSynC are similar, with mSynC doing slightly worse on 7 out of 9 tasks. As we would expect, on the probe for predicting chunk tags, mSynC achieves 96.9 $F_1$ vs. 92.2 $F_1$ for ELMo-transformer, indicating that mSynC is indeed encoding shallow syntax. Overall, the results further confirm that explicit shallow syntax does not offer any benefits over ELMo-transformer. \n Question: What are improvements for these two approaches relative to ELMo-only baselines?",
            "output": [
                "only modest gains on three of the four downstream tasks"
            ]
        },
        {
            "id": "task460-158c740380d94bf399cb0679b8728955",
            "input": "For the purpose of our experiments, we use data released by the APDA shared task organizers. The dataset is divided into train and test by organizers. The training set is distributed with labels for the three tasks of age, dialect, and gender. \n Question: What are the three datasets used in the paper?",
            "output": [
                "Data released for APDA shared task contains 3 datasets."
            ]
        },
        {
            "id": "task460-9fb10cf8816246f8bba31b09d47ed9c6",
            "input": "Therefore, there are 44 tasks (22 INLINEFORM10 2) in total. In addition, there are 2 human summaries for each task. We selected three competitive systems (SumBasic, ILP, and ILP+MC) and therefore we have 3 system-system pairs (ILP+MC vs. ILP, ILP+MC vs. SumBasic, and ILP vs. SumBasic) for each task and each human summary. \n Question: Do they build one model per topic or on all topics?",
            "output": [
                "One model per topic."
            ]
        },
        {
            "id": "task460-0c56ef657f6e49569230453b8bb81938",
            "input": "Our goal is to compare the efectiviness of the proposed approach against the baseline system described in BIBREF13 . We evaluate the false-reject (FR) and false-accept (FA) tradeoff across several end-to-end models of distinct sizes and computational complexities. As can be seen in the Receiver Operating Characteristic (ROC) curves in Figure FIGREF14 , the 2 largest end-to-end models, with 2-stage training, significantly outperform the recognition quality of the much larger and complex Baseline_1850K system. More specifically, E2E_318K_2stage and E2E_700K_2stage show up to 60% relative FR rate reduction over Baseline_1850K in most test conditions.  \n Question: How do they measure the quality of detection?",
            "output": [
                "We evaluate the false-reject (FR) and false-accept (FA) tradeoff across several end-to-end models of distinct sizes and computational complexities."
            ]
        },
        {
            "id": "task460-cc1746049c31496bbe2c5badaaf3e404",
            "input": "Table TABREF23 shows the Pearson, Spearman and Kendall correlation of Rouge and Sera, with pyramid scores. Interestingly, we observe that many variants of Rouge scores do not have high correlations with human pyramid scores. The lowest F-score correlations are for Rouge-1 and Rouge-L (with INLINEFORM0 =0.454). Weak correlation of Rouge-1 shows that matching unigrams between the candidate summary and gold summaries is not accurate in quantifying the quality of the summary. \n Question: What different correlations result when using different variants of ROUGE scores?",
            "output": [
                "we observe that many variants of Rouge scores do not have high correlations with human pyramid scores"
            ]
        },
        {
            "id": "task460-f143961d971b4c9ab670ef126450fb3c",
            "input": "In this section, we describe the experimental study. We fit the sefe model on three datasets and compare it against the efe BIBREF10 . Our quantitative results show that sharing the context vectors provides better results, and that amortization and hierarchical structure give further improvements. Data. We apply the sefe on three datasets: ArXiv papers, U.S. Senate speeches, and purchases on supermarket grocery shopping data. We describe these datasets below, and we provide a summary of the datasets in Table TABREF17 . ArXiv papers: This dataset contains the abstracts of papers published on the ArXiv under the 19 different tags between April 2007 and June 2015. We treat each tag as a group and fit sefe with the goal of uncovering which words have the strongest shift in usage. We split the abstracts into training, validation, and test sets, with proportions of INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 , respectively.\n\nSenate speeches: This dataset contains U.S. Senate speeches from 1994 to mid 2009. In contrast to the ArXiv collection, it is a transcript of spoken language. We group the data into state of origin of the speaker and his or her party affiliation. Only affiliations with the Republican and Democratic Party are considered. As a result, there are 83 groups (Republicans from Alabama, Democrats from Alabama, Republicans from Arkansas, etc.). Some of the state/party combinations are not available in the data, as some of the 50 states have only had Senators with the same party affiliation. We split the speeches into training ( INLINEFORM0 ), validation ( INLINEFORM1 ), and testing ( INLINEFORM2 ).\n\nGrocery shopping data: This dataset contains the purchases of INLINEFORM0 customers. The data covers a period of 97 weeks. After removing low-frequency items, the data contains INLINEFORM1 unique items at the 1.10upc (Universal Product Code) level. We split the data into a training, test, and validation sets, with proportions of INLINEFORM2 , INLINEFORM3 , and INLINEFORM4 , respectively. The training data contains INLINEFORM5 shopping trips and INLINEFORM6 purchases in total. For the text corpora, we fix the vocabulary to the 15k most frequent terms and remove all words that are not in the vocabulary. Following BIBREF2 , we additionally remove each word with probability INLINEFORM0 , where INLINEFORM1 is the word frequency. This downsamples especially the frequent words and speeds up training. (Sizes reported in Table TABREF17 are the number of words remaining after preprocessing.) Models. Our goal is to fit the sefe model on these datasets. For the text data, we use the Bernoulli distribution as the conditional exponential family, while for the shopping data we use the Poisson distribution, which is more appropriate for count data. On each dataset, we compare four approaches based on sefe with two efe BIBREF10 baselines. All are fit using sgd BIBREF34 . In particular, we compare the following methods: \n Question: What experiments are used to demonstrate the benefits of this approach?",
            "output": [
                "On each dataset, we compare four approaches based on sefe with two efe BIBREF10 baselines. All are fit using sgd BIBREF34 . In particular, we compare the following methods:"
            ]
        },
        {
            "id": "task460-39cc4171996b4fc38d96c9e586a20972",
            "input": "We consider three benchmark datasets: ( INLINEFORM0 ) Cora, a paper citation network with text information, built by BIBREF44 .  Hepth, a paper citation network from Arxiv on high energy physics theory, with paper abstracts as text information. ( INLINEFORM2 ) Zhihu, a Q&A network dataset constructed by BIBREF9 , which has 10,000 active users with text descriptions and their collaboration links.  \n Question: Which dataset of texts do they use?",
            "output": [
                "Cora Hepth Zhihu"
            ]
        },
        {
            "id": "task460-631211e60440408295983a4d469171cf",
            "input": "We choose the state-of-the-art Transformer BIBREF1 model and the conventional RNN-Search model BIBREF0 as our test bed. \n Question: Which model architectures do they test their word importance approach on?",
            "output": [
                " Transformer BIBREF1 model and the conventional RNN-Search model BIBREF0"
            ]
        },
        {
            "id": "task460-c56fde1da0d446afb0bd3a0d20e981b2",
            "input": "In particular, we perform an extensive set of experiments on standard benchmarks for bilingual dictionary induction and monolingual and cross-lingual word similarity, as well as on an extrinsic task: cross-lingual hypernym discovery. For both the monolingual and cross-lingual settings, we can notice that our models generally outperform the corresponding baselines.  As can be seen in Table 1 , our refinement method consistently improves over the baselines (i.e., VecMap and MUSE) on all language pairs and metrics. First and foremost, in terms of model-wise comparisons, we observe that our proposed alterations of both VecMap and MUSE improve their quality in a consistent manner, across most metrics and data configurations. \n Question: What are the tasks that this method has shown improvements?",
            "output": [
                "bilingual dictionary induction, monolingual and cross-lingual word similarity, and cross-lingual hypernym discovery"
            ]
        },
        {
            "id": "task460-f8bbc53c642a46d7a75d685082cb59d9",
            "input": "Bengali, Gujarati, Marathi, Malayalam and Tamil are the primary source languages, and translation from these to Hindi constitute the child tasks.  \n Question: On how many language pairs do they show that preordering assisting language sentences helps translation quality?",
            "output": [
                "5"
            ]
        },
        {
            "id": "task460-d25962793b5240f4b53835d846bdca0b",
            "input": "We focus primarily on the smaller data set of 645 hand-labeled articles provided to task participants, both for training and for validation.  We pre-trained BERT-base on the 600,000 articles without labels by using the same Cloze task BIBREF5 that BERT had originally used for pre-training.  \n Question: How long is the dataset?",
            "output": [
                "645, 600000"
            ]
        },
        {
            "id": "task460-04d1164bbf49457aa6312109dc4294e5",
            "input": "Inspired by the psycholinguistic studies BIBREF5 , BIBREF4 , BIBREF7 proposed a semi-character based RNN (ScRNN) that processes a sentence of words with misspelled characters, predicting the correct words at each step. Let $s = \\lbrace w_1, w_2, \\dots , w_n\\rbrace $ denote the input sentence, a sequence of constituent words $w_i$ . Each input word ( $w_i$ ) is represented by concatenating (i) a one hot vector of the first character ( $\\mathbf {w_{i1}}$ ); (ii) a one hot representation of the last character ( $\\mathbf {w_{il}}$ , where $l$ is the length of word $w_i$ ); and (iii) a bag of characters representation of the internal characters ( $\\sum _{j=2}^{l-1}\\mathbf {w_{ij}})$ . ScRNN treats the first and the last characters individually, and is agnostic to the ordering of the internal characters. Each word, represented accordingly, is then fed into a BiLSTM cell. At each sequence step, the training target is the correct corresponding word (output dimension equal to vocabulary size), and the model is optimized with cross-entropy loss. \n Question: What is a semicharacter architecture?",
            "output": [
                "A semi-character based RNN (ScRNN) treats the first and last characters individually, and is agnostic to the ordering of the internal characters"
            ]
        },
        {
            "id": "task460-3b8af46baf624b93a941c727ee872e6d",
            "input": "Besides the Baseline and Oracle, where only ASR 1-best hypothesis is considered, we also perform experiments to utilize ASR $n$-best hypotheses during evaluation. \n Question: What are the series of simple models?",
            "output": [
                "perform experiments to utilize ASR $n$-best hypotheses during evaluation"
            ]
        },
        {
            "id": "task460-22dad0b7a0f94037bae02ae764d733b6",
            "input": "Experimental Setup \n Question: what english datasets were used?",
            "output": [
                "Answer with content missing: (Data section) Penn Treebank (PTB)"
            ]
        },
        {
            "id": "task460-d4992057789c4ff98cf0ffd9be234849",
            "input": "To identify new keywords in the selected microposts, we again leverage crowdsourcing, as humans are typically better than machines at providing specific explanations BIBREF18, BIBREF19. In the crowdsourcing task, workers are first asked to find those microposts where the model predictions are deemed correct. Then, from those microposts, workers are asked to find the keyword that best indicates the class of the microposts as predicted by the model. \n Question: How is the keyword specific expectation elicited from the crowd?",
            "output": [
                "workers are first asked to find those microposts where the model predictions are deemed correct  asked to find the keyword that best indicates the class of the microposts"
            ]
        },
        {
            "id": "task460-5faba20e1c5348459fdd224782f1a3a6",
            "input": "We collect human judgments on Amazon Mechanical Turk via ParlAI BIBREF18. \n Question: Do they use crowdsourcing to collect human judgements?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-7cba36fe07844b3696924c926922ee56",
            "input": "Generic Sarcasm. We first examine the different patterns learned on the Gen dataset.  We observe that the not-sarcastic patterns appear to capture technical and scientific language, while the sarcastic patterns tend to capture subjective language that is not topic-specific. We observe an abundance of adjective and adverb patterns for the sarcastic class, although we do not use adjective and adverb patterns in our regex retrieval method. Rhetorical Questions. We notice that while the not-sarcastic patterns generated for RQs are similar to the topic-specific not-sarcastic patterns we find in the general dataset, there are some interesting features of the sarcastic patterns that are more unique to the RQs.\n\nMany of our sarcastic questions focus specifically on attacks on the mental abilities of the addressee. This generalization is made clear when we extract and analyze the verb, subject, and object arguments using the Stanford dependency parser BIBREF32 for the questions in the RQ dataset. Hyperbole. One common pattern for hyperbole involves adverbs and adjectives, as noted above. We did not use this pattern to retrieve hyperbole, but because each hyperbolic sarcastic utterance contains multiple cues, we learn an expanded class of patterns for hyperbole.  We learn a number of verbal patterns that we had not previously associated with hyperbole, as shown in Table TABREF34 . Interestingly, many of these instantiate the observations of CanoMora2009 on hyperbole and its related semantic fields: creating contrast by exclusion, e.g. no limit and no way, or by expanding a predicated class, e.g. everyone knows. Many of them are also contrastive.  \n Question: What are the linguistic differences between each class?",
            "output": [
                "Each class has different patterns in adjectives, adverbs and verbs for sarcastic and non-sarcastic classes"
            ]
        },
        {
            "id": "task460-06570ebc23e1445ba7ce3ff97c475628",
            "input": "We use the VisDial v1.0 BIBREF0 dataset to train our models, where one example has an image with its caption, 9 question-answer pairs, and follow-up questions and candidate answers for each round. At round $r$, the caption and the previous question-answer pairs become conversational context. The whole dataset is split into 123,287/2,000/8,000 images for train/validation/test, respectively. Unlike the images in the train and validation sets, the images in the test set have only one follow-up question and candidate answers and their corresponding conversational context. \n Question: How big is dataset for this challenge?",
            "output": [
                "133,287 images"
            ]
        },
        {
            "id": "task460-4711feb82e8248d9b96a3dbdc1cf11cf",
            "input": "We evaluated our model on three benchmark datasets, namely the CNN/DailyMail news highlights dataset BIBREF24, the New York Times Annotated Corpus (NYT; BIBREF25), and XSum BIBREF22. \n Question: What are the datasets used for evaluation?",
            "output": [
                "CNN/DailyMail news highlights New York Times Annotated Corpus XSum"
            ]
        },
        {
            "id": "task460-794f0e6a6cc94f619a76518a0e8e08e1",
            "input": "We download 210 users' all twitter posts who are war veterans and clinically diagnosed with PTSD sufferers as well which resulted a total 12,385 tweets.  \n Question: How many twitter users are surveyed using the clinically validated survey?",
            "output": [
                "210"
            ]
        },
        {
            "id": "task460-75bc749480b847bb9cabdf2a2466b286",
            "input": "An acoustic word embedding is a function that takes as input a speech segment corresponding to a word, INLINEFORM0 , where each INLINEFORM1 is a vector of frame-level acoustic features, and outputs a fixed-dimensional vector representing the segment, INLINEFORM2 . \n Question: How do they represent input features of their model to train embeddings?",
            "output": [
                "a vector of frame-level acoustic features"
            ]
        },
        {
            "id": "task460-fc765b8453a548d09aeadf3c1fe1dd04",
            "input": "In this work, the metrics detailed below are proposed and we evaluate their quality through a human evaluation in subsection SECREF32. In addition to the automatic metrics, we proceeded to a human evaluation. We chose to use the data from our SQuAD-based experiments in order to also to measure the effectiveness of the proposed approach to derive Curiosity-driven QG data from a standard, non-conversational, QA dataset. We randomly sampled 50 samples from the test set. Three professional English speakers were asked to evaluate the questions generated by: humans (i.e. the reference questions), and models trained using pre-training (PT) or (RL), and all combinations of those methods.\n\nBefore submitting the samples for human evaluation, the questions were shuffled. Ratings were collected on a 1-to-5 likert scale, to measure to what extent the generated questions were: answerable by looking at their context; grammatically correct; how much external knowledge is required to answer; relevant to their context; and, semantically sound. The results of the human evaluation are reported in Table TABREF33. \n Question: How they evaluate quality of generated output?",
            "output": [
                "Through human evaluation where they are asked to evaluate the generated output on a likert scale."
            ]
        },
        {
            "id": "task460-22b91a5c03f3486cbfd22400723ede67",
            "input": "As Garimella et al. BIBREF23 have made their code public , we reproduced their best method Randomwalk on our datasets and measured the AUC ROC, obtaining a score of 0.935. An interesting finding was that their method had a poor performance over their own datasets. This was due to the fact (already explained in Section SECREF4) that it was not possible to retrieve the complete discussions, moreover, in no case could we restore more than 50% of the tweets. So we decided to remove these discussions and measure again the AUC ROC of this method, obtaining a 0.99 value. Our hypothesis is that the performance of that method was seriously hurt by the incompleteness of the data. We also tested our method on these datasets, obtaining a 0.99 AUC ROC with Walktrap and 0.989 with Louvain clustering. \n Question: What are the state of the art measures?",
            "output": [
                "Randomwalk Walktrap Louvain clustering"
            ]
        },
        {
            "id": "task460-d6c86e69d06b488ea002bd2daad241db",
            "input": "The encoder is a bidirectional Long-Short Term Memory(LSTM) cell BIBREF14 and the decoder a single LSTM cell with attention mechanism.  \n Question: How many attention layers are there in their model?",
            "output": [
                "one"
            ]
        },
        {
            "id": "task460-90a0d37e0cea4793830b8b1368fb5323",
            "input": "For model distillation BIBREF6 , we extract sentences from Wikipedia in languages for which public multilingual is pretrained. For each sentence, we use the open-source BERT wordpiece tokenizer BIBREF4 , BIBREF1 and compute cross-entropy loss for each wordpiece: INLINEFORM0\n\nwhere INLINEFORM0 is the cross-entropy function, INLINEFORM1 is the softmax function, INLINEFORM2 is the BERT model's logit of the current wordpiece, INLINEFORM3 is the small BERT model's logits and INLINEFORM4 is a temperature hyperparameter, explained in Section SECREF11 .\n\nTo train the distilled multilingual model mMiniBERT, we first use the distillation loss above to train the student from scratch using the teacher's logits on unlabeled data. Afterwards, we finetune the student model on the labeled data the teacher is trained on. \n Question: How do they compress the model?",
            "output": [
                "we extract sentences from Wikipedia in languages for which public multilingual is pretrained. For each sentence, we use the open-source BERT wordpiece tokenizer BIBREF4 , BIBREF1 and compute cross-entropy loss for each wordpiece: INLINEFORM0"
            ]
        },
        {
            "id": "task460-6823020d0049487d88115b825dfbd423",
            "input": "Our idea is the BQ generation for MQ and, at the same time, we only want the minimum number of BQ to represent the MQ, so modeling our problem as $LASSO$ optimization problem is an appropriate way \n Question: What they formulate the question generation as?",
            "output": [
                "LASSO optimization problem"
            ]
        },
        {
            "id": "task460-ec538d8e946e4ecdbabcf410d7f919c8",
            "input": "BIBREF12 found in a Chinese corpus that the word label \"End\" has a better performance than \"Begin\". This motivates us to carry out a backward greedy search over each sentence's label sequence to identify word boundaries. If two words segmented in a sentence are identified as nouns, and one word is immediately before the other, we assemble their boundaries, creating a new word candidate for entity recognition. This strategy has the advantage to find named entities with long word length. It also reduces the influence caused by different segmentation criteria. \n Question: What boundary assembling method is used?",
            "output": [
                "This motivates us to carry out a backward greedy search over each sentence's label sequence to identify word boundaries. If two words segmented in a sentence are identified as nouns, and one word is immediately before the other, we assemble their boundaries, creating a new word candidate for entity recognition."
            ]
        },
        {
            "id": "task460-7e6104e60879455e89c8fb69846001e8",
            "input": " In addition to EIN, we created a model (Emotion-based Model) that uses emotional features only and compare it to two baselines. Our aim is to investigate if the emotional features independently can detect false news. The two baselines of this model are Majority Class baseline (MC) and the Random selection baseline (RAN). \n Question: What is the baseline?",
            "output": [
                "Majority Class baseline (MC)  Random selection baseline (RAN)"
            ]
        },
        {
            "id": "task460-a9848b48d9c84fe2af5d1d54c1a212c1",
            "input": "As explained in Section SECREF15 , the corruption introduced in Doc2VecC acts as a data-dependent regularization that suppresses the embeddings of frequent but uninformative words. In contrast, Doc2VecC manages to clamp down the representation of words frequently appear in the training set, but are uninformative, such as symbols and stop words. \n Question: How do they determine which words are informative?",
            "output": [
                "Informative are those that will not be suppressed by regularization performed."
            ]
        },
        {
            "id": "task460-32106d83e62e4a8abfc039574b9982f4",
            "input": "We adopt the mixture of two multilingual translation corpus as our training data: MultiUN BIBREF20 and OpenSubtitles BIBREF21. MultiUN consists of 463,406 official documents in six languages, containing around 300M words for each language. OpenSubtitles is a corpus consisting of movie and TV subtitles, which contains 2.6B sentences over 60 languages. We select four shared languages of the two corpora: English, Spanish, Russian and Chinese. Statistics of the training corpus are shown in Table TABREF14. \n Question: What multilingual parallel data is used for training proposed model?",
            "output": [
                "MultiUN BIBREF20 OpenSubtitles BIBREF21"
            ]
        },
        {
            "id": "task460-e75b4e854e0c4ad9b4df419fb8d81061",
            "input": "Our conversational agent uses two architectures to simulate a specialized reminiscence therapist. The block in charge of generating questions is based on the work Show, Attend and Tell BIBREF13. This work generates descriptions from pictures, also known as image captioning. \n Question: Is machine learning system underneath similar to image caption ML systems?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-7bf9db961df6431c92653ed4e5103c38",
            "input": " 2) Answers are formulated by domain experts with legal training.  \n Question: Are the experts comparable to real-world users?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-41dbd8cac4214aa2956f50a59940ae24",
            "input": "These features are typically obtained by training a deep neural network jointly on several languages for which labelled data is available. The bottom layers of the network are normally shared across all training languages. The network then splits into separate parts for each of the languages, or has a single shared output. The final output layer has phone labels or HMM states as targets. The final shared layer often has a lower dimensionality than the input layer, and is therefore referred to as a `bottleneck'. \n Question: What are bottleneck features?",
            "output": [
                "Bulgarian, Czech, French, German, Korean, Polish, Portuguese, Russian, Thai, Vietnamese South African English These features are typically obtained by training a deep neural network jointly on several languages for which labelled data is available. The final shared layer often has a lower dimensionality than the input layer, and is therefore referred to as a `bottleneck'."
            ]
        },
        {
            "id": "task460-5e572077529243579342ae947d037d53",
            "input": "For the first task, the human raters are asked to evaluate these outputs on the first three aspects, relevance, attractiveness, and language fluency on a Likert scale from 1 to 10 (integer values). For attractiveness, annotators are asked how attractive the headlines are. \n Question: How is attraction score measured?",
            "output": [
                "annotators are asked how attractive the headlines are Likert scale from 1 to 10 (integer values)"
            ]
        },
        {
            "id": "task460-c4328a714449421eb8900875d188b34b",
            "input": "We have observed that the conflict model is very sensitive to even minor differences and compensates in such cases where attention poses high bias towards similarities already there in the sequences.\n\nSequence 1: What are the best ways to learn French ?\n\nSequence 2: How do I learn french genders ?\n\nAttention only: 1\n\nAttention+Conflict: 0\n\nGround Truth: 0\n\nSequence 1: How do I prevent breast cancer ?\n\nSequence 2: Is breast cancer preventable ?\n\nAttention only: 1\n\nAttention+Conflict: 0\n\nGround Truth: 0\n\nWe provide two examples with predictions from the models with only attention and combination of attention and conflict. Each example is accompanied by the ground truth in our data. \n Question: Do they show on which examples how conflict works better than attention?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-ae8395609a3a4e30a114e2d309db602b",
            "input": "As one of our aims is to compare coreference chain properties in automatic translation with those of the source texts and human reference, we derive data from ParCorFull, an English-German corpus annotated with full coreference chains BIBREF46. \n Question: What languages are seen in the news and TED datasets?",
            "output": [
                "English German"
            ]
        },
        {
            "id": "task460-1d3fd802cd054e9a84689bf416389733",
            "input": "the basic model yields good performance for recognizing explicit discourse relations as well, which is comparable with previous best result (92.05% macro F1-score and 93.09% accuracy as reported in BIBREF11 ). After untying parameters in the softmax prediction layer, implicit discourse relation classification performance was improved across all four relations, meanwhile, the explicit discourse relation classification performance was also improved. Then we also created ensemble models by applying majority voting to combine results of ten runs. From table 5 , each ensemble model obtains performance improvements compared with single model. The full model achieves performance boosting of (51.84 - 48.82 = 3.02) and (94.17 - 93.21 = 0.96) in macro F1-scores for predicting implicit and explicit discourse relations respectively.  \n Question: What discourse relations does it work best/worst for?",
            "output": [
                "explicit discourse relations"
            ]
        },
        {
            "id": "task460-a785980d07d34e548c5bdb2559a16528",
            "input": "With a single text, we were already able to predict the electricity consumption with a relative error of less than 5% for both data sets. \n Question: How accurate is model trained on text exclusively?",
            "output": [
                "Relative error is less than 5%"
            ]
        },
        {
            "id": "task460-fcea938a064c4272b34a12e42c79e434",
            "input": "Since the input of our method is textual data, we follow the approach of BIBREF15 and map the text into a fixed-size vector representation. To this end, we use word embeddings that were successfully applied in other domains. We follow BIBREF5 and use pre-trained GloVe word vectors BIBREF16 to initialize the embedding layer (also known as look-up table). Section SECREF18 discusses the embedding layer in more details. \n Question: Which pretrained word vectors did they use?",
            "output": [
                " pre-trained GloVe word vectors "
            ]
        },
        {
            "id": "task460-59ff1279e9e84209bbcf389a29cd2fe7",
            "input": "We performed experiments on synthetically generated dataset since it gives us a better control over the distribution of the data. Specifically we compared the gains obtained using our approach versus the variance of the distribution. We created dataset from the following generative process. [H] Generative Process [1] Generate data\n\nPick k points INLINEFORM0 as domain -1 means and a corresponding set of k points INLINEFORM1 as domain-2 means, and covariance matrices INLINEFORM2\n\niter INLINEFORM0 upto num INLINEFORM1 samples Sample class INLINEFORM2 Sample INLINEFORM3 Sample INLINEFORM4 Add q and a so sampled to the list of q,a pairs We generated the dataset from the above sampling process with means selected on a 2 dimensional grid of size INLINEFORM5 with variance set as INLINEFORM6 in each dimension.10000 sample points were generated. The parameter INLINEFORM7 of the above algorithm was set to 0.5 and k was set to 9 (since the points could be generated from one of the 9 gaussians with centroids on a INLINEFORM8 grid). \n Question: How do they generate the synthetic dataset?",
            "output": [
                "using generative process"
            ]
        },
        {
            "id": "task460-3583fab1649d4482883703ed0e1e39b1",
            "input": "The first classifier model acts as a filter for the second stage of classification. We use both SVM and NB to compare the results and choose SVM later for stage one classification model, owing to a better F-score. The training is performed on tweets labeled with classes , and based on unigrams as features. We create word vectors of strings in the tweet using a filter available in the WEKA API BIBREF9 , and perform cross validation using standard classification techniques. \n Question: What classifier is used for emergency detection?",
            "output": [
                "SVM"
            ]
        },
        {
            "id": "task460-83054b9bbe794cf39d8fba5c22fd883b",
            "input": " In addition, we propose road exam as a new metric to reveal a model's robustness against exposure bias. \n Question: What is the road exam metric?",
            "output": [
                "a new metric to reveal a model's robustness against exposure bias"
            ]
        },
        {
            "id": "task460-3ccd3e82b5e9486dbb6836ad3e7cde87",
            "input": "Each tweet is annotated as no evidence of depression (e.g., “Citizens fear an economic depression\") or evidence of depression (e.g., “depressed over disappointment\"). If a tweet is annotated evidence of depression, then it is further annotated with one or more depressive symptoms, for example, depressed mood (e.g., “feeling down in the dumps\"), disturbed sleep (e.g., “another restless night\"), or fatigue or loss of energy (e.g., “the fatigue is unbearable\") BIBREF10 . \n Question: How is the dataset annotated?",
            "output": [
                "no evidence of depression depressed mood disturbed sleep fatigue or loss of energy"
            ]
        },
        {
            "id": "task460-82bd7e50c9664422bfbb6094284df069",
            "input": "The final result submitted in public leaderboard is 0.73019 and in private leaderboard is 0.58455. It is quite different in bad way. That maybe is the result of the model too overfit on train set tuning on public test set. \n Question: What is public dashboard?",
            "output": [
                "Public dashboard where competitors can see their results during competition, on part of the test set (public test set)."
            ]
        },
        {
            "id": "task460-259ab9ef5f0c417d921a15f37b0fa336",
            "input": "The Bi-LSTM model consists of a Bi-LSTM layer followed by a linear layer to extract input features. The Bi-LSTM layer has a 300-dimensional hidden state for each direction. For the final classification, an additional linear layer is added to output predicted class distributions. The BERT model is obtained by fine-tuning the pre-trained BERT embeddings on NER data with an additional untrained CRF classifier. We fine-tuned all the parameters of BERT including that of the CRF end-to-end. \n Question: What classifiers were used in experiments?",
            "output": [
                "Bi-LSTM BERT"
            ]
        },
        {
            "id": "task460-919db1e3faee463e841254ac045e9c39",
            "input": "he first, OpenIE 4 BIBREF5 , descends from two popular OIE systems OLLIE BIBREF10 and Reverb BIBREF10 . The second was MinIE BIBREF7 , which is reported as performing better than OLLIE, ClauseIE BIBREF9 and Stanford OIE BIBREF9 . \n Question: Which OpenIE systems were used?",
            "output": [
                "OpenIE4 and MiniIE"
            ]
        },
        {
            "id": "task460-3e38d5ea08e44de3a27e71c658b2ea48",
            "input": "The behavior model was implemented using an RNN with LSTM units and trained with the Couples Therapy Corpus. Out of the 33 behavioral codes included in the corpus we applied the behaviors Acceptance, Blame, Negativity, Positivity, and Sadness to train our models. This is motivated from previous works which showed good separability in these behaviors as well as being easy to interpret. The behavior model is pre-trained to identify the presence of each behavior from a sequence of words using a multi-label classification scheme. The pre-trained portion of the behavior model was implemented using a single layer RNN with LSTM units with dimension size 50. \n Question: How is module that analyzes behavioral state trained?",
            "output": [
                "pre-trained to identify the presence of behavior from a sequence of word using the Couples Therapy Corpus"
            ]
        },
        {
            "id": "task460-aae35ca1b023414eaecbea10d1f4058a",
            "input": "All the datasets consist of videos where only one speaker is in front of the camera. The descriptors we used for each of the modalities are as follows: Language All the datasets provide manual transcriptions. Vision Facet BIBREF26 is used to extract a set of features including per-frame basic and advanced emotions and facial action units as indicators of facial muscle movement. Acoustic We use COVAREP BIBREF27 to extract low level acoustic features including 12 Mel-frequency cepstral coefficients (MFCCs), pitch tracking and voiced/unvoiced segmenting features, glottal source parameters, peak slope parameters and maxima dispersion quotients. \n Question: What modalities are being used in different datasets?",
            "output": [
                "Language Vision Acoustic"
            ]
        },
        {
            "id": "task460-3a69f05926324bfe96582640c785dcac",
            "input": "The current state-of-the-art approach BIBREF14 , BIBREF15 uses maximum entropy and CRF models with a combination of language model and hand-crafted features to predict if each character in the hashtag is the beginning of a new word. \n Question: What current state of the art method was used for comparison?",
            "output": [
                "current state-of-the-art approach BIBREF14 , BIBREF15"
            ]
        },
        {
            "id": "task460-853fc2d92a7949d4967c6c54b1354b78",
            "input": "We conducted an experiment to determine whether we could maintain or improve classifier performances by applying the following three-tiered feature elimination approach:\n\nReduction We reduced the dataset encoded for each class by eliminating features that occur less than twice in the full dataset.\n\nSelection We iteratively applied Chi-Square feature selection on the reduced dataset, selecting the top percentile of highest ranked features in increments of 5 percent to train and test the support vector model using a linear kernel and 5-fold, stratified cross-validation.\n\nRank We cumulatively plotted the average F1-score performances of each incrementally added percentile of top ranked features. We report the percentile and count of features resulting in the first occurrence of the highest average F1-score for each class. \n Question: What are the three steps to feature elimination?",
            "output": [
                "Reduction Selection Rank"
            ]
        },
        {
            "id": "task460-1f15df3dabbe4b47a9ef8bf1826aa86e",
            "input": "WN18RR dataset consists of two kinds of relations: the symmetric relations such as $\\_similar\\_to$, which link entities in the category (b); other relations such as $\\_hypernym$ and $\\_member\\_meronym$, which link entities in the category (a). Actually, RotatE can model entities in the category (b) very well BIBREF7. However, HAKE gains a 0.021 higher MRR, a 2.4% higher H@1, and a 2.4% higher H@3 against RotatE, respectively. FB15k-237 dataset has more complex relation types and fewer entities, compared with WN18RR and YAGO3-10. Although there are relations that reflect hierarchy in FB15k-237, there are also lots of relations, such as “/location/location/time_zones” and “/film/film/prequel”, that do not lead to hierarchy. The characteristic of this dataset accounts for why our proposed models doesn't outperform the previous state-of-the-art as much as that of WN18RR and YAGO3-10 datasets. AGO3-10 datasets contains entities with high relation-specific indegree BIBREF18. For example, the link prediction task $(?, hasGender, male)$ has over 1000 true answers, which makes the task challenging. Fortunately, we can regard “male” as an entity at higher level of the hierarchy and the predicted head entities as entities at lower level. In this way, YAGO3-10 is a dataset that clearly has semantic hierarchy property, and we can expect that our proposed models is capable of working well on this dataset. Table TABREF19 validates our expectation. Both ModE and HAKE significantly outperform the previous state-of-the-art. Notably, HAKE gains a 0.050 higher MRR, 6.0% higher H@1 and 4.6% higher H@3 than RotatE, respectively. \n Question: How better does HAKE model peform than state-of-the-art methods?",
            "output": [
                "0.021 higher MRR, a 2.4% higher H@1, and a 2.4% higher H@3 against RotatE, respectively doesn't outperform the previous state-of-the-art as much as that of WN18RR and YAGO3-10 HAKE gains a 0.050 higher MRR, 6.0% higher H@1 and 4.6% higher H@3 than RotatE, respectively"
            ]
        },
        {
            "id": "task460-8391b26fd6b04791af449ba510fe957c",
            "input": "We then show the tweet text and image and we ask them to classify it in one of 6 categories: No attacks to any community, racist, sexist, homophobic, religion based attacks or attacks to other communities. \n Question: What annotations are available in the dataset - tweat used hate speach or not?",
            "output": [
                "No attacks to any community  racist sexist homophobic religion based attacks attacks to other communities"
            ]
        },
        {
            "id": "task460-9de644f308b9434e9e9b86b31ca28f54",
            "input": "this paper proposes to use the most relevant samples from the source dataset to train on the target dataset. One way to find the most similar samples is finding the pair-wise distance between all samples of the development set of the target dataset and source dataset. we propose using a clustering algorithm on the development set. The clustering algorithm used ihere is a hierarchical clustering algorithm. The cosine similarity is used as a criteria to cluster each question and answer. Therefore, these clusters are representative of the development set of the target dataset and the corresponding center for each cluster is representative of all the samples on that cluster. In the next step, the distance of each center is used to calculate the cosine similarity. Finally, the samples in the source dataset which are far from these centers are ignored. In other words, the outliers do not take part in transfer learning. \n Question: How do they transfer the model?",
            "output": [
                "In the MULT method, two datasets are simultaneously trained, and the weights are tuned based on the inputs which come from both datasets. The hyper-parameter $\\lambda \\in (0,1)$ is calculated based on a brute-force search or using general global search. This hyper parameter is used to calculate the final cost function which is computed from the combination of the cost function of the source dataset and the target datasets.  this paper proposes to use the most relevant samples from the source dataset to train on the target dataset. One way to find the most similar samples is finding the pair-wise distance between all samples of the development set of the target dataset and source dataset. we propose using a clustering algorithm on the development set. The clustering algorithm used ihere is a hierarchical clustering algorithm. The cosine similarity is used as a criteria to cluster each question and answer. Therefore, these clusters are representative of the development set of the target dataset and the corresponding center for each cluster is representative of all the samples on that cluster. In the next step, the distance of each center is used to calculate the cosine similarity. Finally, the samples in the source dataset which are far from these centers are ignored. In other words, the outliers do not take part in transfer learning."
            ]
        },
        {
            "id": "task460-ad5a184f15a54d509040eddca9cf8af4",
            "input": "A number of methods utilize an extra type of information as the observed features for entities, by either merging, concatenating, or averaging the entity and its features to compute its embeddings, such as numerical values BIBREF26 (we use KBLN from this work to compare it with our approach using only numerical as extra attributes), images BIBREF27 , BIBREF28 (we use IKRL from the first work to compare it with our approach using only images as extra attributes), text BIBREF29 , BIBREF30 , BIBREF31 , BIBREF32 , BIBREF33 , BIBREF34 , and a combination of text and image BIBREF35 . Further, BIBREF7 address the multilingual relation extraction task to attain a universal schema by considering raw text with no annotation as extra feature and using matrix factorization to jointly embed KB and textual relations BIBREF36 . In addition to treating the extra information as features, graph embedding approaches BIBREF37 , BIBREF38 consider observed attributes while encoding to achieve more accurate embeddings. \n Question: What other multimodal knowledge base embedding methods are there?",
            "output": [
                "merging, concatenating, or averaging the entity and its features to compute its embeddings graph embedding approaches matrix factorization to jointly embed KB and textual relations"
            ]
        },
        {
            "id": "task460-58610b0dafd04dfab4ee54cda317d0f3",
            "input": "To measure the effect of adding Bertram to BERT on downstream tasks, we apply the procedure described in Section SECREF4 to a commonly used textual entailment dataset as well as two text classification datasets: MNLI BIBREF21, AG's News BIBREF22 and DBPedia BIBREF23.  \n Question: What are three downstream task datasets?",
            "output": [
                "MNLI BIBREF21 AG's News BIBREF22 DBPedia BIBREF23"
            ]
        },
        {
            "id": "task460-6576432481de48f085c37220bd3a420f",
            "input": "We manually investigate the errors made by artificial neural networks for morphological inflection in a target language after pretraining on different source languages. For our qualitative analysis, we make use of the validation set. Therefore, we show validation set accuracies in Table TABREF19 for comparison. We manually annotate the outputs for the first 75 development examples for each source–target language combination. All found errors are categorized as belonging to one of the following categories. \n Question: How is the performance on the task evaluated?",
            "output": [
                "Comparison of test accuracies of neural network models on an inflection task and qualitative analysis of the errors"
            ]
        },
        {
            "id": "task460-97d0554055e540ce83049f5ea0a13d93",
            "input": "In this section we report on the experiments performed using the system and data described above. First we will present the English results for the three ABSA editions as well as a comparison with previous work. After that we will do the same for 5 additional languages included in the ABSA 2016 edition: Dutch, French, Russian, Spanish and Turkish. The local and clustering features, as described in Section SECREF11 , are the same for every language and evaluation setting. The only change is the clustering lexicons used for the different languages. As stated in section SECREF11 , the best cluster combination is chosen via 5-fold cross validation (CV) on the training data. We first try every permutation with the Clark and Word2vec clusters. Once the best combination is obtained, we then try with the Brown clusters obtaining thus the final model for each language and dataset. \n Question: Which six languages are experimented with?",
            "output": [
                "Dutch French Russian Spanish  Turkish English "
            ]
        },
        {
            "id": "task460-68e0d22972ce4514a8fa19adc914c220",
            "input": "First, we adapt a dataset extracted from Freebase, i.e., FB24K, which used by BIBREF26. Then, we collect extra entities and relations that from DBpedia which that they should have at least 100 mentions BIBREF7 and they could link to the entities in the FB24K by the sameAs triples. Finally, we build a datasets named as DBP24K. In addition, we build a game datasets from our game knowledge graph, named as Game30K. \n Question: What three datasets are used to measure performance?",
            "output": [
                "FB24K DBP24K Game30K"
            ]
        },
        {
            "id": "task460-03fcff7409c84b748d2ede1d80f93888",
            "input": "n-gram: We consider 3-gram and 4-gram conditional language model baseline with interpolation. We use random grid search for the best coefficients for the interpolated model.\n\nConvolution: We use the fconv architecture BIBREF24 and default hyperparameters from the fairseq BIBREF25 framework. We train the network with ADAM optimizer BIBREF26 with learning rate of 0.25 and dropout probability set to 0.2.\n\nLSTM: We consider LSTM models BIBREF27 with and without attention BIBREF28 and use the tensor2tensor BIBREF29 framework for the LSTM baselines. We use a two-layer LSTM network for both the encoder and the decoder with 128 dimensional hidden vectors.\n\nTransformer: As with LSTMs, we use the tensor2tensor framework for the Transformer model. Our Transformer BIBREF21 model uses 256 dimensions for both input embedding and hidden state, 2 layers and 4 attention heads. For both LSTMs and Transformer, we train the model with ADAM optimizer ($\\beta _{1} = 0.85$, $\\beta _{2} = 0.997$) and dropout probability set to 0.2.\n\nGPT-2: Apart from supervised seq2seq models, we also include results from pre-trained GPT-2 BIBREF30 containing 117M parameters. \n Question: What baseline models are offered?",
            "output": [
                "3-gram and 4-gram conditional language model Convolution LSTM models BIBREF27 with and without attention BIBREF28 Transformer GPT-2"
            ]
        },
        {
            "id": "task460-e6807a47d55a4290ba0111ea371ea3e8",
            "input": "While WSD is predominantly evaluated on English, we are also interested in evaluating our approach on Chinese, to evaluate the effectiveness of our approach in a different language. \n Question: Which language(s) are found in the WSD datasets?",
            "output": [
                " WSD is predominantly evaluated on English, we are also interested in evaluating our approach on Chinese"
            ]
        },
        {
            "id": "task460-a871296696ca4e4298cb4c3cd16804ea",
            "input": "Our framework includes three key modules: Retrieve, Fast Rerank, and BiSET. For each source article, Retrieve aims to return a few candidate templates from the training corpus. Then, the Fast Rerank module quickly identifies a best template from the candidates. Finally, BiSET mutually selects important information from the source article and the template to generate an enhanced article representation for summarization. This module starts with a standard information retrieval library to retrieve a small set of candidates for fine-grained filtering as cao2018retrieve. To do that, all non-alphabetic characters (e.g., dates) are removed to eliminate their influence on article matching. The retrieval process starts by querying the training corpus with a source article to find a few (5 to 30) related articles, the summaries of which will be treated as candidate templates. The above retrieval process is essentially based on superficial word matching and cannot measure the deep semantic relationship between two articles. Therefore, the Fast Rerank module is developed to identify a best template from the candidates based on their deep semantic relevance with the source article. We regard the candidate with highest relevance as the template. As illustrated in Figure FIGREF6 , this module consists of a Convolution Encoder Block, a Similarity Matrix and a Pooling Layer. \n Question: How are templates discovered from training data?",
            "output": [
                "For each source article, Retrieve aims to return a few candidate templates from the training corpus. Then, the Fast Rerank module quickly identifies a best template from the candidates."
            ]
        },
        {
            "id": "task460-f62c46b9bc19447e8a77055d470cdbd3",
            "input": "We thus adapt an off-the-shelf reward learning algorithm BIBREF7 to the supervised setting for automated data manipulation. \n Question: What off-the-shelf reward learning algorithm from RL for joint data manipulation learning and model training is adapted?",
            "output": [
                "BIBREF7"
            ]
        },
        {
            "id": "task460-cc20911a734244ea8f8e174b46abefc3",
            "input": "How the music taste of the audience of popular music changed in the last century? The trend lines of the MUSIC model features, reported in figure FIGREF12, reveal that audiences wanted products more and more contemporary, intense and a little bit novel or sophisticated, but less and less mellow and (surprisingly) unpretentious. In other words, the audiences of popular music are getting more demanding as the quality and variety of the music products increases. \n Question: What trends are found in musical preferences?",
            "output": [
                "audiences wanted products more and more contemporary, intense and a little bit novel or sophisticated, but less and less mellow and (surprisingly) unpretentious"
            ]
        },
        {
            "id": "task460-01ac2fde2aa64ae7b54a9bcd84677c01",
            "input": "One distinguishing feature of educational topics is their breadth of sub-topics and points of view, as they attract researchers, practitioners, parents, students, or policy-makers. We assume that this diversity leads to the linguistic variability of the education topics and thus represents a challenge for NLP. \n Question: What challenges do different registers and domains pose to this task?",
            "output": [
                "linguistic variability"
            ]
        },
        {
            "id": "task460-403afe1b744e4be6945a3d8c5cd835f1",
            "input": "We have two main interfaces that enable human interaction with the computer. There is cross-model interaction, where the machine does all the composition work, and displays three different versions of a story written by three distinct models for a human to compare. The user guides generation by providing a topic for story-writing and by tweaking decoding parameters to control novelty, or diversity. The second interface is intra-model interaction, where a human can select the model to interact with (potentially after having chosen it via cross-model), and can collaborate at all stages to jointly create better stories. \n Question: How is human interaction consumed by the model?",
            "output": [
                "displays three different versions of a story written by three distinct models for a human to compare human can select the model to interact with (potentially after having chosen it via cross-model), and can collaborate at all stages"
            ]
        },
        {
            "id": "task460-46fffce8ead743e3a64f6558ee5500dd",
            "input": "We quantify the use of these strategies by comparing the airtime debaters devote to talking points . For a side INLINEFORM0 , let the self-coverage INLINEFORM1 be the fraction of content words uttered by INLINEFORM2 in round INLINEFORM3 that are among their own talking points INLINEFORM4 ; and the opponent-coverage INLINEFORM5 be the fraction of its content words covering opposing talking points INLINEFORM6 .  We use all conversational features discussed above. For each side INLINEFORM0 we include INLINEFORM1 , INLINEFORM2 , and their sum. We also use the drop in self-coverage given by subtracting corresponding values for INLINEFORM3 , and the number of discussion points adopted by each side. \n Question: what aspects of conversation flow do they look at?",
            "output": [
                "The time devoted to self-coverage, opponent-coverage, and the number of adopted discussion points."
            ]
        },
        {
            "id": "task460-4ce7a83a6d754b0fb8ecd3cc4ebb5306",
            "input": "We focused evaluation over a small but diversified dataset composed by 10 YouTube videos in the English language in the news context. The selected videos cover different topics like technology, human rights, terrorism and politics with a length variation between 2 and 10 minutes.  \n Question: What kind of Youtube video transcripts did they use?",
            "output": [
                "youtube video transcripts on news covering different topics like technology, human rights, terrorism and politics"
            ]
        },
        {
            "id": "task460-3c0e4ac78b4f4f3bb80c16c5aba8c352",
            "input": "It thus seems feasible that by using a similar model, we can produce text samples that are conditioned upon a set of user-specified keywords. To illustrate by example, say a user desires the generated output to contain the keywords, $\\lbrace subway, manhattan\\rbrace $ .  \n Question: What are the user-defined keywords?",
            "output": [
                "Words that a user wants them to appear in the generated output."
            ]
        },
        {
            "id": "task460-8c8bbd53ae6e462c961f57fa1a127e25",
            "input": "Despite these shortcomings in using WER as a metric of success, the WER can reflect our model's effectiveness in generating questions that are similar to those of SQuAD based on the given reading passage and answer. WER can be used for initial analyses that can lead to deeper insights as discussed further below. \n Question: Why did they choose WER as evaluation metric?",
            "output": [
                "WER can reflect our model's effectiveness in generating questions that are similar to those of SQuAD WER can be used for initial analyses"
            ]
        },
        {
            "id": "task460-90958b1f8e594b648d296718dba7b42e",
            "input": "We have created a dataset of discharge summaries and nursing notes, all in the English language, with a focus on frequently readmitted patients, labeled with 15 clinical patient phenotypes believed to be associated with risk of recurrent Intensive Care Unit (ICU) readmission per our domain experts (co-authors LAC, PAT, DAG) as well as the literature. BIBREF10 BIBREF11 BIBREF12 \n Question: How many different phenotypes are present in the dataset?",
            "output": [
                "15 clinical patient phenotypes"
            ]
        },
        {
            "id": "task460-74f277437ff14919914b145f88c4002c",
            "input": "Hypothesis 4 (H4) : The proposed template-based synthesis model outperforms a simple word replacement model. Table TABREF13 supports H4 by showing that the proposed synthesis model outperforms the WordNet baseline in training (rows 7, 8 and 19, 20) except Stat2016, and tuning (10, 11 and 22, 23) over all courses. It also shows that while adding synthetic data from the baseline is not always helpful, adding synthetic data from the template model helps to improve both the training and the tuning process. In both CS and ENGR courses, tuning with synthetic data enhances all ROUGE scores compared to tuning with only the original data. (rows 9 and 11). As for Stat2015, R-1 and R-$L$ improved, while R-2 decreased. For Stat2016, R-2 and R-$L$ improved, and R-1 decreased (rows 21 and 23). Training with both student reflection data and synthetic data compared to training with only student reflection data yields similar improvements, supporting H3 (rows 6, 8 and 18, 20). \n Question: Is the template-based model realistic?  ",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-8beea3bc82a8412e82c31f75090c583f",
            "input": "Together, these users gave Gunrock an average rating of 3.65 (median: 4.0), which was elicited at the end of the conversation (“On a scale from 1 to 5 stars, how do you feel about talking to this socialbot again?\"). \n Question: Do they gather explicit user satisfaction data on Gunrock?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-6fc84f2e97a24bbbb58d8fef7ba38045",
            "input": "We experiment and compare with the following models.\n\nPointer-Gen is the baseline model trained by optimizing $L_\\text{MLE}$ in Equation DISPLAY_FORM13.\n\nPointer-Gen+Pos is the baseline model by training Pointer-Gen only on positive examples whose sensationalism score is larger than 0.5\n\nPointer-Gen+Same-FT is the model which fine-tunes Pointer-Gen on the training samples whose sensationalism score is larger than 0.1\n\nPointer-Gen+Pos-FT is the model which fine-tunes Pointer-Gen on the training samples whose sensationalism score is larger than 0.5\n\nPointer-Gen+RL-ROUGE is the baseline model trained by optimizing $L_\\text{RL-ROUGE}$ in Equation DISPLAY_FORM17, with ROUGE-L BIBREF9 as the reward.\n\nPointer-Gen+RL-SEN is the baseline model trained by optimizing $L_\\text{RL-SEN}$ in Equation DISPLAY_FORM17, with $\\alpha _\\text{sen}$ as the reward. \n Question: Which baselines are used for evaluation?",
            "output": [
                "Pointer-Gen Pointer-Gen+Pos Pointer-Gen+Same-FT Pointer-Gen+Pos-FT Pointer-Gen+RL-ROUGE Pointer-Gen+RL-SEN"
            ]
        },
        {
            "id": "task460-1ac85488143b45fc91745275f9857d25",
            "input": "d  Experiments on two benchmark data sets, the Stanford Sentiment Treebank BIBREF7 and the AG English news corpus BIBREF3 , show that 1) our method achieves very competitive accuracy, 2) some views distinguish themselves from others by better categorizing specific classes, and 3) when our base bag-of-words feature set is augmented with convolutional features, the method establishes a new state-of-the-art for both data sets.  Stanford Sentiment Treebank  AG English news corpus  \n Question: which benchmark tasks did they experiment on?",
            "output": [
                " They used Stanford Sentiment Treebank benchmark for sentiment classification task and   AG English news corpus for the text classification task."
            ]
        },
        {
            "id": "task460-b0d846c31eab42008fd0683b544e6f0e",
            "input": "Compared to single-task baselines, performance improved on the low-resource En-De task and was comparable on high-resource En-Fr task. \n Question: How big are negative effects of proposed techniques on high-resource tasks?",
            "output": [
                "The negative effects were insignificant."
            ]
        },
        {
            "id": "task460-b4055377866445c08540597cf7872a95",
            "input": "This action space is of the order of $A=\\mathcal {O}(|V| \\times |O|^2)$ where $V$ is the number of action verbs, and $O$ is the number of distinct objects in the world that the agent can interact with. As this is too large a space for a RL agent to effectively explore, the knowledge graph is used to prune this space by ranking actions based on their presence in the current knowledge graph and the relations between the objects in the graph as in BIBREF7 \n Question: How is the domain knowledge transfer represented as knowledge graph?",
            "output": [
                "the knowledge graph is used to prune this space by ranking actions based on their presence in the current knowledge graph and the relations between the objects in the graph as in BIBREF7"
            ]
        },
        {
            "id": "task460-42c0bbcc1d734d708a7cc81e404399a2",
            "input": "We use datasets from the NIST DUC-05, DUC-06 and DUC-07 shared tasks BIBREF7, BIBREF19, BIBREF20. Given a question and a cluster of newswire documents, the contestants were asked to generate a 250-word summary answering the question. DUC-05 contains 1,600 summaries (50 questions x 32 systems); in DUC-06, 1,750 summaries are included (50 questions x 35 systems); and DUC-07 has 1,440 summaries (45 questions x 32 systems). \n Question: What dataset do they use?",
            "output": [
                "datasets from the NIST DUC-05, DUC-06 and DUC-07 shared tasks"
            ]
        },
        {
            "id": "task460-317430f5717748e5aceec23cb69a737a",
            "input": "Within ICU notes (e.g., text example in top-left box in Figure 2), we first identify all abbreviations using regular expressions and then try to find all possible expansions of these abbreviations from domain-specific knowledge base as candidates. \n Question: How do they identify abbreviations?",
            "output": [
                "identify all abbreviations using regular expressions"
            ]
        },
        {
            "id": "task460-42363fbf0b5a41aaa95a9f3cdb464aa3",
            "input": "Validated transcripts were sent to professional translators. In order to control the quality of the professional translations, we applied various sanity checks to the translations BIBREF11. We also sanity check the overlaps of train, development and test sets in terms of transcripts and voice clips (via MD5 file hashing), and confirm they are totally disjoint. \n Question: How is the quality of the data empirically evaluated? ",
            "output": [
                "Validated transcripts were sent to professional translators. various sanity checks to the translations  sanity check the overlaps of train, development and test sets"
            ]
        },
        {
            "id": "task460-4c025d116e8d4504a6333fbbd8c0d437",
            "input": "We conduct our experiments on $\\sim $ 8.7M annotated anonymised user utterances. They are annotated and derived from requests across 23 domains. \n Question: Over which datasets/corpora is this work evaluated?",
            "output": [
                "$\\sim $ 8.7M annotated anonymised user utterances"
            ]
        },
        {
            "id": "task460-14fb878267b34f16abd55daaeb39d305",
            "input": "To tackle this issue we applying a back-translation method BIBREF13, where we translate indirect and physical harassment tweets of the train set from english to german, french and greek. After that, we translate them back to english in order to achieve data augmentation. \n Question: What language(s) is/are represented in the dataset?",
            "output": [
                "english"
            ]
        },
        {
            "id": "task460-46ff5ec4b91943998d949805fd5561f3",
            "input": "This section introduces our probabilistic model that infers keyword expectation and trains the target model simultaneously. \n Question: What type of classifiers are used?",
            "output": [
                "probabilistic model"
            ]
        },
        {
            "id": "task460-c69884adb1a94896a6cb4211831d9f6d",
            "input": "Our model achieves a 33.33 average of ROUGE-1, ROUGE-2 and ROUGE-L on the CNN/Daily Mail, which is state-of-the-art. \n Question: What is the ROUGE score of the highest performing model?",
            "output": [
                "33.33 average of ROUGE-1, ROUGE-2 and ROUGE-L "
            ]
        },
        {
            "id": "task460-a931f20e99a34acab771c6cefa76fee6",
            "input": "We discuss two core models for addressing sequence labeling problems and describe, for each, training them in a single-model multilingual setting: (1) the Meta-LSTM BIBREF0 , an extremely strong baseline for our tasks, and (2) a multilingual BERT-based model BIBREF1 . \n Question: What is the multilingual baseline?",
            "output": [
                " the Meta-LSTM BIBREF0"
            ]
        },
        {
            "id": "task460-f753e0e444e2499ca960805552f659cd",
            "input": "In this paper, we propose the first large-scale dataset for QA over social media data.   Table TABREF3 gives an example from our TweetQA dataset. The dataset now includes 10,898 articles, 17,794 tweets, and 13,757 crowdsourced question-answer pairs \n Question: What is the size of this dataset?",
            "output": [
                "13,757"
            ]
        },
        {
            "id": "task460-0389d3d4f08e482b9d50e8d2d18e7458",
            "input": "We computed Fliess' INLINEFORM0 on the trial set for the five annotators on 21 of the tweets. INLINEFORM1 is .83 for Layer A (OFF vs NOT) indicating high agreement. As to normalization and anonymization, no user metadata or Twitter IDs have been stored, and URLs and Twitter mentions have been substituted to placeholders.  \n Question: How many annotators participated?",
            "output": [
                "five annotators"
            ]
        },
        {
            "id": "task460-d6b0905cb3e14f00a3e3348fce3cdd7d",
            "input": "Our system, including software and corpus, is available as an open source project for free research purpose and we believe that it is a good baseline for the development and comparison of future Vietnamese SRL systems.  \n Question: Are their corpus and software public?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-6e76950ea6734146afab8a966ae6ae1a",
            "input": "As illustrated in Figure FIGREF1, our key idea is that we can exploit discourse relations BIBREF4 to efficiently propagate polarity from seed predicates that directly report one's emotions (e.g., “to be glad” is positive). Suppose that events $x_1$ are $x_2$ are in the discourse relation of Cause (i.e., $x_1$ causes $x_2$). If the seed lexicon suggests $x_2$ is positive, $x_1$ is also likely to be positive because it triggers the positive emotion. The fact that $x_2$ is known to be negative indicates the negative polarity of $x_1$. Similarly, if $x_1$ and $x_2$ are in the discourse relation of Concession (i.e., $x_2$ in spite of $x_1$), the reverse of $x_2$'s polarity can be propagated to $x_1$. Even if $x_2$'s polarity is not known in advance, we can exploit the tendency of $x_1$ and $x_2$ to be of the same polarity (for Cause) or of the reverse polarity (for Concession) although the heuristic is not exempt from counterexamples. We transform this idea into objective functions and train neural network models that predict the polarity of a given event. \n Question: How are relations used to propagate polarity?",
            "output": [
                "based on the relation between events, the suggested polarity of one event can determine the possible polarity of the other event "
            ]
        },
        {
            "id": "task460-ca34ff69a82940ec950b54c0232451e2",
            "input": "Previously, datasets openly available in abusive language detection research on Twitter ranged from 10K to 35K in size BIBREF9 , BIBREF11 . This quantity is not sufficient to train the significant number of parameters in deep learning models. Due to this reason, these datasets have been mainly studied by traditional machine learning methods. Most recently, Founta et al. founta2018large introduced Hate and Abusive Speech on Twitter, a dataset containing 100K tweets with cross-validated labels. Although this corpus has great potential in training deep models with its significant size, there are no baseline reports to date.\n\nThis paper investigates the efficacy of different learning models in detecting abusive language. We compare accuracy using the most frequently studied machine learning classifiers as well as recent neural network models. Reliable baseline results are presented with the first comparative study on this dataset. Additionally, we demonstrate the effect of different features and variants, and describe the possibility for further improvements with the use of ensemble models. \n Question: Does the dataset feature only English language data?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-22db3c942eb049dab81b940714a18db1",
            "input": "We pre-trained our model on the Dutch section of the OSCAR corpus, a large multilingual corpus which was obtained by language classification in the Common Crawl corpus BIBREF16. \n Question: What data did they use?",
            "output": [
                "the Dutch section of the OSCAR corpus"
            ]
        },
        {
            "id": "task460-45597582be0d4c7ea74dff28cfb8786d",
            "input": "Our approach consists of structuring input to the transformer network to use and guide the self-attention of the transformers, conditioning it on the entity. Our main mode of encoding the input, the entity-first method, is shown in Figure FIGREF4.  We also have an entity-last variant where the entity is primarily observed just before the classification token to condition the [CLS] token's self-attention accordingly.  As an additional variation, we can either run the transformer once per document with multiple [CLS] tokens (a document-level model as shown in Figure FIGREF4) or specialize the prediction to a single timestep (a sentence-level model). \n Question: In what way is the input restructured?",
            "output": [
                "In four entity-centric ways - entity-first, entity-last, document-level and sentence-level"
            ]
        },
        {
            "id": "task460-172be4032c164d43aca81927f27fabf4",
            "input": "To further study the information encoded in the discourse embeddings, we perform t-SNE clustering BIBREF20 on them, using the best performing model CNN2-DE (global). \n Question: How are discourse embeddings analyzed?",
            "output": [
                "They perform t-SNE clustering to analyze discourse embeddings"
            ]
        },
        {
            "id": "task460-5d8628f24b6c40a58e67feef0fd8e207",
            "input": "Recently, continuous space representations of words and phrases have been incorporated into SMT systems via neural networks. Specifically, addition of monolingual neural network language models BIBREF13 , BIBREF14 , neural network joint models (NNJM) BIBREF4 , and neural network global lexicon models (NNGLM) BIBREF3 have been shown to be useful for SMT Grammatical error correction (GEC) is a challenging task due to the variability of the type of errors and the syntactic and semantic dependencies of the errors on the surrounding context. Most of the grammatical error correction systems use classification and rule-based approaches for correcting specific error types. However, these systems use several linguistic cues as features. The standard linguistic analysis tools like part-of-speech (POS) taggers and parsers are often trained on well-formed text and perform poorly on ungrammatical text. This introduces further errors and limits the performance of rule-based and classification approaches to GEC. As a consequence, the phrase-based statistical machine translation (SMT) approach to GEC has gained popularity because of its ability to learn text transformations from erroneous text to correct text from error-corrected parallel corpora without any additional linguistic information.  We conduct experiments by incorporating NNGLM and NNJM both independently and jointly into our baseline system On top of our baseline system described above, we incorporate the two neural network components, neural network global lexicon model (NNGLM) and neural network joint model (NNJM) as features. Both NNGLM and NNJM are trained using the parallel data used to train the translation model of our baseline system. \n Question: How do they combine the two proposed neural network models?",
            "output": [
                "ncorporating NNGLM and NNJM both independently and jointly into baseline system"
            ]
        },
        {
            "id": "task460-46f3e01f7a71468ebac68b23b4e696af",
            "input": "Note that ESIM models have two LSTM layers, the first (input) LSTM performs the input encoding and the second (inference) LSTM generates the representation for inference. \n Question: How many layers are there in their model?",
            "output": [
                "two LSTM layers"
            ]
        },
        {
            "id": "task460-676a241a12a34625bb6e1da37f076480",
            "input": "To test whether horizontal or vertical consistency is more helpful, we train and evaluate M-Bert on a dataset variant where all questions are in their English version. \n Question: Did they pefrorm any cross-lingual vs single language evaluation?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-0a0c721452fd420c80be6e5478bdeea7",
            "input": "For the sake of interpretability, we will also project back coefficients from the embeddings as well as PCA models into the dummy variable space. \n Question: How do their interpret the coefficients?",
            "output": [
                "The coefficients are projected back to the dummy variable space."
            ]
        },
        {
            "id": "task460-4baf67d471364a858b76d2364b1b694d",
            "input": "To make the annotated data publicly available, we selected 70 news articles from Arabic WikiNews site. These articles cover recent news from year 2013 to year 2015 in multiple genres (politics, economics, health, science and technology, sports, arts, and culture.) Articles contain 18,300 words, and they are evenly distributed among these 7 genres with 10 articles per each.\n\nWord are white-space and punctuation separated, and some spelling errors are corrected (1.33% of the total words) to have very clean test cases. Lemmatization is done by an expert Arabic linguist where spelling corrections are marked, and lemmas are provided with full diacritization as shown in Figure FIGREF2 . \n Question: How was the dataset annotated?",
            "output": [
                "Lemmatization is done by an expert Arabic linguist where spelling corrections are marked, and lemmas are provided with full diacritization"
            ]
        },
        {
            "id": "task460-e12984e893754acc8378a03952389012",
            "input": "Table TABREF19 shows the comparison of our proposed system with the existing state-of-the-art system of SemEval 2016 Task 6 for the sentiment dataset. BIBREF7 used feature-based SVM, BIBREF39 used keyword rules, LitisMind relied on hashtag rules on external data, BIBREF38 utilized a combination of sentiment classifiers and rules, whereas BIBREF37 used a maximum entropy classifier with domain-specific features. We also compare our system with the state-of-the-art systems proposed by BIBREF15 on the emotion dataset. The comparison is demonstrated in Table TABREF22. Maximum entropy, SVM, LSTM, Bi-LSTM, and CNN were the five individual systems used by BIBREF15. \n Question: What is the previous state-of-the-art model?",
            "output": [
                "BIBREF7 BIBREF39 BIBREF37 LitisMind Maximum entropy, SVM, LSTM, Bi-LSTM, and CNN"
            ]
        },
        {
            "id": "task460-5d5ca9d177614d9e830f676e6838a82b",
            "input": " For a fair comparison with previous work, we train the baseline MT system on the data released by BIBREF11.  \n Question: what was the baseline?",
            "output": [
                " MT system on the data released by BIBREF11"
            ]
        },
        {
            "id": "task460-82ada7bd964845b3a3cf3308885b597c",
            "input": "The observed performance of both the BasicDCGAN and MultitaskDCGAN using 3-classes is comparable to the state-of-the-art, with 49.80% compared to 49.99% reported in BIBREF16  \n Question: What model achieves state of the art performance on this task?",
            "output": [
                "BIBREF16"
            ]
        },
        {
            "id": "task460-7a049ea7ea834edd85ff29b9598ebd40",
            "input": "Differently to textual data, our goal in this paper is to explore the large amount of categorical data that is often collected in travel surveys. This includes trip purpose, education level, or family type. We also consider other variables that are not necessarily of categorical nature, but typically end up as dummy encoding, due to segmentation, such as age, income, or even origin/destination pair. \n Question: How do they model travel behavior?",
            "output": [
                "The data from collected travel surveys is used to model travel behavior."
            ]
        },
        {
            "id": "task460-af8a604cd66b42c5aa9d94f2ba6cbc99",
            "input": "Dataset Quality Analysis ::: Inter-Annotator Agreement (IAA)\nTo estimate dataset consistency across different annotations, we measure F1 using our UA metric with 5 generators per predicate. Dataset Quality Analysis ::: Dataset Assessment and Comparison\nWe assess both our gold standard set and the recent Dense set against an integrated expert annotated sample of 100 predicates.  Dataset Quality Analysis ::: Agreement with PropBank Data\nIt is illuminating to observe the agreement between QA-SRL and PropBank (CoNLL-2009) annotations BIBREF7.  \n Question: How was quality measured?",
            "output": [
                "Inter-annotator agreement, comparison against expert annotation, agreement with PropBank Data annotations."
            ]
        },
        {
            "id": "task460-7c0cbf10f4d84466bd8a034bf96d0896",
            "input": "To demonstrate the value of building a dedicated version of BERT for French, we first compare CamemBERT to the multilingual cased version of BERT (designated as mBERT). \n Question: Was CamemBERT compared against multilingual BERT on these tasks?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-60e6acf22ec94fae8f78308097a6013f",
            "input": "We further explore the effect of INLINEFORM0 -gram length in our model (i.e., the filter size for the covolutional layers used by the attention parsing module). In Figure FIGREF39 we plot the AUC scores for link prediction on the Cora dataset against varying INLINEFORM1 -gram length.  \n Question: Do they measure how well they perform on longer sequences specifically?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-15ae5aa67e864111a02d8a93cfd3fe66",
            "input": "Then, it selects a subset of a 1,700-hour ( INLINEFORM2 1.1M instances) unlabeled dataset. \n Question: How much data samples do they start with before obtaining the initial model labels?",
            "output": [
                "1,700-hour ( INLINEFORM2 1.1M instances) unlabeled dataset"
            ]
        },
        {
            "id": "task460-d49397d9a6bb40c6b57f340bf35d6417",
            "input": "We use one public dataset Social Honeypot dataset and one self-collected dataset Weibo dataset to validate the effectiveness of our proposed features. Before directly performing the experiments on the employed datasets, we first delete some accounts with few posts in the two employed since the number of tweets is highly indicative of spammers. For the English Honeypot dataset, we remove stopwords, punctuations, non-ASCII words and apply stemming. For the Chinese Weibo dataset, we perform segmentation with \"Jieba\", a Chinese text segmentation tool. After preprocessing steps, the Weibo dataset contains 2197 legitimate users and 802 spammers, and the honeypot dataset contains 2218 legitimate users and 2947 spammers. \n Question: What is the benchmark dataset and is its quality high?",
            "output": [
                "Social Honeypot dataset (public) and Weibo dataset (self-collected); yes"
            ]
        },
        {
            "id": "task460-a115c99a037b455f8b56c8dffc3a1256",
            "input": "Most recently, GANs were used to generate an original painting in an unsupervised fashion BIBREF24.  \n Question: Is text-to-image synthesis trained is suppervized or unsuppervized manner?",
            "output": [
                "unsupervised "
            ]
        },
        {
            "id": "task460-750973114bf7416aa372fd2cede2bae8",
            "input": "Analogous to word embeddings, sentence embeddings, e.g. the Universal Sentence Encoder BIBREF8 and Sentence-BERT BIBREF6, allow one to calculate the cosine similarity of various different sentences, as for instance the similarity of a question and the corresponding answer. The more appropriate a specific answer is to a given question, the stronger is its cosine similarity expected to be. When considering two opposite answers, it is therefore possible to determine a bias value:\n\nwhere $\\vec{q}$ is the vector representation of the question and $\\vec{a}$ and $\\vec{b}$ the representations of the two answers/choices. \n Question: How is moral bias measured?",
            "output": [
                "Answer with content missing: (formula 1) bias(q, a, b) = cos(a, q) − cos(b, q)\nBias is calculated as substraction of cosine similarities of question and some answer for two opposite answers."
            ]
        },
        {
            "id": "task460-bc4540c7db904a5a9c559ee58b11ff6d",
            "input": "As shown in Table TABREF24, MUSE outperforms all previously models on En-De and En-Fr translation, including both state-of-the-art models of stand alone self-attention BIBREF0, BIBREF13, and convolutional models BIBREF11, BIBREF15, BIBREF10. This result shows that either self-attention or convolution alone is not enough for sequence to sequence learning. The proposed parallel multi-scale attention improves over them both on En-De and En-Fr.\n\nCompared to Evolved Transformer BIBREF19 which is constructed by NAS and also mixes convolutions of different kernel size, MUSE achieves 2.2 BLEU gains in En-Fr translation. \n Question: How big is improvement in performance over Transformers?",
            "output": [
                "2.2 BLEU gains"
            ]
        },
        {
            "id": "task460-6285f5d3fdf3440fbd04c80e382eed12",
            "input": "Compared with 2015, our QA-SRL gold yielded 1593 annotations, with 989 core and 604 adjuncts, while theirs yielded 1315 annotations, 979 core and 336 adjuncts.  \n Question: How big is the dataset?",
            "output": [
                "1593 annotations"
            ]
        },
        {
            "id": "task460-0fbcde138ff04595989785df8dc90251",
            "input": "Our training data consists of 2.09M sentence pairs extracted from LDC corpus. \n Question: Where does the vocabulary come from?",
            "output": [
                "LDC corpus"
            ]
        },
        {
            "id": "task460-f54d095320b44e93a8f80b51454ebe19",
            "input": "In this section, we introduce an invertibility condition for our neural projector to tackle the optimization challenge. Specifically, we constrain our neural projector with two requirements: (1) INLINEFORM0 and (2) INLINEFORM1 exists.  \n Question: What is the invertibility condition?",
            "output": [
                "The neural projector must be invertible."
            ]
        },
        {
            "id": "task460-1bb3032b9a02469c8e12b7dae26b5335",
            "input": "We use three datasets for our experiments: IWSLT14 German-English, Turkish-English, and WMT14 English-German. \n Question: what language pairs are explored?",
            "output": [
                "German-English Turkish-English English-German"
            ]
        },
        {
            "id": "task460-7561e90b217548cdb4d2c5ccba62b965",
            "input": "On the other hand, in the case of MHD, instead of the integration at attention level, we assign multiple decoders for each head and then integrate their outputs to get a final output. \n Question: Does each attention head in the decoder calculate the same output?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-4a23508b043443508b6c6ec6989666ca",
            "input": "Davidson et al. BIBREF2 showed that there were systematic and substantial racial biases in five benchmark Twitter datasets annotated for offensive language detection. By examining more samples and with respect to recently studies BIBREF2, BIBREF24, BIBREF19, it is clear that many errors are due to biases from data collection BIBREF19 and rules of annotation BIBREF24 and not the classifier itself.  \n Question: What are the existing biases?",
            "output": [
                "systematic and substantial racial biases biases from data collection rules of annotation"
            ]
        },
        {
            "id": "task460-672a73bd832c436082ba75b7e9ab0324",
            "input": "The training data released for this task contains 22 drug labels, referred to as Training-22, with gold standard annotations.  As Training-22 is a relatively small dataset, we additionally utilize an external dataset with 180 annotated drug labels dubbed NLM-180 BIBREF5 (more later).  \n Question: What training data did they use?",
            "output": [
                "Training-22 NLM-180"
            ]
        },
        {
            "id": "task460-38db195e4524453b8ba1c4392d1c8795",
            "input": "The language is Chinese, which is not easy for non-Chinese-speaking researchers to work on. \n Question: What language are the conversations in?",
            "output": [
                "The language is Chinese, which is not easy for non-Chinese-speaking researchers to work on."
            ]
        },
        {
            "id": "task460-12462927b60048e38d135f8ca8c4fbc7",
            "input": " To the best of our knowledge, this is the first large-scale reusable sentence representation model obtained by combining a set of training objectives with the level of diversity explored here, i.e. multi-lingual NMT, natural language inference, constituency parsing and skip-thought vectors. \n Question: Which training objectives do they combine?",
            "output": [
                "multi-lingual NMT natural language inference constituency parsing skip-thought vectors"
            ]
        },
        {
            "id": "task460-a7e49faeeb7c489a827293c11b10ea8c",
            "input": "We are using the disaster data from BIBREF5. It contains various dataset including the CrisiLexT6 dataset which contains six crisis events related to English tweets in 2012 and 2013, labeled by relatedness (on-topic and off-topic) of respective crisis. Each crisis event tweets contain almost 10,000 labeled tweets but we are only focused on flood-related tweets thus, we experimented with only two flood event i.e. Queensland flood in Queensland, Australia and Alberta flood in Alberta, Canada and relabeled all on-topic tweets as Related and Off-topic as Unrelated for implicit class labels understanding in this case. \n Question: What dataset did they use?",
            "output": [
                " disaster data from BIBREF5 Queensland flood in Queensland, Australia and Alberta flood in Alberta, Canada"
            ]
        },
        {
            "id": "task460-c9f9519c50374798bfa178f3d3b85ec9",
            "input": "Wikilinks can be seen as a large-scale, naturally-occurring, crowd-sourced dataset where thousands of human annotators provide ground truths for mentions of interest. This means that the dataset contains various kinds of noise, especially due to incoherent contexts. We prepare our dataset from the local-context version of Wikilinks, and resolve ground-truth links using a Wikipedia dump from April 2016. We use the page and redirect tables for resolution, and keep the database pageid column as a unique identifier for Wikipedia entities. We discard mentions where the ground-truth could not be resolved (only 3% of mentions). \n Question: How was a quality control performed so that the text is noisy but the annotations are accurate?",
            "output": [
                "The authors believe that the Wikilinks corpus  contains ground truth annotations while being noisy. They discard mentions that cannot have ground-truth verified by comparison with Wikipedia."
            ]
        },
        {
            "id": "task460-491a4e6a9ff24a649fa55b0847f6979d",
            "input": "In this work we consider directed graphs. Let INLINEFORM0 represent a graph comprised of a set of vertices INLINEFORM1 and a set of edges ( INLINEFORM2 ), which are ordered pairs. Further, each edge can have a real-valued weight assigned. Let INLINEFORM3 represent a document comprised of tokens INLINEFORM4 . The order in which tokens in text appear is known, thus INLINEFORM5 is a totally ordered set. A potential way of constructing a graph from a document is by simply observing word co-occurrences. When two words co-occur, they are used as an edge. However, such approaches do not take into account the sequence nature of the words, meaning that the order is lost. We attempt to take this aspect into account as follows. The given corpus is traversed, and for each element INLINEFORM6 , its successor INLINEFORM7 , together with a given element, forms a directed edge INLINEFORM8 . Finally, such edges are weighted according to the number of times they appear in a given corpus. Thus the graph, constructed after traversing a given corpus, consists of all local neighborhoods (order one), merged into a single joint structure. Global contextual information is potentially kept intact (via weights), even though it needs to be detected via network analysis as proposed next. \n Question: How are graphs derived from a given text?",
            "output": [
                "The given corpus is traversed, and for each element INLINEFORM6 , its successor INLINEFORM7 , together with a given element, forms a directed edge INLINEFORM8 . Finally, such edges are weighted according to the number of times they appear in a given corpus. Thus the graph, constructed after traversing a given corpus, consists of all local neighborhoods (order one), merged into a single joint structure. Global contextual information is potentially kept intact (via weights), even though it needs to be detected via network analysis"
            ]
        },
        {
            "id": "task460-a12d6ccfeb29480585d05354b0ab409d",
            "input": "In addition to it, we extract three hand-crafted polarity scores, which are minimum, mean, and maximum polarity scores, from each review. These polarity scores of words are computed as in (DISPLAY_FORM4). For example, if a review consists of five words, it would have five polarity scores and we utilise only three of these sentiment scores as mentioned. Lastly, we concatenate these three scores to the averaged word vector per review.\n\nThat is, each review is represented by the average word vector of its constituent word embeddings and three supervised scores. We then feed these inputs into the SVM approach. The flowchart of our framework is given in Figure FIGREF11. When combining the unsupervised features, which are word vectors created on a word basis, with supervised three scores extracted on a review basis, we have better state-of-the-art results. \n Question: Which hand-crafted features are combined with word2vec?",
            "output": [
                "three hand-crafted polarity scores, which are minimum, mean, and maximum polarity scores"
            ]
        },
        {
            "id": "task460-65f0fe2ac25f4465b3a431c524d3c193",
            "input": "For comparison purpose, we build two baseline systems for each direction: one is use the traditional phrase-based statistical machine translation (SMT), the other one is the NMT system.  \n Question: what was the baseline?",
            "output": [
                "traditional phrase-based statistical machine translation (SMT) NMT system"
            ]
        },
        {
            "id": "task460-b8a06ded10f9499da225cb8aca356878",
            "input": "We used the Twitter API to gather real-time tweets from September 2018 until February 2019, selecting the ones containing any of the 51 Hatebase terms that are more common in hate speech tweets, as studied in BIBREF9. \n Question: How is data collected, manual collection or Twitter api?",
            "output": [
                "Twitter API"
            ]
        },
        {
            "id": "task460-4dbde4c76ddf48f0a02ec4f5429ba409",
            "input": "The SnapCaptions dataset is composed of 10K user-generated image (snap) and textual caption pairs where named entities in captions are manually labeled by expert human annotators (entity types: PER, LOC, ORG, MISC). \n Question: Which types of named entities do they recognize?",
            "output": [
                "PER, LOC, ORG, MISC"
            ]
        },
        {
            "id": "task460-add9d8d07fda4bf5a559431657bfb44d",
            "input": "Construction of the auxiliary sentence\nFor simplicity, we mainly describe our method with TABSA as an example.\n\nWe consider the following four methods to convert the TABSA task into a sentence pair classification task:\n\nThe sentence we want to generate from the target-aspect pair is a question, and the format needs to be the same. For example, for the set of a target-aspect pair (LOCATION1, safety), the sentence we generate is “what do you think of the safety of location - 1 ?\"\n\nFor the NLI task, the conditions we set when generating sentences are less strict, and the form is much simpler. The sentence created at this time is not a standard sentence, but a simple pseudo-sentence, with (LOCATION1, safety) pair as an example: the auxiliary sentence is: “location - 1 - safety\".\n\nFor QA-B, we add the label information and temporarily convert TABSA into a binary classification problem ( INLINEFORM0 ) to obtain the probability distribution. At this time, each target-aspect pair will generate three sequences such as “the polarity of the aspect safety of location - 1 is positive\", “the polarity of the aspect safety of location - 1 is negative\", “the polarity of the aspect safety of location - 1 is none\". We use the probability value of INLINEFORM1 as the matching score. For a target-aspect pair which generates three sequences ( INLINEFORM2 ), we take the class of the sequence with the highest matching score for the predicted category.\n\nThe difference between NLI-B and QA-B is that the auxiliary sentence changes from a question to a pseudo-sentence. The auxiliary sentences are: “location - 1 - safety - positive\", “location - 1 - safety - negative\", and “location - 1 - safety - none\".\n\nAfter we construct the auxiliary sentence, we can transform the TABSA task from a single sentence classification task to a sentence pair classification task. As shown in Table TABREF19 , this is a necessary operation that can significantly improve the experimental results of the TABSA task. \n Question: How do they generate the auxiliary sentence?",
            "output": [
                "The sentence we want to generate from the target-aspect pair is a question, and the format needs to be the same. For the NLI task, the conditions we set when generating sentences are less strict, and the form is much simpler. For QA-B, we add the label information and temporarily convert TABSA into a binary classification problem ( INLINEFORM0 ) to obtain the probability distribution auxiliary sentence changes from a question to a pseudo-sentence"
            ]
        },
        {
            "id": "task460-56cc96f239fa4beb93e8468f17aaaacd",
            "input": "We try 3 variants of each training set to fine-tune our models: (i) the original one in English (Orig), (ii) an English paraphrase of it generated through back-translation using Spanish or Finnish as pivot (BT-ES and BT-FI), and (iii) a machine translated version in Spanish or Finnish (MT-ES and MT-FI). \n Question: What languages do they use in their experiments?",
            "output": [
                "English Spanish Finnish"
            ]
        },
        {
            "id": "task460-5994718a65b8444e98a1b1222f1f1e0a",
            "input": "However, probably the most surprising result in the field is Zipf's law, which states that if one ranks words by their frequency in a large text, the resulting rank frequency distribution is approximately a power law, for all languages BIBREF0, BIBREF11. Another interesting characterization of texts is the Heaps-Herdan law, which describes how the vocabulary -that is, the set of different words- grows with the size of a text, the number of which, empirically, has been found to grow as a power of the text size BIBREF18, BIBREF19. It is worth noting that it has been argued that this law is a consequence Zipf's law. BIBREF20, BIBREF21 \n Question: How do Zipf and Herdan-Heap's laws differ?",
            "output": [
                "Zipf's law describes change of word frequency rate, while Heaps-Herdan describes different word number in large texts (assumed that Hepas-Herdan is consequence of Zipf's)"
            ]
        },
        {
            "id": "task460-09d0addcd5e94acc87c6d53798c0e75c",
            "input": "Results and Analysis \n Question: what quantitative analysis is done?",
            "output": [
                "Answer with content missing: (Evaluation section) Given that in CLIR the primary goal is to get a better ranked list of documents against a translated query, we only report Mean Average Precision (MAP)."
            ]
        },
        {
            "id": "task460-e71bddab6aa64d90bb726ebd45afe1fe",
            "input": "Using an encoder-decoder backbone, our model may be regarded as an extension of the constituent parsing model of BIBREF18 as shown in Figure FIGREF4. The difference is that in our model both constituent and dependency parsing share the same token representation and shared self-attention layers and each has its own individual Self-Attention Layers and subsequent processing layers. Our model includes four modules: token representation, self-attention encoder, constituent and dependency parsing decoder. Constituent Parsing Decoder Dependency Parsing Decoder \n Question: What are the models used to perform constituency and dependency parsing?",
            "output": [
                "token representation self-attention encoder, Constituent Parsing Decoder  Dependency Parsing Decoder"
            ]
        },
        {
            "id": "task460-aa5ddb45d68641c69ab8082ca63bad16",
            "input": "In this experiment set, our BERT implementation is compared to several systems that participated in the MEDDOCAN challenge: a CRF classifier BIBREF18, a spaCy entity recogniser BIBREF18, and NLNDE BIBREF12, the winner of the shared task and current state of the art for sensitive information detection and classification in Spanish clinical text.  However, attending to the obtained results, BERT remains only 0.3 F1-score points behind, and would have achieved the second position among all the MEDDOCAN shared task competitors. Taking into account that only 3% of the gold labels remain incorrectly annotated, the task can be considered almost solved, and it is not clear if the differences among the systems are actually significant, or whether they stem from minor variations in initialisation or a long-tail of minor labelling inconsistencies. \n Question: Does BERT reach the best performance among all the algorithms compared?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-bf86477465534df19c8ceffa6b1593d1",
            "input": "The seq2seq model with global attention gives the best results with an average target BLEU score of 29.65 on the style transfer dataset, compared with an average target BLEU score of 26.97 using the seq2seq model with pointer networks. \n Question: What is best BLEU score of language style transfer authors got?",
            "output": [
                "seq2seq model with global attention gives the best results with an average target BLEU score of 29.65"
            ]
        },
        {
            "id": "task460-d0e82415765b4a49a24a3465a11f1ee8",
            "input": "The annotator carried out all annotation. \n Question: How many annotators tagged each tweet?",
            "output": [
                "One"
            ]
        },
        {
            "id": "task460-8d67c5573a85457dabb1f660ccc7c739",
            "input": "We find that BIBREF27 zhao2018unsupervised make use of a set of discrete variables that define high-level attributes of a response. Although they interpret meanings of the learned discrete latent variables by clustering data according to certain classes (e.g. dialog acts), such latent variables still have no exact meanings. In our model, we connect each latent variable with a word in the vocabulary, thus each latent variable has an exact semantic meaning. Besides, they focus on multi-turn dialogue generation and presented an unsupervised discrete sentence representation learning method learned from the context while our concentration is primarily on single-turn dialogue generation with no context information. \n Question: How does discrete latent variable has an explicit semantic meaning to improve the CVAE on short-text conversation?",
            "output": [
                "we connect each latent variable with a word in the vocabulary, thus each latent variable has an exact semantic meaning."
            ]
        },
        {
            "id": "task460-95f03c348bba46c5b8c0378bf84ffd26",
            "input": "Long-short Term Hybrid Memory and Multi-attention Block. Long-short Term Hybrid Memory (LSTHM) is an extension of the Long-short Term Memory (LSTM) by reformulating the memory component to carry hybrid information.  In the LSTHM model, we seek to build a memory mechanism for each modality which in addition to storing view-specific dynamics, is also able to store the cross-view dynamics that are important for that modality. This allows the memory to function in a hybrid manner. \n Question: What is the difference between Long-short Term Hybrid Memory and LSTMs?",
            "output": [
                "Long-short Term Hybrid Memory (LSTHM) is an extension of the Long-short Term Memory (LSTM) "
            ]
        },
        {
            "id": "task460-43d47267e6af4ba0a9cc8e2c15d90daa",
            "input": "We used the public tool, word2vec, released by Mikolov-2013 to obtain the word embeddings. Their neural network approach is similar to the feed-forward neural networks BIBREF5 , BIBREF6 . In the Skip-gram model architecture we used, we have chosen 200 as the dimension of the obtained word vectors. \n Question: What type and size of word embeddings were used?",
            "output": [
                "word2vec 200 as the dimension of the obtained word vectors"
            ]
        },
        {
            "id": "task460-0245002936c84396ae276d075abbbd7d",
            "input": "Finally, a comparison was made between the Skip-gram model W10N20 obtained at the 50th epoch and the other two W2V in Italian present in the literature (BIBREF9 and BIBREF10). The first test (Table TABREF15) was performed considering all the analogies present, and therefore evaluating as an error any analogy that was not executable (as it related to one or more words absent from the vocabulary).\n\nAs it can be seen, regardless of the metric used, our model has significantly better results than the other two models, both overall and within the two macro-areas. \n Question: Are the word embeddings evaluated?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-e4420132bc454e56bc7e1a62a9ac958a",
            "input": "Spelling correction tests described in literature have tended to focus on one approach applied to a specific corpus. Limited examples include works on spellchecking mammography reports and tweets BIBREF7 , BIBREF4 . These works emphasized the importance of tailoring correction systems to specific problems of corpora they are applied to. For example, mammography reports suffer from poor typing, which in this case is a repetitive work done in relative hurry. Tweets, on the other hand, tend to contain emoticons and neologisms that can trick solutions based on rules and dictionaries, such as LanguageTool. The latter is, by itself, fairly well suited for Polish texts, since a number of extensions to the structure of this application was inspired by problems with morphology of Polish language BIBREF3 . \n Question: Which specific error correction solutions have been proposed for specialized corpora in the past?",
            "output": [
                "spellchecking mammography reports and tweets BIBREF7 , BIBREF4"
            ]
        },
        {
            "id": "task460-0694daa58eb9476885888b48204aa13d",
            "input": "A data collection process involving location neutral keywords used by gang members, with an expanded search of their retweet, friends and follower networks, led to identifying 400 authentic gang member profiles on Twitter. Our study discovered that the text in their tweets and profile descriptions, their emoji use, their profile images, and music interests embodied by links to YouTube music videos, can help a classifier distinguish between gang and non-gang member profiles. \n Question: How is the ground truth of gang membership established in this dataset?",
            "output": [
                " text in their tweets and profile descriptions, their emoji use, their profile images, and music interests embodied by links to YouTube music videos, can help a classifier distinguish between gang and non-gang member profiles"
            ]
        },
        {
            "id": "task460-a602b02f5c224fa5b5f1a451921705f8",
            "input": "The model consists of one layer of LSTM followed by three dense layers. The LSTM layer uses a dropout value of 0.2.  \n Question: Do they use dropout?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-5aeb9d06b65b48ceadaa08b8b4eeab44",
            "input": "We show that by determining and integrating heterogeneous set of features from different modalities – aesthetic features from posted images (colorfulness, hue variance, sharpness, brightness, blurriness, naturalness), choice of profile picture (for gender, age, and facial expression), the screen name, the language features from both textual content and profile's description (n-gram, emotion, sentiment), and finally sociability from ego-network, and user engagement – we can reliably detect likely depressed individuals in a data set of 8,770 human-annotated Twitter users. \n Question: What is the source of the textual data? ",
            "output": [
                "Users' tweets"
            ]
        },
        {
            "id": "task460-157cc270ed284f5bb6fba15fbb7b155d",
            "input": "Across all 26 datasets, RCRN outperforms not only standard BiLSTMs but also 3L-BiLSTMs which have approximately equal parameterization. \n Question: Does their model have more parameters than other models?",
            "output": [
                "approximately equal parameterization"
            ]
        },
        {
            "id": "task460-b8b2c4309802432bb1ba9d073ddc30de",
            "input": "is the standard word counting method whereby the feature vector represents the term frequency of the words in a sentence. is similar to BoW, except that it is derived by the counting of the words in the sentence weighted by individual word's term-frequency and inverse-document-frequency BIBREF31 . is derived from clustering of words-embeddings with k-means into 5000 clusters, and follow by BoW representation of the words in 5000 clusters. \n Question: What other non-neural baselines do the authors compare to? ",
            "output": [
                "bag of words, tf-idf, bag-of-means"
            ]
        },
        {
            "id": "task460-2845f8fb488c4089bb610748230f0b66",
            "input": "Our Reddit data was gathered using Reddit's public API, collecting the most recent jokes. Every time the scraper ran, it also updated the upvote score of the previously gathered jokes. This data collection occurred every hour through the months of March and April 2019. Since the data was already split into body and punchline sections from Reddit, we created separate datasets containing the body of the joke exclusively and the punchline of the joke exclusively. Additionally, we created a dataset that combined the body and punchline together.\n\nSome sample jokes are shown in Table 1, above. The distribution of joke scores varies wildly, ranging from 0 to 136,354 upvotes. We found that there is a major jump between the 0-200 upvote range and the 200 range and onwards, with only 6% of jokes scoring between 200-20,000. We used this natural divide as the cutoff to decide what qualified as a funny joke, giving us 13884 not-funny jokes and 2025 funny jokes. \n Question: How they evaluate if joke is humorous or not?",
            "output": [
                "The distribution of joke scores varies wildly, ranging from 0 to 136,354 upvotes. We found that there is a major jump between the 0-200 upvote range and the 200 range and onwards, with only 6% of jokes scoring between 200-20,000. We used this natural divide as the cutoff to decide what qualified as a funny joke, giving us 13884 not-funny jokes and 2025 funny jokes."
            ]
        },
        {
            "id": "task460-e7bd729b788443b49d49fcd3bdebd8ff",
            "input": "Finally, the morphosyntactic annotation was performed automatically, which may lead to errors, especially in the case of noisy web text. \n Question: Did they use a crowdsourcing platform for annotations?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-22bf8cc54e7b4f8c9f3451f3ea544392",
            "input": "A document $d$ can be defined as a finite sequence of terms (independent textual entities within a document, for example, words), namely $d=(t_1,t_2,\\dots ,t_n)$. \n Question: What representations for textual documents do they use?",
            "output": [
                "finite sequence of terms"
            ]
        },
        {
            "id": "task460-b3ed441621774075a5c400779ebe3bf0",
            "input": "Thus we consider the task of answer generation for TweetQA and we use several standard metrics for natural language generation to evaluate QA systems on our dataset, namely we consider BLEU-1 BIBREF16 , Meteor BIBREF17 and Rouge-L BIBREF18 in this paper. \n Question: What evaluation metrics do they use?",
            "output": [
                "BLEU-1 Meteor  Rouge-L "
            ]
        },
        {
            "id": "task460-977781e0b9d246188af6c0d24928f186",
            "input": "Axelrod's causal mapping method comprises a set of conventions to graphically represent networks of causes and effects (the nodes in a network) as well as the qualitative aspects of this relation (the network’s directed edges, notably assertions of whether the causal linkage is positive or negative). \n Question: What are the causal mapping methods employed?",
            "output": [
                "Axelrod's causal mapping method"
            ]
        },
        {
            "id": "task460-d91558ec261a4d049c49a518e8996218",
            "input": "While a host of input modalities have been considered in other NLG tasks, such as text summarization BIBREF24 , image captioning BIBREF25 and table-to-text generation BIBREF26 , traditional QG mainly focused on textual inputs, especially declarative sentences, explained by the original application domains of question answering and education, which also typically featured textual inputs.\n\nRecently, with the growth of various QA applications such as Knowledge Base Question Answering (KBQA) BIBREF27 and Visual Question Answering (VQA) BIBREF28 , NQG research has also widened the spectrum of sources to include knowledge bases BIBREF29 and images BIBREF10 . \n Question: What are all the input modalities considered in prior work in question generation?",
            "output": [
                "Textual inputs, knowledge bases, and images."
            ]
        },
        {
            "id": "task460-ad63ff20b81d4b65ab4cb33683f04335",
            "input": " The selection model selects the best answer from the set $\\lbrace a_i\\rbrace _{i=1}^N$ observed during the interaction by predicting the difference of the F1 score to the average F1 of all variants. We train another neural network to pick the best answer from the candidates. We frame the task as binary classification, distinguishing between above and below average performance. In training, we compute the F1 score of the answer for every instance. If the rewrite produces an answer with an F1 score greater than the average score of the other rewrites the instance is assigned a positive label. \n Question: how are multiple answers from multiple reformulated questions aggregated?",
            "output": [
                "The selection model selects the best answer from the set $\\lbrace a_i\\rbrace _{i=1}^N$ observed during the interaction by predicting the difference of the F1 score to the average F1 of all variants."
            ]
        },
        {
            "id": "task460-c1fc1d75cfcc4d2ba9d3d35f61c62fae",
            "input": "Pre-trained English NER model We construct the English NER system following BIBREF7 . This system uses a bidirectional LSTM as a character-level language model to take context information for word embedding generation.  For pre-trained English NER system, we use the default NER model of Flair. \n Question: Which pre-trained English NER model do they use?",
            "output": [
                "Bidirectional LSTM based NER model of Flair"
            ]
        },
        {
            "id": "task460-a6d8b099b8624c58bcde6b708290bb88",
            "input": "In addition to the traditional VAE structure, we introduces an extra context-aware latent variable in CWVAE to learn the event background knowledge. In the pretrain stage, CWVAE is trained on an auxiliary dataset (consists of three narrative story corpora and contains rich event background knowledge), to learn the event background information by using the context-aware latent variable. Subsequently, in the finetune stage, CWVAE is trained on the task-specific dataset to adapt the event background information to each specific aspect of If-Then inferential target (e.g., intents, reactions, etc.). \n Question: How does the context-aware variational autoencoder learn event background information?",
            "output": [
                " CWVAE is trained on an auxiliary dataset to learn the event background information by using the context-aware latent variable.  Then, in finetute stage, CWVAE is trained on the task-specific dataset to adapt the event background information to each specific aspect of If-Then inferential target."
            ]
        },
        {
            "id": "task460-0719dd389b2e48508e2c9c45b187e1f8",
            "input": "In this work, we propose to generate unanswerable questions by editing an answerable question and conditioning on the corresponding paragraph that contains the answer. So the generated unanswerable questions are more lexically similar and relevant to the context. Moreover, by using the answerable question as a prototype and its answer span as a plausible answer, the generated examples can provide more discriminative training signal to the question answering model. \n Question: Does their approach require a dataset of unanswerable questions mapped to similar answerable questions?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-093c268fc8e44e7dbde2634dc98f5d24",
            "input": "These models define parametrized similarity scoring functions $: Q\\times T\\rightarrow \\mathbb {R}$ , where $Q$ is the set of natural language questions and $T$ is the set of paraphrases of logical forms. \n Question: Does a neural scoring function take both the question and the logical form as inputs?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-29eb79a76c8d42b190e3a6fc8564e059",
            "input": "Due to the high cost of pre-training BERT language model, we directly adopt parameters pre-trained by Google in Chinese general corpus. The named entity recognition is applied on both pathology report texts and query texts. \n Question: What data is the language model pretrained on?",
            "output": [
                "Chinese general corpus"
            ]
        },
        {
            "id": "task460-efaa541a2a90461bb4c17888f8cb63df",
            "input": "This method, which we refer to as progressive dynamic hurdles (PDH), allows models that are consistently performing well to train for more steps. It begins as ordinary tournament selection evolutionary architecture search with early stopping, with each child model training for a relatively small $s_0$ number of steps before being evaluated for fitness. However, after a predetermined number of child models, $m$ , have been evaluated, a hurdle, $h_0$ , is created by calculating the the mean fitness of the current population. For the next $m$ child models produced, models that achieve a fitness greater than $h_0$ after $s_0$ train steps are granted an additional $s_1$ steps of training and then are evaluated again to determine their final fitness. Once another $m$ models have been considered this way, another hurdle, $h_1$ , is constructed by calculating the mean fitness of all members of the current population that were trained for the maximum number of steps. For the next $m$ child models, training and evaluation continues in the same fashion, except models with fitness greater than $m$0 after $m$1 steps of training are granted an additional $m$2 number of train steps, before being evaluated for their final fitness. This process is repeated until a satisfactory number of maximum training steps is reached. \n Question: what is the proposed Progressive Dynamic Hurdles method?",
            "output": [
                "allows models that are consistently performing well to train for more steps"
            ]
        },
        {
            "id": "task460-10ee17b96cf2459cba28c6167142459f",
            "input": "We propose the following challenge to the community: We must develop formal definition and evaluation for faithfulness that allows us the freedom to say when a method is sufficiently faithful to be useful in practice.\n\nWe note two possible approaches to this end:\n\nAcross models and tasks: The degree (as grayscale) of faithfulness at the level of specific models and tasks. Perhaps some models or tasks allow sufficiently faithful interpretation, even if the same is not true for others.\n\nFor example, the method may not be faithful for some question-answering task, but faithful for movie review sentiment, perhaps based on various syntactic and semantic attributes of those tasks.\n\nAcross input space: The degree of faithfulness at the level of subspaces of the input space, such as neighborhoods of similar inputs, or singular inputs themselves. If we are able to say with some degree of confidence whether a specific decision's explanation is faithful to the model, even if the interpretation method is not considered universally faithful, it can be used with respect to those specific areas or instances only. \n Question: What faithfulness criteria does they propose?",
            "output": [
                "Across models and tasks: The degree (as grayscale) of faithfulness at the level of specific models and tasks. Across input space: The degree of faithfulness at the level of subspaces of the input space, such as neighborhoods of similar inputs, or singular inputs themselves."
            ]
        },
        {
            "id": "task460-6862647522b842d89bce931b9357293b",
            "input": "Since it is infeasible to manually annotate all of the comments, we process this dataset with the goal of extracting threads that involve suspected trolling attempts and the direct responses to them.  We had two human annotators who were trained on snippets (i.e., (suspected trolling attempt, responses) pairs) taken from 200 conversations and were allowed to discuss their findings. After this training stage, we asked them to independently label the four aspects for each snippet.  \n Question: Do they use a crowdsourcing platform for annotation?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-80552595e44a4a1886af834fc75b4685",
            "input": "We evaluate performance of our models on the SQUAD BIBREF16 dataset (denoted $\\mathcal {S}$ ). We use the same split as that of BIBREF4 , where a random subset of 70,484 instances from $\\mathcal {S}\\ $ are used for training ( ${\\mathcal {S}}^{tr}$ ), 10,570 instances for validation ( ${\\mathcal {S}}^{val}$ ), and 11,877 instances for testing ( ${\\mathcal {S}}^{te}$ ). \n Question: Which datasets are used to train this model?",
            "output": [
                "SQUAD"
            ]
        },
        {
            "id": "task460-e63993b0e511444d9a49a5b676f4a88f",
            "input": "In this paper, in spite of previous statements, we present a system that uses rule-based and dictionary-based methods combined (in a way we prefer to call resource-based).  \n Question: What does their system consist of?",
            "output": [
                "rule-based and dictionary-based methods "
            ]
        },
        {
            "id": "task460-ba8ac864c49149a5b026babf557c7f95",
            "input": "We evaluate a multi-task approach and three algorithms that explicitly model the task dependencies. \n Question: Will these findings be robust through different datasets and different question answering algorithms?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-cf6de692749140109c3bf324db68d55f",
            "input": "Gazetteers: 6)Contributor first names; 7)Contributor last names; 8)Contributor types (\"soprano\", \"violinist\", etc.); 9)Classical work types (\"symphony\", \"overture\", etc.); 10)Musical instruments; 11)Opus forms (\"op\", \"opus\"); 12)Work number forms (\"no\", \"number\"); 13)Work keys (\"C\", \"D\", \"E\", \"F\" , \"G\" , \"A\", \"B\", \"flat\", \"sharp\"); 14)Work Modes (\"major\", \"minor\", \"m\"). \n Question: What task-specific features are used?",
            "output": [
                "6)Contributor first names 7)Contributor last names 8)Contributor types (\"soprano\", \"violinist\", etc.) 9)Classical work types (\"symphony\", \"overture\", etc.) 10)Musical instruments 11)Opus forms (\"op\", \"opus\") 12)Work number forms (\"no\", \"number\") 13)Work keys (\"C\", \"D\", \"E\", \"F\" , \"G\" , \"A\", \"B\", \"flat\", \"sharp\") 14)Work Modes (\"major\", \"minor\", \"m\")"
            ]
        },
        {
            "id": "task460-48cfc54f242b4a5b8892fb084a5f1d3b",
            "input": "As can be seen, the proposed DSC loss outperforms the best baseline results by a large margin, i.e., outperforming BERT-tagger by +1.86 in terms of F1 score on CTB5, +1.80 on CTB6 and +2.19 on UD1.4. \n Question: What are method's improvements of F1 w.r.t. baseline BERT tagger for Chinese POS datasets?",
            "output": [
                "+1.86 in terms of F1 score on CTB5 +1.80 on CTB6 +2.19 on UD1.4"
            ]
        },
        {
            "id": "task460-90c3a3a5670443acb3102c22222b7cb8",
            "input": "We use Waseem and Hovy BIBREF5, Davidson et al. BIBREF9, and Waseem et al. BIBREF10 as baselines and compare the results with our different fine-tuning strategies using pre-trained BERTbase model.  \n Question: What baseline is used?",
            "output": [
                "Waseem and Hovy BIBREF5 Davidson et al. BIBREF9 Waseem et al. BIBREF10"
            ]
        },
        {
            "id": "task460-04f3709db86b4779946aa29c1d68f1e1",
            "input": "The regressor and classifier used Support Vector Regression (SVR) and Support Vector Classification (SVC) respectively. The table includes a neural regressor (NNR) and a neural classifier (NNC). The neural classifier is trained in two set ups: “NNC top 5” uses classification labels as described in Section SECREF3, and “NNC SU4 F1” uses the regression labels, that is, the ROUGE-SU4 F1 scores of each sentence. \n Question: What classification approaches were experimented for this task?",
            "output": [
                "NNC SU4 F1 NNC top 5 Support Vector Classification (SVC)"
            ]
        },
        {
            "id": "task460-4526a68650ed4cb7b16e9319414f9d39",
            "input": "Questions are gathered from anonymized, aggregated queries to the Google search engine. Queries that are likely to be yes/no questions are heuristically identified: we found selecting queries where the first word is in a manually constructed set of indicator words and are of sufficient length, to be effective.\n\nQuestions are only kept if a Wikipedia page is returned as one of the first five results, in which case the question and Wikipedia page are given to a human annotator for further processing.\n\nAnnotators label question/article pairs in a three-step process. First, they decide if the question is good, meaning it is comprehensible, unambiguous, and requesting factual information. This judgment is made before the annotator sees the Wikipedia page. Next, for good questions, annotators find a passage within the document that contains enough information to answer the question. Annotators can mark questions as “not answerable\" if the Wikipedia article does not contain the requested information. Finally, annotators mark whether the question's answer is “yes\" or “no\". Annotating data in this manner is quite expensive since annotators need to search entire Wikipedia documents for relevant evidence and read the text carefully. \n Question: how was the dataset built?",
            "output": [
                "Questions are gathered from anonymized, aggregated queries to the Google search engine. Queries that are likely to be yes/no questions are heuristically identified: we found selecting queries where the first word is in a manually constructed set of indicator words and are of sufficient length, to be effective. Questions are only kept if a Wikipedia page is returned as one of the first five results, in which case the question and Wikipedia page are given to a human annotator for further processing. Annotators label question/article pairs in a three-step process. First, they decide if the question is good, meaning it is comprehensible, unambiguous, and requesting factual information. This judgment is made before the annotator sees the Wikipedia page. Next, for good questions, annotators find a passage within the document that contains enough information to answer the question. Annotators can mark questions as “not answerable\" if the Wikipedia article does not contain the requested information. Finally, annotators mark whether the question's answer is “yes\" or “no\""
            ]
        },
        {
            "id": "task460-a1c45dcf6a4a47b6a84f9a2852ae0be7",
            "input": "This step consists of generating a query out of the claim and querying a search engine (here, we experiment with Google and Bing) in order to retrieve supporting documents. Rather than querying the search engine with the full claim (as on average, a claim is two sentences long), we generate a shorter query following the lessons highlighted in BIBREF0 . We rank the words by means of tf-idf. We compute the idf values on a 2015 Wikipedia dump and the English Gigaword. BIBREF0 suggested that a good way to perform high-quality search is to only consider the verbs, the nouns and the adjectives in the claim; thus, we exclude all words in the claim that belong to other parts of speech. Moreover, claims often contain named entities (e.g., names of persons, locations, and organizations); hence, we augment the initial query with all the named entities from the claim's text. We use IBM's AlchemyAPI to identify named entities. Ultimately, we generate queries of 5–10 tokens, which we execute against a search engine. We then collect the snippets and the URLs in the results, skipping any result that points to a domain that is considered unreliable. \n Question: How are the potentially relevant text fragments identified?",
            "output": [
                " Generate a query out of the claim and querying a search engine, rank the words by means of TF-IDF, use IBM's AlchemyAPI to identify named entities, generate queries of 5–10 tokens, which execute against a search engine, and collect the snippets and the URLs in the results, skipping any result that points to a domain that is considered unreliable."
            ]
        },
        {
            "id": "task460-6dda2bee42034b469cfd982821e22e54",
            "input": "In order to train a neural g2p system, one needs a large quantity of pronunciation data. A standard dataset for g2p is the Carnegie Mellon Pronouncing Dictionary BIBREF12 . However, that is a monolingual English resource, so it is unsuitable for our multilingual task. Instead, we use the multilingual pronunciation corpus collected by deri2016grapheme for all experiments. This corpus consists of spelling–pronunciation pairs extracted from Wiktionary. It is already partitioned into training and test sets. Corpus statistics are presented in Table TABREF10 In addition to the raw IPA transcriptions extracted from Wiktionary, the corpus provides an automatically cleaned version of transcriptions. Cleaning is a necessary step because web-scraped data is often noisy and may be transcribed at an inconsistent level of detail. The data cleaning used here attempts to make the transcriptions consistent with the phonemic inventories used in Phoible BIBREF4 .  \n Question: what datasets did they use?",
            "output": [
                "the Carnegie Mellon Pronouncing Dictionary BIBREF12 the multilingual pronunciation corpus collected by deri2016grapheme  ranscriptions extracted from Wiktionary"
            ]
        },
        {
            "id": "task460-3ceea78b29aa409faa2cfb913f196637",
            "input": "One way to analyze the model is to compute model gradients with respect to input features BIBREF26, BIBREF25. Figure FIGREF37 shows that in this particular example, the most important model inputs are verbs possibly associated with the entity butter, in addition to the entity's mentions themselves. It further shows that the model learns to extract shallow clues of identifying actions exerted upon only the entity being tracked, regardless of other entities, by leveraging verb semantics. \n Question: What evidence do they present that the model attends to shallow context clues?",
            "output": [
                "Using model gradients with respect to input features they presented that the most important model inputs are verbs associated with entities which shows that the model attends to shallow context clues"
            ]
        },
        {
            "id": "task460-e25aaacf55d04d33aaea75e8cab3d797",
            "input": "For de-identification tasks, the three metrics we will use to evaluate the performance of our architecture are Precision, Recall and INLINEFORM0 score as defined below. \n Question: What evaluation metrics do they use?",
            "output": [
                "Precision, Recall and INLINEFORM0 score"
            ]
        },
        {
            "id": "task460-fead6e0190af4005b2b7ed6faae6e5c1",
            "input": "We initiated crawling with 100 questions randomly selected from different topics so that different genre of questions can be covered. The crawling of the questions follow a BFS pattern through the related question links. We obtained 822,040 unique questions across 80,253 different topics with a total of 1,833,125 answers to these questions. \n Question: Does the experiments focus on a specific domain?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-75310088cb014662b9d21ed9ba4fd0c9",
            "input": "TOEFL listening comprehension test is for human English learners whose native language is not English. This paper reports how today's machine can perform with such a test.\n\nThe listening comprehension task considered here is highly related to Spoken Question Answering (SQA) BIBREF0 , BIBREF1 .  \n Question: What is the new task proposed in this work?",
            "output": [
                " listening comprehension task "
            ]
        },
        {
            "id": "task460-24dc9d1ad0494133a1e2bbd442071758",
            "input": "For the other techniques, we extract paragraphs containing any word from a predetermined list of LGTBQ terms (shown in Table TABREF19) \n Question: How do they identify discussions of LGBTQ people in the New York Times?",
            "output": [
                "act paragraphs containing any word from a predetermined list of LGTBQ terms "
            ]
        },
        {
            "id": "task460-c2104f71db6f4f12ab7b81edf7de1553",
            "input": "Therefore, in our analysis, we focus on the chain features related to the phenomena of shining through and explicitation. \n Question: Which coreference phenomena are analyzed?",
            "output": [
                "shining through explicitation"
            ]
        },
        {
            "id": "task460-396e5f33a8b44c5aa64646d1bf1c9bb0",
            "input": "The functional dissimilarity score was computed using sentences from the test set in CoNLL 2017 Universal Dependencies task BIBREF20 for the relevant languages with the provided UPOS sequences. We computed the nearest neighbours experiment for all languages in the training data for the above models. \n Question: Which evaluation metrics do they use for language modelling?",
            "output": [
                " functional dissimilarity score nearest neighbours experiment"
            ]
        },
        {
            "id": "task460-0abf2b8089404810bd55d5364707fa30",
            "input": "With the purpose of comparing the proposed model with a popular state-of-the-art approach for image classification, for the LabelMe dataset, the following baseline was introduced:\n\nBosch 2006 (mv): This baseline is similar to one in BIBREF33 . The authors propose the use of pLSA to extract the latent topics, and the use of k-nearest neighbor (kNN) classifier using the documents' topics distributions. For this baseline, unsupervised LDA is used instead of pLSA, and the labels from the different annotators for kNN (with INLINEFORM0 ) are aggregated using majority voting (mv).\n\nThe results obtained by the different approaches for the LabelMe data are shown in Fig. FIGREF94 , where the svi version is using mini-batches of 200 documents.\n\nAnalyzing the results for the Reuters-21578 and LabelMe data, we can observe that MA-sLDAc outperforms all the baselines, with slightly better accuracies for the batch version, especially in the Reuters data. Interestingly, the second best results are consistently obtained by the multi-annotator approaches, which highlights the need for accounting for the noise and biases of the answers of the different annotators. Both the batch and the stochastic variational inference (svi) versions of the proposed model (MA-sLDAc) are compared with the following baselines:\n\n[itemsep=0.02cm]\n\nLDA + LogReg (mv): This baseline corresponds to applying unsupervised LDA to the data, and learning a logistic regression classifier on the inferred topics distributions of the documents. The labels from the different annotators were aggregated using majority voting (mv). Notice that, when there is a single annotator label per instance, majority voting is equivalent to using that label for training. This is the case of the 20-Newsgroups' simulated annotators, but the same does not apply for the experiments in Section UID89 .\n\nLDA + Raykar: For this baseline, the model of BIBREF21 was applied using the documents' topic distributions inferred by LDA as features.\n\nLDA + Rodrigues: This baseline is similar to the previous one, but uses the model of BIBREF9 instead.\n\nBlei 2003 (mv): The idea of this baseline is to replicate a popular state-of-the-art approach for document classification. Hence, the approach of BIBREF0 was used. It consists of applying LDA to extract the documents' topics distributions, which are then used to train a SVM. Similarly to the previous approach, the labels from the different annotators were aggregated using majority voting (mv).\n\nsLDA (mv): This corresponds to using the classification version of sLDA BIBREF2 with the labels obtained by performing majority voting (mv) on the annotators' answers. \n Question: what are the state of the art approaches?",
            "output": [
                "Bosch 2006 (mv) LDA + LogReg (mv) LDA + Raykar LDA + Rodrigues Blei 2003 (mv) sLDA (mv)"
            ]
        },
        {
            "id": "task460-c9c7f497df1d4a0c8558bc10862de2c2",
            "input": "We defined the reward as being 1 for successfully completing the task, and 0 otherwise. A discount of $0.95$ was used to incentivize the system to complete dialogs faster rather than slower, yielding return 0 for failed dialogs, and $G = 0.95^{T-1}$ for successful dialogs, where $T$ is the number of system turns in the dialog.   0.95^{T-1} reward  0.95^{T-1}  \n Question: What is the reward model for the reinforcement learning appraoch?",
            "output": [
                "reward 1 for successfully completing the task, with a discount by the number of turns, and reward 0 when fail"
            ]
        },
        {
            "id": "task460-f6f56436ed274925b70d84ecc26310eb",
            "input": "Although such an approach has been used in different studies during feature engineering, the selection of word vectors and the number of clusters remain a trial-end-error procedure.  \n Question: Which other hyperparameters, other than number of clusters are typically evaluated in this type of research?",
            "output": [
                "selection of word vectors"
            ]
        },
        {
            "id": "task460-f8957c6445834eb1a80876116f1c763c",
            "input": "For the sentiment analysis, we look at our problem in a multi-label setting, our two labels being sentiment polarity and the candidate/category in consideration. \n Question: How many label options are there in the multi-label task?",
            "output": [
                " two labels "
            ]
        },
        {
            "id": "task460-aa4dd072dcf248c89c9eec12e4dedf15",
            "input": "SCA BIBREF5 softly augments a randomly chosen word in a sentence by its contextual mixture of multiple related words, i.e., replacing the one-hot representation of a word by a distribution provided by a language model over the vocabulary. \n Question: How does soft contextual data augmentation work?",
            "output": [
                "softly augments a randomly chosen word in a sentence by its contextual mixture of multiple related words replacing the one-hot representation of a word by a distribution provided by a language model over the vocabulary"
            ]
        },
        {
            "id": "task460-ba95b1f850be4960812f2ddc1accac6a",
            "input": "The baseline approach is based on BIBREF20 . It divides the task of interpreting commands for behavioral navigation into two steps: path generation, and path verification. For path generation, this baseline uses a standard sequence-to-sequence model augmented with an attention mechanism, similar to BIBREF23 , BIBREF6 . For path verification, the baseline uses depth-first search to find a route in the graph that matches the sequence of predicted behaviors. If no route matches perfectly, the baseline changes up to three behaviors in the predicted sequence to try to turn it into a valid path. \n Question: What baselines did they compare their model with?",
            "output": [
                "the baseline where path generation uses a standard sequence-to-sequence model augmented with attention mechanism and path verification uses depth-first search"
            ]
        },
        {
            "id": "task460-9b13f728caab4e7fa487dd01c006dfc3",
            "input": "The most intuitive way to represent syntactic information is to use individual dependency relations directly, like dependency head and dependency relation label, denoted as Dep and Rel for short. In order to preserve the structural information of dependency trees as much as possible, we take the syntactic path between candidate arguments and predicates in dependency trees as linguistic knowledge. Referring to BIBREF9, we use the Tree-based Position Feature (TPF) as Dependency Path (DepPath) and use the Shortest Dependency Path (SDP) as Relation Path (RelPath). \n Question: What different approaches of encoding syntactic information authors present?",
            "output": [
                "dependency head and dependency relation label, denoted as Dep and Rel for short Tree-based Position Feature (TPF) as Dependency Path (DepPath) Shortest Dependency Path (SDP) as Relation Path (RelPath)"
            ]
        },
        {
            "id": "task460-958d7821b9134c18bef3cb909b7e454e",
            "input": "The output of a system with the target words in the predicted order is compared to the gold ranking of the DURel data set. As the metric to assess how well the model's output fits the gold ranking Spearman's $\\rho $ was used. The higher Spearman's rank-order correlation the better the system's performance. \n Question: How is evaluation performed?",
            "output": [
                "As the metric to assess how well the model's output fits the gold ranking Spearman's $\\rho $ was used"
            ]
        },
        {
            "id": "task460-58691c0114e24a7a9d2562bf63da80aa",
            "input": "Features. We argue that different kinds of features like the sentiment of the text, morality, and other text-based features are critical to detect the nonfactual Twitter accounts by utilizing their occurrence during reporting the news in an account's timeline. We employ a rich set of features borrowed from previous works in fake news, bias, and rumors detection BIBREF0, BIBREF1, BIBREF8, BIBREF9.\n\n[leftmargin=4mm]\n\nEmotion: We build an emotions vector using word occurrences of 15 emotion types from two available emotional lexicons. We use the NRC lexicon BIBREF10, which contains $\\sim $14K words labeled using the eight Plutchik's emotions BIBREF11. The other lexicon is SentiSense BIBREF12 which is a concept-based affective lexicon that attaches emotional meanings to concepts from the WordNet lexical database. It has $\\sim $5.5 words labeled with emotions from a set of 14 emotional categories We use the categories that do not exist in the NRC lexicon.\n\nSentiment: We extract the sentiment of the tweets by employing EffectWordNet BIBREF13, SenticNet BIBREF14, NRC BIBREF10, and subj_lexicon BIBREF15, where each has the two sentiment classes, positive and negative.\n\nMorality: Features based on morality foundation theory BIBREF16 where words are labeled in one of the following 10 categories (care, harm, fairness, cheating, loyalty, betrayal, authority, subversion, sanctity, and degradation).\n\nStyle: We use canonical stylistic features, such as the count of question marks, exclamation marks, consecutive characters and letters, links, hashtags, users' mentions. In addition, we extract the uppercase ratio and the tweet length.\n\nWords embeddings: We extract words embeddings of the words of the tweet using $Glove\\-840B-300d$ BIBREF17 pretrained model. The tweet final representation is obtained by averaging its words embeddings. \n Question: What features are extracted?",
            "output": [
                "Sentiment Morality Style Words embeddings"
            ]
        },
        {
            "id": "task460-49225c5bb93e4ad190e40c7cac8170e9",
            "input": "Our contribution leverages pre-trained word embeddings (GloVe, trained on wikipedia+gigaword corpus), the DepecheMood affective lexicon, and convolutional neural networks. \n Question: What embeddings do they use?",
            "output": [
                "GloVe"
            ]
        },
        {
            "id": "task460-025cb88f53a14595b5bdb1b25d411e8d",
            "input": "Our evaluation comprises of a rich set of 19 different algorithms to recommend tags for e-books, which we group into (i) popularity-based, (ii) similarity-based (i.e., using content information), and (iii) hybrid approaches. \n Question: what algorithms did they use?",
            "output": [
                "popularity-based similarity-based hybrid"
            ]
        },
        {
            "id": "task460-17e336e474ce4167a7b9708bfc87cbe9",
            "input": "Traditional text-to-speech (TTS) systems are composed of complex pipelines BIBREF0 , these often include accoustic frontends, duration model, acoustic prediction model and vocoder models. The architecture of our model utilizes RNN-based Seq2Seq model for generating mel spectrogram from text. The architecture is similar to that of Tacotron 2 BIBREF4  Direct comparison of model parameters between ours and the open-source tacotron 2, our model contains 4.5 million parameters, whereas the Tacotron 2 contains around 13 million parameters with default setting. By helping our model learn attention alignment faster, we can afford to use a smaller overall model to achieve similar quality speech quality. \n Question: Do they reduce the number of parameters in their architecture compared to other direct text-to-speech models?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-ed1cc890a0b640c89659c48f5c10f05a",
            "input": "The dataset comprises a total of 353 conversations from 40 speakers (11 nurses, 16 patients, and 13 caregivers) with consent to the use of anonymized data for research. In Section SECREF16 , we build templates and expression pools using linguistic analysis followed by manual verification. \n Question: How big is their created dataset?",
            "output": [
                "353 conversations from 40 speakers (11 nurses, 16 patients, and 13 caregivers) we build templates and expression pools using linguistic analysis"
            ]
        },
        {
            "id": "task460-34b0eec6a6574b4cb9283be19a811825",
            "input": "Our model encodes the information from audio and text sequences using dual RNNs and then combines the information from these sources using a feed-forward neural model to predict the emotion class. \n Question: How do they combine audio and text sequences in their RNN?",
            "output": [
                "combines the information from these sources using a feed-forward neural model"
            ]
        },
        {
            "id": "task460-10508bdab6524f22a918317ecefb84ef",
            "input": "We apply Han to such conversations in a sequential manner by feeding each user turn to Han as they occur, to determine if the conversation should escalate. If so, the user will be transferred to a live chat representative to continue the conversation. The user must wait for the human representative to review the IVA chat history and resume the failed task. \n Question: How do they gather human reviews?",
            "output": [
                "human representative to review the IVA chat history and resume the failed task"
            ]
        },
        {
            "id": "task460-3c19c54f4c84478b865c66f9727ded89",
            "input": "we follow Lapata2005Automatic to measure coherence as sentence similarity \n Question: Did the authors evaluate their system output for coherence?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-11217343bb034d978912580b2685a1cd",
            "input": "Experiments ::: Baselines\nWe evaluate MPAD against multiple state-of-the-art baseline models, including hierarchical ones, to enable fair comparison with the hierarchical MPAD variants.\n\ndoc2vec BIBREF37. Doc2vec (or paragraph vector) is an extension of word2vec that learns vectors for documents in a fully unsupervised manner. Document embeddings are then fed to a logistic regression classifier.\n\nCNN BIBREF38. The convolutional neural network architecture, well-known in computer vision, is applied to text. There is one spatial dimension and the word embeddings are used as channels (depth dimensions).\n\nDAN BIBREF39. The Deep Averaging Network passes the unweighted average of the embeddings of the input words through multiple dense layers and a final softmax.\n\nTree-LSTM BIBREF40 is a generalization of the standard LSTM architecture to constituency and dependency parse trees.\n\nDRNN BIBREF41. Recursive neural networks are stacked and applied to parse trees.\n\nLSTMN BIBREF42 is an extension of the LSTM model where the memory cell is replaced by a memory network which stores word representations.\n\nC-LSTM BIBREF43 combines convolutional and recurrent neural networks. The region embeddings provided by a CNN are fed to a LSTM.\n\nSPGK BIBREF44 also models documents as word co-occurrence networks. It computes a graph kernel that compares shortest paths extracted from the word co-occurrence networks and then uses a SVM to categorize documents.\n\nWMD BIBREF45 is an application of the well-known Earth Mover's Distance to text. A k-nearest neighbor classifier is used.\n\nS-WMD BIBREF46 is a supervised extension of the Word Mover's Distance.\n\nSemantic-CNN BIBREF47. Here, a CNN is applied to semantic units obtained by clustering words in the embedding space.\n\nLSTM-GRNN BIBREF26 is a hierarchical model where sentence embeddings are obtained with a CNN and a GRU-RNN is fed the sentence representations to obtain a document vector.\n\nHN-ATT BIBREF27 is another hierarchical model, where the same encoder architecture (a bidirectional GRU-RNN) is used for both sentences and documents, with different parameters. A self-attention mechanism is applied to the RNN annotations at each level. \n Question: What is the state-of-the-art system?",
            "output": [
                "doc2vec  CNN DAN Tree-LSTM DRNN LSTMN C-LSTM SPGK WMD S-WMD Semantic-CNN LSTM-GRNN HN-ATT"
            ]
        },
        {
            "id": "task460-34b323c992f5415a96ab200dd22caeb4",
            "input": "Because of this, we do not claim that this dataset can be considered a ground truth. \n Question: How is the ground truth for fake news established?",
            "output": [
                "Ground truth is not established in the paper"
            ]
        },
        {
            "id": "task460-b013bd9cd64f43378cc181b0d4f48635",
            "input": "The only requirement is that the model accepts as input, an embedding layer (for entities and relations). If a model fulfills this requirement (which a large number of neural models on knowledge graphs do), we can just use Dolores embeddings as a drop-in replacement. We just initialize the corresponding embedding layer with Dolores embeddings. \n Question: Is fine-tuning required to incorporate these embeddings into existing models?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-d274d16eb7134b0eb119ae39f84cf06f",
            "input": "Here is a list of architectures for which reference implementations and pretrained weights are currently provided in Transformers. These models fall into two main categories: generative models (GPT, GPT-2, Transformer-XL, XLNet, XLM) and models for language understanding (Bert, DistilBert, RoBERTa, XLM).\n\nBERT (BIBREF13) is a bi-directional Transformer-based encoder pretrained with a linear combination of masked language modeling and next sentence prediction objectives.\n\nRoBERTa (BIBREF5) is a replication study of BERT which showed that carefully tuning hyper-parameters and training data size lead to significantly improved results on language understanding.\n\nDistilBERT (BIBREF32) is a smaller, faster, cheaper and lighter version BERT pretrained with knowledge distillation.\n\nGPT (BIBREF34) and GPT2 (BIBREF9) are two large auto-regressive language models pretrained with language modeling. GPT2 showcased zero-shot task transfer capabilities on various tasks such as machine translation or reading comprehension.\n\nTransformer-XL (BIBREF35) introduces architectural modifications enabling Transformers to learn dependency beyond a fixed length without disrupting temporal coherence via segment-level recurrence and relative positional encoding schemes.\n\nXLNet (BIBREF4) builds upon Transformer-XL and proposes an auto-regressive pretraining scheme combining BERT's bi-directional context flow with auto-regressive language modeling by maximizing the expected likelihood over permutations of the word sequence.\n\nXLM (BIBREF8) shows the effectiveness of pretrained representations for cross-lingual language modeling (both on monolingual data and parallel data) and cross-lingual language understanding.\n\nWe systematically release the model with the corresponding pretraining heads (language modeling, next sentence prediction for BERT) for adaptation using the pretraining objectives. Some models fine-tuned on downstream tasks such as SQuAD1.1 are also available. Overall, more than 30 pretrained weights are provided through the library including more than 10 models pretrained in languages other than English. Some of these non-English pretrained models are multi-lingual models (with two of them being trained on more than 100 languages) . \n Question: What state-of-the-art general-purpose pretrained models are made available under the unified API? ",
            "output": [
                "BERT RoBERTa DistilBERT GPT GPT2 Transformer-XL XLNet XLM"
            ]
        },
        {
            "id": "task460-203b3837f468413284169f38ae99ab36",
            "input": "In this work we train bilingual UWS models using the endangered language Mboshi as target and different well-resourced languages as aligned information. Results show that similar languages rank better in terms of segmentation performance, and that by combining the information learned by different models, segmentation is further improved. This might be due to the different language-dependent structures that are captured by using more than one language.  \n Question: How does the well-resourced language impact the quality of the output?",
            "output": [
                "Results show that similar languages rank better in terms of segmentation performance, and that by combining the information learned by different models, segmentation is further improved."
            ]
        },
        {
            "id": "task460-a2e1710880f34dc4bc998ccf5796c296",
            "input": "To discover topics from the collected tweets, we used a topic modeling approach that fuzzy clusters the semantically related words such as assigning “diabetes\", “cancer\", and “influenza\" into a topic that has an overall “disease\" theme BIBREF44 , BIBREF45 . Among topic models, Latent Dirichlet Allocation (LDA) BIBREF49 is the most popular effective model BIBREF50 , BIBREF19 as studies have shown that LDA is an effective computational linguistics model for discovering topics in a corpus BIBREF51 , BIBREF52 . We used the Mallet implementation of LDA BIBREF49 , BIBREF56 with its default settings to explore opinions in the tweets. \n Question: How were topics of interest about DDEO identified?",
            "output": [
                "using topic modeling model Latent Dirichlet Allocation (LDA)"
            ]
        },
        {
            "id": "task460-f6fc6b3493194a52acf9d28a76cbf1d6",
            "input": "We present and evaluate a model that we have developed for this, called QuaSP+Zero, that modifies the QuaSP+ parser as follows: During decoding, at points where the parser is selecting which property to include in the LF (e.g., Figure FIGREF31 ), it does not just consider the question tokens, but also the relationship between those tokens and the properties INLINEFORM0 used in the qualitative model. This approach follows the entity-linking approach used by BIBREF11 Krishnamurthy2017NeuralSP, where the similarity between question tokens and (words associated with) entities - called the entity linking score - help decide which entities to include in the LF during parsing. \n Question: How does the QuaSP+Zero model work?",
            "output": [
                "does not just consider the question tokens, but also the relationship between those tokens and the properties"
            ]
        },
        {
            "id": "task460-a09caf8e3b104aa492b37ef8f856bdd6",
            "input": "Some authors use pretrained embeddings (especially when their data set is too small to train their own embeddings) or try to modify these embeddings and adjust to their set. But the biggest drawback of these approaches is that the corpus for training embeddings can be not related to the specific task where embeddings are utilized. A lot of medical concepts are not contained in well-known embeddings bases. Furthermore, the similarity of words may vary in different contexts. Then, we compute embeddings of concepts (by GloVe) for interview descriptions and for examination descriptions separately. \n Question: Do they fine-tune the used word embeddings on their medical texts?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-a2ae1145fbea447883c1bc2b71a0aba3",
            "input": "Our work is further connected to the topic of interpretability and transparency of machine learning models BIBREF11, BIBREF35, BIBREF12, for which humans are increasingly involved, for instance for post-hoc evaluations of the model's interpretability.  \n Question: How are the interpretability merits of the approach demonstrated?",
            "output": [
                "By involving humans for post-hoc evaluation of model's interpretability"
            ]
        },
        {
            "id": "task460-c90d58ffcb534921a813f1dea2453151",
            "input": "The final code-mixed tweets were forwarded to a group of three annotators who were university students and fluent in both English and Hindi. \n Question: How many annotators tagged each text?",
            "output": [
                "three "
            ]
        },
        {
            "id": "task460-841ff586042843a59759dc0c9b609e8a",
            "input": "In order to further analysis the embeddings learned by our method, we use t-SNE tool BIBREF27 to visualize the learned embedding. \n Question: What further analysis is done?",
            "output": [
                "we use t-SNE tool BIBREF27 to visualize the learned embedding"
            ]
        },
        {
            "id": "task460-aac0c368a73b4f1d991d6e942d77a602",
            "input": "The data set we evaluate on in this work is WMT English-French NewsTest2014, which has 380M words of parallel training data and a 3003 sentence test set. The NewsTest2013 set is used for validation. \n Question: Do they only test on one dataset?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-5e1d560f9a8345fea53d18e7b8f5b2b6",
            "input": "We employ precision, recall, F1 and accuracy for evaluation metrics. \n Question: what evaluation metrics are reported?",
            "output": [
                "precision, recall, F1 and accuracy"
            ]
        },
        {
            "id": "task460-a6cd38822bc546b4abc79dfb1935dc38",
            "input": "It is is now clear that some configurations used in some architectures (e.g., additional RNN layers) are actually irrelevant and can be removed altogether without harming accuracy. \n Question: What are least important components identified in the the training of VQA models?",
            "output": [
                " some configurations used in some architectures (e.g., additional RNN layers) are actually irrelevant"
            ]
        },
        {
            "id": "task460-fda8675d553a4989851823c244f8302d",
            "input": "We start by crawling the content of a few notable debating websites: idebate.com, debatewise.org, procon.org. \n Question: What debate websites did they look at?",
            "output": [
                "idebate.com debatewise.org procon.org"
            ]
        },
        {
            "id": "task460-f177fedd82e04f008aa728367259c6bd",
            "input": "The baseline system for the SLC task is a very simple logistic regression classifier with default parameters, where we represent the input instances with a single feature: the length of the sentence.  The baseline for the FLC task generates spans and selects one of the 18 techniques randomly.  \n Question: What was the baseline for this task?",
            "output": [
                "The baseline system for the SLC task is a very simple logistic regression classifier with default parameters. The baseline for the FLC task generates spans and selects one of the 18 techniques randomly."
            ]
        },
        {
            "id": "task460-d72263fd79fa4ebb8765b88218d7d737",
            "input": "We use the Ultrax Typically Developing dataset (UXTD) from the publicly available UltraSuite repository BIBREF19 . This dataset contains synchronized acoustic and ultrasound data from 58 typically developing children, aged 5-12 years old (31 female, 27 male).  \n Question: How many speakers do they have in the dataset?",
            "output": [
                "58"
            ]
        },
        {
            "id": "task460-a6454d433a9d4bf8bfedc9358a50cb5b",
            "input": "The second Turkish dataset is the Twitter corpus which is formed of tweets about Turkish mobile network operators. Those tweets are mostly much noisier and shorter compared to the reviews in the movie corpus. In total, there are 1,716 tweets. 973 of them are negative and 743 of them are positive. These tweets are manually annotated by two humans, where the labels are either positive or negative. \n Question: What details are given about the Twitter dataset?",
            "output": [
                "Those tweets are mostly much noisier and shorter compared to the reviews in the movie corpus. In total, there are 1,716 tweets. 973 of them are negative and 743 of them are positive."
            ]
        },
        {
            "id": "task460-3e428f9c821a456998b0c8550b38027c",
            "input": "Information Gathering: At step $t$ of the information gathering phase, the agent can issue one of the following four actions to interact with the paragraph $p$, where $p$ consists of $n$ sentences and where the current observation corresponds to sentence $s_k,~1 \\le k \\le n$:\n\nprevious: jump to $ \\small {\\left\\lbrace \\begin{array}{ll} s_n & \\text{if $k = 1$,}\\\\ s_{k-1} & \\text{otherwise;} \\end{array}\\right.} $\n\nnext: jump to $ \\small {\\left\\lbrace \\begin{array}{ll} s_1 & \\text{if $k = n$,}\\\\ s_{k+1} & \\text{otherwise;} \\end{array}\\right.} $\n\nCtrl+F $<$query$>$: jump to the sentence that contains the next occurrence of “query”;\n\nstop: terminate information gathering phase. \n Question: What commands does their setup provide to models seeking information?",
            "output": [
                "previous next Ctrl+F $<$query$>$ stop"
            ]
        },
        {
            "id": "task460-87227ae4b05f4f8c8a543bf35da5f2b7",
            "input": " Our street view environment was integrated into ParlAI BIBREF6 and used to collect a large-scale dataset on Mechanical Turk involving human perception, action and communication. \n Question: What data did they use?",
            "output": [
                " dataset on Mechanical Turk involving human perception, action and communication"
            ]
        },
        {
            "id": "task460-18e2b31dde8c41149e2e1c63feef0c4e",
            "input": "The CRF module achieved the best result on the Thai sentence segmentation task BIBREF8 ; therefore, we adopt the Bi-LSTM-CRF model as our baseline.  \n Question: Which deep learning architecture do they use for sentence segmentation?",
            "output": [
                "Bi-LSTM-CRF"
            ]
        },
        {
            "id": "task460-cec83ce0789244cda7f2864daf122086",
            "input": "To study how the multimodal context can boost the performance compared to an unimodal context we evaluate different models: a Feature Concatenation Model (FCM), a Spatial Concatenation Model (SCM) and a Textual Kernels Model (TKM). \n Question: What different models for multimodal detection were proposed?",
            "output": [
                "Feature Concatenation Model (FCM) Spatial Concatenation Model (SCM) Textual Kernels Model (TKM)"
            ]
        },
        {
            "id": "task460-60360157a988481b8f03bbce0cc73492",
            "input": "Our goal is to supply an NMT system with knowledge regarding the speaker and interlocutor of first-person sentences, in order to produce the desired target-side morphology when the information is not available in the source sentence. The approach we take in the current work is that of black-box injection, in which we attempt to inject knowledge to the input in order to influence the output of a trained NMT system, without having access to its internals or its training procedure as proposed by vanmassenhove-hardmeier-way:2018:EMNLP. To verify this, we experiment with translating the sentences with the following variations: No Prefix—The baseline translation as returned by the GMT system. “He said:”—Signaling a male speaker. We expect to further skew the system towards masculine forms. “She said:”—Signaling a female speaker and unknown audience. As this matches the actual speaker's gender, we expect an improvement in translation of first-person pronouns and verbs with first-person pronouns as subjects. “I said to them:”—Signaling an unknown speaker and plural audience. “He said to them:”—Masculine speaker and plural audience. “She said to them:”—Female speaker and plural audience—the complete, correct condition. We expect the best translation accuracy on this setup. “He/she said to him/her”—Here we set an (incorrect) singular gender-marked audience, to investigate our ability to control the audience morphology. \n Question: What are the components of the black-box context injection system?",
            "output": [
                "supply an NMT system with knowledge regarding the speaker and interlocutor of first-person sentences"
            ]
        },
        {
            "id": "task460-da45843284e24cdb837eba863827fd6a",
            "input": "We recruited 176 AMT workers to participate in our conceptualization task. \n Question: What was the task given to workers?",
            "output": [
                "conceptualization task"
            ]
        },
        {
            "id": "task460-1b9844ff8fdd4922bb4142c04623a804",
            "input": "We compare the performance of translation approaches based on four metrics:\n\n[align=left,leftmargin=0em,labelsep=0.4em,font=]\n\nAs in BIBREF20 , EM is 1 if a predicted plan matches exactly the ground truth; otherwise it is 0.\n\nThe harmonic average of the precision and recall over all the test set BIBREF26 .\n\nThe minimum number of insertions, deletions or swap operations required to transform a predicted sequence of behaviors into the ground truth sequence BIBREF27 .\n\nGM is 1 if a predicted plan reaches the ground truth destination (even if the full sequence of behaviors does not match exactly the ground truth). Otherwise, GM is 0. \n Question: What evaluation metrics are used?",
            "output": [
                "exact match, f1 score, edit distance and goal match"
            ]
        },
        {
            "id": "task460-125d24a82d11404eb1f6b5386ff80a03",
            "input": "This paper experimented with two different models and compared them against each other.  \n Question: what was the baseline?",
            "output": [
                "There is no baseline."
            ]
        },
        {
            "id": "task460-ec1a1b04fe624f85aa98180830b1630b",
            "input": "Even if the optional lemmatization step is applied, one can still aim at further reducing the graph complexity by merging similar vertices. This step is called meta vertex construction. The meta-vertex construction step works as follows. Let INLINEFORM0 represent the set of vertices, as defined above. A meta vertex INLINEFORM1 is comprised of a set of vertices that are elements of INLINEFORM2 , i.e. INLINEFORM3 . Let INLINEFORM4 denote the INLINEFORM5 -th meta vertex. We construct a given INLINEFORM6 so that for each INLINEFORM7 , INLINEFORM8 's initial edges (prior to merging it into a meta vertex) are rewired to the newly added INLINEFORM9 . Note that such edges connect to vertices which are not a part of INLINEFORM10 . Thus, both the number of vertices, as well as edges get reduced substantially. This feature is implemented via the following procedure: Meta vertex candidate identification. Edit distance and word lengths distance are used to determine whether two words should be merged into a meta vertex (only if length distance threshold is met, the more expensive edit distance is computed). The meta vertex creation. As common identifiers, we use the stemmed version of the original vertices and if there is more than one resulting stem, we select the vertex from the identified candidates that has the highest centrality value in the graph and its stemmed version is introduced as a novel vertex (meta vertex). \n Question: How are meta vertices computed?",
            "output": [
                "Meta vertex candidate identification. Edit distance and word lengths distance are used to determine whether two words should be merged into a meta vertex (only if length distance threshold is met, the more expensive edit distance is computed). The meta vertex creation. As common identifiers, we use the stemmed version of the original vertices and if there is more than one resulting stem, we select the vertex from the identified candidates that has the highest centrality value in the graph and its stemmed version is introduced as a novel vertex (meta vertex)."
            ]
        },
        {
            "id": "task460-6c26df1920fc49a6a2867da4383ff43d",
            "input": "We sparsify this graph to the union of a minimum spanning tree and a k-Nearest Neighbors (MST-kNN) graph BIBREF14, a geometric construction that removes less important similarities but preserves global connectivity for the graph and, hence, for the dataset. The MST-kNN graph is then analysed with Markov Stability BIBREF15, BIBREF16, BIBREF17, BIBREF18, a multi-resolution graph partitioning method that identifies relevant subgraphs (i.e., clusters of documents) at different levels of granularity. \n Question: What cluster identification method is used in this paper?",
            "output": [
                "A combination of Minimum spanning trees, K-Nearest Neighbors and Markov Stability BIBREF15, BIBREF16, BIBREF17, BIBREF18"
            ]
        },
        {
            "id": "task460-fcbe420725c14b13acda600bf0cf75e8",
            "input": "We use two datasets for experiments (Table TABREF29 ): (a) STAN INLINEFORM0 , created by BIBREF10 BansalBV15, which consists of 1,108 unique English hashtags from 1,268 randomly selected tweets in the Stanford Sentiment Analysis Dataset BIBREF36 along with their crowdsourced segmentations and our additional corrections; and (b) STAN INLINEFORM1 , our new expert curated dataset, which includes all 12,594 unique English hashtags and their associated tweets from the same Stanford dataset. \n Question: Do the hashtag and SemEval datasets contain only English data?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-52eddccce4a643de814176fa316b6b84",
            "input": "We evaluated baselines and our model using accuracy as the metric on the ROCStories dataset, and summarized these results in Table 2 . \n Question: Which metrics are they evaluating with?",
            "output": [
                "accuracy"
            ]
        },
        {
            "id": "task460-006332a382a345f398d795322b48c96e",
            "input": "We use GraphParser without paraphrases as our baseline. This gives an idea about the impact of using paraphrases We compare our paraphrasing models with monolingual machine translation based model for paraphrase generation BIBREF24 , BIBREF36 . In particular, we use Moses BIBREF37 to train a monolingual phrase-based MT system on the Paralex corpus. Finally, we use Moses decoder to generate 10-best distinct paraphrases for the test questions. \n Question: What are the baselines?",
            "output": [
                "GraphParser without paraphrases monolingual machine translation based model for paraphrase generation"
            ]
        },
        {
            "id": "task460-92ca2a25491f4599b5a6a6f7a87264c9",
            "input": "The annotation process was a trial-and-error, with cycles composed of annotation, discussing confusing entities, updating the annotation guide schematic and going through the corpus section again to correct entities following guide changes. \n Question: Did they experiment with the corpus?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-bebc3d00843941e185660cada5213a32",
            "input": "INSIGHT 1: Political handles are more likely to engage in profile changing behavior as compared to their followers.  We analyze the trends in Figure FIGREF12 and find that the political handles do not change their usernames at all. This is in contrast to the trend in Figure FIGREF15 where we see that there are a lot of handles that change their usernames multiple times.  INSIGHT 3: Political handles tend to make new changes related to previous attribute values. However, the followers make comparatively less related changes to previous attribute values. \n Question: How do profile changes vary for influential leads and their followers over the social movement?",
            "output": [
                "Influential leaders are more likely to change their profile attributes than their followers; the leaders do not change their usernames, while their followers change their usernames a lot; the leaders  tend to make new changes related to previous attribute values, while the followers make comparatively less related changes to previous attribute values."
            ]
        },
        {
            "id": "task460-bd196c9d3f3f45d7b2bd9cf67ed923cf",
            "input": "Moreover, this work also handles the cases of singular/plural, subject pronoun/object pronoun, etc. For instance, the pronoun “he\" is used for the subject as “he\" but is used for the object as “him\". \n Question: What else is tried to be solved other than 12 tenses, model verbs and negative form?",
            "output": [
                "cases of singular/plural, subject pronoun/object pronoun, etc."
            ]
        },
        {
            "id": "task460-4744c4b6b9ec489d9cd4402f7e6c9501",
            "input": "For all the tasks in our experimental study, we use 36 millions English tweets collected between August and September 2017.  \n Question: Do they report results only on English datasets?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-3a921c2c73a24cdcb49c58d7bb1bca86",
            "input": "$LS \\cup KLD \\cup CONN$ and $KLD \\cup LS \\cup LS_{inter}$ are the best systems with the highest recall and F1-score respectively. \n Question: What was previous state-of-the-art approach?",
            "output": [
                "TextCNN, TextRNN, SASE, DPCNN, and BERT $LS \\cup KLD \\cup CONN$ and $KLD \\cup LS \\cup LS_{inter}$ are the best systems with the highest recall and F1-score respectively"
            ]
        },
        {
            "id": "task460-7410a977d3044d299671c8a7f975afc5",
            "input": "To systematically evaluate the importance of the clinical sentiment values extracted from the free text in EHRs, we first build a baseline model using the structured features, which are similar to prior studies on readmission risk prediction BIBREF6. \n Question: Do they compare to previous models?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-4d27cd6a545f4494a7086c05c0056385",
            "input": " They perform data augmentation by back translation between English and Chinese, which seems to be one of the distinguishing factors resulting in a much higher accuracy. \n Question: What data augmentation techniques are used?",
            "output": [
                "back translation between English and Chinese"
            ]
        },
        {
            "id": "task460-286fabb5cc534482baba994706a93451",
            "input": "We show the statistics of the dataset we use in tab:statistics, and the construction procedures will be introduced in this section. In Wikidata BIBREF8 , facts can be described as (Head item/property, Property, Tail item/property).  We first choose the most common 202 relations and 120000 entities from Wikidata as our initial data. ReVerb BIBREF9 is a program that automatically identifies and extracts binary relationships from English sentences. We use the extractions from running ReVerb on Wikipedia. FB15K BIBREF3 is a subset of freebase. TACRED BIBREF10 is a large supervised relation extraction dataset obtained via crowdsourcing. We directly use these two dataset, no extra processing steps were applied. \n Question: Which knowledge bases do they use?",
            "output": [
                "Wikidata ReVerb FB15K TACRED"
            ]
        },
        {
            "id": "task460-6830b99d79b541fe810f954c5de917f2",
            "input": "Machine translation finds use in cheminformatics in “translation\" from one language (e.g. reactants) to another (e.g. products). The variational Auto-encoder (VAE) is another widely adopted text generation architecture BIBREF101. Generative Adversarial Network (GAN) models generate novel molecules by using two components: the generator network generates novel molecules, and the discriminator network aims to distinguish between the generated molecules and real molecules BIBREF107. \n Question: Are this models usually semi/supervised or unsupervised?",
            "output": [
                "Both supervised and unsupervised, depending on the task that needs to be solved."
            ]
        },
        {
            "id": "task460-0f802360e43346cb9e3adf77d76c2db7",
            "input": "In addition, we propose a new standard experimental protocol for the IBM-UB-1 dataset BIBREF25 (Sec. SECREF50 ) to enable easier comparison between approaches in the future. The IAM-OnDB dataset BIBREF42 is probably the most used evaluation dataset for online handwriting recognition. It consists of 298 523 characters in 86 272 word instances from a dictionary of 11 059 words written by 221 writers. We use the standard IAM-OnDB dataset separation: one training set, two validations sets and a test set containing 5 363, 1 438, 1 518 and 3 859 written lines, respectively. We tune the decoder weights using the validation set with 1 438 items and report error rates on the test set. We provide an evaluation of our production system trained on our in-house datasets applied to a number of publicly available benchmark datasets from the literature. Note that for all experiments presented in this section we evaluate our current live system without any tuning specifec to the tasks at hand.\n\nThe ICDAR-2013 Competition for Online Handwriting Chinese Character Recognition BIBREF45 introduced a dataset for classifying the most common Chinese characters. We report the error rates in comparison to published results from the competition and more recent work done by others in Table TABREF56 . In the ICFHR2018 Competition on Vietnamese Online Handwritten Text Recognition using VNOnDB BIBREF50 , our production system was evaluated against other systems. The system used in the competition is the one reported and described in this paper. Due to licensing restrictions we were unable to do any experiments on the competition training data, or specific tuning for the competition, which was not the case for the other systems mentioned here. \n Question: What datasets did they use?",
            "output": [
                "IBM-UB-1 dataset BIBREF25 IAM-OnDB dataset BIBREF42 The ICDAR-2013 Competition for Online Handwriting Chinese Character Recognition BIBREF45 ICFHR2018 Competition on Vietnamese Online Handwritten Text Recognition using VNOnDB BIBREF50"
            ]
        },
        {
            "id": "task460-a5204516baa744888e67580ce9c6dc6c",
            "input": "After excluding all the words that are not used by at least three separate users in our training set, we build our AllWords model by counting the frequencies of all the remaining words and training a multinomial Naive Bayes classifier.  \n Question: What model did they use for their system?",
            "output": [
                "AllWords model by counting the frequencies of all the remaining words and training a multinomial Naive Bayes classifier"
            ]
        },
        {
            "id": "task460-ef6e508a65974bbdb67973301af4a242",
            "input": "N-grams, POS, Hierarchical features: A baseline bag-of-words model incorporating both tagged and untagged unigrams and bigams.  CNN: Kim BIBREF28 demonstrated near state-of-the-art performance on a number of sentence classification tasks (including TREC question classification) by using pre-trained word embeddings BIBREF40 as feature extractors in a CNN model. \n Question: What previous methods is their model compared to?",
            "output": [
                "bag-of-words model CNN"
            ]
        },
        {
            "id": "task460-a240a1bd945f47ecbd78e2a7097fb336",
            "input": "We set up the baselines and proposed models as follows: LSTM with text embedding: CNNs and LSTMs are widely used to encode textual contents for sentiment analysis in BIBREF45 , BIBREF46 and many online tutorials. Here we select the standard LSTM with pre-trained word embedding as input, and add one fully-connected layer with sigmoid activation top of the LSTM encoder (same as all other models), denoted as T-LSTM. LSTM with emoji embedding: We consider the emoji as one special word and input both pre-trained text and emoji embeddings into the same LSTM network, namely E-LSTM. Similarly, we concatenate the pre-trained bi-sense emoji embedding as one special word to feed into the LSTM network. This model is called BiE-LSTM. Attention-based LSTM with emojis:We also use the word-emoji embedding to calculate the emoji-word attention following Equation EQREF20 and EQREF21 , and the only difference is that we replace the attention-derived senti-emoji embedding with the pre-trained word-emoji embedding by fasttext, denoted as ATT-E-LSTM. \n Question: What is the baseline for experiments?",
            "output": [
                "LSTM with text embedding LSTM with emoji embedding Attention-based LSTM with emojis"
            ]
        },
        {
            "id": "task460-475ee26159da442b95ec31bfbb259c46",
            "input": "Our baseline system (Baseline_1850K) is taken from BIBREF13 . It consists of a DNN trained to predict subword targets within the keywords. The input to the DNN consists of a sequence with INLINEFORM0 frames of left and INLINEFORM1 frames of right context; each with a stride of INLINEFORM2 . The topology consists of a 1-D convolutional layer with 92 filters (of shape 8x8 and stride 8x8), followed by 3 fully-connected layers with 512 nodes and a rectified linear unit activation each. A final softmax output predicts the 7 subword targets, obtained from the same forced alignment process described in SECREF5 . This results in the baseline DNN containing 1.7M parameters, and performing 1.8M multiply-accumulate operations per inference (every 30ms of streaming audio). A keyword spotting score between 0 and 1 is computed by first smoothing the posterior values, averaging them over a sliding window of the previous 100 frames with respect to the current INLINEFORM3 ; the score is then defined as the largest product of the smoothed posteriors in the sliding window as originally proposed in BIBREF6 . \n Question: What previous approaches are considered?",
            "output": [
                "Our baseline system (Baseline_1850K) is taken from BIBREF13 . "
            ]
        },
        {
            "id": "task460-6671ce7401d841ba908925ffac5e2238",
            "input": "Given the relatively small quest length for TextWorld games—games can be completed in as little as 5 steps—we generate 50 such games and partition them into train and test sets in a 4:1 ratio. We choose the game, 9:05 as our target task game due to similarities in structure in addition to the vocabulary overlap.  For the horror domain, we choose Lurking Horror to train the question-answering system on. The source and target task games are chosen as Afflicted and Anchorhead respectively. \n Question: What games are used to test author's methods?",
            "output": [
                "Lurking Horror Afflicted Anchorhead 9:05 TextWorld games"
            ]
        },
        {
            "id": "task460-bd958184c5a7418b8b5efccbb7a4008f",
            "input": "The text and image encodings were combined by concatenation, which resulted in a feature vector of 4,864 dimensions. This multimodal representation was afterward fed as input into a multi-layer perceptron (MLP) with two hidden layer of 100 neurons with a ReLU activation function. \n Question: Is the dataset multimodal?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-e8f1d2f2ae394339b1c1912ceea247e1",
            "input": "We compare eight different models in our experiments. Four of them have a Projected Layer (see Fig. FIGREF2), while the others do not have, and this is the only difference between these two groups of our models. So, we actually include four models in our experiments (having a projected layer or not). Firstly, LastStateRNN is the classic RNN model, where the last state passes through an MLP and then the LR Layer estimates the corresponding probability. In contrast, in the AvgRNN model we consider the average vector of all states that come out of the cells. The AttentionRNN model is the one that it has been presented in BIBREF9. Moreover, we introduce the MultiAttentionRNN model for the harassment language detection, which instead of one attention, it includes four attentions, one for each category. \n Question: What are the different variations of the attention-based approach which are examined?",
            "output": [
                "classic RNN model, avgRNN model, attentionRNN model and multiattention RNN model with and without a projected layer"
            ]
        },
        {
            "id": "task460-c19559e88ed24aceb1df8829f06311ad",
            "input": "So, we can impose the constraint that our model's representation of the input's syntax cannot contain this context-invariant information. This regularization is strictly preferable to allowing all aspects of word meaning to propagate into the input's syntax representation. Without such a constraint, all inputs could, in principle, be given their own syntactic categories. \n Question: Does having constrained neural units imply word meanings are fixed across different context?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-f98dde6519c841afa428d52d1ba4dbef",
            "input": "We also run a manual evaluation which shows for the En-It task a slight quality degradation in exchange of a statistically significant reduction in the average length ratio, from 1.05 to 1.01. \n Question: Do they conduct any human evaluation?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-d032224ecfb54db59db749a2abad9790",
            "input": "A quantitative measure is required to reliably evaluate the achieved improvement. One of the methods proposed to measure the interpretability is the word intrusion test BIBREF41 . But, this method is expensive to apply since it requires evaluations from multiple human evaluators for each embedding dimension. In this study, we use a semantic category-based approach based on the method and category dataset (SEMCAT) introduced in BIBREF27 to quantify interpretability. Specifically, we apply a modified version of the approach presented in BIBREF40 in order to consider possible sub-groupings within the categories in SEMCAT.  \n Question: What experiments do they use to quantify the extent of interpretability?",
            "output": [
                "Human evaluation for interpretability using the word intrusion test and automated evaluation for interpretability using a semantic category-based approach based on the method and category dataset (SEMCAT)."
            ]
        },
        {
            "id": "task460-5f8f67c89eb5401e88795842722886f2",
            "input": "Benchmark Evaluation ::: Classifier Models\nSVM: A linear support vector machine with bag-of-words sentence representations.\n\nMLP: A multi-layer perceptron with USE embeddings BIBREF4 as input.\n\nFastText: A shallow neural network that averages embeddings of n-grams BIBREF5.\n\nCNN: A convolutional neural network with non-static word embeddings initialized with GloVe BIBREF6.\n\nBERT: A neural network that is trained to predict elided words in text and then fine-tuned on our data BIBREF1.\n\nPlatforms: Several platforms exist for the development of task-oriented agents. We consider Google's DialogFlow and Rasa NLU with spacy-sklearn. \n Question: Which classifiers are evaluated?",
            "output": [
                "SVM MLP FastText CNN BERT Google's DialogFlow Rasa NLU"
            ]
        },
        {
            "id": "task460-079a402c0cf64e96b48c7b90cab9c0ae",
            "input": "By providing the Wizard with several dialogue options (aside from free text), we guided the conversation and could introduce actions that change an internal system state. This proposes several advantages:\n\nA guided dialogue allows for set procedures to be learned and reduces the amount of data needed for a machine learning model for dialogue management to converge.\n\nProviding several dialogue options to the Wizard increases the pace of the interaction and allows them to understand and navigate more complex scenarios. Dialogue structure: we introduced structured dialogues through a Finite State Machine (FSM) that controls the current dialogue state and offers multiple suitable and relevant state transitions (actions) to the Wizard depending on the point in the interaction, the state of the world and the history. A graph of dialogue states, transitions and utterances is loaded when the system is initialised, and each chat room has its own dialogue state, which changes through actions. In addition to the transitions that the FSM provides, there are other fixed dialogue options always available such as “Hold on, 2 seconds”, “Okay” or “Sorry, can you repeat that?” as a shortcut for commonly used dialogue acts, as well as the option to type a message freely. The dialogue has several paths to reach the same states with varying levels of Operator control or engagement that enriched the heterogeneity of conversations. \n Question: What is meant by semiguided dialogue, what part of dialogue is guided?",
            "output": [
                "The Wizard can select one of several predefined messages to send, or type their own message if needed. Free text messages do not change the dialogue state in the FSM, so it is important to minimise their use by providing enough dialogue options to the Wizard."
            ]
        },
        {
            "id": "task460-139e29d0865642a895e5ad55bd69ece1",
            "input": "Then a comparison to a maximum entropy (MaxEnt) proposed in BIBREF23 type of discriminative model and finally a deep neural network (DNN) architecture proposed in BIBREF24 as reported also in BIBREF22 is presented.\n\n \n Question: What state-of-the-art models are compared against?",
            "output": [
                "a deep neural network (DNN) architecture proposed in BIBREF24  maximum entropy (MaxEnt) proposed in BIBREF23 type of discriminative model"
            ]
        },
        {
            "id": "task460-a0ba549006cc407884e147c659ea0ba1",
            "input": "For task 1, we use F1-score as evaluation metric. We use manual evaluation for task 2. There are five evaluation metrics for task 2 as following.\n\nTask completion ratio: The number of completed tasks divided by the number of total tasks.\n\nUser satisfaction degree: There are five scores -2, -1, 0, 1, 2, which denote very dissatisfied, dissatisfied, neutral, satisfied and very satisfied, respectively.\n\nResponse fluency: There are three scores -1, 0, 1, which indicate nonfluency, neutral, fluency.\n\nNumber of dialogue turns: The number of utterances in a task-completed dialogue.\n\nGuidance ability for out of scope input: There are two scores 0, 1, which represent able to guide or unable to guide. \n Question: What metrics are used in the evaluation?",
            "output": [
                "For task 1, we use F1-score Task completion ratio User satisfaction degree Response fluency Number of dialogue turns Guidance ability for out of scope input"
            ]
        },
        {
            "id": "task460-b36d51d35c6540939b7c07d6a2b24bee",
            "input": "We perform experiments using the following state-of-the-art models: (1) SEQ2SEQ: a sequence-to-sequence model with attention mechanisms BIBREF21, (2) CVAE: a conditional variational auto-encoder model with KL-annealing and a BOW loss BIBREF2, (3) Transformer: an encoder-decoder architecture relying solely on attention mechanisms BIBREF22, (4) HRED: a generalized sequence-to-sequence model with the hierarchical RNN encoder BIBREF23, (5) DialogWAE: a conditional Wasserstein auto-encoder, which models the distribution of data by training a GAN within the latent variable space BIBREF6. \n Question: What state of the art models were used in experiments?",
            "output": [
                "SEQ2SEQ CVAE Transformer HRED DialogWAE"
            ]
        },
        {
            "id": "task460-6680e0e43f5e45238507daf61170a91e",
            "input": "We introduce here the use of an L-PCFG with two layers of latent states: one layer is intended to capture the usual syntactic information, and the other aims to capture semantic and topical information by using a large set of states with specific feature functions. \n Question: What latent variables are modeled in the PCFG?",
            "output": [
                "syntactic information semantic and topical information"
            ]
        },
        {
            "id": "task460-da54e38b9f974bbf9fd53d585a3ed74f",
            "input": "Below, we explore different ways to back off when the ScRNN predicts UNK (a frequent outcome for rare and unseen words):\n\nPass-through: word-recognizer passes on the (possibly misspelled) word as is.\n\nBackoff to neutral word: Alternatively, noting that passing $\\colorbox {gray!20}{\\texttt {UNK}}$ -predicted words through unchanged exposes the downstream model to potentially corrupted text, we consider backing off to a neutral word like `a', which has a similar distribution across classes.\n\nBackoff to background model: We also consider falling back upon a more generic word recognition model trained upon a larger, less-specialized corpus whenever the foreground word recognition model predicts UNK. Figure 1 depicts this scenario pictorially. \n Question: How do the backoff strategies work?",
            "output": [
                "In pass-through, the recognizer passes on the possibly misspelled word, backoff to neutral word backs off to a word with similar distribution across classes and backoff to background model backs off to a more generic word recognition model trained with larger and less specialized corpus."
            ]
        },
        {
            "id": "task460-f89efb0b87a949f5b269bbc4762bc5e6",
            "input": "Next, for emergent language, we show that the MASC architecture can achieve very high localization accuracy, significantly outperforming the baseline that does not include this mechanism. \n Question: What evaluation metrics did the authors look at?",
            "output": [
                "localization accuracy"
            ]
        },
        {
            "id": "task460-338c2237e6ae46498a2e1438397d83c7",
            "input": "Throughout all our experiments, we used BERT-Base BIBREF2 to provide the state-of-the-art contextualized modeling of the input text.\n\nSemantic Retrieval: We treated the neural semantic retrieval at both the paragraph and sentence level as binary classification problems with models' parameters updated by minimizing binary cross entropy loss. \n Question: How do they model the neural retrieval modules?",
            "output": [
                "BERT-Base BIBREF2 to provide the state-of-the-art contextualized modeling"
            ]
        },
        {
            "id": "task460-6a1026b1526a4b46a9cd0a7750681284",
            "input": "The proposed system consists of a Bi-directional Long Short-Term Memory (BiLSTM) BIBREF16, a two-level attention mechanism BIBREF29, BIBREF30 and a shared representation for emotion and sentiment analysis tasks. The BiLSTM encodes the word representation of each word. This representation is shared between the subsystems of sentiment and emotion analysis. Each of the shared representations is then fed to the primary attention mechanism of both the subsystems. The primary attention mechanism finds the best representation for each word for each task. The secondary attention mechanism acts on top of the primary attention to extract the best sentence representation by focusing on the suitable context for each task. Finally, the representations of both the tasks are fed to two different feed-forward neural networks to produce two outputs - one for sentiment analysis and one for emotion analysis. Each component is explained in the subsequent subsections. \n Question: How is multi-tasking performed?",
            "output": [
                "The proposed system consists of a Bi-directional Long Short-Term Memory (BiLSTM) BIBREF16, a two-level attention mechanism BIBREF29, BIBREF30 and a shared representation for emotion and sentiment analysis tasks. Each of the shared representations is then fed to the primary attention mechanism"
            ]
        },
        {
            "id": "task460-8511674376304a35bc24b9126af9e547",
            "input": "IDEA\nBIBREF9 Two different BERT models were developed. For Friends, pre-training was done using a sliding window of two utterances to provide dialogue context. Both Next Sentence Prediction (NSP) phase on the complete unlabeled scripts from all 10 seasons of Friends, which are available for download. In addition, the model learned the emotional disposition of each of six main six main characters in Friends (Rachel, Monica, Phoebe, Joey, Chandler and Ross) by adding a special token to represent the speaker. For EmotionPush, pre-training was performed on Twitter data, as it is similar in nature to chat based dialogues. In both cases, special attention was given to the class imbalance issue by applying “weighted balanced warming” on the loss function. \n Question: What model was used by the top team?",
            "output": [
                "Two different BERT models were developed"
            ]
        },
        {
            "id": "task460-b857ad82e8d8460689b17333af068d25",
            "input": "We decided to use sentences involving at least one race- or gender-associated word. The sentences were intended to be short and grammatically simple. We also wanted some sentences to include expressions of sentiment and emotion, since the goal is to test sentiment and emotion systems. \n Question: What criteria are used to select the 8,640 English sentences?",
            "output": [
                "Sentences involving at least one race- or gender-associated word,  sentence  have to be short and grammatically simple,  sentence have to  include expressions of sentiment and emotion."
            ]
        },
        {
            "id": "task460-a53f43a5fe5d44238b54015200b4e02a",
            "input": "We present here a step in this direction: a probabilistic semantic parser that uses a large knowledge base (NELL) to form a prior probability distribution on the meanings of sentences it parses, and that \"understands\" each sentence either by identifying its existing beliefs that correspond to the sentence's meaning, or by creating new beliefs. \n Question: What knowledge bases do they use?",
            "output": [
                "NELL"
            ]
        },
        {
            "id": "task460-2de66f47b5184a04b497eed1ab8cd3d7",
            "input": "The Digital Library in the TextGrid Repository represents an extensive collection of German texts in digital form BIBREF3. \n Question: What is the corpus used in the study?",
            "output": [
                "TextGrid Repository"
            ]
        },
        {
            "id": "task460-40029447a86244f8b7baf08de85f62eb",
            "input": "We evaluate CBA, LSA, DCA, and GMMv2b using mean opinion score (MOS) naturalness judgments produced by a crowd-sourced pool of raters. Scores range from 1 to 5, with 5 representing “completely natural speech”. \n Question: How they compare varioius mechanisms in terms of naturalness?",
            "output": [
                "using mean opinion score (MOS) naturalness judgments produced by a crowd-sourced pool of raters"
            ]
        },
        {
            "id": "task460-5aa7e3e17b484621b815ff4ffc905c96",
            "input": "As previously mentioned, there is no German hate speech corpus available for our needs, especially not for the very recent topic of the refugee crisis in Europe. We therefore had to compile our own corpus. \n Question: What languages are were included in the dataset of hateful content?",
            "output": [
                "German"
            ]
        },
        {
            "id": "task460-7dd9a43212c24c2d903e3575f9f96697",
            "input": "We evaluate our framework on fastText embeddings trained on Wikipedia (2017), UMBC webbase corpus and statmt.org news dataset (16B tokens) BIBREF11. For simplicity, only the first 22000 words are used in all embeddings, though preliminary results indicate the findings extend to the full corpus. For our novel methods of mitigating bias, a shallow neural network is used to adjust the embedding. The single layer of the model is an embedding layer with weights initialized to those of the original embedding. For the composite method, these weights are initialized to those of the embedding after probabilistic bias mitigation. A batch of word indices is fed into the model, which are then embedded and for which a loss value is calculated, allowing back-propagation to adjust the embeddings. For each of the models, a fixed number of iterations is used to prevent overfitting, which can eventually hurt performance on the embedding benchmarks (See Figure FIGREF12). We evaluated the embedding after 1000 iterations, and stopped training if performance on a benchmark decreased significantly. We construct a list of candidate words to debias, taken from the words used in the WEAT gender bias statistics. Words in this list should be gender neutral, and are related to the topics of career, arts, science, math, family and professions (see appendix). We note that this list can easily be expanded to include a greater proportion of words in the corpus. For example, BIBREF4 suggested a method for identifying inappropriately gendered words using unsupervised learning. We compare this method of bias mitigation with the no bias mitigation (\"Orig\"), geometric bias mitigation (\"Geo\"), the two pieces of our method alone (\"Prob\" and \"KNN\") and the composite method (\"KNN+Prob\"). We note that the composite method performs reasonably well according the the RIPA metric, and much better than traditional geometric bias mitigation according to the neighborhood metric, without significant performance loss according to the accepted benchmarks. To our knowledge this is the first bias mitigation method to perform reasonably both on both metrics. \n Question: How is embedding quality assessed?",
            "output": [
                "We compare this method of bias mitigation with the no bias mitigation (\"Orig\"), geometric bias mitigation (\"Geo\"), the two pieces of our method alone (\"Prob\" and \"KNN\") and the composite method (\"KNN+Prob\"). We note that the composite method performs reasonably well according the the RIPA metric, and much better than traditional geometric bias mitigation according to the neighborhood metric, without significant performance loss according to the accepted benchmarks. To our knowledge this is the first bias mitigation method to perform reasonably both on both metrics."
            ]
        },
        {
            "id": "task460-4f7a54de7cc8469ba67bd3870ba0bced",
            "input": "Recently, a number of researchers have endeavored to explore methods for simultaneous translation in the context of NMT BIBREF6, BIBREF7, BIBREF8, BIBREF9. Some of them propose sophisticated training frameworks explicitly designed for simultaneous translation BIBREF5, BIBREF10.  \n Question: Has there been previous work on SNMT?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-beca048b4bac4a99a549c34e85816725",
            "input": "To evaluate the proposed BiLSTM model with attention (BiLSTM+att), it is compared with three of its own variants: a BiLSTM without attention (BiLSTM) as well as a single forward-LSTM layer with attention (LSTM+att) and without attention (LSTM). Additional baselines are defined by BIBREF32 who already proposed an LSTM-based architecture that only uses non-temporal features, and the SVM-based estimation model as originally used for reward estimation by BIBREF24. \n Question: What model do they use a baseline to estimate satisfaction?",
            "output": [
                "a BiLSTM without attention (BiLSTM) as well as a single forward-LSTM layer with attention (LSTM+att) and without attention (LSTM) baselines are defined by BIBREF32 who already proposed an LSTM-based architecture that only uses non-temporal features, and the SVM-based estimation model as originally used for reward estimation by BIBREF24"
            ]
        },
        {
            "id": "task460-66b6edb8d88c4ca6bbe3a150257661c5",
            "input": "Specifically, PlEWi supplied 550,755 [error, correction] pairs, from which 298,715 were unique. \n Question: How is PIEWi annotated?",
            "output": [
                "[error, correction] pairs"
            ]
        },
        {
            "id": "task460-5e04975095e3454280bc34a7778123ab",
            "input": "Many NLP tasks utilize POS as features, but human annotated POS sequences are difficult and expensive to obtain. Thus, it is important to know if we can learn sentences-level syntactic embeddings for low-sources languages without treebanks.\n\nWe performed zero-shot transfer of the syntactic embeddings for French, Portuguese and Indonesian. French and Portuguese are simulated low-resource languages, while Indonesian is a true low-resource language. \n Question: Do they evaluate on downstream tasks?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-ee15b1cfa4334fa8ad05d6f682db844a",
            "input": "The Hausa data used in this paper is part of the LORELEI language pack. It consists of Broad Operational Language Translation (BOLT) data gathered from news sites, forums, weblogs, Wikipedia articles and twitter messages. We use a split of 10k training and 1k test instances. The Yorùbá NER data used in this work is the annotated corpus of Global Voices news articles recently released by BIBREF22. The dataset consists of 1,101 sentences (26k tokens) divided into 709 training sentences, 113 validation sentences and 279 test sentences based on 65%/10%/25% split ratio. \n Question: How much labeled data is available for these two languages?",
            "output": [
                "10k training and 1k test 1,101 sentences (26k tokens)"
            ]
        },
        {
            "id": "task460-3ebd1c477a684b7182b8b4ac04fb8be8",
            "input": "Which events authors choose to include in their history, which they leave out, and the way the events chosen relate to the march are crucial factors in conveying bias. \n Question: What factors contribute to interpretive biases according to this research?",
            "output": [
                "Which events authors choose to include in their history, which they leave out, and the way the events chosen relate to the march"
            ]
        },
        {
            "id": "task460-44870d21abc245dab0ba506182253081",
            "input": "In this paper, we manually annotate the predicate–argument structures for the 600 L2-L1 pairs as the basis for the semantic analysis of learner Chinese. \n Question: Who manually annotated the semantic roles for the set of learner texts?",
            "output": [
                "Authors"
            ]
        },
        {
            "id": "task460-ed83d978645246ca868461ef99672270",
            "input": "Our dataset is annotated based on Chinese pathology reports provided by the Department of Gastrointestinal Surgery, Ruijin Hospital. It contains 17,833 sentences, 826,987 characters and 2,714 question-answer pairs. All question-answer pairs are annotated and reviewed by four clinicians with three types of questions, namely tumor size, proximal resection margin and distal resection margin.  \n Question: Is all text in this dataset a question, or are there unrelated sentences in between questions?",
            "output": [
                "the dataset consists of pathology reports including sentences and questions and answers about tumor size and resection margins so it does include additional sentences "
            ]
        },
        {
            "id": "task460-1dfc9551335243b7a56909274bdc024f",
            "input": "The proposed Multimodal Differential Network (MDN) consists of a representation module and a joint mixture module. We use a triplet network BIBREF41 , BIBREF42 in our representation module. The triplet network consists of three sub-parts: target, supporting, and contrasting networks. All three networks share the same parameters. The Mixture module brings the image and caption embeddings to a joint feature embedding space. \n Question: How do the authors define a differential network?",
            "output": [
                "The proposed Multimodal Differential Network (MDN) consists of a representation module and a joint mixture module."
            ]
        },
        {
            "id": "task460-0edd7fb6262a478783b43d6214ccb038",
            "input": "We use word alignments, similarly to other annotation projection work, to project the AMR alignments to the target languages. Our approach depends on an underlying assumption that we make: if a source word is word-aligned to a target word and it is AMR aligned with an AMR node, then the target word is also aligned to that AMR node. Word alignments were generated using fast_align BIBREF10 , while AMR alignments were generated with JAMR BIBREF11 . \n Question: How is annotation projection done when languages have different word order?",
            "output": [
                "Word alignments are generated for parallel text, and aligned words are assumed to also share AMR node alignments."
            ]
        },
        {
            "id": "task460-359660f9308e4c4083b948bae72f6e95",
            "input": "To evaluate the performance of our approach, we used a subset of the SNIPS BIBREF12 dataset, which is readily available in RASA nlu format.  We use accuracy of intent and entity recognition as our task and metric.  \n Question: How are their changes evaluated?",
            "output": [
                "The changes are evaluated based on accuracy of intent and entity recognition on SNIPS dataset"
            ]
        },
        {
            "id": "task460-a9f76a831ae94f60bf6418309e7cc9ac",
            "input": "To train our model, we generated a dataset of 20,000 demonstrated 7 DOF trajectories (6 robot joints and 1 gripper dimension) in our simulated environment together with a sentence generator capable of creating natural task descriptions for each scenario. In order to create the language generator, we conducted an human-subject study to collect sentence templates of a placement task as well as common words and synonyms for each of the used features. By utilising these data, we are able to generate over 180,000 unique sentences, depending on the generated scenario. To test our model, we generated 500 new scenario testing each of the three features to identify the correct target among other bowls.  \n Question: Does proposed end-to-end approach learn in reinforcement or supervised learning manner?",
            "output": [
                "supervised learning"
            ]
        },
        {
            "id": "task460-2acce9cc85364d9d89a1560b69db58dc",
            "input": "We also include RNNLM BIBREF11 , a recurrent neural network based language model in the comparison. \n Question: Which language models do they compare against?",
            "output": [
                "RNNLM BIBREF11"
            ]
        },
        {
            "id": "task460-ec527bd7fddf4c9e89ff0a8509a8188b",
            "input": "For this knowledge, we create a knowledge base by counting how many times a predicate-argument tuple appears in a corpus and use the resulted number to represent the preference strength. Specifically, we use the English Wikipedia as the base corpus for such counting. \n Question: What is the source of external knowledge?",
            "output": [
                "counts of predicate-argument tuples from English Wikipedia"
            ]
        },
        {
            "id": "task460-b691ce55ee1042c2892f371a923259a2",
            "input": "We end up with two salient roles called Anchors and Punctual speakers:\n\nthe Anchor speakers (A) are above the threshold of 1% for both criteria, meaning they are intervening often and for a long time thus holding an important place in interaction;\n\nthe Punctual speakers (PS) on the contrary are below the threshold of 1% for both the total number of turns and the total speech time. \n Question: How many categories do authors define for speaker role?",
            "output": [
                " two salient roles called Anchors and Punctual speakers"
            ]
        },
        {
            "id": "task460-7a2f6be715b9485e927d563d6112c186",
            "input": "FastTextBIBREF4: It uses bag of words and bag of n-grams as features for text classification, capturing partial information about the local word order efficiently. BiLSTM: Unlike feedforward neural networks, recurrent neural networks like BiLSTMs use memory based on history information to learn long-distance features and then predict the output. We use a two-layer BiLSTM architecture with GloVe word embeddings as a strong RNN baseline. BERT BIBREF5: It is a contextualized word representation model that uses bidirectional transformers, pretrained on a large $3.3B$ word corpus. We use the $BERT_{large}$ model finetuned on the training dataset. \n Question: What is the baseline for the experiments?",
            "output": [
                "FastText BiLSTM BERT"
            ]
        },
        {
            "id": "task460-e07be05d09d1499f8f0d5217b926f2be",
            "input": "We work with a dataset composed of 3,206 news articles, each labeled real or fake, with a perfect 50/50 split between 1,603 real and fake articles, respectively. Fake articles were sourced from online sites that were tagged as fake news sites by the non-profit independent media fact-checking organization Verafiles and the National Union of Journalists in the Philippines (NUJP). Real articles were sourced from mainstream news websites in the Philippines, including Pilipino Star Ngayon, Abante, and Bandera. \n Question: What is the source of the dataset?",
            "output": [
                "Online sites tagged as fake news site by Verafiles and NUJP and news website in the Philippines, including Pilipino Star Ngayon, Abante, and Bandera"
            ]
        },
        {
            "id": "task460-6701fcaf65834e93afa8863b567fcd98",
            "input": "Sentiment: For each cluster, its overall sentiment score is quantified by the mean of the sentiment scores among all tweets. \n Question: How is sentiment polarity measured?",
            "output": [
                "For each cluster, its overall sentiment score is quantified by the mean of the sentiment scores among all tweets"
            ]
        },
        {
            "id": "task460-d0ea6974891642d18a7fa6ee2c8d9a8c",
            "input": "We evaluate the quality of the document embeddings learned by the different variants of CAHAN and the HAN baseline on three of the large-scale document classification datasets introduced by BIBREF14 and used in the original HAN paper BIBREF5. \n Question: Do they compare to other models appart from HAN?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-b054ff562cea44b48277fdd5bf1d7dd9",
            "input": "Originally, the seed dictionaries typically spanned several thousand word pairs BIBREF15 , BIBREF18 , BIBREF19 , but more recent work has shown that CLWEs can be induced with even weaker supervision from small dictionaries spanning several hundred pairs BIBREF20 , identical strings BIBREF21 , or even only shared numerals BIBREF22 . \n Question: How are seed dictionaries obtained by fully unsupervised methods?",
            "output": [
                "the latest CLWE developments almost exclusively focus on fully unsupervised approaches BIBREF23 , BIBREF24 , BIBREF25 , BIBREF26 , BIBREF27 , BIBREF28 , BIBREF29 , BIBREF30 : they fully abandon any source of (even weak) supervision and extract the initial seed dictionary by exploiting topological similarities between pre-trained monolingual embedding spaces"
            ]
        },
        {
            "id": "task460-a391aaf2bd5a4f5fbcccb329a4dc4f16",
            "input": "Through our experiments, we make subtle points related to: (a) the performance of our features, (b) how our approach compares against human ability to detect drunk-texting, (c) most discriminative stylistic features, and (d) an error analysis that points to future work. \n Question: Do the authors mention any confounds to their study?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-43b79fd4287a4415838653f4e94b954f",
            "input": "As with INLINEFORM0 , Greens-EFA, S&D, and EPP exhibit the highest cohesion, even though their ranking is permuted when compared to the ranking obtained with INLINEFORM1 . At the other end of the scale, we observe the same situation as with INLINEFORM2 . The non-aligned members NI have the lowest cohesion, followed by EFDD and ENL. The only place where the two methods disagree is the level of cohesion of GUE-NGL. The Alpha attributes GUE-NGL a rather high level of cohesion, on a par with ALDE, whereas the ERGM attributes them a much lower cohesion. \n Question: What insights does the analysis give about the cohesion of political groups in the European parliament?",
            "output": [
                "Greens-EFA, S&D, and EPP exhibit the highest cohesion non-aligned members NI have the lowest cohesion, followed by EFDD and ENL two methods disagree is the level of cohesion of GUE-NGL"
            ]
        },
        {
            "id": "task460-62284ee30ce9433e85b185cb7489d4fe",
            "input": "In this experiment, we compute the correlation of the proposed NED measure with the patient-perceived emotional bond ratings. Since the proposed measure is asymmetric in nature, we compute the measures for both patient-to-therapist and therapist-to-patient entrainment. We report Pearson's correlation coefficients ( INLINEFORM0 ) for this experiment in Table TABREF26 along with their INLINEFORM1 -values.  \n Question: How do they correlate NED with emotional bond levels?",
            "output": [
                "They compute Pearson’s correlation between NED measure for patient-to-therapist and patient-perceived emotional bond rating and NED measure for therapist-to-patient and patient-perceived emotional bond rating"
            ]
        },
        {
            "id": "task460-8da9c8c2940046a8aabe3a2931d70acd",
            "input": "For the comparison we chose the best text model for each representation. As expected we obtain the largest improvement ($22-26\\%$ E.R) when text-based unsupervised models are combined with image representations. \n Question: How much better is inference that has addition of image representation compared to text-only representations? ",
            "output": [
                " largest improvement ($22-26\\%$ E.R) when text-based unsupervised models are combined with image representations"
            ]
        },
        {
            "id": "task460-2c3a327b6fc2460bae6b93ae7db5a20c",
            "input": "From the in-corpus experiments, we obtain good results on SarcasmCorpus, which is the only corpus containing Amazon reviews. Unfortunately, when we train our models in a cross-corpora or all-corpora setting, our results drop dramatically, especially in the cross-corpora case. These results mean that the sarcasm in SarcasmCorpus is conveyed through features that are not present in the other corpora. \n Question: In which domains is sarcasm conveyed in different ways?",
            "output": [
                "Amazon reviews"
            ]
        },
        {
            "id": "task460-47de61790c424d59b71584d9d65ff54b",
            "input": "We assess our dataset using traditional and deep learning methods. Our simplest model is a linear SVM trained on word unigrams. SVMs have produced state-of-the-art results for many text classification tasks BIBREF13 . We also train a bidirectional Long Short-Term-Memory (BiLSTM) model, which we adapted from the sentiment analysis system of sentimentSystem,rasooli2018cross and altered to predict offensive labels instead. It consists of (1) an input embedding layer, (2) a bidirectional LSTM layer, (3) an average pooling layer of input features. The concatenation of the LSTM's and average pool layer is passed through a dense layer and the output is passed through a softmax function. We set two input channels for the input embedding layers: pre-trained FastText embeddings BIBREF14 , as well as updatable embeddings learned by the model during training. Finally, we also apply a Convolutional Neural Network (CNN) model based on the architecture of BIBREF15 , using the same multi-channel inputs as the above BiLSTM. \n Question: What models are used in the experiment?",
            "output": [
                "linear SVM bidirectional Long Short-Term-Memory (BiLSTM) Convolutional Neural Network (CNN)"
            ]
        },
        {
            "id": "task460-fa651b7e9229493c9c15e1cabb1c239c",
            "input": "Our target data set consists of a corpus of discharge summaries, admission notes, individual encounter notes, and other clinical notes from 220 patients in the OnTrackTM program at McLean Hospital. OnTrackTM is an outpatient program, focusing on treating adults ages 18 to 30 who are experiencing their first episodes of psychosis. The length of time in the program varies depending on patient improvement and insurance coverage, with an average of two to three years. The program focuses primarily on early intervention via individual therapy, group therapy, medication evaluation, and medication management. See Table TABREF2 for a demographic breakdown of the 220 patients, for which we have so far extracted approximately 240,000 total EHR paragraphs spanning from 2011 to 2014 using Meditech, the software employed by McLean for storing and organizing EHR data.\n\nThese patients are part of a larger research cohort of approximately 1,800 psychosis patients, which will allow us to connect the results of this EHR study with other ongoing research studies incorporating genetic, cognitive, neurobiological, and functional outcome data from this cohort.\n\nWe also use an additional data set for training our vector space model, comprised of EHR texts queried from the Research Patient Data Registry (RPDR), a centralized regional data repository of clinical data from all institutions in the Partners HealthCare network. These records are highly comparable in style and vocabulary to our target data set. The corpus consists of discharge summaries, encounter notes, and visit notes from approximately 30,000 patients admitted to the system's hospitals with psychiatric diagnoses and symptoms. This breadth of data captures a wide range of clinical narratives, creating a comprehensive foundation for topic extraction. \n Question: What datasets did the authors use?",
            "output": [
                " a corpus of discharge summaries, admission notes, individual encounter notes, and other clinical notes from 220 patients in the OnTrackTM program at McLean Hospital an additional data set for training our vector space model, comprised of EHR texts queried from the Research Patient Data Registry (RPDR)"
            ]
        },
        {
            "id": "task460-134db6d5614d4e6faa5549f5fa91e1be",
            "input": "Modern Standard Arabic (MSA) and Classical Arabic (CA) have two types of vowels, namely long vowels, which are explicitly written, and short vowels, aka diacritics, which are typically omitted in writing but are reintroduced by readers to properly pronounce words. We show that our model achieves a case ending error rate (CEER) of 3.7% for MSA and 2.5% for CA. For MSA, this CEER is more than 60% lower than other state-of-the-art systems such as Farasa and the RDI diacritizer, which are trained on the same dataset and achieve CEERs of 10.7% and 14.4% respectively.  \n Question: what are the previous state of the art?",
            "output": [
                "Farasa RDI"
            ]
        },
        {
            "id": "task460-3604dc9c69d844418998a584a1ca8027",
            "input": "In Section SECREF5 , we discuss the resulting data collection of questions, texts and answers via crowdsourcing on Amazon Mechanical Turk (henceforth MTurk). \n Question: what crowdsourcing platform was used?",
            "output": [
                "Amazon Mechanical Turk"
            ]
        },
        {
            "id": "task460-4eb99f3195e3408e92e27d9f80ca06c7",
            "input": "We constructed the ensembled predictions by choosing the answer from the network that had the highest probability and choosing no answer if any of the networks predicted no answer. \n Question: What ensemble methods are used for best model?",
            "output": [
                "choosing the answer from the network that had the highest probability and choosing no answer if any of the networks predicted no answer"
            ]
        },
        {
            "id": "task460-d50c9ec771424b6da5dcfe3e82c46907",
            "input": "Despite this, JESSI performs second on Subtask A with an F1 score of 77.78% among 33 other team submissions. It also performs well on Subtask B with an F1 score of 79.59%. \n Question: How did they do compared to other teams?",
            "output": [
                "second on Subtask A with an F1 score of 77.78% among 33 other team submissions performs well on Subtask B with an F1 score of 79.59%"
            ]
        },
        {
            "id": "task460-64598648b1a14052b62e9cdca9f29ddf",
            "input": "Practical evaluation of GTD is currently only possible on synthetic data. We construct a range of datasets designed for image captioning evaluation. We call this diagnostic evaluation benchmark ShapeWorldICE (ShapeWorld for Image Captioning Evaluation). We illustrate the evaluation of specific image captioning models on ShapeWorldICE. \n Question: Are the images from a specific domain?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-e4847d3b4d6c4273b52fa063329a7f15",
            "input": "We represent each instance of image, question, and user choice as a triplet consisting of image feature, question feature, and the label vector for the user's answer. In addition, collecting multiple choices from identical users enables us to represent any two instances by the same user as a pair of triplets, assuming source-target relation. With these pairs of triplets, we can train the system to predict a user's choice on a new image and a new question, given the same user's choice on the previous image and its associated question. As discussed earlier, we attempt to reflect user's interest by asking questions that provide visual context. In other words, a question whose answer is so obvious that it can be answered in an identical way would not be valid as an interactive question.   If the most likely candidate $c_i=\\max p_{ans}$ , where $c_i \\in C$ , has a probability of being answer over a certain threshold $\\alpha $ , then the question is considered to have a single obvious answer, and is thus considered ineligible.  In our experiments, we set $\\alpha $ as 0.33. We also excluded the yes/no type of questions.  \n Question: What are the features of used to customize target user interaction? ",
            "output": [
                "image feature question feature label vector for the user's answer"
            ]
        },
        {
            "id": "task460-d54d2c5b26b74474b9d4afe9f3562ae1",
            "input": "Although the candidates agreed to the use of their interviews, the dataset will not be released to public outside of the scope of this study due to the videos being personal data subject to high privacy constraints. \n Question: Have the candidates given their consent to have their videos used for the research?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-a99fc9032f334ccc8c85c9ee06786615",
            "input": "The key idea behind our proposed interactive MRC (iMRC) is to restrict the document context that a model observes at one time. Concretely, we split a supporting document into its component sentences and withhold these sentences from the model. Given a question, the model must issue commands to observe sentences in the withheld set; we equip models with actions such as Ctrl+F (search for token) and stop for searching through partially observed documents. A model searches iteratively, conditioning each command on the input question and the sentences it has observed previously. Thus, our task requires models to `feed themselves' rather than spoon-feeding them with information. This casts MRC as a sequential decision-making problem amenable to reinforcement learning (RL). \n Question: Do they provide decision sequences as supervision while training models?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-a759a197b742414c914aa79bdd02058f",
            "input": "To create our training dataset, we followed an approach similar to LASER. The dataset contains 6 languages: English, Spanish, German, Dutch, Korean and Chinese Mandarin. The dataset was created by using translations provided by Tatoeba and OpenSubtitles BIBREF16. \n Question: Which corpus do they use?",
            "output": [
                "The dataset was created by using translations provided by Tatoeba and OpenSubtitles BIBREF16."
            ]
        },
        {
            "id": "task460-4a0ebf8647e7433880246b13fd5ca078",
            "input": "We use the dropout technique of Gal & Ghahramani gal as a baseline because it is the most similar dropout technique to our approach and denote it VBD (variational binary dropout). \n Question: What is binary variational dropout?",
            "output": [
                "the dropout technique of Gal & Ghahramani gal"
            ]
        },
        {
            "id": "task460-d77f371d9bb744fea93a74edb8519c08",
            "input": "Experiments ::: Results ::: Natural Language Inference: XNLI\nOn the XNLI benchmark, CamemBERT obtains improved performance over multilingual language models on the TRANSLATE-TRAIN setting (81.2 vs. 80.2 for XLM) while using less than half the parameters (110M vs. 250M). However, its performance still lags behind models trained on the original English training set in the TRANSLATE-TEST setting, 81.2 vs. 82.91 for RoBERTa. It should be noted that CamemBERT uses far fewer parameters than RoBERTa (110M vs. 355M parameters). \n Question: Which tasks does CamemBERT not improve on?",
            "output": [
                "its performance still lags behind models trained on the original English training set in the TRANSLATE-TEST setting, 81.2 vs. 82.91 for RoBERTa"
            ]
        },
        {
            "id": "task460-33931570d97b41dcb0e689fbb3b5b8e0",
            "input": "We evaluate our proposed approach for joint sentiment and emotion analysis on the benchmark dataset of SemEval 2016 Task 6 BIBREF7 and Stance Sentiment Emotion Corpus (SSEC) BIBREF15. \n Question: What are the datasets used for training?",
            "output": [
                "SemEval 2016 Task 6 BIBREF7 Stance Sentiment Emotion Corpus (SSEC) BIBREF15"
            ]
        },
        {
            "id": "task460-baabe9ef13d2494b926e15ea96772197",
            "input": "In this study, we focused on the LDA approaches to recommendation systems and given the importance of research, we have studied recent impressive articles on this subject and presented a taxonomy of recommendation systems based on LDA of the recent research. we evaluated ISWC and WWW conferences articles from DBLP website and used the Gibbs sampling algorithm as an evaluation parameter. We succeeded in discovering the relationship between LDA topics and paper features and also obtained the researchers' interest in research field. To perform approximate inference and learning LDA, there are many inference methods for LDA topic model such as Gibbs sampling, collapsed Variational Bayes, Expectation Maximization. Gibbs sampling is a popular technique because of its simplicity and low latency. However, for large numbers of topics, Gibbs sampling can become unwieldy. In this paper, we use Gibbs Sampling in our experiment in section 5. \n Question: How they utilize LDA and Gibbs sampling to evaluate ISWC and WWW publications?",
            "output": [
                "the LDA approaches to recommendation systems and given the importance of research, we have studied recent impressive articles on this subject and presented a taxonomy of recommendation systems based on LDA of the recent research we evaluated ISWC and WWW conferences articles from DBLP website and used the Gibbs sampling algorithm as an evaluation parameter"
            ]
        },
        {
            "id": "task460-873a6efa98214f1fbc6e9daf9c15997a",
            "input": "The E2E NLG challenge dataset BIBREF21 is utilized in our experiments, which is a crowd-sourced dataset of 50k instances in the restaurant domain. Our models are trained on the official training set and verified on the official testing set.  \n Question: What datasets did they use?",
            "output": [
                "The E2E NLG challenge dataset BIBREF21"
            ]
        },
        {
            "id": "task460-11b2e5c30a564a1a82183fe282c8526e",
            "input": "For instance, question and aux are correlated because main-clause questions in English require subject-aux inversion and in many cases the insertion of auxiliary do (Do lions meow?).  Expletives, or “dummy” arguments, are semantically inert arguments. The most common expletives in English are it and there, although not all occurrences of these items are expletives. \n Question: Do they report results only on English data?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-06581aae1020464194f8fd72d3af771d",
            "input": "BIBREF17: The Lemming model is a log-linear model that performs joint morphological tagging and lemmatization.  \n Question: What were the non-neural baselines used for the task?",
            "output": [
                "The Lemming model in BIBREF17"
            ]
        },
        {
            "id": "task460-c86a72e1da5241f88d44a98142cecdeb",
            "input": "Our definition of bias is now: [Interpretive Bias] An interpretive bias in an epistemic ME game is the probability distribution over types given by the belief function of the conversationalists or players, or the Jury. Note that in an ME game there are typically several interpretive biases at work: each player has her own bias, as does the Jury. \n Question: Which interpretative biases are analyzed in this paper?",
            "output": [
                "in an ME game there are typically several interpretive biases at work: each player has her own bias, as does the Jury"
            ]
        },
        {
            "id": "task460-c72bba1bd78c4200b9d6680000411a31",
            "input": "We feed these representations into a clustering algorithm that produces exactly eleven clusters of five cities each and evaluate them against both Calvino's original labels and crowdsourced human judgments. While the overall correlation with Calvino's labels is low, both computers and humans can reliably identify some thematic groups associated with concrete objects. \n Question: How do they obtain human judgements?",
            "output": [
                "Using crowdsourcing "
            ]
        },
        {
            "id": "task460-e5bd5a6aafb542ef97a740ffe737287f",
            "input": "Following previous studies BIBREF1, we collect event-related microposts from Twitter using 11 and 8 seed events (see Section SECREF2) for CyberAttack and PoliticianDeath, respectively. Unlabeled microposts are collected by using the keyword `hack' for CyberAttack, while for PoliticianDeath, we use a set of keywords related to `politician' and `death' (such as `bureaucrat', `dead' etc.) \n Question: Do they report results only on English data?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-4c08195a5996451fa0ad79e474c2519f",
            "input": "We also applied our model to MCTest dataset which requires machines to answer multiple-choice reading comprehension questions about fictional stories.  \n Question: Do they experiment with their proposed model on any other dataset other than MovieQA?",
            "output": [
                "Yes"
            ]
        },
        {
            "id": "task460-a725d3813b684206bb353f2ea9ad5170",
            "input": "On another note, we apply our formalization to evaluate multilingual $\\textsc {bert} $'s syntax knowledge on a set of six typologically diverse languages. Although it does encode a large amount of information about syntax (more than $81\\%$ in all languages), it only encodes at most $5\\%$ more information than some trivial baseline knowledge (a type-level representation). This indicates that the task of POS labeling (word-level POS tagging) is not an ideal task for contemplating the syntactic understanding of contextual word embeddings. We see that—in all analysed languages—type level embeddings can already capture most of the uncertainty in POS tagging. We also see that BERT only shares a small amount of extra information with the task, having small (or even negative) gains in all languages. Finally, when put into perspective, multilingual $\\textsc {bert} $'s representations do not seem to encode much more information about syntax than a trivial baseline. $\\textsc {bert} $ only improves upon fastText in three of the six analysed languages—and even in those, it encodes at most (in English) $5\\%$ additional information. \n Question: Was any variation in results observed based on language typology?",
            "output": [
                "It is observed some variability - but not significant. Bert does not seem to gain much more syntax information than with type level information."
            ]
        },
        {
            "id": "task460-dab72620d5544b098f7035a2cd7e9e01",
            "input": "We evaluate our model on a publicly available dataset, KARA ONE BIBREF17 , composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw). The dataset consists of 14 participants, with each prompt presented 11 times to each individual.  \n Question: What data was presented to the subjects to elicit event-related responses?",
            "output": [
                "7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)"
            ]
        },
        {
            "id": "task460-32a2c38fe9024ef3ac9decc56f1b5410",
            "input": "We then proceed to connect these mentions i) if they co-occur within the same document (we will refer to this as DOC-BASED edges), ii) if the pair of named entity mentions is identical (MATCH edges—these may connect nodes across and within documents), or iii) if they are in the same coreference chain, as predicted by the external coreference system (COREF edges). \n Question: Did they use a relation extraction method to construct the edges in the graph?",
            "output": [
                "No"
            ]
        },
        {
            "id": "task460-7205689c1389479a99e714c98102174b",
            "input": "For evaluation of clustering performance, we use Calinski-Harabasz score BIBREF42 , also known as the variance ratio criterion. CH score is defined as the ratio between the within-cluster dispersion and the between-cluster dispersion. CH score has a range of $[0, +\\infty ]$ and a higher CH score corresponds to a better clustering.  For visual validation, we plot and inspect the t-Distributed Stochastic Neighbor Embedding (t-SNE) BIBREF52 and Uniform Manifold Approximation and Projection (UMAP) BIBREF53 mappings of the learned representations as well. \n Question: How do they evaluate their method?",
            "output": [
                "Calinski-Harabasz score t-SNE UMAP"
            ]
        },
        {
            "id": "task460-495820c332354b1994c0e56788b08b5c",
            "input": "For instance, BIBREF16 and BIBREF17 showed that a hypothesis-only baseline performs better than chance due to cues on their lexical choice and sentence length. Similarly, BIBREF18 showed that NLI models tend to predict entailment for sentence pairs with a high lexical overlap. \n Question: What are examples of these artifacts?",
            "output": [
                "hypothesis-only baseline performs better than chance due to cues on their lexical choice and sentence length NLI models tend to predict entailment for sentence pairs with a high lexical overlap"
            ]
        },
        {
            "id": "task460-1e99111e73a84df1a01998c0e5cae9e4",
            "input": "In the second round, we collected 293 annotations from 12 annotators. After Korektor, there are 4262 unique sentences (including 150 seed sentences) that form the COSTRA 1.0 dataset. \n Question: How are possible sentence transformations represented in dataset, as new sentences?",
            "output": [
                "Yes, as new sentences."
            ]
        },
        {
            "id": "task460-fed72f1c79f14d79b5960291df2011a7",
            "input": "Following these methods, we evaluate NCEL on the following five datasets: (1) CoNLL-YAGO BIBREF22 : the CoNLL 2003 shared task including testa of 4791 mentions in 216 documents, and testb of 4485 mentions in 213 documents. (2) TAC2010 BIBREF39 : constructed for the Text Analysis Conference that comprises 676 mentions in 352 documents for testing. (3) ACE2004 BIBREF23 : a subset of ACE2004 co-reference documents including 248 mentions in 35 documents, which is annotated by Amazon Mechanical Turk. (4) AQUAINT BIBREF40 : 50 news articles including 699 mentions from three different news agencies. (5) WW BIBREF19 : a new benchmark with balanced prior distributions of mentions, leading to a hard case of disambiguation. It has 6374 mentions in 310 documents automatically extracted from Wikipedia. \n Question: Which datasets do they use?",
            "output": [
                "CoNLL-YAGO TAC2010 ACE2004 AQUAINT WW"
            ]
        },
        {
            "id": "task460-a1f37afade4f4b0db194914cf6dd9729",
            "input": "For automatically evaluating our methods, we propose to use widely used metric for image/video captioning. This is because the proposed CommonGen task can be regarded as also a caption task where the context are incomplete scenes with given concept-sets. Therefore, we choose BLEU-3/4 BIBREF28, ROUGE-2/L BIBREF29, CIDEr BIBREF30, and SPICE BIBREF31 as the main metrics. Apart from these classic metrics, we also include a novel embedding-based metric named BERTScore BIBREF32. To make the comparisons more clear, we show the delta of BERTScore results by subtracting the score of merely using input concept-sets as target sentences, named $\\triangle $BERTS. \n Question: What automatic metrics are used for this task?",
            "output": [
                "BLEU-3/4 ROUGE-2/L CIDEr SPICE BERTScore"
            ]
        },
        {
            "id": "task460-79062ec4b8c34621bf47c3ae22de362b",
            "input": "In both cases, we denote with the word model one of the possible combinations of classic/statistical LSA and a classifier. The used classifiers are Support Vector Machine (SVM), Logistic regression (Log.Reg), Random Forest (RF) and gradient boosting (XGB). Where applicable, we compare our results with existing results in the literature. \n Question: What are the different methods used for different corpora?",
            "output": [
                "Support Vector Machine (SVM), Logistic regression (Log.Reg), Random Forest (RF) and gradient boosting (XGB)"
            ]
        },
        {
            "id": "task460-69612a49280648679edb9f3a6338205e",
            "input": "SMT techniques require a parallel corpus in thr source and thr target language. A text-code parallel corpus similar to Fig. FIGREF12 is used in training. This parallel corpus has 18805 aligned data in it . In source data, the expression of each line code is written in the English language. In target data, the code is written in Python programming language. \n Question: What dataset do they use?",
            "output": [
                "A parallel corpus where the source is an English expression of code and the target is Python code."
            ]
        },
        {
            "id": "task460-1a63da1be4cc410db9047bd6be53149e",
            "input": "We assume that for each corpora INLINEFORM0 , we are given word embeddings for each word INLINEFORM1 , where INLINEFORM2 is the dimension of each word embedding. We are also given a classification task on documents that is represented by a parametric model INLINEFORM3 taking document embeddings as feature vectors. We construct a GAN model which combines different sets of word embeddings INLINEFORM4 , INLINEFORM5 , into a single set of word embeddings INLINEFORM6 . Note that INLINEFORM7 are given but INLINEFORM8 is trained. Here we consider INLINEFORM9 as the generator, and the goal of the discriminator is to distinguish documents represented by the original embeddings INLINEFORM10 and the same documents represented by the new embeddings INLINEFORM11 . \n Question: Which GAN do they use?",
            "output": [
                "We construct a GAN model which combines different sets of word embeddings INLINEFORM4 , INLINEFORM5 , into a single set of word embeddings INLINEFORM6 . "
            ]
        },
        {
            "id": "task460-f9a192cb693e4eab94da4cb0cd67de9b",
            "input": "We collected tweets related to five different DDoS attacks on three different American banks. For each attack, all the tweets containing the bank's name posted from one week before the attack until the attack day were collected. There are in total 35214 tweets in the dataset. Only the tweets from the Bank of America attack on 09/19/2012 were used in this experiment. In this subsection we evaluate how good the model generalizes. To achieve that, the dataset is divided into two groups, one is about the attacks on Bank of America and the other group is about PNC and Wells Fargo. The only difference between this experiment and the experiment in section 4.4 is the dataset. In this experiment setting $D_a$ contains only the tweets collected on the days of attack on PNC and Wells Fargo. $D_b$ only contains the tweets collected before the Bank of America attack. \n Question: What is the training and test data used?",
            "output": [
                "Tweets related to a Bank of America DDos attack were used as training data. The test datasets contain tweets related to attacks to Bank of America, PNC and Wells Fargo."
            ]
        },
        {
            "id": "task460-facf4158a6944bcc95adf4de17c6e330",
            "input": "The task of attribute embedding lays is embedding every value in attribute triples into a continuous vector space while preserving the semantic information. To capture both high-order structural information of KGs, we used an attention-based embedding propagation method. The final embedding of entities, relations and values are feed into two different deep neural network for two different tasks including link predication and entity classification. \n Question: How does KANE capture both high-order structural and attribute information of KGs in an efficient, explicit and unified manner?",
            "output": [
                "To capture both high-order structural information of KGs, we used an attention-based embedding propagation method."
            ]
        },
        {
            "id": "task460-ae2e813cb3d54e378a0025059c7d03ce",
            "input": "As shown in Figure FIGREF3 , our model contains two key components, namely Truncated History-Attention (THA) and Selective Transformation Network (STN), for capturing aspect detection history and opinion summary respectively. THA and STN are built on two LSTMs that generate the initial word representations for the primary ATE task and the auxiliary opinion detection task respectively. THA is designed to integrate the information of aspect detection history into the current aspect feature to generate a new history-aware aspect representation. STN first calculates a new opinion representation conditioned on the current aspect candidate. Then, we employ a bi-linear attention network to calculate the opinion summary as the weighted sum of the new opinion representations, according to their associations with the current aspect representation. Finally, the history-aware aspect representation and the opinion summary are concatenated as features for aspect prediction of the current time step. \n Question: How do they determine the opinion summary?",
            "output": [
                "the weighted sum of the new opinion representations, according to their associations with the current aspect representation"
            ]
        },
        {
            "id": "task460-2654e4a102f04ecfac8c236b6db771a9",
            "input": "Inspired by BIBREF12, we integrate in this paper a boundary assembling step into the state-of-the-art LSTM model for Chinese word segmentation, and feed the output into a CRF model for NER, resulting in a 2% absolute improvement on the overall F1 score over current state-of-the-art methods. \n Question: What state-of-the-art deep neural network is used?",
            "output": [
                "LSTM model"
            ]
        },
        {
            "id": "task460-9e4c9599e75843eb83e15fd351edc17a",
            "input": "The median age is 17 for depressed class versus 19 for control class suggesting either likely depressed-user population is younger, or depressed youngsters are more likely to disclose their age for connecting to their peers (social homophily.) Our findings are consistent with the medical literature BIBREF10 as according to BIBREF52 more women than men were given a diagnosis of depression. \n Question: What insights into the relationship between demographics and mental health are provided?",
            "output": [
                "either likely depressed-user population is younger, or depressed youngsters are more likely to disclose their age more women than men were given a diagnosis of depression"
            ]
        },
        {
            "id": "task460-6f4781e3a31e4458ba9f107b523b6dd7",
            "input": "In this paper we propose two neural embedding models in order to learn continuous concept vectors based on the skip-gram model BIBREF11 . Our first model is the Concept Raw Context model (CRC) which utilizes concept mentions in a large scale KB to jointly learn embeddings of both words and concepts. Our second model is the Concept-Concept Context model (3C) which learns the embeddings of concepts from their conceptual contexts (i.e., contexts containing surrounding concepts only).  \n Question: What are the two neural embedding models?",
            "output": [
                "Our first model is the Concept Raw Context model (CRC) which utilizes concept mentions in a large scale KB to jointly learn embeddings of both words and concepts. Our second model is the Concept-Concept Context model (3C) which learns the embeddings of concepts from their conceptual contexts (i.e., contexts containing surrounding concepts only)"
            ]
        },
        {
            "id": "task460-4a18355ab6d94a6586f9afb4f690d010",
            "input": "In this paper, we introduce a novel policy model to output multiple actions per turn (called multi-act), generating a sequence of tuples and expanding agents' expressive power. Each tuple is defined as $(\\textit {continue}, \\textit {act}, \\textit {slots})$, where continue indicates whether to continue or stop producing new acts, act is an act type (e.g., inform or request), and slots is a set of slots (names) associated with the current act type. Correspondingly, a novel decoder (Figure FIGREF5) is proposed to produce such sequences. Each tuple is generated by a cell called gated Continue Act Slots (gCAS, as in Figure FIGREF7), which is composed of three sequentially connected gated units handling the three components of the tuple. This decoder can generate multi-acts in a double recurrent manner BIBREF18.  \n Question: What is specific to gCAS cell?",
            "output": [
                "It has three sequentially connected units to output continue, act and slots generating multi-acts in a doble recurrent manner."
            ]
        },
        {
            "id": "task460-687a8c51491e418b834b7d23424fc217",
            "input": "The UNGA speeches dataset, compiled by Baturo et al. UNGAspeeches, contains the text from 7,507 speeches given between 1970-2015 inclusive. \n Question: how many speeches are in the dataset?",
            "output": [
                "7,507"
            ]
        },
        {
            "id": "task460-2c3dd81788cf4c32a0301fb14a0ba85d",
            "input": "For the low resource translation task, we used the BTEC corpus as the training data, which consists of 30k sentence pairs with 0.27M Chinese words and 0.33M English words. As development and test sets, we used the CSTAR03 and IWSLT04 held out sets, respectively.  We used the data from the NIST2008 Open Machine Translation Campaign. The training data consisted of 1.8M sentence pairs, the development set was nist02 (878 sentences), and the test sets are were nist05 (1082 sentences), nist06 (1664 sentences) and nist08 (1357 sentences). \n Question: Which dataset do they use?",
            "output": [
                "BTEC corpus the CSTAR03 and IWSLT04 held out sets the NIST2008 Open Machine Translation Campaign"
            ]
        },
        {
            "id": "task460-b4020c3e6f9d43399b85dcbda6e9d3cd",
            "input": "We conduct experiments on WikiSQL BIBREF8 , which provides 87,726 annotated question-SQL pairs over 26,375 web tables. We do our study on SimpleQuestions BIBREF10 , which includes 108,442 simple questions, each of which is accompanied by a subject-relation-object triple. We conduct experiments on SequentialQA BIBREF9 which is derived from the WikiTableQuestions dataset BIBREF19 . \n Question: What datasets are used in this paper?",
            "output": [
                "WikiSQL SimpleQuestions SequentialQA"
            ]
        },
        {
            "id": "task460-9da599e94e31486c830b420740672001",
            "input": "LiLi should have the following capabilities: \n Question: What are the components of the general knowledge learning engine?",
            "output": [
                "Answer with content missing: (list)\nLiLi should have the following capabilities:\n1. to formulate an inference strategy for a given query that embeds processing and interactive actions.\n2. to learn interaction behaviors (deciding what to ask and when to ask the user).\n3. to leverage the acquired knowledge in the current and future inference process.\n4. to perform 1, 2 and 3 in a lifelong manner for continuous knowledge learning."
            ]
        },
        {
            "id": "task460-986ac38bbc744476ae89a57739cc56ec",
            "input": "We carried out our experiments on the Universal Dependencies v1.2 treebanks BIBREF21 , hereafter UD1.2, from which morphosyntactically annotated corpora can be trivially extracted.  We considered UD1.2 corpora for the following 16 languages: Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish.  \n Question: which datasets did they experiment with?",
            "output": [
                "Universal Dependencies v1.2 treebanks for the following 16 languages: Bulgarian, Croatian, Czech, Danish, English, French, German,\nIndonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish, and Swedish"
            ]
        },
        {
            "id": "task460-21554b5929844d9aab9749a9cb10ba85",
            "input": "Moreover, because English does not mark grammatical gender, approaches developed for English are not transferable to morphologically rich languages that exhibit gender agreement BIBREF8 . \n Question: Why does not the approach from English work on other languages?",
            "output": [
                "Because, unlike other languages, English does not mark grammatical genders"
            ]
        },
        {
            "id": "task460-ad0c4e35998e4953b6d04b82cf42250c",
            "input": "In this subsection, we see the influence of each component of a model on performance by removing or replacing its components. the SNLI dataset is used for experiments, and the best performing configuration is used as a baseline for modifications. We consider the following variants: (i) models that use plain stacked LSTMs, (ii) models with different INLINEFORM0 , (iii) models without INLINEFORM1 , and (iv) models that integrate lower contexts via peephole connections. \n Question: What were the baselines?",
            "output": [
                "(i) models that use plain stacked LSTMs (ii) models with different INLINEFORM0 (iii) models without INLINEFORM1 (iv) models that integrate lower contexts via peephole connections"
            ]
        }
    ],
    "Instance License": [
        "CC BY 4.0"
    ]
}